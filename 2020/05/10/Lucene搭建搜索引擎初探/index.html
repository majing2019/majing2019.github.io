<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  
  
  
    <meta name="description" content="记录生活">
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <title>
    Lucene搭建搜索引擎初探 |
    
    大嘴怪的小世界</title>
  
    <link rel="shortcut icon" href="/favicon.ico">
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<script src="/js/pace.min.js"></script>

<meta name="generator" content="Hexo 4.2.1"></head>

<body>
<main class="content">
  <section class="outer">
  

<article id="post-Lucene搭建搜索引擎初探" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Lucene搭建搜索引擎初探
    </h1>
  
  




      </header>
    

    
      <div class="article-meta">
        <a href="/2020/05/10/Lucene%E6%90%AD%E5%BB%BA%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E5%88%9D%E6%8E%A2/" class="article-date">
  <time datetime="2020-05-10T14:23:43.000Z" itemprop="datePublished">2020-05-10</time>
</a>
        
      </div>
    

    
      
    <div class="tocbot"></div>





    

    <div class="article-entry" itemprop="articleBody">
      


      

      
        <p>最近要做例句搜索的优化，因此重新看一看lucene，边学习边搭demo。由于平时使用惯了python，所以这一次使用pylucene做demo。本文着重于lucene的介绍，一些内容主要参考了niyanchun的博客，并增加了几个pylucene的示例代码。</p>
<a id="more"></a>
<h1 id="配置Pylucene环境"><a href="#配置Pylucene环境" class="headerlink" title="配置Pylucene环境"></a>配置Pylucene环境</h1><h3 id="安装pylucene"><a href="#安装pylucene" class="headerlink" title="安装pylucene"></a><strong>安装pylucene</strong></h3><ul>
<li>wget <a href="https://mirror.bit.edu.cn/apache/lucene/pylucene/pylucene-8.1.1-src.tar.gz" target="_blank" rel="noopener">https://mirror.bit.edu.cn/apache/lucene/pylucene/pylucene-8.1.1-src.tar.gz</a></li>
<li>tar zxvf pylucene-8.1.1-src.tar.gz  </li>
</ul>
<h3 id="安装JCC"><a href="#安装JCC" class="headerlink" title="安装JCC"></a><strong>安装JCC</strong></h3><ul>
<li>cd pylucene-8.1.1/jcc</li>
<li>setup.py中修改jdk位置</li>
<li>如果是MACOS<ul>
<li>export CC=/usr/bin/clang</li>
<li>export CXX=/usr/bin/clang++</li>
</ul>
</li>
<li>python setup.py build</li>
<li>python setup.py install</li>
</ul>
<h3 id="安装ANT"><a href="#安装ANT" class="headerlink" title="安装ANT"></a><strong>安装ANT</strong></h3><ul>
<li>wget <a href="https://mirror.bit.edu.cn/apache//ant/binaries/apache-ant-1.9.14-bin.tar.gz" target="_blank" rel="noopener">https://mirror.bit.edu.cn/apache//ant/binaries/apache-ant-1.9.14-bin.tar.gz</a></li>
<li>tar zxvf apache-ant-1.9.14-bin.tar.gz</li>
<li>export ANT_HOME=…/apache-ant-1.9.14</li>
<li>export PATH=$PATH:$ANT_HOME/bin</li>
<li>export ANT_OPTS=”-Xms1300m -Xmx2048m -XX:PermSize=128M -XX:MaxNewSize=256m -XX:MaxPermSize=256m”</li>
</ul>
<h3 id="安装lucene"><a href="#安装lucene" class="headerlink" title="安装lucene"></a><strong>安装lucene</strong></h3><ul>
<li>cd pylucene-8.1.1</li>
<li>vi Makefile<ul>
<li>PREFIX_PYTHON=…./conda-env</li>
<li>PYTHON=$(PREFIX_PYTHON)/bin/python</li>
<li>ANT=…/apache-ant-1.9.14/bin/ant</li>
<li>JCC=$(PYTHON) -m jcc.<strong>main</strong></li>
<li>NUM_FILES=8</li>
</ul>
</li>
<li>make</li>
<li>make install</li>
</ul>
<h1 id="术语总结"><a href="#术语总结" class="headerlink" title="术语总结"></a>术语总结</h1><p>索引整体的逻辑结构图，如下所示：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/1.png" alt="图片"></p>
<h2 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h2><p>对于初学全文检索的人来说，索引这个词非常具有迷惑性，主要原因是它有两个词性：</p>
<ul>
<li>动词：做动词时，一般英文写为“<em>indexing</em>”，比如“<em>索引一个文件</em>”翻译为“<em>indexing a file</em>”，它指的是我们将原始数据经过一系列的处理，最终形成可以高效全文检索（对于Lucene，就是生成倒排索引）的过程。这个过程就称之为<strong>索引（indexing）</strong>。</li>
<li>名词：做名词时，写为“<em>index</em>”。经过indexing最终形成的结果（一般以文件形式存在）称之为<strong>索引（index）</strong>。</li>
</ul>
<p>所以，见到索引这个词，你一定要分清楚是动词还是名词。后面为了清楚，凡是作为动词的时候我使用indexing，作为名词的时候使用index。Index是Lucene中的顶级逻辑结构，它是一个逻辑概念，如果对应到具体的实物，就是一个目录，目录里面所有的文件组成一个index。注意，这个目录里面不会再嵌套目录，只会包含多个文件。具体index的构成细节后面会专门写一篇文章来介绍。对应到代码里面，就是org.apache.lucene.store.Directory这个抽象类。最后要说明一点的是，Lucene中的Index和ElasticSearch里面的Index不是一个概念，ElasticSearch里面的shard对应的才是Lucene的Index。</p>
<h2 id="文档（Document）和字段（Field）"><a href="#文档（Document）和字段（Field）" class="headerlink" title="文档（Document）和字段（Field）"></a>文档（Document）和字段（Field）</h2><p>一个Index里面会包含若干个文档，文档就像MySQL里面的一行（record）或者HBase里面的一列。文档是Lucene里面索引和搜索的原子单位，就像我们在MySQL里面写数据的时候，肯定是以行为单位的；读的时候也是以行为单位的。当然我们可以指定只读/写行里面某些字段，但仍是以行为单位的，Lucene也是一样，以文档为最小单位。代码里面是这样说明的：”<em>Documents are the unit of indexing and search</em>“.每个文档都会有一个唯一的文档ID。</p>
<p>文档是一个灵活的概念，不同的业务场景对应的具体含义不同。对于搜索引擎来说，一个文档可能就代表爬虫爬到的一个网页，很多个网页（文档）组成了一个索引。而对于提供检索功能的邮件客户端来说，一个文档可能就代表一封邮件，很多封邮件（文档）组成了一个索引。再比如假设我们要构建一个带全文检索功能的商品管理系统，那一件商品就是一个文档，很多个商品组成了一个索引。对于日志处理，一般是一行日志代表一个文档。</p>
<p>文档里面包含若干个字段，真正的数据是存储在字段里面的。一个字段包含三个要素：<strong><em>名称、类型、值</em></strong>。我们要索引数据，必须将数据以文本形式存储到字段里之后才可以。Lucene的字段由一个key-value组成，就像map一样。value支持多种类型，如果value是一个map类型，那就是嵌套字段了。</p>
<p>最后需要注意的是，不同于传统的关系型数据库，Lucene不要求一个index里面的所有文档的字段要一样，如果你喜欢，每一条文档的结构都可以不一样（当然实际中不建议这样操作），而且不需要事先定义，这个特性一般称之为“<strong><em>flexible schema</em></strong>”。传统的关系型数据库要求一个表里面的所有字段的结构必须一致，而且必事先定义好，一般称之为“<strong><em>strict schema</em></strong>”或者”<strong><em>fixed schema</em></strong>“。比如，有一个名为“<em>mixture</em>”的索引包含3条Document，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &#123; &quot;name&quot;: &quot;Ni Yanchun&quot;, &quot;gender&quot;: &quot;male&quot;, &quot;age&quot;: 28  &#125;,</span><br><span class="line">    &#123; &quot;name&quot;: &quot;Donald John Trump&quot;, &quot;gender&quot;: &quot;male&quot;, &quot;birthday&quot;: &quot;1946.06.14&quot;&#125;,</span><br><span class="line">    &#123; &quot;isbn&quot;: &quot;978-1-933988-17-7&quot;, &quot;price&quot;: 60, &quot;publish&quot;: &quot;2010&quot;, &quot;topic&quot;: [&quot;lucene&quot;, &quot;search&quot;]&#125;</span><br></pre></td></tr></table></figure>
<p>}</p>
<p>可以看到，3条Document的字段并不完全一样，这在Lucene中是合法的。</p>
<h2 id="Token和Term"><a href="#Token和Term" class="headerlink" title="Token和Term"></a>Token和Term</h2><p>Token存储在字段中的文本数据经过分词器分词后（准确的说是经过Tokenizer处理之后）产生的一系列词或者词组。比如假设有个”content”字段的存储的值为”My name is Ni Yanchun”，这个字段经过Lucene的标准分词器分词后的结果是：”my”, “name”, “is”, “ni”, “yanchun”。这里的每个词就是一个token，当然实际上除了词自身外，token还会包含一些其它属性。后面的文章中会介绍这些属性。</p>
<p>一个token加上它原来所属的字段的名称构成了<strong>Term</strong>。比如”content”和”my”组成一个term，”content”和”name”组成另外一个term。我们检索的时候搜的就是Term，而不是Token或者Document（但搜到term之后，会找到包含这个term的Document，然后返回整个Document，而不是返回单个Term）。</p>
<h2 id="Index-Segment"><a href="#Index-Segment" class="headerlink" title="Index Segment"></a>Index Segment</h2><p>在上面的图中，Document分别被一些绿框括了起来，这个称为Segment。Indexing的时候，并不是将所有数据写到一起，而是再分了一层，这层就是segment。Indexing的时候，会先将Document缓存，然后定期flush到文件。每次flush就会生成一个Segment。所以一个Index包含若干个Segment，每个Segment包含一部分Document。为了减少文件描述符的使用，这些小的Segment会定期的合并为（merge）大的Segment，数据量不大的时候，合并之后一个index可能只有一个Segment。搜索的时候，会搜索各个Segment，然后合并搜索结果。</p>
<h1 id="Analyzer"><a href="#Analyzer" class="headerlink" title="Analyzer"></a>Analyzer</h1><blockquote>
<p>参考：<br><a href="https://gist.github.com/Sennahoi/740753384999add46fc1" target="_blank" rel="noopener">https://gist.github.com/Sennahoi/740753384999add46fc1</a><br><a href="https://niyanchun.com/lucene-learning-4.html" target="_blank" rel="noopener">https://niyanchun.com/lucene-learning-4.html</a></p>
</blockquote>
<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>Analyzer像一个数据加工厂，输入是原始的文本数据，输出是经过各种工序加工的term，然后这些terms以倒排索引的方式存储起来，形成最终用于搜索的Index。所以Analyzer也是我们控制数据能以哪些方式检索的重要点。</p>
<h3 id="内置的Analyzer对比"><a href="#内置的Analyzer对比" class="headerlink" title="内置的Analyzer对比"></a>内置的Analyzer对比</h3><p>Lucene已经帮我们内置了许多Analyzer，我们先来挑几个常见的对比一下他们的分析效果吧：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">package com.niyanchun;</span><br><span class="line"></span><br><span class="line">import org.apache.lucene.analysis.Analyzer;</span><br><span class="line">import org.apache.lucene.analysis.TokenStream;</span><br><span class="line">import org.apache.lucene.analysis.core.KeywordAnalyzer;</span><br><span class="line">import org.apache.lucene.analysis.core.SimpleAnalyzer;</span><br><span class="line">import org.apache.lucene.analysis.core.WhitespaceAnalyzer;</span><br><span class="line">import org.apache.lucene.analysis.en.EnglishAnalyzer;</span><br><span class="line">import org.apache.lucene.analysis.standard.StandardAnalyzer;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line">import java.util.concurrent.atomic.AtomicInteger;</span><br><span class="line">public class AnalyzerCompare &#123;</span><br><span class="line"></span><br><span class="line">    private static final Analyzer[] ANALYZERS &#x3D; new Analyzer[]&#123;</span><br><span class="line">            new WhitespaceAnalyzer(), &#x2F;&#x2F; 仅根据空白字符（whitespace）进行分词。</span><br><span class="line">            new KeywordAnalyzer(), &#x2F;&#x2F; 不做任何分词，把整个原始输入作为一个token。所以可以看到输出只有1个token，就是原始句子。</span><br><span class="line">            new SimpleAnalyzer(), &#x2F;&#x2F; 根据非字母（non-letters）分词，并且将token全部转换为小写。所以该分词的输出的terms都是由小写字母组成的。</span><br><span class="line">            new StandardAnalyzer(EnglishAnalyzer.getDefaultStopSet()) &#x2F;&#x2F; 基于JFlex进行语法分词，然后删除停用词，并且将token全部转换为小写。标准分词器会处理停用词，但默认其停用词库为空，这里我们使用英文的停用词&#125;;</span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        String content &#x3D; &quot;My name is Ni Yanchun, I&#39;m 28 years old. You can contact me with the email niyanchun@outlook.com&quot;;</span><br><span class="line">        System.out.println(&quot;原始数据:\n&quot; + content + &quot;\n\n分析结果：&quot;);</span><br><span class="line">        for (Analyzer analyzer : ANALYZERS) &#123;</span><br><span class="line">            showTerms(analyzer, content);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private static void showTerms(Analyzer analyzer, String content) throws IOException &#123;</span><br><span class="line"></span><br><span class="line">        try (TokenStream tokenStream &#x3D; analyzer.tokenStream(&quot;content&quot;, content)) &#123;</span><br><span class="line">            StringBuilder sb &#x3D; new StringBuilder();</span><br><span class="line">            AtomicInteger tokenNum &#x3D; new AtomicInteger();</span><br><span class="line">            tokenStream.reset();</span><br><span class="line">            while (tokenStream.incrementToken()) &#123;</span><br><span class="line">                tokenStream.reflectWith(((attClass, key, value) -&gt; &#123;</span><br><span class="line">                    if (&quot;term&quot;.equals(key)) &#123;</span><br><span class="line">                        tokenNum.getAndIncrement();</span><br><span class="line">                        sb.append(&quot;\&quot;&quot;).append(value).append(&quot;\&quot;, &quot;);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;));</span><br><span class="line">            &#125;</span><br><span class="line">            tokenStream.end();</span><br><span class="line"></span><br><span class="line">            System.out.println(analyzer.getClass().getSimpleName() + &quot;:\n&quot; + tokenNum + &quot; tokens: [&quot; + sb.toString().substring(0, sb.toString().length() - 2) + &quot;]&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这段代码的功能是使用常见的四种分词器（WhitespaceAnalyzer，KeywordAnalyzer，SimpleAnalyzer，StandardAnalyzer）对“My name is Ni Yanchun, I’m 28 years old. You can contact me with the email niyanchun@outlook.com”这句话进行analyze，输出最终的terms。其中需要注意的是，标准分词器会去掉停用词（stop word），但其内置的停用词库为空，所以我们传了一个英文默认的停用词库。运行代码之后的输出如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">原始数据:</span><br><span class="line">My name is Ni Yanchun, I&#39;m 28 years old. You can contact me with the email niyanchun@outlook.com</span><br><span class="line"></span><br><span class="line">分析结果：</span><br><span class="line">WhitespaceAnalyzer:</span><br><span class="line">17 tokens: [&quot;My&quot;, &quot;name&quot;, &quot;is&quot;, &quot;Ni&quot;, &quot;Yanchun,&quot;, &quot;I&#39;m&quot;, &quot;28&quot;, &quot;years&quot;, &quot;old.&quot;, &quot;You&quot;, &quot;can&quot;, &quot;contact&quot;, &quot;me&quot;, &quot;with&quot;, &quot;the&quot;, &quot;email&quot;, &quot;niyanchun@outlook.com&quot;]</span><br><span class="line">KeywordAnalyzer:</span><br><span class="line">1 tokens: [&quot;My name is Ni Yanchun, I&#39;m 28 years old. You can contact me with the email niyanchun@outlook.com&quot;]</span><br><span class="line">SimpleAnalyzer:</span><br><span class="line">19 tokens: [&quot;my&quot;, &quot;name&quot;, &quot;is&quot;, &quot;ni&quot;, &quot;yanchun&quot;, &quot;i&quot;, &quot;m&quot;, &quot;years&quot;, &quot;old&quot;, &quot;you&quot;, &quot;can&quot;, &quot;contact&quot;, &quot;me&quot;, &quot;with&quot;, &quot;the&quot;, &quot;email&quot;, &quot;niyanchun&quot;, &quot;outlook&quot;, &quot;com&quot;]</span><br><span class="line">StandardAnalyzer:</span><br><span class="line">15 tokens: [&quot;my&quot;, &quot;name&quot;, &quot;ni&quot;, &quot;yanchun&quot;, &quot;i&#39;m&quot;, &quot;28&quot;, &quot;years&quot;, &quot;old&quot;, &quot;you&quot;, &quot;can&quot;, &quot;contact&quot;, &quot;me&quot;, &quot;email&quot;, &quot;niyanchun&quot;, &quot;outlook.com&quot;]</span><br></pre></td></tr></table></figure>
<h3 id="Analyzer原理"><a href="#Analyzer原理" class="headerlink" title="Analyzer原理"></a>Analyzer原理</h3><p>前面我们说了Analyzer就像一个加工厂，包含很多道工序。这些工序在Lucene里面分为两大类：Tokenizer和TokenFilter。Tokenizer永远是Analyzer的第一道工序，有且只有一个。它的作用是读取输入的原始文本，然后根据工序的内部定义，将其转化为一个个token输出。TokenFilter只能接在Tokenizer之后，因为它的输入只能是token。然后它将输入的token进行加工，输出加工之后的token。一个Analyzer中，TokenFilter可以没有，也可以有多个。也就是说一个Analyzer内部的流水线是这样的：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/2.png" alt="图片"></p>
<p>比如StandardAnalyzer的流水线是这样的：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/3.png" alt="图片"></p>
<p>所以，Analyzer的原理还是比较简单的，Tokenizer读入文本转化为token，然后后续的TokenFilter将token按需加工，输出需要的token。我们可以自由组合已有的Tokenizer和TokenFilter来满足自己的需求，也可以实现自己的Tokenizer和TokenFilter。</p>
<h2 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h2><h3 id="Analyzer和TokenStream"><a href="#Analyzer和TokenStream" class="headerlink" title="Analyzer和TokenStream"></a>Analyzer和TokenStream</h3><p>Analyzer对应的实现类是org.apache.lucene.analysis.Analyzer，这是一个抽象类。它的主要作用是构建一个org.apache.lucene.analysis.TokenStream对象，该对象用于分析文本。代码中的类描述是这样的：</p>
<blockquote>
<p>An Analyzer builds TokenStreams, which analyze text. It thus represents a policy for extracting index terms from text.</p>
</blockquote>
<p>因为它是一个抽象类，所以实际使用的时候需要继承它，实现具体的类。比如第一部分我们使用的4个内置Analyzer都是直接或间接继承的该类。继承的子类需要实现createComponents方法，之前说的一系列工序就是加在这个方法里的，可以认为一道工序就是整个流水线中的一个Component。Analyzer抽象类还实现了一个tokenStream方法，并且是final的。该方法会将一系列工序转化为TokenStream对象输出。比如SimpleAnalyzer的实现如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">public final class SimpleAnalyzer extends Analyzer &#123;</span><br><span class="line">  public SimpleAnalyzer() &#123;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  @Override</span><br><span class="line">  protected TokenStreamComponents createComponents(final String fieldName) &#123;</span><br><span class="line">    Tokenizer tokenizer &#x3D; new LetterTokenizer();</span><br><span class="line">    return new TokenStreamComponents(tokenizer, new LowerCaseFilter(tokenizer));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  @Override</span><br><span class="line">  protected TokenStream normalize(String fieldName, TokenStream in) &#123;</span><br><span class="line">    return new LowerCaseFilter(in);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>TokenStream的作用就是流式的产生token。这些token可能来自于indexing时文档里面的字段数据，也可能来自于检索时的检索语句。其实就是之前说的indexing和查询的时候都会调用Analyzer。</p>
<h3 id="Tokenizer和TokenFilter"><a href="#Tokenizer和TokenFilter" class="headerlink" title="Tokenizer和TokenFilter"></a>Tokenizer和TokenFilter</h3><p>TokenStream有两个非常重要的抽象子类：org.apache.lucene.analysis.Tokenizer和org.apache.lucene.analysis.TokenFilter。这两个类的实质其实都是一样的，都是对Token进行处理。不同之处就是前面介绍的，Tokenizer是第一道工序，所以它的输入是原始文本，输出是token；而TokenFilter是后面的工序，它的输入是token，输出也是token。实质都是对token的处理，所以实现它两个的子类都需要实现incrementToken方法，也就是在这个方法里面实现处理token的具体逻辑。incrementToken方法是在TokenStream类中定义的。比如前面提到的StandardTokenizer就是实现Tokenizer的一个具体子类；LowerCaseFilter和StopFilter就是实现TokenFilter的具体子类。</p>
<p>最后要说一下，Analyzer的流程越长，处理逻辑越复杂，性能就越差，实际使用中需要注意权衡。Analyzer的原理及代码就分析到这里，因为篇幅，一些源码没有在文章中全部列出，如果你有兴趣，建议去看下常见的Analyzer的实现的源码，一定会有收获。</p>
<h2 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h2><p>那么在python中我们怎么使用呢？下面举个小例子，使用pylucene其实和java的lucene没有太大差别，因为实际都是调用的java的类库。这里我没自定义过Analyzer，都是将输入进行处理后，变成用空格分好的结果，再送入到WhitespaceAnalyzer。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">from org.apache.lucene.analysis.core import WhitespaceAnalyzer</span><br><span class="line">from org.apache.lucene.index import IndexWriter, IndexWriterConfig</span><br><span class="line">from org.apache.lucene.store import SimpleFSDirectory</span><br><span class="line">from org.apache.lucene.document import Document, Field</span><br><span class="line">from org.apache.lucene.document import TextField</span><br><span class="line">from org.apache.lucene.search import BooleanQuery</span><br><span class="line">from org.apache.lucene.queryparser.classic import QueryParser</span><br><span class="line"># 声明</span><br><span class="line">lucene_analyzer &#x3D; WhitespaceAnalyzer()</span><br><span class="line"># 在建索引中使用analyzer</span><br><span class="line">sentence &#x3D; &#39;i am so confused .&#39;</span><br><span class="line">config &#x3D; IndexWriterConfig(self.lucene_analyzer)</span><br><span class="line">index_writer &#x3D; IndexWriter(SimpleFSDirectory(INDEXIDR), config)</span><br><span class="line">document &#x3D; Document()</span><br><span class="line">document.add(Field(&#39;sentence&#39;, sentence, TextField.TYPE_STORED))</span><br><span class="line">index_writer.addDocument(document)</span><br><span class="line"></span><br><span class="line"># 在构建query时使用analyzer</span><br><span class="line">query &#x3D; &#39;confuse&#39;</span><br><span class="line">boolean_query &#x3D; BooleanQuery.Builder()</span><br><span class="line">simple_query &#x3D; QueryParser(&quot;sentence&quot;, lucene_analyzer).parse(query)</span><br></pre></td></tr></table></figure>
<h1 id="倒排索引、Token与词向量"><a href="#倒排索引、Token与词向量" class="headerlink" title="倒排索引、Token与词向量"></a>倒排索引、Token与词向量</h1><h2 id="倒排索引（Inverted-Index）和正向索引（Forward-Index）"><a href="#倒排索引（Inverted-Index）和正向索引（Forward-Index）" class="headerlink" title="倒排索引（Inverted Index）和正向索引（Forward Index）"></a>倒排索引（Inverted Index）和正向索引（Forward Index）</h2><p>我们用一个例子来看什么是倒排索引，什么是正向索引。假设有两个文档（前面的数字为文档ID）：</p>
<ul>
<li>a good student.</li>
<li>a gifted student.</li>
</ul>
<p>这两个文档经过Analyzer之后（这里我们不去停顿词），分别得到以下两个索引表：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/4.png" alt="图片"></p>
<p>这两个表都是key-value形式的Map结构，该数据结构的最大特点就是可以根据key快速访问value。我们分别分析以下这两个表。</p>
<p>表1中，Map的key是一个个词，也就是上文中Analyzer的输出。value是包含该词的文档的ID。这种映射的好处就是如果我们知道了词，就可以很快的查出有哪些文档包含该词。大家想一下我们平时的检索是不是就是这种场景：我们知道一些关键字，然后想查有哪些网页包含该关键词。表1这种<strong>词到文档的映射</strong>结构就称之为<strong>倒排索引</strong>。</p>
<p>表2中，Map的key是文档id，而value是该文档中包含的所有词。这种结构的映射的好处是只要我们知道了文档（ID），就能知道这个文档里面包含了哪些词。这种<strong>文档到词的映射</strong>结构称之为<strong>正向索引</strong>。</p>
<p>倒排索引是文档检索系统最常用的数据结构，Lucene用的就是这种数据结构。那对于检索有了倒排索引是不是就够用了呢？我们来看一个搜索结果：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/5.png" alt="图片"></p>
<p>这里我搜索了我年少时的偶像S.H.E，一个台湾女团，Google返回了一些包含该关键字的网页，同时它将网页中该关键字用红色字体标了出来。几乎所有的搜索引擎都有该功能。大家想一下，使用上述的倒排索引结构能否做到这一点？</p>
<p>答案是做不到的。倒排索引的结构只能让我们快速判断一个文档（上述例子中一个网页就是一个文档）是否包含该关键字，但无法知道关键字出现在文档中的哪个位置。那搜索引擎是如何知道的呢？其实使用的是另外一个结构——词向量，词向量和倒排索引的信息都是在Analyze阶段计算出来的。在介绍词向量之前，我们先来看一下Analyze的输出结果——Token。</p>
<h2 id="Token"><a href="#Token" class="headerlink" title="Token"></a>Token</h2><p>Token除了包含词以外，还存在一些其它属性，下面就让我们来看看完整的token长什么样？</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.lucene.analysis.Analyzer;</span><br><span class="line">import org.apache.lucene.analysis.TokenStream;</span><br><span class="line">import org.apache.lucene.analysis.core.WhitespaceAnalyzer;</span><br><span class="line">import org.apache.lucene.analysis.standard.StandardAnalyzer;</span><br><span class="line">import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;</span><br><span class="line"></span><br><span class="line">public class AnalysisDebug &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        Analyzer analyzer &#x3D; new StandardAnalyzer();</span><br><span class="line">        String sentence &#x3D; &quot;a good student, a gifted student.&quot;;</span><br><span class="line">        try (TokenStream tokenStream &#x3D; analyzer.tokenStream(&quot;sentence&quot;, sentence)) &#123;</span><br><span class="line">            tokenStream.reset();</span><br><span class="line"></span><br><span class="line">            while (tokenStream.incrementToken()) &#123;</span><br><span class="line">                System.out.println(&quot;token: &quot; + tokenStream.reflectAsString(false));</span><br><span class="line">            &#125;</span><br><span class="line">            tokenStream.end();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们借助TokenStream对象输出经过StandardAnalyzer处理的数据，程序运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">token: term&#x3D;a,bytes&#x3D;[61],startOffset&#x3D;0,endOffset&#x3D;1,positionIncrement&#x3D;1,positionLength&#x3D;1,type&#x3D;&lt;ALPHANUM&gt;,termFrequency&#x3D;1</span><br><span class="line">token: term&#x3D;good,bytes&#x3D;[67 6f 6f 64],startOffset&#x3D;2,endOffset&#x3D;6,positionIncrement&#x3D;1,positionLength&#x3D;1,type&#x3D;&lt;ALPHANUM&gt;,termFrequency&#x3D;1</span><br><span class="line">token: term&#x3D;student,bytes&#x3D;[73 74 75 64 65 6e 74],startOffset&#x3D;7,endOffset&#x3D;14,positionIncrement&#x3D;1,positionLength&#x3D;1,type&#x3D;&lt;ALPHANUM&gt;,termFrequency&#x3D;1</span><br><span class="line">token: term&#x3D;a,bytes&#x3D;[61],startOffset&#x3D;16,endOffset&#x3D;17,positionIncrement&#x3D;1,positionLength&#x3D;1,type&#x3D;&lt;ALPHANUM&gt;,termFrequency&#x3D;1</span><br><span class="line">token: term&#x3D;gifted,bytes&#x3D;[67 69 66 74 65 64],startOffset&#x3D;18,endOffset&#x3D;24,positionIncrement&#x3D;1,positionLength&#x3D;1,type&#x3D;&lt;ALPHANUM&gt;,termFrequency&#x3D;1</span><br><span class="line">token: term&#x3D;student,bytes&#x3D;[73 74 75 64 65 6e 74],startOffset&#x3D;25,endOffset&#x3D;32,positionIncrement&#x3D;1,positionLength&#x3D;1,type&#x3D;&lt;ALPHANUM&gt;,termFrequency&#x3D;1</span><br></pre></td></tr></table></figure>
<p>这个输出结果是非常值得探究的。可以看到sentence字段的文本数据”a good student, a gifted student”经过StandardAnalyzer分析之后输出了6个token，每个token由一些属性组成，这些属性对应的定义类在org.apache.lucene.analysis.tokenattributes包下面，有兴趣的可以查阅。这里我们简单介绍一下这些属性：</p>
<ul>
<li><strong>term</strong>：解析出来的词。<strong>注意这里的term不同于我们之前介绍的Term，它仅指提取出来的词</strong>。</li>
<li><strong>bytes</strong>：词的字节数组形式。</li>
<li><strong>startOffset, endOffset</strong>：词开始和结束的位置，从0开始计数。大家可以数一下。</li>
<li><strong>positionIncrement</strong>：当前词和上个词的距离，默认为1，表示词是连续的。如果有些token被丢掉了，这个值就会大于1了。可以将上述代码中注释掉的那行放开，同时将原来不带停用词的analyzer注释掉，这样解析出的停用词token就会被移除，你就会发现有些token的该字段的值会变成2。该字段主要用于支持”phrase search”, “span search”以及”highlight”，这些搜索都需要知道关键字在文档中的position，以后介绍搜索的时候再介绍。另外这个字段还有一个非常重要的用途就是支持同义词查询。我们将该某个token的positionIncrement置为0，就表示该token和上个token没有距离，搜索的时候，不论搜这两个token任何一个，都会返回它们两对应的文档。假设第一个token是土豆，下一个token是马铃薯，马铃薯对应的token的positionIncrement为0，那我们搜马铃薯时，也会给出土豆相关的信息，反之亦然。</li>
<li><strong>positionLength</strong>：该字段跨了多少个位置。代码注释中说极少有Analyzer会产生该字段，基本都是使用默认值1.</li>
<li><strong>type</strong>：字段类型。需要注意的是这个类型是由每个Analyzer的Tokenizer定义的，不同的Analyer定义的类型可能不同。比如StandardAnalyzer使用的StandardTokenizer定义了这几种类型：<ALPHANUM>、<NUM>、<SOUTHEAST_ASIAN>、<IDEOGRAPHIC>、<HIRAGANA>、<KATAKANA>、<HANGUL>、<EMOJI>。</li>
<li><strong>termFrequency</strong>：词频。注意这里的词频不是token在句子中出现的频率，而是让用户自定义的，比如我们想让某个token在评分的时候更重要一些，那我们就可以将其词频设置大一些。如果不设置，默认都会初始化为1。比如上面输出结果中有两个”a”字段，词频都为初始值1，这个在后续的流程会合并，合并之后，词频会变为2。</li>
</ul>
<p>除了以上属性外，还有一个可能存在的属性就是payload，我们可以在这个字段里面存储一些信息。以上就是一个完整的Token。</p>
<h2 id="词向量（Term-Vector）"><a href="#词向量（Term-Vector）" class="headerlink" title="词向量（Term Vector）"></a>词向量（Term Vector）</h2><p>Analyzer分析出来的Token并不会直接写入Index，还需要做一些转化：</p>
<ul>
<li>取token中的词，以及包含该词的字段信息、文档信息（doc id），形成词到字段信息、文档信息的映射，也就是我们前面介绍的倒排索引。</li>
<li>取token中的词，以及包含该词的positionIncrement、startOffset、endOffset、termFrequency信息，组成从token到后面四个信息的映射，这就是<strong>词向量</strong>。</li>
</ul>
<p>所以，倒排索引和词向量都是从term到某个value的映射，只是value的值不一样。这里需要注意，倒排索引是所有文档范围内的，而词向量是某个文档范围的。简言之就是一个index对应一个倒排索引，而一个document就有一个词向量。<strong>有了倒排索引，我们就知道搜索关键字包含在index的哪些document的字段中。有了词向量，我们就知道关键字在匹配到的document的具体位置。</strong>下面让我们从代码角度来验证一下上面的理论。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.lucene.analysis.Analyzer;</span><br><span class="line">import org.apache.lucene.analysis.standard.StandardAnalyzer;</span><br><span class="line">import org.apache.lucene.document.Document;</span><br><span class="line">import org.apache.lucene.document.Field;</span><br><span class="line">import org.apache.lucene.document.FieldType;</span><br><span class="line">import org.apache.lucene.index.*;</span><br><span class="line">import org.apache.lucene.search.DocIdSetIterator;</span><br><span class="line">import org.apache.lucene.store.Directory;</span><br><span class="line">import org.apache.lucene.store.FSDirectory;</span><br><span class="line"></span><br><span class="line">import java.nio.file.Paths;</span><br><span class="line"></span><br><span class="line">public class TermVectorShow &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        &#x2F;&#x2F; 构建索引</span><br><span class="line">        final String indexPath &#x3D; &quot;indices&#x2F;tv-show&quot;;</span><br><span class="line">        Directory indexDir &#x3D; FSDirectory.open(Paths.get(indexPath));</span><br><span class="line"></span><br><span class="line">        Analyzer analyzer &#x3D; new StandardAnalyzer();</span><br><span class="line">        IndexWriterConfig iwc &#x3D; new IndexWriterConfig(analyzer);</span><br><span class="line">        iwc.setOpenMode(IndexWriterConfig.OpenMode.CREATE);</span><br><span class="line">        IndexWriter writer &#x3D; new IndexWriter(indexDir, iwc);</span><br><span class="line"></span><br><span class="line">        String sentence &#x3D; &quot;a good student, a gifted student&quot;;</span><br><span class="line">        &#x2F;&#x2F; 默认不会保存词向量，这里我们通过一些设置来保存词向量的相关信息</span><br><span class="line">        FieldType fieldType &#x3D; new FieldType();</span><br><span class="line">        fieldType.setStored(true);</span><br><span class="line">        fieldType.setStoreTermVectors(true);</span><br><span class="line">        fieldType.setStoreTermVectorOffsets(true);</span><br><span class="line">        fieldType.setStoreTermVectorPositions(true);</span><br><span class="line">        fieldType.setIndexOptions(</span><br><span class="line">             IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);</span><br><span class="line">        Field field &#x3D; new Field(&quot;content&quot;, sentence, fieldType);</span><br><span class="line">        Document document &#x3D; new Document();</span><br><span class="line">        document.add(field);</span><br><span class="line">        writer.addDocument(document);</span><br><span class="line">        writer.close();</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 从索引读取Term Vector信息</span><br><span class="line">        IndexReader indexReader &#x3D; DirectoryReader.open(indexDir);</span><br><span class="line">        Terms termVector &#x3D; indexReader.getTermVector(0, &quot;content&quot;);</span><br><span class="line">        TermsEnum termIter &#x3D; termVector.iterator();</span><br><span class="line">        while (termIter.next() !&#x3D; null) &#123;</span><br><span class="line">            PostingsEnum postingsEnum &#x3D; termIter.postings(null, PostingsEnum.ALL);</span><br><span class="line">            while (postingsEnum.nextDoc() !&#x3D; DocIdSetIterator.NO_MORE_DOCS) &#123;</span><br><span class="line">                int freq &#x3D; postingsEnum.freq();</span><br><span class="line">                System.out.printf(&quot;term: %s, freq: %d,&quot;, termIter.term().utf8ToString(), freq);</span><br><span class="line">                while (freq &gt; 0) &#123;</span><br><span class="line">                    System.out.printf(&quot; nextPosition: %d,&quot;, postingsEnum.nextPosition());</span><br><span class="line">                    System.out.printf(&quot; startOffset: %d, endOffset: %d&quot;,</span><br><span class="line">                            postingsEnum.startOffset(), postingsEnum.endOffset());</span><br><span class="line">                    freq--;</span><br><span class="line">                &#125;</span><br><span class="line">                System.out.println();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这段代码实现的功能是先indexing 1条document，形成index，然后我们读取index，从中获取那条document content字段的词向量。需要注意，indexing时默认是不存储词向量相关信息的，我们需要通过FieldType做显式的设置，否则你读取出来的Term Vector会是null。我们看一下程序的输出结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">term: a, freq: 2, nextPosition: 0, startOffset: 0, endOffset: 1 nextPosition: 3, startOffset: 16, endOffset: 17</span><br><span class="line">term: gifted, freq: 1, nextPosition: 4, startOffset: 18, endOffset: 24</span><br><span class="line">term: good, freq: 1, nextPosition: 1, startOffset: 2, endOffset: 6</span><br><span class="line">term: student, freq: 2, nextPosition: 2, startOffset: 7, endOffset: 14 nextPosition: 5, startOffset: 25, endOffset: 32</span><br></pre></td></tr></table></figure>
<p>这里我们indexing的数据和上一节token部分的数据是一样的，而且都使用的是StandardAnalyzer，所以我们可以对比着看上一节输出的token和这里输出的term vector数据。可以看到，之前重复的token（a和student）到这里已经被合并了，并且词频也相应的变成了2。然后我们看一下position信息和offset信息也是OK的。而像token中的positionLength、type等信息都丢弃了。<br>词向量的信息量比较大，所以默认并不记录，我们想要保存时需要针对每个字段做显式的设置，Lucene 8.2.0中包含如下一些选项（见org.apache.lucene.index.IndexOptions枚举类）：</p>
<ul>
<li>NONE：不索引</li>
<li>DOCS：只索引字段，不保存词频等位置信息</li>
<li>DOCS_AND_FREQS：索引字段并保存词频信息，但不保存位置信息</li>
<li>DOCS_AND_FREQS_AND_POSITIONS：索引字段并保存词频及位置信息</li>
<li>DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS：索引字段并保存词频、位置、偏移量等信息</li>
</ul>
<p>phrase search和span search需要position信息支持，所以一般全文搜索引擎默认会采用DOCS_AND_FREQS_AND_POSITIONS策略，这样基本就能覆盖常用的搜索需求了。而需要高亮等功能的时候，才需要记录offset信息。</p>
<h1 id="字段及其属性"><a href="#字段及其属性" class="headerlink" title="字段及其属性"></a>字段及其属性</h1><p>在创建Field的时候，第一个参数是字段名，第二个是字段值，第三个就是字段属性了。字段的属性决定了字段如何Analyze，以及Analyze之后存储哪些信息，进而决定了以后我们可以使用哪些方式进行检索。</p>
<h2 id="Field类"><a href="#Field类" class="headerlink" title="Field类"></a>Field类</h2><p>Field对应的类是org.apache.lucene.document.Field，该类实现了org.apache.lucene.document.IndexableField接口，代表用于indexing的一个字段。Field类比较底层一些，所以Lucene实现了许多Field子类，用于不同的场景，比如下图是IDEA分析出来的Field的子类：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/6.png" alt="图片"></p>
<p>如果有某个子类能满足我们的场景，那推荐使用子类。在介绍常用子类之前，需要了解一下字段的三大类属性：</p>
<ul>
<li>是否indexing（只有indexing的数据才能被搜索）</li>
<li>是否存储（即是否保存字段的原始值）</li>
<li>是否保存term vector</li>
</ul>
<p>这些属性就是由之前文章中介绍的org.apache.lucene.index.IndexOptions枚举类定义的：</p>
<ul>
<li>NONE：不索引</li>
<li>DOCS：只索引字段，不保存词频等位置信息</li>
<li>DOCS_AND_FREQS：索引字段并保存词频信息，但不保存位置信息</li>
<li>DOCS_AND_FREQS_AND_POSITIONS：索引字段并保存词频及位置信息</li>
<li>DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS：索引字段并保存词频、位置、偏移量等信息</li>
</ul>
<p>Field的各个子类就是实现了对不同类型字段的存储，同时选择了不同的字段属性，这里列举几个常用的：</p>
<ul>
<li>TextField：存储字符串类型的数据。indexing+analyze；不存储原始数据；不保存term vector。适用于需要全文检索的数据，比如邮件内容，网页内容等。</li>
<li>StringField：存储字符串类型的数据。indexing但不analyze，即整个字符串就是一个token，之前介绍的KeywordAnalyzer就属于这种；不存储原始数据；不保存term vector。适用于文章标题、人名、ID等只需精确匹配的字符串。</li>
<li>IntPoint, LongPoint, FloatPoint, DoublePoint：用于存储各种不同类型的数值型数据。indexing；不存储原始数据；不保存term vector。适用于数值型数据的存储。</li>
</ul>
<p>所以，对于Field及其子类我们需要注意以下两点：</p>
<ul>
<li>几乎所有的Field子类（除StoredField）都默认不存储原始数据，如果需要存储原始数据，就要额外增加一个StoredField类型的字段，专门用于存储原始数据。注意该类也是Field的一个子类。当然也有一些子类的构造函数中提供了参数来控制是否存储原始数据，比如StringField，创建实例时可以通过传递Field.Store.YES参数来存储原始数据。</li>
<li>Field子类的使用场景和对应的属性都已经设置好了，如果子类不能满足我们的需求，就需要对字段属性进行自定义，但子类的属性一般是不允许更改的，需要直接使用Field类，再配合FieldType类进行自定义化。</li>
</ul>
<h2 id="FieldType类"><a href="#FieldType类" class="headerlink" title="FieldType类"></a>FieldType类</h2><p>org.apache.lucene.document.FieldType类实现了org.apache.lucene.index.IndexableFieldType接口，用于描述字段的属性，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">FieldType fieldType &#x3D; new FieldType();</span><br><span class="line">fieldType.setStored(true);</span><br><span class="line">fieldType.setStoreTermVectors(true);</span><br><span class="line">fieldType.setStoreTermVectorOffsets(true);</span><br><span class="line">fieldType.setStoreTermVectorPositions(true);</span><br><span class="line">fieldType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);</span><br><span class="line">Field field &#x3D; new Field(&quot;content&quot;, sentence, fieldType);</span><br></pre></td></tr></table></figure>
<p>该类定义了一些成员变量，这些成员变量就是字段的一些属性，这里列一下代码中的成员变量：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">private boolean stored;</span><br><span class="line">private boolean tokenized &#x3D; true;</span><br><span class="line">private boolean storeTermVectors;</span><br><span class="line">private boolean storeTermVectorOffsets;</span><br><span class="line">private boolean storeTermVectorPositions;</span><br><span class="line">private boolean storeTermVectorPayloads;</span><br><span class="line">private boolean omitNorms;</span><br><span class="line">private IndexOptions indexOptions &#x3D; IndexOptions.NONE;</span><br><span class="line">private boolean frozen;</span><br><span class="line">private DocValuesType docValuesType &#x3D; DocValuesType.NONE;</span><br><span class="line">private int dataDimensionCount;</span><br><span class="line">private int indexDimensionCount;</span><br><span class="line">private int dimensionNumBytes;</span><br><span class="line">private Map&lt;String, String&gt; attributes;</span><br></pre></td></tr></table></figure>
<p>大部分属性含义已经比较清楚了，这里再简单介绍一下其含义：</p>
<ul>
<li><em>stored</em>：是否存储字段，默认为false。</li>
<li><em>tokenized</em>：是否analyze，默认为true。</li>
<li><em>storeTermVectors</em>：是否存储TermVector（如果是true，也不存储offset、position、payload信息），默认为false。</li>
<li><em>storeTermVectorOffsets</em>：是否存储offset信息，默认为false。</li>
<li><em>storeTermVectorPositions</em>：是否存储position信息，默认为false。</li>
<li><em>storeTermVectorPayloads</em>：是否存储payload信息，默认为false。</li>
<li><em>omitNorms</em>：是否忽略norm信息，默认为false。那什么是norm信息呢？Norm的全称是“Normalization”，理解起来非常简单，按照TF-IDF的计算方式，包含同一个搜索词的多个文本，文本越短其权重（或者叫相关性）越高。比如有两个文本都包含搜索词，一个文本只有100个词，另外文本一个有1000个词，那按照TF-IDF的算法，第一个文本跟搜索词的相关度比第二个文本高。这个信息就是norm信息。如果我们将其忽略掉，那在计算相关性的时候，会认为长文本和短文本的权重得分是一样的。</li>
<li><em>indexOptions</em>：即org.apache.lucene.index.IndexOptions，已经介绍过了。默认值为DOCS_AND_FREQS_AND_POSITIONS。</li>
<li><em>frozen</em>：该值设置为true之后，字段的各个属性就不允许再更改了，比如Field的TextField、StringField等子类都将该值设置为true了，因为他们已经将字段的各个属性定制好了。</li>
<li><em>dataDimensionCount</em>、<em>indexDimensionCount</em>、<em>dimensionNumBytes</em>：这几个和数值型的字段类型有关系，Lucene 6.0开始，对于数值型都改用Point来组织。dataDimensionCount和indexDimensionCount都可以理解为是Point的维度，类似于数组的维度。dimensionNumBytes则是Point中每个值所使用的字节数，比如IntPoint和FloatPoint是4个字节，LongPoint和DoublePoint则是8个字节。</li>
<li><em>attributes</em>：可以选择性的以key-value的形式给字段增加一些元数据信息，但注意这个key-value的map不是线程安全的。</li>
<li><em>docValuesType</em>：指定字段的值指定以何种类型索引DocValue。那什么是DocValue？DocValue就是从文档到Term的一个正向索引，需要这个东西是因为排序、聚集等操作需要根据文档快速访问文档内的字段。Lucene 4.0之前都是在查询的时候将所有文档的相关字段信息加载到内存缓存，一方面用的时候才加载，所以慢，另一方面对于内存压力很大。4.0中引入了DocValue的概念，在indexing阶段除了创建倒排索引，也可以选择性的创建一个正向索引，这个正向索引就是DocValue，主要用于排序、聚集等操作。其好处是存储在磁盘上，减小了内存压力，而且因为是事先计算好的，所以使用时速度也很快。弊端就是磁盘使用量变大（需要耗费 document个数*每个document的字段数 个字节），同时indexing的速度慢了。</li>
</ul>
<p>对于我们使用（包括ES、Solr等基于Lucene的软件）而言，只需要知道我们检索一个字段的时候可以控制保存哪些信息，以及这些信息在什么场景下使用，能带来什么好处，又会产生什么弊端即可。举个例子：比如我们在设计字段的时候，如果一个字段不会用来排序，也不会做聚集，那就没有必要生成DocValue，既能节省磁盘空间，又能提高写入速度。另外对于norm信息，如果你的场景只关注是否包含，那无需保存norm信息，但如果也关注相似度评分，并且文本长短是一个需要考虑的因素，那就应该保存norm信息。</p>
<h2 id="基本使用-1"><a href="#基本使用-1" class="headerlink" title="基本使用"></a>基本使用</h2><blockquote>
<p>参考：<br><a href="https://blog.51cto.com/8744704/2086852" target="_blank" rel="noopener">https://blog.51cto.com/8744704/2086852</a><br><a href="https://www.cnblogs.com/leeSmall/p/9011405.html" target="_blank" rel="noopener">https://www.cnblogs.com/leeSmall/p/9011405.html</a><br><a href="https://www.amazingkoala.com.cn/Lucene/DocValues/2019/0412/48.html" target="_blank" rel="noopener">https://www.amazingkoala.com.cn/Lucene/DocValues/2019/0412/48.html</a><br><a href="https://www.cnblogs.com/cnjavahome/p/9192467.html" target="_blank" rel="noopener">https://www.cnblogs.com/cnjavahome/p/9192467.html</a></p>
</blockquote>
<p>在例句搜索中主要使用了以下几种FieldType:</p>
<ul>
<li>TextField</li>
<li>IntPoint<ul>
<li>把整型存入索引中，必须同时加入NumericDocValuesField和StoredField，是看IntPoint源码注释中写的，不知道为什么一定要这样写</li>
</ul>
</li>
<li>FloatPoint<ul>
<li>把浮点数存入索引中，必须同时加入FloatDocValuesField和StoredField</li>
</ul>
</li>
<li>数组类型<ul>
<li>复杂类型存储</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">confidence &#x3D; int(data_json[&#39;confidence&#39;])</span><br><span class="line">document.add(IntPoint(&quot;confidence&quot;, confidence))</span><br><span class="line">document.add(NumericDocValuesField(&quot;confidence&quot;, confidence))</span><br><span class="line">document.add(StoredField(&quot;confidence&quot;, confidence))</span><br><span class="line"></span><br><span class="line">score &#x3D; float(data_json[&#39;score&#39;])</span><br><span class="line">document.add(FloatPoint(&quot;score&quot;, score))</span><br><span class="line">document.add(FloatDocValuesField(&quot;score&quot;, score))</span><br><span class="line">document.add(StoredField(&quot;score&quot;, score))</span><br><span class="line"></span><br><span class="line">document.add(SortedSetDocValuesField(&quot;keyword&quot;, BytesRef(keyword_text)))</span><br><span class="line">document.add(StoredField(&quot;keyword&quot;, keyword_text))</span><br><span class="line">document.add(Field(&quot;keyword&quot;, keyword_text, TextField.TYPE_STORED))</span><br></pre></td></tr></table></figure>
<h1 id="索引存储文件介绍"><a href="#索引存储文件介绍" class="headerlink" title="索引存储文件介绍"></a>索引存储文件介绍</h1><h3 id="索引文件格式"><a href="#索引文件格式" class="headerlink" title="索引文件格式"></a>索引文件格式</h3><p>不论是Solr还是ES，底层index的存储都是完全使用Lucene原生的方式，没有做改变，所以本文会以ES为例来介绍。需要注意的是Lucene的index在ES中称为shard，本文中提到的index都指的是Lucene的index，即ES中的shard。先来看一个某个index的数据目录：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/7.png" alt="图片"></p>
<p>可以看到一个索引包含了很多文件，似乎很复杂。但仔细观察之后会发现乱中似乎又有些规律：很多文件前缀一样，只是后缀不同，比如有很多_3c开头的文件。回想一下之前文章的介绍，index由若干个segment组成，而<strong>一个index目录下前缀相同表示这些文件都属于同一个segment</strong>。</p>
<p>那各种各样的后缀又代表什么含义呢？Lucene存储segment时有两种方式：</p>
<ul>
<li><strong>multifile格式</strong>。该模式下会产生很多文件，不同的文件存储不同的信息，其弊端是读取index时需要打开很多文件，可能造成文件描述符超出系统限制。</li>
<li><strong>compound格式</strong>。一般简写为CFS(Compound File System)，该模式下会将很多小文件合并成一个大文件，以减少文件描述符的使用。</li>
</ul>
<p>我们先来介绍multifile格式下的各个文件：</p>
<ul>
<li>write.lock：每个index目录都会有一个该文件，用于防止多个IndexWriter同时写一个文件。</li>
<li>segments_N：该文件记录index所有segment的相关信息，比如该索引包含了哪些segment。IndexWriter每次commit都会生成一个（N的值会递增），新文件生成后旧文件就会删除。所以也说该文件用于保存commit point信息。</li>
</ul>
<p>上面这两个文件是针对当前index的，所以每个index目录下都只会有1个（segments_N可能因为旧的没有及时删除临时存在两个）。下面介绍的文件都是针对segment的，每个segment就会有1个。</p>
<ul>
<li>.si：<em>Segment Info</em>的缩写，用于记录segment的一些元数据信息。</li>
<li>.fnm：<em>Fields</em>，用于记录fields设置类信息，比如字段的index option信息，是否存储了norm信息、DocValue等。</li>
<li>.fdt：<em>Field Data</em>，存储字段信息。当通过StoredField或者Field.Store.YES指定存储原始field数据时，这些数据就会存储在该文件中。</li>
<li>.fdx：<em>Field Index</em>，.fdt文件的索引/指针。通过该文件可以快速从.fdt文件中读取field数据。</li>
<li>.doc：<em>Frequencies</em>，存储了一个documents列表，以及它们的term frequency信息。</li>
<li>.pos：<em>Positions</em>，和.doc类似，但保存的是position信息。</li>
<li>.pay：Payloads<em>，和</em>.doc类似，但保存的是payloads和offset信息。</li>
<li>.tim：<em>Term Dictionary</em>，存储所有文档analyze出来的term信息。同时还包含term对应的document number以及若干指向.doc, .pos, .pay的指针，从而可以快速获取term的term vector信息。。</li>
<li>.tip：<em>Term Index</em>，该文件保存了Term Dictionary的索引信息，使得可以对Term Dictionary进行随机访问。</li>
<li>.nvd, .nvm：<em>Norms</em>，这两个都是用来存储Norms信息的，前者用于存储norms的数据，后者用于存储norms的元数据。</li>
<li>.dvd, .dvm：<em>Per-Document Values</em>，这两个都是用来存储DocValues信息的，前者用于数据，后者用于存储元数据。</li>
<li>.tvd：<em>Term Vector Data</em>，用于存储term vector数据。</li>
<li>.tvx：<em>Term Vector Index</em>，用于存储Term Vector Data的索引数据。</li>
<li>.liv：<em>Live Documents</em>，用于记录segment中哪些documents没有被删除。一般不存在该文件，表示segment内的所有document都是live的。如果有documents被删除，就会产生该文件。以前是使用一个.del后缀的文件来记录被删除的documents，现在改为使用该文件了。</li>
<li>.dim,.dii：<em>Point values</em>，这两个文件用于记录indexing的Point信息，前者保存数据，后者保存索引/指针，用于快速访问前者。</li>
</ul>
<p>上面介绍了很多文件类型，实际中不一定都有，如果indexing阶段不保存字段的term vector信息，那存储term vector的相关文件可能就不存在。如果一个index的segment非常多，那将会有非常非常多的文件，检索时，这些文件都是要打开的，很可能会造成文件描述符不够用，所以Lucene引入了前面介绍的CFS格式，它把上述每个segment的众多文件做了一个合并压缩（.liv和.si没有被合并，依旧单独写文件），最终形成了两个新文件：.cfs和.cfe，前者用于保存数据，后者保存了前者的一个Entry Table，用于快速访问。所以，如果使用CFS的话，最终对于每个segment，最多就只存在.cfs, .cfe, .si, .liv4个文件了。Lucene从1.4版本开始，默认使用CFS来保存segment数据，但开发者仍然可以选择使用multifile格式。一般来说，对于小的segment使用CFS，对于大的segment，使用multifile格式。比如Lucene的org.apache.lucene.index.MergePolicy构造函数中就提供merge时在哪些条件下使用CFS：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">  &#x2F;**</span><br><span class="line">   * Default ratio for compound file system usage. Set to &lt;tt&gt;1.0&lt;&#x2F;tt&gt;, always use </span><br><span class="line">   * compound file system.</span><br><span class="line">   *&#x2F;</span><br><span class="line">  protected static final double DEFAULT_NO_CFS_RATIO &#x3D; 1.0;</span><br><span class="line"></span><br><span class="line">  &#x2F;**</span><br><span class="line">   * Default max segment size in order to use compound file system. Set to &#123;@link Long#MAX_VALUE&#125;.</span><br><span class="line">   *&#x2F;</span><br><span class="line">  protected static final long DEFAULT_MAX_CFS_SEGMENT_SIZE &#x3D; Long.MAX_VALUE;</span><br><span class="line"></span><br><span class="line">  &#x2F;** If the size of the merge segment exceeds this ratio of</span><br><span class="line">   *  the total index size then it will remain in</span><br><span class="line">   *  non-compound format *&#x2F;</span><br><span class="line">  protected double noCFSRatio &#x3D; DEFAULT_NO_CFS_RATIO;</span><br><span class="line">  </span><br><span class="line">  &#x2F;** If the size of the merged segment exceeds</span><br><span class="line">   *  this value then it will not use compound file format. *&#x2F;</span><br><span class="line">  protected long maxCFSSegmentSize &#x3D; DEFAULT_MAX_CFS_SEGMENT_SIZE;</span><br><span class="line"></span><br><span class="line">  &#x2F;**</span><br><span class="line">   * Creates a new merge policy instance.</span><br><span class="line">   *&#x2F;</span><br><span class="line">  public MergePolicy() &#123;</span><br><span class="line">    this(DEFAULT_NO_CFS_RATIO, DEFAULT_MAX_CFS_SEGMENT_SIZE);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  &#x2F;**</span><br><span class="line">   * Creates a new merge policy instance with default settings for noCFSRatio</span><br><span class="line">   * and maxCFSSegmentSize. This ctor should be used by subclasses using different</span><br><span class="line">   * defaults than the &#123;@link MergePolicy&#125;</span><br><span class="line">   *&#x2F;</span><br><span class="line">  protected MergePolicy(double defaultNoCFSRatio, long defaultMaxCFSSegmentSize) &#123;</span><br><span class="line">    this.noCFSRatio &#x3D; defaultNoCFSRatio;</span><br><span class="line">    this.maxCFSSegmentSize &#x3D; defaultMaxCFSSegmentSize;</span><br></pre></td></tr></table></figure>
<h3 id=""><a href="#" class="headerlink" title="}"></a>}</h3><p>栗子</p>
<p>首先在ES中创建一个索引：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">PUT nyc-test</span><br><span class="line">&#123;</span><br><span class="line">  &quot;settings&quot;: &#123;</span><br><span class="line">    &quot;number_of_shards&quot;: 1,</span><br><span class="line">    &quot;number_of_replicas&quot;: 0,</span><br><span class="line">    &quot;refresh_interval&quot;: -1</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里设置1个shard，0个副本，并且将refresh_interval设置为-1，表示不自动刷新。创建完之后就可以在es的数据目录找到该索引，es的后台索引的目录结构为：&lt;数据目录&gt;/nodes/0/indices/&lt;索引UUID&gt;/<shard>/index，这里的shard就是Lucene的index。我们看下刚创建的index的目录：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">-&gt; % ll</span><br><span class="line">总用量 4.0K</span><br><span class="line">-rw-rw-r-- 1 allan allan 230 10月 11 21:45 segments_2</span><br><span class="line">-rw-rw-r-- 1 allan allan   0 10月 11 21:45 write.lock</span><br></pre></td></tr></table></figure>
<p>可以看到，现在还没有写入任何数据，所以只有index级别的segments_N和write.lock文件，没有segment级别的文件。写入1条数据并查看索引目录的变化：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">PUT nyc-test&#x2F;doc&#x2F;1</span><br><span class="line">&#123;</span><br><span class="line">  &quot;name&quot;: &quot;Jack&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 查看索引目录</span><br><span class="line">-&gt; % ll</span><br><span class="line">总用量 4.0K</span><br><span class="line">-rw-rw-r-- 1 allan allan   0 10月 11 22:20 _0.fdt</span><br><span class="line">-rw-rw-r-- 1 allan allan   0 10月 11 22:20 _0.fdx</span><br><span class="line">-rw-rw-r-- 1 allan allan 230 10月 11 22:19 segments_2</span><br><span class="line">-rw-rw-r-- 1 allan allan   0 10月 11 22:19 write.lock</span><br></pre></td></tr></table></figure>
<p>可以看到出现了1个segment的数据，因为ES把数据缓存在内存里面，所以文件大小为0。然后再写入1条数据，并查看目录变化：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">PUT nyc-test&#x2F;doc&#x2F;2</span><br><span class="line">&#123;</span><br><span class="line">  &quot;name&quot;: &quot;Allan&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 查看目录</span><br><span class="line">-&gt; % ll</span><br><span class="line">总用量 4.0K</span><br><span class="line">-rw-rw-r-- 1 allan allan   0 10月 11 22:20 _0.fdt</span><br><span class="line">-rw-rw-r-- 1 allan allan   0 10月 11 22:20 _0.fdx</span><br><span class="line">-rw-rw-r-- 1 allan allan 230 10月 11 22:19 segments_2</span><br><span class="line">-rw-rw-r-- 1 allan allan   0 10月 11 22:19 write.lock</span><br></pre></td></tr></table></figure>
<p>因为ES缓存机制的原因，目录没有变化。显式的refresh一下，让内存中的数据落地：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">POST nyc-test&#x2F;_refresh</span><br><span class="line"></span><br><span class="line">-&gt; % ll</span><br><span class="line">总用量 16K</span><br><span class="line">-rw-rw-r-- 1 allan allan  405 10月 11 22:22 _0.cfe</span><br><span class="line">-rw-rw-r-- 1 allan allan 2.5K 10月 11 22:22 _0.cfs</span><br><span class="line">-rw-rw-r-- 1 allan allan  393 10月 11 22:22 _0.si</span><br><span class="line">-rw-rw-r-- 1 allan allan  230 10月 11 22:19 segments_2</span><br><span class="line">-rw-rw-r-- 1 allan allan    0 10月 11 22:19 write.lock</span><br></pre></td></tr></table></figure>
<p>ES的refresh操作会将内存中的数据写入到一个新的segment中，所以refresh之后写入的两条数据形成了一个segment，并且使用CFS格式存储了。然后再插入1条数据，接着update这条数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># 触发Lucene commit</span><br><span class="line">POST nyc-test&#x2F;_flush?wait_if_ongoing</span><br><span class="line"></span><br><span class="line"># 查看目录</span><br><span class="line">-&gt; % ll</span><br><span class="line">总用量 32K</span><br><span class="line">-rw-rw-r-- 1 allan allan  405 10月 11 22:22 _0.cfe</span><br><span class="line">-rw-rw-r-- 1 allan allan 2.5K 10月 11 22:22 _0.cfs</span><br><span class="line">-rw-rw-r-- 1 allan allan  393 10月 11 22:22 _0.si</span><br><span class="line">-rw-rw-r-- 1 allan allan   67 10月 11 22:24 _1_1.liv</span><br><span class="line">-rw-rw-r-- 1 allan allan  405 10月 11 22:24 _1.cfe</span><br><span class="line">-rw-rw-r-- 1 allan allan 2.5K 10月 11 22:24 _1.cfs</span><br><span class="line">-rw-rw-r-- 1 allan allan  393 10月 11 22:24 _1.si</span><br><span class="line">-rw-rw-r-- 1 allan allan  361 10月 11 22:25 segments_3</span><br><span class="line">-rw-rw-r-- 1 allan allan    0 10月 11 22:19 write.lock</span><br><span class="line"></span><br><span class="line"># 查看segment信息</span><br><span class="line">GET _cat&#x2F;segments&#x2F;nyc-test?v</span><br><span class="line"></span><br><span class="line">index    shard prirep ip        segment generation docs.count docs.deleted  size size.memory committed searchable version compound</span><br><span class="line">nyc-test 0     p      10.8.4.42 _0               0          2            0 3.2kb        1184 true      true       7.4.0   true</span><br><span class="line">nyc-test 0     p      10.8.4.42 _1               1          1            2 3.2kb        1184 true      true       7.4.0   true</span><br></pre></td></tr></table></figure>
<p>触发Lucene commit之后，可以看到segments_2变成了segments_3。然后调用_cat接口查看索引的segment信息也能看到目前有2个segment，而且都已经commit过了，并且compound是true，表示是CFS格式存储的。当然Lucene的segment是可以合并的。我们通过ES的forcemerge接口进行合并，并且将所有segment合并成1个segment，forcemerge的时候会自动调用flush，即会触发Lucene commit：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">POST nyc-test&#x2F;_forcemerge?max_num_segments&#x3D;1</span><br><span class="line"></span><br><span class="line">-&gt; % ll</span><br><span class="line">总用量 60K</span><br><span class="line">-rw-rw-r-- 1 allan allan  69 10月 11 22:27 _2.dii</span><br><span class="line">-rw-rw-r-- 1 allan allan 123 10月 11 22:27 _2.dim</span><br><span class="line">-rw-rw-r-- 1 allan allan 142 10月 11 22:27 _2.fdt</span><br><span class="line">-rw-rw-r-- 1 allan allan  83 10月 11 22:27 _2.fdx</span><br><span class="line">-rw-rw-r-- 1 allan allan 945 10月 11 22:27 _2.fnm</span><br><span class="line">-rw-rw-r-- 1 allan allan 110 10月 11 22:27 _2_Lucene50_0.doc</span><br><span class="line">-rw-rw-r-- 1 allan allan  80 10月 11 22:27 _2_Lucene50_0.pos</span><br><span class="line">-rw-rw-r-- 1 allan allan 287 10月 11 22:27 _2_Lucene50_0.tim</span><br><span class="line">-rw-rw-r-- 1 allan allan 145 10月 11 22:27 _2_Lucene50_0.tip</span><br><span class="line">-rw-rw-r-- 1 allan allan 100 10月 11 22:27 _2_Lucene70_0.dvd</span><br><span class="line">-rw-rw-r-- 1 allan allan 469 10月 11 22:27 _2_Lucene70_0.dvm</span><br><span class="line">-rw-rw-r-- 1 allan allan  59 10月 11 22:27 _2.nvd</span><br><span class="line">-rw-rw-r-- 1 allan allan 100 10月 11 22:27 _2.nvm</span><br><span class="line">-rw-rw-r-- 1 allan allan 572 10月 11 22:27 _2.si</span><br><span class="line">-rw-rw-r-- 1 allan allan 296 10月 11 22:27 segments_4</span><br><span class="line">-rw-rw-r-- 1 allan allan   0 10月 11 22:19 write.lock</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">GET _cat&#x2F;segments&#x2F;nyc-test?v</span><br><span class="line"></span><br><span class="line">index    shard prirep ip        segment generation docs.count docs.deleted  size size.memory committed searchable version compound</span><br><span class="line">nyc-test 0     p      10.8.4.42 _2               2          3            0 3.2kb        1224 true      true       7.4.0   false</span><br></pre></td></tr></table></figure>
<p>可以看到，force merge之后只有一个segment了，并且使用了multifile格式存储，而不是compound。当然这并非Lucene的机制，而是ES自己的设计。<br>最后用图总结一下：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/8.png" alt="图片"></p>
<p>想了解更详细的，可以阅读：<a href="https://lucene.apache.org/core/8_2_0/core/org/apache/lucene/codecs/lucene80/package-summary.html#package.description" target="_blank" rel="noopener">https://lucene.apache.org/core/8_2_0/core/org/apache/lucene/codecs/lucene80/package-summary.html#package.description</a></p>
<h1 id="Query"><a href="#Query" class="headerlink" title="Query"></a>Query</h1><blockquote>
<p>参考：<br><a href="https://niyanchun.com/lucene-learning-8.html" target="_blank" rel="noopener">https://niyanchun.com/lucene-learning-8.html</a><br><a href="https://niyanchun.com/lucene-learning-9.html" target="_blank" rel="noopener">https://niyanchun.com/lucene-learning-9.html</a><br><a href="https://pythonhosted.org/lupyne/examples.html" target="_blank" rel="noopener">https://pythonhosted.org/lupyne/examples.html</a></p>
</blockquote>
<p>在Lucene中，Term是查询的基本单元(unit)，所有查询类的父类是org.apache.lucene.search.Query，本文会介绍下图中这些主要的Query子类：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/9.png" alt="图片"></p>
<p>DisjunctionMaxQuery主要用于控制评分机制，SpanQuery代表一类查询，有很多的实现。这两类查询不是非常常用。</p>
<h2 id="TermQuery"><a href="#TermQuery" class="headerlink" title="TermQuery"></a>TermQuery</h2><p>TermQuery是最基础最常用的的一个查询了，对应的类是org.apache.lucene.search.TermQuery。其功能很简单，就是查询哪些文档中包含指定的term。看下面代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line"> * Query Demo.</span><br><span class="line"> *</span><br><span class="line"> * @author NiYanchun</span><br><span class="line"> **&#x2F;</span><br><span class="line">public class QueryDemo &#123;</span><br><span class="line"></span><br><span class="line">    &#x2F;**</span><br><span class="line">     * 搜索的字段</span><br><span class="line">     *&#x2F;</span><br><span class="line">    private static final String SEARCH_FIELD &#x3D; &quot;contents&quot;;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        &#x2F;&#x2F; 索引保存目录</span><br><span class="line">        final String indexPath &#x3D; &quot;indices&#x2F;poems-index&quot;;</span><br><span class="line">        &#x2F;&#x2F; 读取索引</span><br><span class="line">        IndexReader indexReader &#x3D; DirectoryReader.open(FSDirectory.open(Paths.get(indexPath)));</span><br><span class="line">        IndexSearcher searcher &#x3D; new IndexSearcher(indexReader);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; TermQuery</span><br><span class="line">        termQueryDemo(searcher);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private static void termQueryDemo(IndexSearcher searcher) throws IOException &#123;</span><br><span class="line">        System.out.println(&quot;TermQuery, search for &#39;death&#39;:&quot;);</span><br><span class="line">        TermQuery termQuery &#x3D; new TermQuery(new Term(SEARCH_FIELD, &quot;death&quot;));</span><br><span class="line"></span><br><span class="line">        resultPrint(searcher, termQuery);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private static void resultPrint(IndexSearcher searcher, Query query) throws IOException &#123;</span><br><span class="line">        TopDocs topDocs &#x3D; searcher.search(query, 10);</span><br><span class="line">        if (topDocs.totalHits.value &#x3D;&#x3D; 0) &#123;</span><br><span class="line">            System.out.println(&quot;not found!\n&quot;);</span><br><span class="line">            return;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        ScoreDoc[] hits &#x3D; topDocs.scoreDocs;</span><br><span class="line"></span><br><span class="line">        System.out.println(topDocs.totalHits.value + &quot; result(s) matched: &quot;);</span><br><span class="line">        for (ScoreDoc hit : hits) &#123;</span><br><span class="line">            Document doc &#x3D; searcher.doc(hit.doc);</span><br><span class="line">            System.out.println(&quot;doc&#x3D;&quot; + hit.doc + &quot; score&#x3D;&quot; + hit.score + &quot; file: &quot; + doc.get(&quot;path&quot;));</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面代码先读取索引文件，然后执行了一个term查询，查询所有包含death关键词的文档。为了方便打印，我们封装了一个resultPrint函数用于打印查询结果。<em>On Death</em>一诗包含了<em>death</em>关键字，所以程序执行结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">TermQuery, search for &#39;death&#39;:</span><br><span class="line">1 result(s) matched: </span><br><span class="line">doc&#x3D;3 score&#x3D;0.6199532 file: data&#x2F;poems&#x2F;OnDeath.txt</span><br></pre></td></tr></table></figure>
<h2 id="BooleanQuery"><a href="#BooleanQuery" class="headerlink" title="BooleanQuery"></a>BooleanQuery</h2><p>BooleanQuery用于将若干个查询按照与或的逻辑关系组织起来，支持嵌套。目前支持4个逻辑关系：</p>
<ul>
<li><strong><em>SHOULD</em></strong>：逻辑<strong>或</strong>的关系，文档满足任意一个查询即视为匹配。</li>
<li><strong><em>MUST</em></strong>：逻辑<strong>与</strong>的关系，文档必须满足所有查询才视为匹配。</li>
<li><strong><em>FILTER</em></strong>：逻辑<strong>与</strong>的关系，与must的区别是不计算score，所以性能会比must好。如果只关注是否匹配，而不关注匹配程度（即得分），应该优先使用filter。</li>
<li><strong><em>MUST NOT</em></strong>：逻辑与的关系，且取反。文档不满足所有查询的条件才视为匹配。</li>
</ul>
<p>使用方式也比较简单，以下的代码使用BooleanQuery查询contents字段包含<em>love</em>但不包含<em>seek</em>的词：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">private static void booleanQueryDemo(IndexSearcher searcher) throws IOException &#123;</span><br><span class="line">    System.out.println(&quot;BooleanQuery, must contain &#39;love&#39; but absolutely not &#39;seek&#39;: &quot;);</span><br><span class="line">    BooleanQuery.Builder builder &#x3D; new BooleanQuery.Builder();</span><br><span class="line">    builder.add(new TermQuery(new Term(SEARCH_FIELD, &quot;love&quot;)), BooleanClause.Occur.MUST);</span><br><span class="line">    builder.add(new TermQuery(new Term(SEARCH_FIELD, &quot;seek&quot;)), BooleanClause.Occur.MUST_NOT);</span><br><span class="line">    BooleanQuery booleanQuery &#x3D; builder.build();</span><br><span class="line"></span><br><span class="line">    resultPrint(searcher, booleanQuery);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><em>Love’s Secret</em>和<em>Freedom and Love</em>两首诗中均包含了<em>love</em>一词，但前者还包含了<em>seek</em>一词，所以最终的搜索结果为<em>Freedom and Love</em>。</p>
<h2 id="PhraseQuery"><a href="#PhraseQuery" class="headerlink" title="PhraseQuery"></a>PhraseQuery</h2><p>PhraseQuery用于搜索term序列，比如搜索“hello world”这个由两个term组成的一个序列。对于Phrase类的查询需要掌握两个点：</p>
<ul>
<li>Phrase查询需要term的position信息，所以如果indexing阶段没有保存position信息，就无法使用phrase类的查询。</li>
<li>理解slop的概念：Slop就是两个term或者两个term序列的edit distance。后面的FuzzyQuery也用到了该概念，这里简单介绍一下。</li>
</ul>
<p>PhraseQuery使用的是Levenshtein distance，且默认的slop值是0，也就是只检索完全匹配的term序列。看下面这个例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">private static void phraseQueryDemo(IndexSearcher searcher) throws IOException &#123;</span><br><span class="line">    System.out.println(&quot;\nPhraseQuery, search &#39;love that&#39;&quot;);</span><br><span class="line"></span><br><span class="line">    PhraseQuery.Builder builder &#x3D; new PhraseQuery.Builder();</span><br><span class="line">    builder.add(new Term(SEARCH_FIELD, &quot;love&quot;));</span><br><span class="line">    builder.add(new Term(SEARCH_FIELD, &quot;that&quot;));</span><br><span class="line">    PhraseQuery phraseQueryWithSlop &#x3D; builder.build();</span><br><span class="line"></span><br><span class="line">    resultPrint(searcher, phraseQueryWithSlop);</span><br><span class="line">&#125;</span><br><span class="line">&#x2F;&#x2F; 运行结果</span><br><span class="line">PhraseQuery, search &#39;love that&#39;</span><br><span class="line">1 result(s) matched: </span><br><span class="line">doc&#x3D;2 score&#x3D;0.7089927 file: data&#x2F;poems&#x2F;Love&#39;sSecret.txt</span><br></pre></td></tr></table></figure>
<p><em>Love‘s Secret</em>里面有这么一句：”<em>Love that never told shall be</em>“，是能够匹配”<em>love that</em>“的。我们也可以修改slop的值，使得与搜索序列的edit distance小于等于slop的文档都可以被检索到，同时距离越小的文档评分越高。看下面例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">private static void phraseQueryWithSlopDemo(IndexSearcher searcher) throws IOException &#123;</span><br><span class="line">    System.out.println(&quot;PhraseQuery with slop: &#39;love &lt;slop&gt; never&quot;);</span><br><span class="line">    PhraseQuery phraseQueryWithSlop &#x3D; new PhraseQuery(1, SEARCH_FIELD, &quot;love&quot;, &quot;never&quot;);</span><br><span class="line"></span><br><span class="line">    resultPrint(searcher, phraseQueryWithSlop);</span><br><span class="line">&#125;</span><br><span class="line">&#x2F;&#x2F; 运行结果</span><br><span class="line">PhraseQuery with slop: &#39;love &lt;slop&gt; never</span><br><span class="line">1 result(s) matched: </span><br><span class="line">doc&#x3D;2 score&#x3D;0.43595996 file: data&#x2F;poems&#x2F;Love&#39;sSecret.txt</span><br></pre></td></tr></table></figure>
<h2 id="MultiPhraseQuery"><a href="#MultiPhraseQuery" class="headerlink" title="MultiPhraseQuery"></a>MultiPhraseQuery</h2><p>不论是官方文档或是网上的资料，对于MultiPhraseQuery讲解的都比较少。但其实它的功能很简单，举个例子就明白了：我们提供两个由term组成的数组：[“love”, “hate”], [“him”, “her”]，然后把这两个数组传给MultiPhraseQuery，它就会去检索 “love him”, “love her”, “hate him”, “hate her”的组合，每一个组合其实就是一个上面介绍的PhraseQuery。当然MultiPhraseQuery也可以接受更高维的组合。</p>
<p>由上面的例子可以看到PhraseQuery其实是MultiPhraseQuery的一种特殊形式而已，如果给MultiPhraseQuery传递的每个数组里面只有一个term，那就退化成PhraseQuery了。在MultiPhraseQuery中，一个数组内的元素匹配时是 <strong>或(OR)</strong> 的关系，也就是这些term共享同一个position。 还记得之前的文章中我们说过在同一个position放多个term，可以实现同义词的搜索。的确MultiPhraseQuery实际中主要用于同义词的查询。比如查询一个“我爱土豆”，那可以构造这样两个数组传递给MultiPhraseQuery查询：[“喜欢”，“爱”], [“土豆”，”马铃薯”，”洋芋”]，这样查出来的结果就会更全面一些。最后来个例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">private static void multiPhraseQueryDemo(IndexSearcher searcher) throws IOException &#123;</span><br><span class="line">    System.out.println(&quot;MultiPhraseQuery:&quot;);</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; On Death 一诗中有这样一句: I know not what into my ear</span><br><span class="line">    &#x2F;&#x2F; Fog 一诗中有这样一句: It sits looking over harbor and city</span><br><span class="line">    &#x2F;&#x2F; 以下的查询可以匹配 &quot;know harbor, know not, over harbor, over not&quot; 4种情况</span><br><span class="line">    MultiPhraseQuery.Builder builder &#x3D; new MultiPhraseQuery.Builder();</span><br><span class="line">    Term[] termArray1 &#x3D; new Term[2];</span><br><span class="line">    termArray1[0] &#x3D; new Term(SEARCH_FIELD, &quot;know&quot;);</span><br><span class="line">    termArray1[1] &#x3D; new Term(SEARCH_FIELD, &quot;over&quot;);</span><br><span class="line">    Term[] termArray2 &#x3D; new Term[2];</span><br><span class="line">    termArray2[0] &#x3D; new Term(SEARCH_FIELD, &quot;harbor&quot;);</span><br><span class="line">    termArray2[1] &#x3D; new Term(SEARCH_FIELD, &quot;not&quot;);</span><br><span class="line">    builder.add(termArray1);</span><br><span class="line">    builder.add(termArray2);</span><br><span class="line">    MultiPhraseQuery multiPhraseQuery &#x3D; builder.build();</span><br><span class="line"></span><br><span class="line">    resultPrint(searcher, multiPhraseQuery);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 程序输出</span><br><span class="line">MultiPhraseQuery:</span><br><span class="line">2 result(s) matched: </span><br><span class="line">doc&#x3D;0 score&#x3D;2.7032354 file: data&#x2F;poems&#x2F;Fog.txt</span><br><span class="line">doc&#x3D;3 score&#x3D;2.4798129 file: data&#x2F;poems&#x2F;OnDeath.txt</span><br></pre></td></tr></table></figure>
<h2 id="PrefixQuery、WildcardQuery、RegexpQuery"><a href="#PrefixQuery、WildcardQuery、RegexpQuery" class="headerlink" title="PrefixQuery、WildcardQuery、RegexpQuery"></a>PrefixQuery、WildcardQuery、RegexpQuery</h2><p>这三个查询提供模糊模糊查询的功能：</p>
<ul>
<li>PrefixQuery只支持指定前缀模糊查询，用户指定一个前缀，查询时会匹配所有该前缀开头的term。</li>
<li>WildcardQuery比PrefixQuery更进一步，支持 <strong>*</strong>（匹配0个或多个字符）和 <strong>?</strong>（匹配一个字符） 两个通配符。从效果上看，PrefixQuery是WildcardQuery的一种特殊情况，但其底层不是基于WildcardQuery，而是另外一种单独的实现。</li>
<li>RegexpQuery是比WildcardQuery更宽泛的查询，它支持正则表达式。支持的正则语法范围见org.apache.lucene.util.automaton.RegExp类。</li>
</ul>
<p>需要注意，WildcardQuery和RegexpQuery的性能会差一些，因为它们需要遍历很多文档。特别是极力不推荐以模糊匹配开头。当然这里的差是相对其它查询来说的，我粗略测试过，2台16C+32G的ES，比较简短的文档，千万级以下的查询也能毫秒级返回。最后看几个使用的例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">private static void prefixQueryDemo(IndexSearcher searcher) throws IOException &#123;</span><br><span class="line">    System.out.println(&quot;PrefixQuery, search terms begin with &#39;co&#39;&quot;);</span><br><span class="line">    PrefixQuery prefixQuery &#x3D; new PrefixQuery(new Term(SEARCH_FIELD, &quot;co&quot;));</span><br><span class="line"></span><br><span class="line">    resultPrint(searcher, prefixQuery);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private static void wildcardQueryDemo(IndexSearcher searcher) throws IOException &#123;</span><br><span class="line">    System.out.println(&quot;WildcardQuery, search terms &#39;har*&#39;&quot;);</span><br><span class="line">    WildcardQuery wildcardQuery &#x3D; new WildcardQuery(new Term(SEARCH_FIELD, &quot;har*&quot;));</span><br><span class="line"></span><br><span class="line">    resultPrint(searcher, wildcardQuery);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private static void regexpQueryDemo(IndexSearcher searcher) throws IOException &#123;</span><br><span class="line">    System.out.println(&quot;RegexpQuery, search regexp &#39;l[ao]*&#39;&quot;);</span><br><span class="line">    RegexpQuery regexpQuery &#x3D; new RegexpQuery(new Term(SEARCH_FIELD, &quot;l[ai].*&quot;));</span><br><span class="line"></span><br><span class="line">    resultPrint(searcher, regexpQuery);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 程序输出</span><br><span class="line">PrefixQuery, search terms begin with &#39;co&#39;</span><br><span class="line">2 result(s) matched: </span><br><span class="line">doc&#x3D;0 score&#x3D;1.0 file: data&#x2F;poems&#x2F;Fog.txt</span><br><span class="line">doc&#x3D;2 score&#x3D;1.0 file: data&#x2F;poems&#x2F;Love&#39;sSecret.txt</span><br><span class="line"></span><br><span class="line">WildcardQuery, search terms &#39;har*&#39;</span><br><span class="line">1 result(s) matched: </span><br><span class="line">doc&#x3D;0 score&#x3D;1.0 file: data&#x2F;poems&#x2F;Fog.txt</span><br><span class="line"></span><br><span class="line">RegexpQuery, search regexp &#39;l[ao]*&#39;</span><br><span class="line">2 result(s) matched: </span><br><span class="line">doc&#x3D;0 score&#x3D;1.0 file: data&#x2F;poems&#x2F;Fog.txt</span><br><span class="line">doc&#x3D;3 score&#x3D;1.0 file: data&#x2F;poems&#x2F;OnDeath.txt</span><br></pre></td></tr></table></figure>
<h2 id="FuzzyQuery"><a href="#FuzzyQuery" class="headerlink" title="FuzzyQuery"></a>FuzzyQuery</h2><p>FuzzyQuery和PhraseQuery一样，都是基于上面介绍的edit distance做匹配的，差异是在PhraseQuery中搜索词的是一个term序列，此时edit distance中定义的一个symbol就是一个词；而FuzzyQuery的搜索词就是一个term，所以它对应的edit distance中的symbol就是一个字符了。另外使用时还有几个注意点：</p>
<ul>
<li>PhraseQuery采用Levenshtein distance计算edit distance，即相邻symbol交换是2个slop，而FuzzyQuery默认使用Damerau–Levenshtein distance，所以相邻symbol交换是1个slop，但支持用户使用Levenshtein distance。</li>
<li>FuzzyQuery限制最大允许的edit distance为2（LevenshteinAutomata.MAXIMUM_SUPPORTED_DISTANCE值限定），因为对于更大的edit distance会匹配出特别多的词，但FuzzyQuery的定位是解决诸如美式英语和英式英语在拼写上的细微差异。</li>
<li>FuzzyQuery匹配的时候还有个要求就是搜索的term和待匹配的term的edit distance必须小于它们二者长度的最小值。比如搜索词为”abcd”，设定允许的maxEdits（允许的最大edit distance）为2，那么按照edit distance的计算方式”ab”这个词是匹配的，因为它们的距离是2，不大于设定的maxEdits。但是，由于 2 &lt; min( len(“abcd”), len(“ab”) ) = 2不成立，所以算不匹配。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">private static void fuzzyQueryDemo(IndexSearcher searcher) throws IOException &#123;</span><br><span class="line">    System.out.println(&quot;FuzzyQuery, search &#39;remembre&#39;&quot;);</span><br><span class="line">    &#x2F;&#x2F; 这里把remember拼成了remembre</span><br><span class="line">    FuzzyQuery fuzzyQuery &#x3D; new FuzzyQuery(new Term(SEARCH_FIELD, &quot;remembre&quot;), 1);</span><br><span class="line"></span><br><span class="line">    resultPrint(searcher, fuzzyQuery);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 程序输出</span><br><span class="line">FuzzyQuery, search &#39;remembre&#39;</span><br><span class="line">1 result(s) matched: </span><br><span class="line">doc&#x3D;1 score&#x3D;0.4473783 file: data&#x2F;poems&#x2F;FreedomAndLove.txt</span><br></pre></td></tr></table></figure>
<h2 id="PointRangeQuery"><a href="#PointRangeQuery" class="headerlink" title="PointRangeQuery"></a>PointRangeQuery</h2><p>前面介绍Field的时候，我们介绍过几种常用的数值型Field：IntPoint、LongPoint、FloatPoint、DoublePoint。PointRangeQuery就是给数值型数据提供范围查询的一个Query，功能和原理都很简单，我们直接看一个完整的例子吧：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line"> * Point Query Demo.</span><br><span class="line"> *</span><br><span class="line"> * @author NiYanchun</span><br><span class="line"> **&#x2F;</span><br><span class="line">public class PointQueryDemo &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        &#x2F;&#x2F; 索引保存目录</span><br><span class="line">        final String indexPath &#x3D; &quot;indices&#x2F;point-index&quot;;</span><br><span class="line">        Directory indexDir &#x3D; FSDirectory.open(Paths.get(indexPath));</span><br><span class="line">        IndexWriterConfig iwc &#x3D; new IndexWriterConfig(new StandardAnalyzer());</span><br><span class="line">        iwc.setOpenMode(IndexWriterConfig.OpenMode.CREATE);</span><br><span class="line">        IndexWriter writer &#x3D; new IndexWriter(indexDir, iwc);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 向索引中插入10条document，每个document包含一个field字段，字段值是0~10之间的数字</span><br><span class="line">        for (int i &#x3D; 0; i &lt; 10; i++) &#123;</span><br><span class="line">            Document doc &#x3D; new Document();</span><br><span class="line">            Field pointField &#x3D; new IntPoint(&quot;field&quot;, i);</span><br><span class="line">            doc.add(pointField);</span><br><span class="line">            writer.addDocument(doc);</span><br><span class="line">        &#125;</span><br><span class="line">        writer.close();</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 查询</span><br><span class="line">        IndexReader indexReader &#x3D; DirectoryReader.open(FSDirectory.open(Paths.get(indexPath)));</span><br><span class="line">        IndexSearcher searcher &#x3D; new IndexSearcher(indexReader);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 查询field字段值在[5, 8]范围内的文档</span><br><span class="line">        Query query &#x3D; IntPoint.newRangeQuery(&quot;field&quot;, 5, 8);</span><br><span class="line">        TopDocs topDocs &#x3D; searcher.search(query, 10);</span><br><span class="line"></span><br><span class="line">        if (topDocs.totalHits.value &#x3D;&#x3D; 0) &#123;</span><br><span class="line">            System.out.println(&quot;not found!&quot;);</span><br><span class="line">            return;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        ScoreDoc[] hits &#x3D; topDocs.scoreDocs;</span><br><span class="line"></span><br><span class="line">        System.out.println(topDocs.totalHits.value + &quot; result(s) matched: &quot;);</span><br><span class="line">        for (ScoreDoc hit : hits) &#123;</span><br><span class="line">            System.out.println(&quot;doc&#x3D;&quot; + hit.doc + &quot; score&#x3D;&quot; + hit.score);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 程序输出</span><br><span class="line">4 result(s) matched: </span><br><span class="line">doc&#x3D;5 score&#x3D;1.0</span><br><span class="line">doc&#x3D;6 score&#x3D;1.0</span><br><span class="line">doc&#x3D;7 score&#x3D;1.0</span><br><span class="line">doc&#x3D;8 score&#x3D;1.0</span><br></pre></td></tr></table></figure>
<h2 id="TermRangeQuery"><a href="#TermRangeQuery" class="headerlink" title="TermRangeQuery"></a>TermRangeQuery</h2><p>TermRangeQuery和PointRangeQuery功能类似，不过它比较的是字符串，而非数值。比较基于org.apache.lucene.util.BytesRef.compareTo(BytesRef other)方法。直接看例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">private static void termRangeQueryDemo(IndexSearcher searcher) throws IOException &#123;</span><br><span class="line">    System.out.println(&quot;TermRangeQuery, search term between &#39;loa&#39; and &#39;lov&#39;&quot;);</span><br><span class="line">    &#x2F;&#x2F; 后面的true和false分别表示 loa &lt;&#x3D; 待匹配的term &lt; lov</span><br><span class="line">    TermRangeQuery termRangeQuery &#x3D; new TermRangeQuery(SEARCH_FIELD, new BytesRef(&quot;loa&quot;), new BytesRef(&quot;lov&quot;), true, false);</span><br><span class="line"></span><br><span class="line">    resultPrint(searcher, termRangeQuery);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 程序输出</span><br><span class="line">TermRangeQuery, search term between &#39;loa&#39; and &#39;lov&#39;</span><br><span class="line">1 result(s) matched: </span><br><span class="line">doc&#x3D;0 score&#x3D;1.0 file: data&#x2F;poems&#x2F;Fog.txt    &#x2F;&#x2F; Fog中的term &#39;looking&#39; 符合搜索条件</span><br></pre></td></tr></table></figure>
<h2 id="ConstantScoreQuery"><a href="#ConstantScoreQuery" class="headerlink" title="ConstantScoreQuery"></a>ConstantScoreQuery</h2><p>ConstantScoreQuery很简单，它的功能是将其它查询包装起来，并将它们查询结果中的评分改为一个常量值（默认为1.0）。上面FuzzyQuery一节里面最后举得例子中返回的查询结果score=0.4473783，现在我们用ConstantScoreQuery包装一下看下效果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">private static void constantScoreQueryDemo(IndexSearcher searcher) throws IOException &#123;</span><br><span class="line">    System.out.println(&quot;ConstantScoreQuery:&quot;);</span><br><span class="line">    ConstantScoreQuery constantScoreQuery &#x3D; new ConstantScoreQuery(</span><br><span class="line">            new FuzzyQuery(new Term(SEARCH_FIELD, &quot;remembre&quot;), 1));</span><br><span class="line"></span><br><span class="line">    resultPrint(searcher, constantScoreQuery);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 运行结果</span><br><span class="line">ConstantScoreQuery:</span><br><span class="line">1 result(s) matched: </span><br><span class="line">doc&#x3D;1 score&#x3D;1.0 file: data&#x2F;poems&#x2F;FreedomAndLove.txt</span><br></pre></td></tr></table></figure>
<p>另外有个知识点需要注意：ConstantScoreQuery嵌套Filter和BooleanQuery嵌套Filter的查询结果不考虑评分的话是一样的，但前面在BooleanQuery中介绍过Filter，其功能与MUST相同，但不计算评分；而ConstantScoreQuery就是用来设置一个评分的。所以两者的查询结果是一样的，但ConstantScoreQuery嵌套Filter返回结果是附带评分的，而BooleanQuery嵌套Filter的返回结果是没有评分的（score字段的值为0）。</p>
<h2 id="MatchAllDocsQuery"><a href="#MatchAllDocsQuery" class="headerlink" title="MatchAllDocsQuery"></a>MatchAllDocsQuery</h2><p>这个查询很简单，就是匹配所有文档，用于没有特定查询条件，只想预览部分数据的场景。直接看例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">private static void matchAllDocsQueryDemo(IndexSearcher searcher) throws IOException &#123;</span><br><span class="line">    System.out.println(&quot;MatchAllDocsQueryDemo:&quot;);</span><br><span class="line">    MatchAllDocsQuery matchAllDocsQuery &#x3D; new MatchAllDocsQuery();</span><br><span class="line"></span><br><span class="line">    resultPrint(searcher, matchAllDocsQuery);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 程序输出</span><br><span class="line">MatchAllDocsQueryDemo:</span><br><span class="line">4 result(s) matched: </span><br><span class="line">doc&#x3D;0 score&#x3D;1.0 file: data&#x2F;poems&#x2F;Fog.txt</span><br><span class="line">doc&#x3D;1 score&#x3D;1.0 file: data&#x2F;poems&#x2F;FreedomAndLove.txt</span><br><span class="line">doc&#x3D;2 score&#x3D;1.0 file: data&#x2F;poems&#x2F;Love&#39;sSecret.txt</span><br><span class="line">doc&#x3D;3 score&#x3D;1.0 file: data&#x2F;poems&#x2F;OnDeath.txt</span><br></pre></td></tr></table></figure>
<p>想看更多资料，可参考：<a href="https://lucene.apache.org/core/8_2_0/core/org/apache/lucene/search/package-summary.html" target="_blank" rel="noopener">https://lucene.apache.org/core/8_2_0/core/org/apache/lucene/search/package-summary.html</a></p>
<h1 id="QueryParser"><a href="#QueryParser" class="headerlink" title="QueryParser"></a>QueryParser</h1><p>QueryParser定义了一些查询语法，通过这些语法几乎可以实现前文介绍的所有Query API提供的功能，但它的存在并不是为了替换那些API，而是用在一些交互式场景中。比如下面代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">public class SearchFilesMinimal &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        &#x2F;&#x2F; 索引保存目录</span><br><span class="line">        final String indexPath &#x3D; &quot;&#x2F;Users&#x2F;allan&#x2F;Git&#x2F;allan&#x2F;github&#x2F;CodeSnippet&#x2F;Java&#x2F;lucene-learning&#x2F;indices&#x2F;poems-index&quot;;</span><br><span class="line">        &#x2F;&#x2F; 搜索的字段</span><br><span class="line">        final String searchField &#x3D; &quot;contents&quot;;</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 从索引目录读取索引信息</span><br><span class="line">        IndexReader indexReader &#x3D; DirectoryReader.open(FSDirectory.open(Paths.get(indexPath)));</span><br><span class="line">        &#x2F;&#x2F; 创建索引查询对象</span><br><span class="line">        IndexSearcher searcher &#x3D; new IndexSearcher(indexReader);</span><br><span class="line">        &#x2F;&#x2F; 使用标准分词器</span><br><span class="line">        Analyzer analyzer &#x3D; new StandardAnalyzer();</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 从终端获取查询语句</span><br><span class="line">        BufferedReader in &#x3D; new BufferedReader(new InputStreamReader(System.in));</span><br><span class="line">        &#x2F;&#x2F; 创建查询语句解析对象</span><br><span class="line">        QueryParser queryParser &#x3D; new QueryParser(searchField, analyzer);</span><br><span class="line">        while (true) &#123;</span><br><span class="line">            System.out.println(&quot;Enter query: &quot;);</span><br><span class="line"></span><br><span class="line">            String input &#x3D; in.readLine();</span><br><span class="line">            if (input &#x3D;&#x3D; null) &#123;</span><br><span class="line">                break;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            input &#x3D; input.trim();</span><br><span class="line">            if (input.length() &#x3D;&#x3D; 0) &#123;</span><br><span class="line">                break;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F; 解析用户输入的查询语句：build query</span><br><span class="line">            Query query &#x3D; queryParser.parse(input);</span><br><span class="line">            System.out.println(&quot;searching for: &quot; + query.toString(searchField));</span><br><span class="line">            &#x2F;&#x2F; 查询</span><br><span class="line">            TopDocs results &#x3D; searcher.search(query, 10);</span><br><span class="line">            &#x2F;&#x2F; 省略后面查询结果打印的代码</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在这段代码中，先读取了已经创建好的索引文件，然后创建了一个QueryParser实例(queryParser)。接着不断读取用户输入(input)，并传给QueryParser的parse方法，该方法通过用户的输入构建一个Query对象用于查询。<br>QueryParser的构造函数为QueryParser(String f, Analyzer a)，第1个参数指定一个默认的查询字段，如果后面输入的<em>input</em>里面没有指定查询字段，则默认查询该该字段，比如输入hello表示在默认字段中查询”<em>hello</em>“，而content: hello则表示在<em>content</em>字段中查询”<em>hello</em>“。第2个参数指定一个分析器，一般该分析器应该选择和索引阶段同样的Analyzer。</p>
<p><strong>另外有两个点需要特别注意：</strong></p>
<ul>
<li><strong>QueryParser默认使用TermQuery进行多个Term的OR关系查询</strong>（后文布尔查询那里会再介绍）。比如输入hello world，表示先将hello world分词（一般会分为hello和world两个词），然后使用TermQuery查询。如果需要全词匹配（即使用PhraseQuery），则需要将搜索词用<strong>双引号</strong>引起来，比如”hello world”。</li>
<li><strong>指定搜索字段时，该字段仅对紧随其后的第一个词或第一个用双引号引起来的串有效</strong>。比如title:hello world这个输入，<em>title</em>仅对<em>hello</em>有效，即搜索时只会在<em>title</em>字段中搜索<em>hello</em>，然后在默认搜索字段中搜索<em>world</em>。如果想要在一个字段中搜索多个词或多个用双引号引起来的词组时，将这些词用小括号括起来即可，比如title:(hello world)。</li>
</ul>
<h3 id="Wildcard搜索"><a href="#Wildcard搜索" class="headerlink" title="Wildcard搜索"></a>Wildcard搜索</h3><p>通配符搜索和WildcardQuery API一样，仅支持?和<em>两个通配符，前者用于匹配1个字符，后者匹配0到多个字符。输入title:te?t，则可以匹配到</em>title<em>中的”</em>test<em>“、”</em>text*”等词。</p>
<p><strong>注意：使用QueryParser中的wildcard搜索时，不允许以?和*开头，否则会抛异常，但直接使用WildcardQuery API时，允许以通配符开头，只是因为性能原因，不推荐使用。</strong>这样设计的原因我猜是因为QueryParser的输入是面向用户的，用户对于通配符开头造成的后果并不清楚，所以直接禁掉；而WildcardQuery是给开发者使用的，开发者在开发阶段很清楚如果允许这样做造成的后果是否可以接受，如果不能接受，也是可以通过接口禁掉开头就是用通配符的情况。</p>
<h3 id="Regexp搜索"><a href="#Regexp搜索" class="headerlink" title="Regexp搜索"></a>Regexp搜索</h3><p>正则搜索和RegexpQuery一样，不同之处在于QueryParser中输入的正则表达式需要使用<strong>两个斜线</strong>(“/“)包围起来，比如匹配”moat”或”boat”的正则为/[mb]oat/。</p>
<h3 id="Fuzzy搜索"><a href="#Fuzzy搜索" class="headerlink" title="Fuzzy搜索"></a>Fuzzy搜索</h3><p>在QueryParser中，通过在搜索词后面加<strong>波浪字符</strong>来实现FuzzyQuery，比如love~，默认edit distance是2，可以在波浪符后面加具体的整数值来修改默认值，合法的值为0、1、2.</p>
<h3 id="Phrase-slop搜索"><a href="#Phrase-slop搜索" class="headerlink" title="Phrase slop搜索"></a>Phrase slop搜索</h3><p>PhraseQuery中可以指定slop（默认值为0，精确匹配）来实现相似性搜索，QueryParser中同样可以，使用方法与Fuzzy类似——<strong>将搜索字符串用双引号引起来，然后在末尾加上波浪符</strong>，比如”jakarta apache”~10。这里对数edit distance没有限制，合法值为非负数，默认值为0.</p>
<h3 id="Range搜索"><a href="#Range搜索" class="headerlink" title="Range搜索"></a>Range搜索</h3><p>QueryParser的范围搜索同时支持TermRangeQuery和数值型的范围搜索，排序使用的是<strong>字典序</strong>。<strong>开区间使用大括号，闭区间使用方括号</strong>。比如搜索修改日期介于2019年9月份和10月份的文档：mod_date:[20190901 TO 20191031]，再比如搜索标题字段中包含<em>hate</em>到<em>love</em>的词（但不包含这两个词）的文档：title:{hate TO love}.</p>
<h3 id="提升权重-boost"><a href="#提升权重-boost" class="headerlink" title="提升权重(boost)"></a>提升权重(boost)</h3><p>查询时可以通过给搜索的关键字或双引号引起来的搜索串后面添加脱字符(^)及一个正数来提升其计算相关性时的权重（默认为1），比如love^5 China或”love China”^0.3。</p>
<h3 id="Boolean操作符"><a href="#Boolean操作符" class="headerlink" title="Boolean操作符"></a>Boolean操作符</h3><p>QueryParser中提供了5种布尔操作符：AND、+、OR、NOT、-，所有的<strong>操作符必须大写</strong>。</p>
<ul>
<li><strong>OR是默认的操作符</strong>，表示满足任意一个term即可。比如搜索love China，<em>love</em>和<em>China</em>之间就是OR的关系，检索时文档匹配任意一个词即视为匹配。OR也可以使用可用||代替。</li>
<li>AND表示必须满足<strong>所有term</strong>才可以，可以使用&amp;&amp;代替。</li>
<li>+用在term之前，表示该term必须存在。比如+love China表示匹配文档中必须包含<em>love</em>，<em>China</em>则可包含也可不含。</li>
<li>-用在term之前，表示该term必须不存在。比如-“hate China” “love China”表示匹配文档中包含”<em>love China</em>“，但不包含”<em>hate China</em>“的词。</li>
</ul>
<h3 id="分组"><a href="#分组" class="headerlink" title="分组"></a>分组</h3><p>前面已经介绍过，可以使用<strong>小括号</strong>进行分组，通过分组可以表达一些复杂的逻辑。举两个例子：</p>
<ul>
<li>(jakarta OR apache) AND website表示匹配文档中必须包含<em>webiste</em>，同时需要至少包含<em>jakarta</em>或<em>apache</em>二者之一。</li>
<li>title:(+return +”pink panther”)表示匹配文档中的title字段中必须同时存在<em>return</em>和<em>“pink panther”</em>串。</li>
</ul>
<h3 id="特殊字符"><a href="#特殊字符" class="headerlink" title="特殊字符"></a>特殊字符</h3><p>从前面的介绍可知，有很多符号在QueryParser中具有特殊含义，目前所有的特殊符号包括：+- &amp;&amp; || ! ( ) { } [ ] ^ “ ~ <em> ? :  /。如果搜索关键字中存在这些特殊符号，则需要使用反斜线()转义。比如搜索(1+1)</em>2则必须写为(1+1)*2。</p>
<p>相比于Lucene的其它搜索API，QueryParser提供了一种方式，让普通用户可以不需要写代码，只是掌握一些语法就可以进行复杂的搜索，在一些交互式检索场景中，还是非常方便的。</p>
<h3 id="基本使用-2"><a href="#基本使用-2" class="headerlink" title="基本使用"></a>基本使用</h3><p>下面展示一个使用pylucene构建一个基于term和关键词的Query，这里把keyword命中进行了加权，使用分数和长度进行排序，并将结果写进result中的过程：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">boolean_query &#x3D; BooleanQuery.Builder()</span><br><span class="line">simple_query &#x3D; QueryParser(</span><br><span class="line">    &quot;en_tokenized&quot;,</span><br><span class="line">    self.lucene_analyzer).parse(query)</span><br><span class="line">keyword_query &#x3D; QueryParser(</span><br><span class="line">    &quot;keyword&quot;,</span><br><span class="line">    self.lucene_analyzer).parse(query)</span><br><span class="line">boost_keyword_query &#x3D; BoostQuery(keyword_query, 2.0)</span><br><span class="line">boolean_query.add(simple_query, BooleanClause.Occur.SHOULD)</span><br><span class="line">boolean_query.add(boost_keyword_query, BooleanClause.Occur.SHOULD)</span><br><span class="line"># searcher</span><br><span class="line">lucene_searcher &#x3D; IndexSearcher(</span><br><span class="line">    DirectoryReader.open(self.indir))</span><br><span class="line">sorter &#x3D; Sort([</span><br><span class="line">    SortField.FIELD_SCORE,</span><br><span class="line">    SortField(&#39;origin_score&#39;, SortField.Type.FLOAT, True),</span><br><span class="line">    SortField(&#39;en_sent_lenth&#39;, SortField.Type.INT, True)])</span><br><span class="line"></span><br><span class="line"># rerank</span><br><span class="line">collector &#x3D; TopFieldCollector.create(sorter, self.maxrecal, self.maxrecal)</span><br><span class="line">lucene_searcher.search(boolean_query.build(), collector)</span><br><span class="line">scoreDocs &#x3D; collector.topDocs().scoreDocs</span><br><span class="line">result &#x3D; []</span><br><span class="line">for hit in scoreDocs:</span><br><span class="line">    doc &#x3D; lucene_searcher.doc(hit.doc)</span><br><span class="line">    result.append(...)</span><br></pre></td></tr></table></figure>
<h1 id="相似度评分机制"><a href="#相似度评分机制" class="headerlink" title="相似度评分机制"></a>相似度评分机制</h1><h2 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h2><h3 id="Bad-of-Words模型"><a href="#Bad-of-Words模型" class="headerlink" title="Bad-of-Words模型"></a>Bad-of-Words模型</h3><p>先介绍一下NLP和IR领域里面非常简单且使用极其广泛的bag-fo-words model，即词袋模型。假设有这么一句话：<em>“John likes to watch movies. Mary likes movies too.”</em>。那这句话用JSON格式的词袋模型表示的话就是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">BoW &#x3D; &#123;&quot;John&quot;:1,&quot;likes&quot;:2,&quot;to&quot;:1,&quot;watch&quot;:1,&quot;movies&quot;:2,&quot;Mary&quot;:1,&quot;too&quot;:1&#125;;</span><br></pre></td></tr></table></figure>
<p>可以看到，词袋模型关注的是词的出现次数，而没有记录词的位置信息。所以不同的语句甚至相反含义的语句其词袋可能是一样的，比如<em>“Mary is quicker than John”</em>和<em>“John is quicker than Mary”</em>这两句话，其词袋是一样的，但含义是完全相反的。所以凡是完全基于词袋模型的一些算法一般也存在这样该问题。</p>
<h3 id="Term-frequency"><a href="#Term-frequency" class="headerlink" title="Term frequency"></a>Term frequency</h3><p>词频就是一个词（term）在一个文档中（document）出现的次数（frequency），记为tf_{t,d}。这是一种最简单的定义方式，实际使用中还有一些变种：</p>
<ul>
<li>布尔词频：如果词在文档中出现，则tf_{t,d}=1，否则为0。</li>
<li>根据文档长短做调整的词频：tf_{t,d}/lenth，其中length为文档中的总词数。</li>
<li>对数词频：log(1+tf_{t,d})，加1是防止对0求对数（0没有对数）。 一般选取常用对数或者自然对数。</li>
</ul>
<p>词频的优点是简单，但缺点也很显然：</p>
<ol>
<li>词频中没有包含词的位置信息，所以从词频的角度来看，<em>“Mary is quicker than John”</em>和<em>“John is quicker than Mary”</em>两条文档是完全一致的，但显然它们的含义是完全相反的。</li>
<li>词频没有考虑不同词的重要性一般是不一样的，比如停用词的词频都很高，但它们并不重要。</li>
</ol>
<h3 id="Inverse-document-frequency"><a href="#Inverse-document-frequency" class="headerlink" title="Inverse document frequency"></a>Inverse document frequency</h3><p>一个词的逆文档频率用于衡量该词提供了多少信息，计算方式定义如下：$i d f_{t}=\log \frac{N}{d f_{t}}=-\log \frac{d f_{t}}{N}$</p>
<p>其中，t代表term，D代表文档，N代表语料库中文档总数，df_t代表语料库中包含t的文档的数据，即<strong>文档频率</strong>（document frequency）。如果语料库中不包含t，那df_t就等于0，为了避免除零操作，可以采用后面的公式，将df_t作为分子，也有的变种给df_t加了1。</p>
<p>对于固定的语料库，N是固定的，一个词的df_t越大，其idf(t,D)</p>
<p> 就越小。所以那些很稀少的词的idf值会很高，而像停用词这种出现频率很高的词idf值很低。</p>
<h3 id="TF-IDF-Model"><a href="#TF-IDF-Model" class="headerlink" title="TF-IDF Model"></a>TF-IDF Model</h3><p>TF-IDF就是将TF和IDF结合起来，其实就是简单的相乘：$t f i d f(t, d)=t f_{t, d} \cdot i d f_{t}$。从公式可以分析出来，一个词t在某个文档d中的tf-idf值：</p>
<ul>
<li>当该词在<strong>少数文档</strong>中<strong>出现很多次</strong>的时候，其值接近最大值；（<em>场景1</em>）</li>
<li>当该词在文档中出现次数少或者在很多文档中都出现时，其值较小；（<em>场景2</em>）</li>
<li>当该词几乎在所有文档中都出现时，其值接近最小值。（<em>场景3</em>）</li>
</ul>
<p>下面用一个例子来实战一下，还是以文中的4首英文短诗中的前3首为例。假设这3首诗组成了我们的语料库，每首诗就是一个文档（doc1：<em>Fog</em>、doc2：<em>Freedom And Love</em>、doc3：<em>Love’s Secret</em>），诗里面的每个单词就是一个个词（我们把标题也包含在里面）。然后我们选取<em>“the”、 “freedom”、”love”</em>三个词来分别计算它们在每个文档的TF-IDF，计算中使用自然对数形式。</p>
<ul>
<li>“<em>the</em>“在doc1中出现了1次，在doc2中出现了2次，在doc3中出现了1次，整个语料库有3个文档，包含”the”的文档也是3个。所以：</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/14.png" alt="图片"></p>
<ul>
<li>“<em>freedom</em>“在doc1中出现了0次，在doc2中出现了1次，在doc3中出现了0次，语料库中包含”freedom”的文档只有1个。所以：</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/15.png" alt="图片"></p>
<ul>
<li>“<em>love</em>“在doc1中现了0次，在doc2中出现了3次，在doc3中出现了5次，整个语料库有3个文档，包含”love”的文档有2个。所以：</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/16.png" alt="图片"></p>
<p>我们简单分析一下结果：”<em>the</em>“在所有文档中都出现了，所以其tf-idf值最低，为0，验证了上面公式分析中的场景3；”<em>freedom</em>“只有在第2个文档中出现了，所以其它两个的tf-idf值为0，表示不包含该词；”<em>love</em>“在第2、3个文档中都出现了，但在第3个文档中出现的频率更高，所以其tf-idf值最高。所以tf-idf算法的结果还是能很好的表示实际结果的。</p>
<h2 id="Vector-Space-Model"><a href="#Vector-Space-Model" class="headerlink" title="Vector Space Model"></a>Vector Space Model</h2><p>通过TF-IDF算法，我们可以计算出每个词在语料库中的权重，而通过VSM（Vector Space Model），则可以计算两个文档的相似度。</p>
<p>假设有两个文档：</p>
<ul>
<li>文档1：”Jack Ma regrets setting up Alibaba.”</li>
<li>文档2：”Richard Liu does not know he has a beautiful wife.”</li>
</ul>
<p>这是原始的文档，然后通过词袋模型转化后为：</p>
<ul>
<li>BoW1 = {“jack”:1, “ma”:1, “regret”:1, “set”:1, “up”:1, “alibaba”:1}</li>
<li>BoW2 = {“richard”:1, “liu”:1, “does”:1, “not”:1, “know”:1, “he”:1, “has”:1, “a”: 1, “beautiful”:1, “wife”:1}</li>
</ul>
<p>接着，分别用TF-IDF算法计算每个文档词袋中每个词的tf-idf值（值是随便写的，仅供原理说明）：</p>
<ul>
<li>tf-idf_doc1 = { 0.41, 0.12, 0.76, 0.83, 0.21, 0.47 }</li>
<li>tf-idf_doc2 = { 0.12, 0.25, 0.67, 0.98, 0.43, 0.76, 0.89, 0.51, 0.19, 0.37 }</li>
</ul>
<p>如果将上面的tf-idf_doc1和tf-idf_doc2看成是2个向量，那我们就通过上面的方式将原始的文档转换成了向量，这个向量就是VSM中的Vector。在VSM中，一个Vector就代表一个文档，记为V(q)，Vector中的每个值就是原来文档中term的权重（这个权重一般使用tf-idf计算，也可以通过其他方式计算）。这样语料库中的很多文档就会产生很多的向量，这些向量一起构成了一个向量空间，也就是Vector Space。</p>
<p>假设有一个查询语句为”Jack Alibaba”，我们可以用同样的方式将其转化一个向量，假设这个向量叫查询向量V(q)。<strong>这样在语料库中检索和 q相近文档的问题就转换成求语料库中每个向量V(d)与</strong>V(q)<strong>的相似度问题了</strong>。而衡量两个向量相似度最常用的方法就是余弦相似度，用公式表示就是：<script type="math/tex">\operatorname{cosineSimilarity(q,d)}=\frac{V(q) \cdot V(d)}{|V(q)||V(d)|}=v(q) \cdot v(d)</script>，这个就是Vector Space Model。</p>
<h2 id="TfidfSimilarity"><a href="#TfidfSimilarity" class="headerlink" title="TfidfSimilarity"></a>TfidfSimilarity</h2><blockquote>
<p>参考：<br><a href="https://en.wikipedia.org/wiki/Boolean_model_of_information_retrieval" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Boolean_model_of_information_retrieval</a></p>
</blockquote>
<p>Lucene使用<a href="http://en.wikipedia.org/wiki/Standard_Boolean_model" target="_blank" rel="noopener">Boolean model (BM) of Information Retrieval</a>模型来计算一个文档是否和搜索词匹配，对于匹配的文档使用基于VSM的评分算法来计算得分。具体的实现类是org.apache.lucene.search.similarities.TFIDFSimilarity，但做了一些修正。本文不讨论BM算法，只介绍评分算法。TFIDFSimilarity采用的评分公式如下：<script type="math/tex">\operatorname{Score}(q, d)=\sum_{t \in q}\left(t f_{t, d} \cdot i d f_{t}^{2} \cdot t . \text { get } \operatorname{Boost}() \cdot \text { norm }(t, d)\right)</script>，我们从外到内剖析一下这个公式：</p>
<ul>
<li>最外层的累加。搜索语句一般是由多个词组成的，比如”Jack Alibaba”就是有”Jack”和”Alibaba”两个词组成。计算搜索语句和每个匹配文档的得分的时候就是计算搜索语句中每个词和匹配文档的得分，然后累加起来就是搜索语句和该匹配文档的得分。这就是最外层的累加。</li>
<li>t.getBoost()：之前的系列文章中介绍过，在查询或者索引阶段我们可以人为设定某些term的权重，t.getBoost()获取的就是这个阶段设置的权重。所以查询或索引阶段设置的权重也就是在这个时候起作用的。</li>
<li>norm(t, d)：之前的系列文章中也介绍过，查询的时候一个文档的长短也是会影响词的重要性，匹配次数一样的情况下，越长的文档评分越低。这个也好理解，比如我们搜”Alibaba”，有两个文档里面都出现了一次该词，但其中一个文档总共包含100万个词，而另外一个只包含10个词，很显然，极大多数情况下，后者与搜索词的相关度是比前者高的。实际计算的时候使用的公式如下：<script type="math/tex">\operatorname{norm}(t, d)=\frac{1}{\sqrt{\operatorname{length}}}</script>，length是文档d的长度。</li>
<li>计算<script type="math/tex">t f_{t, d} \cdot i d f_{t}^{2}</script>：Lucene假设一个词在搜索语句中的词频为1（即使出现多次也不影响，就是重复计算多次而已），所以可以把这个公式拆开写：<script type="math/tex">t f_{t, d} \cdot i d f_{t}^{2}=t f_{t, d} \cdot i d f_{t} \cdot 1 \cdot i d f_{t}=\left(t f_{t, d} \cdot i d f_{t}\right) \cdot\left(t f_{t, q} \cdot i d f_{t}\right)</script>，这里的<script type="math/tex">\left(t f_{t, d} \cdot i d f_{t}\right) \cdot\left(t f_{t, q} \cdot i d f_{t}\right)</script>就对应上面的<script type="math/tex">-v(d) \cdot v(q)</script>!</li>
<li>在Lucene中，采用的TF计算公式为：<script type="math/tex">t f_{t, d}=\sqrt{\text {frequency}}</script>，IDF计算公式为：<script type="math/tex">i d f_{t}=1+\log \frac{N+1}{d f_{t}+1}</script></li>
</ul>
<p>其实TFIDFSimilarity是一个抽象类，真正实现上述相似度计算的是org.apache.lucene.search.similarities.ClassicSimilarity类，上面列举的公式在其对应的方法中也可以找到。除了基于TFIDF这种方式外，Lucene还支持另外一种相似度算法BM25，并且从6.0.0版本开始，BM25已经替代ClassicSimilarity，作为默认的评分算法。</p>
<h2 id="BM25Similarity"><a href="#BM25Similarity" class="headerlink" title="BM25Similarity"></a>BM25Similarity</h2><p>BM25全称“Best Match 25”，其中“25”是指现在BM25中的计算公式是第25次迭代优化。该算法是几位大牛在1994年TREC-3（Third <strong>T</strong>ext <strong>RE</strong>trieval <strong>C</strong>onference）会议上提出的，它将文本相似度问题转化为概率模型，可以看做是TF-IDF的改良版，我们看下它是如何进行改良的。</p>
<h3 id="对IDF的改良"><a href="#对IDF的改良" class="headerlink" title="对IDF的改良"></a>对IDF的改良</h3><p>BM25中的IDF公式为：<script type="math/tex">i d f_{t}^{B M 25}=\log \left(1+\frac{N-d f_{t}+0.5}{d f_{t}+0.5}\right)</script>。原版BM25的log中是没有加1的，Lucene为了防止产生负值，做了一点小优化。虽然对公式进行了更改，但其实和原来的公式没有实质性的差异，下面是新旧函数曲线对比：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/10.png" alt="图片"></p>
<h3 id="对TF的改良1"><a href="#对TF的改良1" class="headerlink" title="对TF的改良1"></a>对TF的改良1</h3><p>BM25中TF的公式为：<script type="math/tex">t f_{t, d}^{B M 25}=((k+1) * t f) /(k+t f)</script>，其中tf是传统的词频值。先来看下改良前后的函数曲线对比吧（下图中k=1.2）：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/11.png" alt="图片"></p>
<p>可以看到，传统的tf计算公式中，词频越高，tf值就越大，没有上限。但BM中的tf，随着词频的增长，tf值会无限逼近(k+1)，相当于是有上限的。这就是二者的区别。一般 k</p>
<p>k取 1.2，Lucene中也使用1.2作为k的默认值。</p>
<h3 id="对TF的改良2"><a href="#对TF的改良2" class="headerlink" title="对TF的改良2"></a>对TF的改良2</h3><p>在传统的计算公式中，还有一个norm。BM25将这个因素加到了TF的计算公式中，结合了norm因素的BM25中的TF计算公式为：<script type="math/tex">t f_{t, d}^{B M 25}=((k+1) * t f) /(k *(1.0-b+b * L)+t f)</script>，和之前相比，就是给分母上面的k加了一个乘数(1.0 - b + b * L)，其中的L的计算公式为：$L=|d|/avgDl$，其中，|d|是当前文档的长度，avgDl是语料库中所有文档的平均长度。b是一个常数，用来控制L对最总评分影响的大小，一般取0~1之间的数（取0则代表完全忽略L）。Lucene中b的默认值为 0.75.</p>
<p>通过这些细节上的改良，BM25在很多实际场景中的表现都优于传统的TF-IDF，所以从Lucene 6.0.0版本开始，上位成为默认的相似度评分算法。</p>
<h1 id="代码实践"><a href="#代码实践" class="headerlink" title="代码实践"></a>代码实践</h1><h2 id="Java"><a href="#Java" class="headerlink" title="Java"></a>Java</h2><p>原始数据为4首英文短诗，每个诗对应一个文件，文件名为诗名。这里列出内容，方便后面讨论。</p>
<ul>
<li>Fog（迷雾）：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">The fog comes</span><br><span class="line">on little cat feet.</span><br><span class="line">It sits looking over harbor and city</span><br><span class="line">on silent haunches</span><br><span class="line">and then, moves on.</span><br></pre></td></tr></table></figure>
<ul>
<li>Freedom And Love（自由与爱情）：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">How delicious is the winning</span><br><span class="line">Of a kiss at loves beginning,</span><br><span class="line">When two mutual hearts are sighing</span><br><span class="line">For the knot there&#39;s no untying.</span><br><span class="line">Yet remember, &#39;mist your wooing,</span><br><span class="line">Love is bliss, but love has ruining;</span><br><span class="line">Other smiles may make you fickle,</span><br><span class="line">Tears for charm may tickle.</span><br></pre></td></tr></table></figure>
<ul>
<li>Love’s Secret（爱情的秘密）：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Never seek to tell thy love,</span><br><span class="line">Love that never told shall be;</span><br><span class="line">For the gentle wind does move</span><br><span class="line">Silently, invisibly.</span><br><span class="line">I told my love, I told my love,</span><br><span class="line">I told her all my heart,</span><br><span class="line">Trembling, cold, in ghastly fears.</span><br><span class="line">Ah! she did depart!</span><br><span class="line">Soon after she was gone from me,</span><br><span class="line">A traveller came by,</span><br><span class="line">Silently, invisibly:</span><br><span class="line">He took her with a sigh.</span><br></pre></td></tr></table></figure>
<ul>
<li>On Death（死亡）：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Death stands above me, whispering low</span><br><span class="line">I know not what into my ear:</span><br><span class="line">Of his strange language all I know</span><br><span class="line">Is, there is not a word of fear.</span><br></pre></td></tr></table></figure>
<p>因为原始数据已经是文本格式了，所以我们构建索引的流程如下：<br><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/12.png" alt="图片"></p>
<p>其中的<strong>分析</strong>就是我们之前说的分词。然后先看代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 省略包等信息，完整文件见源文件</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * Minimal Index Files code.</span><br><span class="line"> **&#x2F;</span><br><span class="line">public class IndexFilesMinimal &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        &#x2F;&#x2F; 原数据存放路径</span><br><span class="line">        final String docsPath &#x3D; &quot;&#x2F;Users&#x2F;allan&#x2F;Git&#x2F;allan&#x2F;github&#x2F;CodeSnippet&#x2F;Java&#x2F;lucene-learning&#x2F;data&#x2F;poems&quot;;</span><br><span class="line">        &#x2F;&#x2F; 索引保存目录</span><br><span class="line">        final String indexPath &#x3D; &quot;&#x2F;Users&#x2F;allan&#x2F;Git&#x2F;allan&#x2F;github&#x2F;CodeSnippet&#x2F;Java&#x2F;lucene-learning&#x2F;indices&#x2F;poems-index&quot;;</span><br><span class="line"></span><br><span class="line">        final Path docDir &#x3D; Paths.get(docsPath);</span><br><span class="line">        Directory indexDir &#x3D; FSDirectory.open(Paths.get(indexPath));</span><br><span class="line">        &#x2F;&#x2F; 使用标准分析器</span><br><span class="line">        Analyzer analyzer &#x3D; new StandardAnalyzer();</span><br><span class="line"></span><br><span class="line">        IndexWriterConfig iwc &#x3D; new IndexWriterConfig(analyzer);</span><br><span class="line">        &#x2F;&#x2F; 每次都重新创建索引</span><br><span class="line">        iwc.setOpenMode(IndexWriterConfig.OpenMode.CREATE);</span><br><span class="line">        &#x2F;&#x2F; 创建IndexWriter用于写索引</span><br><span class="line">        IndexWriter writer &#x3D; new IndexWriter(indexDir, iwc);</span><br><span class="line"></span><br><span class="line">        System.out.println(&quot;index start...&quot;);</span><br><span class="line">        &#x2F;&#x2F; 遍历数据目录，对目录下的每个文件进行索引</span><br><span class="line">        Files.walkFileTree(docDir, new SimpleFileVisitor&lt;Path&gt;() &#123;</span><br><span class="line">            @Override</span><br><span class="line">            public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException &#123;</span><br><span class="line">                indexDoc(writer, file);</span><br><span class="line">                return FileVisitResult.CONTINUE;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        writer.close();</span><br><span class="line"></span><br><span class="line">        System.out.println(&quot;index ends.&quot;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private static void indexDoc(IndexWriter writer, Path file) throws IOException &#123;</span><br><span class="line">        try (InputStream stream &#x3D; Files.newInputStream(file)) &#123;</span><br><span class="line">            System.out.println(&quot;indexing file &quot; + file);</span><br><span class="line">            &#x2F;&#x2F; 创建文档对象</span><br><span class="line">            Document doc &#x3D; new Document();</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F; 将文件绝对路径加入到文档中</span><br><span class="line">            Field pathField &#x3D; new StringField(&quot;path&quot;, file.toString(), Field.Store.YES);</span><br><span class="line">            doc.add(pathField);</span><br><span class="line">            &#x2F;&#x2F; 将文件内容加到文档中</span><br><span class="line">            Field contentsField &#x3D; new TextField(&quot;contents&quot;, new BufferedReader(new InputStreamReader(stream)));</span><br><span class="line">            doc.add(contentsField);</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F; 将文档写入索引中</span><br><span class="line">            writer.addDocument(doc);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这段代码的功能是遍历4首诗对应的文件，对其进行分词、索引，最终形成索引文件，供以后检索。里面有几个API比较关键，这里稍作一下介绍：</p>
<ul>
<li><em>FSDirectory</em>：该类实现了索引文件存储到文件系统的功能。我们无需关注底层文件系统的类型，该类会帮我们处理好。当然还有其它几个类型的Directory，以后再介绍。</li>
<li><em>StandardAnalyzer</em>：Lucene内置的标准分词器，其分词的方法是去掉停用词（stop word），全部转化为小写，根据空白字符分成一个个词/词组。Lucene还支持好几种其它分词器，我们也可以实现自己的分词器，以后再介绍。</li>
<li><em>IndexWriter</em>：该类是索引（<em>此处为动词</em>）文件的核心类，负责索引的创建和维护。</li>
</ul>
<p>我们可以这样理解Lucene里面的组织形式：索引（Index）是最顶级的概念，可以理解为MySQL里面的表；索引里面包含很多个Document，一个Document可以理解为MySQL中的一行记录；一个Document里面可以包含很多个Field，每一个Field都是一个类似Map的结构，由字段名和字段内容组成，内容可再嵌套。在MySQL中，表结构是确定的，每一行记录的格式都是一样的，但Lucene没有这个要求，每个Document里面的字段可以完全不一样，即所谓的”<em>flexible schema</em>“。</p>
<p>在上述代码运行完之后，我们就生成了一个名叫<strong>poems-index</strong>的索引，该索引里面包含4个Document，每个Document对应一首短诗。每个Document由<em>path</em>和<em>contents</em>两个字段组成，<em>path</em>里面存储的是诗歌文件的绝对路径，<em>contents</em>里面存储的是诗歌的内容。最终生成的索引目录包含如下一些文件：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/13png.png" alt="图片"></p>
<p>这样后台索引构建的工作就算完成了，接下来我们来看一下如何利用索引进行高效的搜索：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 省略包等信息，完整文件见源文件</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * Minimal Search Files code</span><br><span class="line"> **&#x2F;</span><br><span class="line">public class SearchFilesMinimal &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        &#x2F;&#x2F; 索引保存目录</span><br><span class="line">        final String indexPath &#x3D; &quot;&#x2F;Users&#x2F;allan&#x2F;Git&#x2F;allan&#x2F;github&#x2F;CodeSnippet&#x2F;Java&#x2F;lucene-learning&#x2F;indices&#x2F;poems-index&quot;;</span><br><span class="line">        &#x2F;&#x2F; 搜索的字段</span><br><span class="line">        final String searchField &#x3D; &quot;contents&quot;;</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 从索引目录读取索引信息</span><br><span class="line">        IndexReader indexReader &#x3D; DirectoryReader.open(FSDirectory.open(Paths.get(indexPath)));</span><br><span class="line">        &#x2F;&#x2F; 创建索引查询对象</span><br><span class="line">        IndexSearcher searcher &#x3D; new IndexSearcher(indexReader);</span><br><span class="line">        &#x2F;&#x2F; 使用标准分词器</span><br><span class="line">        Analyzer analyzer &#x3D; new StandardAnalyzer();</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 从终端获取查询语句</span><br><span class="line">        BufferedReader in &#x3D; new BufferedReader(new InputStreamReader(System.in));</span><br><span class="line">        &#x2F;&#x2F; 创建查询语句解析对象</span><br><span class="line">        QueryParser queryParser &#x3D; new QueryParser(searchField, analyzer);</span><br><span class="line">        while (true) &#123;</span><br><span class="line">            System.out.println(&quot;Enter query: &quot;);</span><br><span class="line"></span><br><span class="line">            String input &#x3D; in.readLine();</span><br><span class="line">            if (input &#x3D;&#x3D; null) &#123;</span><br><span class="line">                break;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            input &#x3D; input.trim();</span><br><span class="line">            if (input.length() &#x3D;&#x3D; 0) &#123;</span><br><span class="line">                break;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F; 解析用户输入的查询语句：build query</span><br><span class="line">            Query query &#x3D; queryParser.parse(input);</span><br><span class="line">            System.out.println(&quot;searching for: &quot; + query.toString(searchField));</span><br><span class="line">            &#x2F;&#x2F; 查询</span><br><span class="line">            TopDocs results &#x3D; searcher.search(query, 10);</span><br><span class="line">            ScoreDoc[] hits &#x3D; results.scoreDocs;</span><br><span class="line">            if (results.totalHits.value &#x3D;&#x3D; 0) &#123;</span><br><span class="line">                System.out.println(&quot;no result matched!&quot;);</span><br><span class="line">                continue;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F; 输出匹配到的结果</span><br><span class="line">            System.out.println(results.totalHits.value + &quot; results matched: &quot;);</span><br><span class="line">            for (ScoreDoc hit : hits) &#123;</span><br><span class="line">                Document doc &#x3D; searcher.doc(hit.doc);</span><br><span class="line">                System.out.println(&quot;doc&#x3D;&quot; + hit.doc + &quot; score&#x3D;&quot; + hit.score + &quot; file: &quot; + doc.get(&quot;path&quot;));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这段代码的核心流程是先从上一步创建的索引目录加载构建好的索引，然后获取用户输入并解析为查询语句（build query），接着运行查询（run query），如果有匹配到的，就输出匹配的结果。这里对查询比较重要的API做下简单说明：</p>
<ul>
<li><em>IndexReader</em>：打开一个索引；</li>
<li><em>IndexSearcher</em>：搜索<em>IndexReader</em>打开的索引，返回<em>TopDocs</em>对象；</li>
<li><em>QueryParser</em>：该类的parse方法解析用户输入的查询语句，返回一个<em>Query</em>对象；</li>
</ul>
<p>下面我们来运行一下程序：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Enter query: </span><br><span class="line">love</span><br><span class="line">searching for: love</span><br><span class="line">2 results matched: </span><br><span class="line">doc&#x3D;0 score&#x3D;0.48849338 file: &#x2F;Users&#x2F;allan&#x2F;Git&#x2F;allan&#x2F;github&#x2F;CodeSnippet&#x2F;Java&#x2F;lucene-learning&#x2F;data&#x2F;poems&#x2F;Love&#39;sSecret.txt</span><br><span class="line">doc&#x3D;1 score&#x3D;0.41322997 file: &#x2F;Users&#x2F;allan&#x2F;Git&#x2F;allan&#x2F;github&#x2F;CodeSnippet&#x2F;Java&#x2F;lucene-learning&#x2F;data&#x2F;poems&#x2F;FreedomAndLove.txt</span><br></pre></td></tr></table></figure>
<p>我们输入关键字”<em>love</em>“，搜索出来两个Document，分别对应Love’s Secret和Freedom And Love。<em>doc=</em>后面的数字是Document的ID，唯一标识一个Document。<em>score</em>后面的数字是搜索结果与我们搜索的关键字的相关度。然后我们再输入”<em>LOVE</em>“（注意字母都大写了）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Enter query: </span><br><span class="line">Love</span><br><span class="line">searching for: love</span><br><span class="line">2 results matched: </span><br><span class="line">doc&#x3D;0 score&#x3D;0.48849338 file: &#x2F;Users&#x2F;allan&#x2F;Git&#x2F;allan&#x2F;github&#x2F;CodeSnippet&#x2F;Java&#x2F;lucene-learning&#x2F;data&#x2F;poems&#x2F;Love&#39;sSecret.txt</span><br><span class="line">doc&#x3D;1 score&#x3D;0.41322997 file: &#x2F;Users&#x2F;allan&#x2F;Git&#x2F;allan&#x2F;github&#x2F;CodeSnippet&#x2F;Java&#x2F;lucene-learning&#x2F;data&#x2F;poems&#x2F;FreedomAndLove.txt</span><br></pre></td></tr></table></figure>
<p>可以看到搜索结果与之前是一样的，这是因为我们搜索时使用了和构建索引时相同的分词器<em>StandardAnalyzer</em>，该分词器会将所有词转化为小写。然后我们再尝试一下其它搜索：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Enter query: </span><br><span class="line">fog</span><br><span class="line">searching for: fog</span><br><span class="line">1 results matched: </span><br><span class="line">doc&#x3D;2 score&#x3D;0.67580885 file: &#x2F;Users&#x2F;allan&#x2F;Git&#x2F;allan&#x2F;github&#x2F;CodeSnippet&#x2F;Java&#x2F;lucene-learning&#x2F;data&#x2F;poems&#x2F;Fog.txt</span><br><span class="line"></span><br><span class="line">Enter query: </span><br><span class="line">above</span><br><span class="line">searching for: above</span><br><span class="line">1 results matched: </span><br><span class="line">doc&#x3D;3 score&#x3D;0.6199532 file: &#x2F;Users&#x2F;allan&#x2F;Git&#x2F;allan&#x2F;github&#x2F;CodeSnippet&#x2F;Java&#x2F;lucene-learning&#x2F;data&#x2F;poems&#x2F;OnDeath.txt</span><br><span class="line"></span><br><span class="line">Enter query: </span><br><span class="line">death</span><br><span class="line">searching for: death</span><br><span class="line">1 results matched: </span><br><span class="line">doc&#x3D;3 score&#x3D;0.6199532 file: &#x2F;Users&#x2F;allan&#x2F;Git&#x2F;allan&#x2F;github&#x2F;CodeSnippet&#x2F;Java&#x2F;lucene-learning&#x2F;data&#x2F;poems&#x2F;OnDeath.txt</span><br><span class="line"></span><br><span class="line">Enter query: </span><br><span class="line">abc</span><br><span class="line">searching for: abc</span><br><span class="line">no result matched!</span><br></pre></td></tr></table></figure>
<p>都工作正常，最后一个关键字”<em>abc</em>“没有搜到，因为原文中也没有这个词。我们再来看一个复杂点的查询：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Enter query: </span><br><span class="line">+love -seek</span><br><span class="line">searching for: +love -seek</span><br><span class="line">1 results matched: </span><br><span class="line">doc&#x3D;1 score&#x3D;0.41322997 file: &#x2F;Users&#x2F;allan&#x2F;Git&#x2F;allan&#x2F;github&#x2F;CodeSnippet&#x2F;Java&#x2F;lucene-learning&#x2F;data&#x2F;poems&#x2F;FreedomAndLove.txt</span><br></pre></td></tr></table></figure>
<p>这里我们输入的关键字为”<em>+love -seek</em>“，这是一个高级一点的查询，含义是“包含love但不包含seek”，于是就只搜出来Freedom And Love一首诗了。</p>
<h2 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br></pre></td><td class="code"><pre><span class="line">import lucene</span><br><span class="line">from java.nio.file import Paths</span><br><span class="line"># from org.apache.lucene.analysis.cjk import CJKAnalyzer</span><br><span class="line">from org.apache.lucene.document import Document, Field, FieldType, StoredField</span><br><span class="line">from org.apache.lucene.document import TextField, FloatPoint, IntPoint</span><br><span class="line">from org.apache.lucene.document import NumericDocValuesField</span><br><span class="line">from org.apache.lucene.document import FloatDocValuesField</span><br><span class="line">from org.apache.lucene.document import SortedSetDocValuesField</span><br><span class="line">from org.apache.lucene.index import FieldInfo, IndexWriter, IndexWriterConfig</span><br><span class="line">from org.apache.lucene.store import SimpleFSDirectory</span><br><span class="line">from org.apache.lucene.util import Version</span><br><span class="line">from org.apache.lucene.search import IndexSearcher, Sort, SortField</span><br><span class="line">from org.apache.lucene.search import BooleanQuery</span><br><span class="line">from org.apache.lucene.search import BooleanClause</span><br><span class="line">from org.apache.lucene.search import TopFieldCollector, BoostQuery</span><br><span class="line">from org.apache.lucene.queryparser.classic import QueryParser</span><br><span class="line">from org.apache.lucene.index import DirectoryReader</span><br><span class="line">from org.apache.lucene.analysis.core import WhitespaceAnalyzer</span><br><span class="line">from org.apache.lucene.util import BytesRef</span><br><span class="line">from strsimpy.levenshtein import Levenshtein</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class LuceneECSearch(object):</span><br><span class="line">    def __init__(self, indir, seg_model_path, mode&#x3D;&#39;search&#39;,</span><br><span class="line">                 maxrecall&#x3D;10000, maxdoc&#x3D;30):</span><br><span class="line">        lucene.initVM()</span><br><span class="line">        self.indir &#x3D; indir</span><br><span class="line">        self.lucene_analyzer &#x3D; WhitespaceAnalyzer()</span><br><span class="line">        if mode &#x3D;&#x3D; &#39;search&#39;:</span><br><span class="line">            self.indir &#x3D; SimpleFSDirectory(Paths.get(indir))</span><br><span class="line">        self.maxdoc &#x3D; maxdoc</span><br><span class="line">        self.maxrecal &#x3D; maxrecall</span><br><span class="line">        self.levenshtein &#x3D; Levenshtein()</span><br><span class="line"></span><br><span class="line">    def search(self, query):</span><br><span class="line">        # 构造Query</span><br><span class="line">        query &#x3D; self.processor.process(query, &#39;en&#39;)</span><br><span class="line">        boolean_query &#x3D; BooleanQuery.Builder()</span><br><span class="line">        simple_query &#x3D; QueryParser(</span><br><span class="line">            &quot;en_tokenized&quot;,</span><br><span class="line">            self.lucene_analyzer).parse(query)</span><br><span class="line">        keyword_query &#x3D; QueryParser(</span><br><span class="line">            &quot;keyword&quot;,</span><br><span class="line">            self.lucene_analyzer).parse(query)</span><br><span class="line">        boost_keyword_query &#x3D; BoostQuery(keyword_query, 2.0)</span><br><span class="line">        boolean_query.add(simple_query, BooleanClause.Occur.SHOULD)</span><br><span class="line">        boolean_query.add(boost_keyword_query, BooleanClause.Occur.SHOULD)</span><br><span class="line"></span><br><span class="line">        # searcher</span><br><span class="line">        lucene_searcher &#x3D; IndexSearcher(</span><br><span class="line">            DirectoryReader.open(self.indir))</span><br><span class="line">        sorter &#x3D; Sort([</span><br><span class="line">            SortField.FIELD_SCORE,</span><br><span class="line">            SortField(&#39;origin_score&#39;, SortField.Type.FLOAT, True),</span><br><span class="line">            SortField(&#39;en_sent_lenth&#39;, SortField.Type.INT, True)])</span><br><span class="line"></span><br><span class="line">        # rerank</span><br><span class="line">        collector &#x3D; TopFieldCollector.create(</span><br><span class="line">            sorter, self.maxrecal, self.maxrecal)</span><br><span class="line">        lucene_searcher.search(boolean_query.build(), collector)</span><br><span class="line">        scoreDocs &#x3D; collector.topDocs().scoreDocs</span><br><span class="line">        result &#x3D; []</span><br><span class="line">        for hit in scoreDocs:</span><br><span class="line">            doc &#x3D; lucene_searcher.doc(hit.doc)</span><br><span class="line">            keyword_info &#x3D; json.loads(doc.get(&#39;keyword_info&#39;))</span><br><span class="line">            keyword_score &#x3D; 0.0</span><br><span class="line">            for word, score in keyword_info.items():</span><br><span class="line">                word_splits &#x3D; word.split(&#39; &#39;)</span><br><span class="line">                for split in word_splits:</span><br><span class="line">                    if split &#x3D;&#x3D; query:</span><br><span class="line">                        keyword_score +&#x3D; score</span><br><span class="line">            json_answer &#x3D; &#123;</span><br><span class="line">                &#39;en_tokenized&#39;: doc.get(&quot;en_tokenized&quot;),</span><br><span class="line">                &#39;en_sent&#39;: doc.get(&quot;en_sent&quot;),</span><br><span class="line">                &#39;en_sent_lenth&#39;: doc.get(&quot;en_sent_lenth&quot;),</span><br><span class="line">                &#39;cn_tokenized&#39;: doc.get(&quot;cn_tokenized&quot;),</span><br><span class="line">                &#39;cn_sent&#39;: doc.get(&quot;cn_sent&quot;),</span><br><span class="line">                &#39;confidence&#39;: int(doc.get(&quot;confidence&quot;)),</span><br><span class="line">                &#39;origin_score&#39;: float(doc.get(&quot;origin_score&quot;)),</span><br><span class="line">                &#39;new_score&#39;: float(doc.get(&quot;new_score&quot;))&#125;</span><br><span class="line">            result.append(json_answer)</span><br><span class="line">        result &#x3D; self.rerank(query, result)</span><br><span class="line">        return result</span><br><span class="line"></span><br><span class="line">    def rerank(self, query, candidates):</span><br><span class="line">        candidates.sort(key&#x3D;lambda x: x[&quot;origin_score&quot;], reverse&#x3D;True)</span><br><span class="line">        # candidates &#x3D; candidates[:self.maxdoc * 2]</span><br><span class="line">        candidates.sort(key&#x3D;lambda x: x[&quot;new_score&quot;], reverse&#x3D;True)</span><br><span class="line">        # 先把有释义的拿出来</span><br><span class="line">        return candidates</span><br><span class="line"></span><br><span class="line">    def build(self, docdir, modeldir):</span><br><span class="line">        if os.path.exists(self.indir):</span><br><span class="line">            shutil.rmtree(self.indir)</span><br><span class="line">        lucene.initVM()</span><br><span class="line">        INDEXIDR &#x3D; Paths.get(self.indir)</span><br><span class="line">        indexdir &#x3D; SimpleFSDirectory(INDEXIDR)</span><br><span class="line"></span><br><span class="line">        config &#x3D; IndexWriterConfig(self.lucene_analyzer)</span><br><span class="line">        index_writer &#x3D; IndexWriter(indexdir, config)</span><br><span class="line"></span><br><span class="line">        cnt &#x3D; 0</span><br><span class="line">        with open(docdir, &#39;r&#39;, encoding&#x3D;&#39;utf-8&#39;) as f:</span><br><span class="line">            for line in f.readlines():</span><br><span class="line">                line &#x3D; line.strip()</span><br><span class="line">                try:</span><br><span class="line">                    data_json &#x3D; json.loads(line)</span><br><span class="line">                except Exception:</span><br><span class="line">                    print(&#39;Json load error!&#39;)</span><br><span class="line">                    continue</span><br><span class="line">                try:</span><br><span class="line">                    document &#x3D; Document()</span><br><span class="line"></span><br><span class="line">                    en_tokenized &#x3D; data_json[&#39;en_tokenized&#39;]</span><br><span class="line">                    # TODO 去掉停用词</span><br><span class="line">                    document.add(Field(&quot;en_tokenized&quot;, en_tokenized,</span><br><span class="line">                                       TextField.TYPE_STORED))</span><br><span class="line"></span><br><span class="line">                    en_sent &#x3D; data_json[&#39;en_sent&#39;]</span><br><span class="line">                    document.add(Field(&quot;en_sent&quot;, en_sent,</span><br><span class="line">                                       TextField.TYPE_STORED))</span><br><span class="line"></span><br><span class="line">                    en_sent_lenth &#x3D; len(en_tokenized)</span><br><span class="line">                    document.add(IntPoint(&quot;en_sent_lenth&quot;, en_sent_lenth))</span><br><span class="line">                    document.add(NumericDocValuesField(</span><br><span class="line">                        &quot;en_sent_lenth&quot;, en_sent_lenth))</span><br><span class="line">                    document.add(StoredField(&quot;en_sent_lenth&quot;, en_sent_lenth))</span><br><span class="line"></span><br><span class="line">                    cn_tokenized &#x3D; data_json[&#39;cn_tokenized&#39;]</span><br><span class="line">                    document.add(Field(&quot;cn_tokenized&quot;, cn_tokenized,</span><br><span class="line">                                       TextField.TYPE_STORED))</span><br><span class="line"></span><br><span class="line">                    cn_sent &#x3D; data_json[&#39;cn_sent&#39;]</span><br><span class="line">                    document.add(Field(&quot;cn_sent&quot;, cn_sent,</span><br><span class="line">                                       TextField.TYPE_STORED))</span><br><span class="line"></span><br><span class="line">                    confidence &#x3D; int(data_json[&#39;confidence&#39;])</span><br><span class="line">                    document.add(IntPoint(&quot;confidence&quot;, confidence))</span><br><span class="line">                    document.add(NumericDocValuesField(</span><br><span class="line">                        &quot;confidence&quot;, confidence))</span><br><span class="line">                    document.add(StoredField(&quot;confidence&quot;, confidence))</span><br><span class="line"></span><br><span class="line">                    origin_score &#x3D; float(data_json[&#39;origin_score&#39;])</span><br><span class="line">                    document.add(FloatPoint(&quot;origin_score&quot;, origin_score))</span><br><span class="line">                    document.add(FloatDocValuesField(&quot;origin_score&quot;, origin_score))</span><br><span class="line">                    document.add(StoredField(&quot;origin_score&quot;, origin_score))</span><br><span class="line"></span><br><span class="line">                    keyword_info &#x3D; &#123;&#125;</span><br><span class="line">                    for keyword in data_json[&#39;keyword&#39;]:</span><br><span class="line">                        keyword_text &#x3D; keyword[&#39;text&#39;]</span><br><span class="line">                        keyword_score &#x3D; keyword[&#39;score&#39;]</span><br><span class="line">                        document.add(</span><br><span class="line">                            SortedSetDocValuesField(</span><br><span class="line">                                &quot;keyword&quot;, BytesRef(keyword_text)))</span><br><span class="line">                        document.add(</span><br><span class="line">                            StoredField(&quot;keyword&quot;, keyword_text))</span><br><span class="line">                        document.add(</span><br><span class="line">                            Field(&quot;keyword&quot;, keyword_text,</span><br><span class="line">                                  TextField.TYPE_STORED))</span><br><span class="line">                        keyword_info[keyword_text] &#x3D; keyword_score</span><br><span class="line">                    keyword_info_str &#x3D; json.dumps(</span><br><span class="line">                        keyword_info, ensure_ascii&#x3D;False)</span><br><span class="line">                    document.add(</span><br><span class="line">                        Field(</span><br><span class="line">                            &quot;keyword_info&quot;,</span><br><span class="line">                            keyword_info_str,</span><br><span class="line">                            TextField.TYPE_STORED))</span><br><span class="line">                    </span><br><span class="line">                    new_score &#x3D; ... # 经过模型计算出来的        </span><br><span class="line">0                    new_score &#x3D; </span><br><span class="line">                    document.add(FloatPoint(&quot;new_score&quot;, new_score))</span><br><span class="line">                    document.add(</span><br><span class="line">                        FloatDocValuesField(&quot;new_score&quot;, new_score))</span><br><span class="line">                    document.add(</span><br><span class="line">                        StoredField(&quot;new_score&quot;, new_score))</span><br><span class="line">                    index_writer.addDocument(document)</span><br><span class="line">                except Exception:</span><br><span class="line">                    print(&#39;Index write error!&#39;)</span><br><span class="line">                    continue</span><br><span class="line">                cnt +&#x3D; 1</span><br><span class="line">                if cnt % 1000 &#x3D;&#x3D; 0:</span><br><span class="line">                    print(&#39;Writing &#39;, cnt)</span><br><span class="line"></span><br><span class="line">        index_writer.commit()</span><br></pre></td></tr></table></figure>
<pre><code>    index_writer.close()
</code></pre>
      
    </div>
    <footer class="article-footer">
      <a data-url="majsunflower.cn/2020/05/10/Lucene%E6%90%AD%E5%BB%BA%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E5%88%9D%E6%8E%A2/" data-id="ckab1g9wu002kemwv6lrfhms7"
         class="article-share-link">分享</a>
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/lucene/" rel="tag">lucene</a></li></ul>

    </footer>

  </div>

  
    
  <nav class="article-nav">
    
      <a href="/2020/05/17/%E8%AF%8D%E5%AF%B9%E8%AF%8D%E7%BF%BB%E8%AF%91%E7%9A%84%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF/" class="article-nav-link">
        <strong class="article-nav-caption">前一篇</strong>
        <div class="article-nav-title">
          
            词对词翻译的那些事儿
          
        </div>
      </a>
    
    
      <a href="/2020/04/19/EM%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/" class="article-nav-link">
        <strong class="article-nav-caption">后一篇</strong>
        <div class="article-nav-title">EM学习——基础学习</div>
      </a>
    
  </nav>


  

  
    
  <div class="gitalk" id="gitalk-container"></div>
  
<link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">

  
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

  
<script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>

  <script type="text/javascript">
    var gitalk = new Gitalk({
      clientID: 'a0115c330d8e2a88dc59',
      clientSecret: '2e456ec13123a898d7b34ad8e117f543a6f379ea',
      repo: 'majing2019.github.io',
      owner: 'majing2019',
      admin: ['majing2019'],
      // id: location.pathname,      // Ensure uniqueness and length less than 50
      id: md5(location.pathname),
      distractionFreeMode: false,  // Facebook-like distraction free mode
      pagerDirection: 'last'
    })

  gitalk.render('gitalk-container')
  </script>

  

</article>



</section>
  <footer class="footer">
  <div class="outer">
    <div class="float-right">
      <ul class="list-inline">
  
    <li><i class="fe fe-smile-alt"></i> <span id="busuanzi_value_site_uv"></span></li>
  
    <li><i class="fe fe-bookmark"></i> <span id="busuanzi_value_page_pv"></span></li>
  
</ul>
    </div>
    <ul class="list-inline">
      <li>&copy; 2020 大嘴怪的小世界</li>
      <li>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></li>
      <li>Theme  <a href="https://github.com/zhwangart/hexo-theme-ocean" target="_blank" rel="noopener">Ocean</a></li>
    </ul>
  </div>
</footer>

</main>
<aside class="sidebar">
  <button class="navbar-toggle"></button>
<nav class="navbar">
  
    <div class="logo">
      <a href="/"><img src="/images/shark.svg" alt="大嘴怪的小世界"></a>
    </div>
  
  <ul class="nav nav-main">
    
      <li class="nav-item">
        <a class="nav-item-link" href="/">主页</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/archives">归档</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/gallery">相册</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/about">关于</a>
      </li>
    
    <li class="nav-item">
      <a class="nav-item-link nav-item-search" title="搜索">
        <i class="fe fe-search"></i>
        搜索
      </a>
    </li>
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      <div class="totop" id="totop">
  <i class="fe fe-rocket"></i>
</div>
    </li>
    <li class="nav-item">
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
</aside>

<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>


  
<script src="/fancybox/jquery.fancybox.min.js"></script>




  
<script src="/js/tocbot.min.js"></script>

  <script>
    // Tocbot_v4.7.0  http://tscanlin.github.io/tocbot/
    tocbot.init({
      tocSelector: '.tocbot',
      contentSelector: '.article-entry',
      headingSelector: 'h1, h2, h3, h4, h5, h6',
      hasInnerContainers: true,
      scrollSmooth: true,
      positionFixedSelector: '.tocbot',
      positionFixedClass: 'is-position-fixed',
      fixedSidebarOffset: 'auto',
    });
  </script>




<script src="/js/ocean.js"></script>


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  
  
  
    <meta name="description" content="记录生活">
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <title>
    论文阅读：《Jointly Learning to Align and Translate with Transformer》 |
    
    大嘴怪的小世界</title>
  
    <link rel="shortcut icon" href="/favicon.ico">
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<script src="/js/pace.min.js"></script>

<meta name="generator" content="Hexo 4.2.1"></head>

<body>
<main class="content">
  <section class="outer">
  

<article id="post-论文阅读：《Jointly-Learning-to-Align-and-Translate-with-Transformer》" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      论文阅读：《Jointly Learning to Align and Translate with Transformer》
    </h1>
  
  




      </header>
    

    
      <div class="article-meta">
        <a href="/2020/03/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E3%80%8AJointly-Learning-to-Align-and-Translate-with-Transformer%E3%80%8B/" class="article-date">
  <time datetime="2020-03-29T12:09:45.000Z" itemprop="datePublished">2020-03-29</time>
</a>
        
      </div>
    

    
      
    <div class="tocbot"></div>





    

    <div class="article-entry" itemprop="articleBody">
      


      

      
        <p>最近在对齐方面看的比较多，这一篇是去年看到的使用多任务学习提高对齐效果的文章。今天仔细读一遍。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://arxiv.org/pdf/1909.02074.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1909.02074.pdf</a></p>
</blockquote>
<h1 id="论文简介"><a href="#论文简介" class="headerlink" title="论文简介"></a>论文简介</h1><p>神经机器翻译已经统治了翻译领域，其中的attention机制是从词对齐借鉴出来的，但是NMT中的attention和词对齐又有很大不同。Attention更倾向于attend到context word而不是source word本身，而且现在的multi-layer、multi-head机制又使得attention非常复杂。</p>
<p>由于词对齐可以用在很多地方，比如对于实体名词的翻译，或者对于low-resource语言的借助词表的翻译有很大作用。因此本文提出了一个多任务学习的方法，使用NMT的negative log likelihood（NLL）loss和alignment loss结合作为多任务学习的loss。而且和NMT的auto-regressive式的模型不同，NMT在翻译时需要借助past target context的信息，而对于词对齐来说是不够用的，因此本文在多任务中使用了不同的context信息进行生成。</p>
<h1 id="问题定义和基线模型"><a href="#问题定义和基线模型" class="headerlink" title="问题定义和基线模型"></a>问题定义和基线模型</h1><p>给定source sentence：f(1,J)=f1,….,fj,…fJ和target translation：e(1,I)=f1,….,fj,…fI，则对齐就是位置的笛卡尔序列：$\mathcal{A} \subseteq\{(j, i): j=1, \ldots, J ; i=1, \ldots, I\}$。词对齐任务就是找到这样的多对多的对应关系。Transformer模型计算过程如下：</p>
<p>$\tilde{\mathbf{q}}_{n}^{i}=\mathbf{q}^{i} W_{n}^{Q}, \tilde{K}_{n}=K W_{n}^{K}, \tilde{V}_{n}=V W_{n}^{V}​$</p>
<p>$H_{n}^{i}=\text { Attention }\left(\tilde{\mathbf{q}}_{n}^{i}, \tilde{K}_{n}, \tilde{V}_{n}\right)$</p>
<p>$\mathcal{M}\left(\mathbf{q}^{i}, K, V\right)=\operatorname{Concat}\left(H_{1}^{i}, \ldots, H_{N}^{i}\right) W^{O}$</p>
<p>$\text { Attention }\left(\tilde{\mathbf{q}}_{n}^{i}, \tilde{K}_{n}, \tilde{V}_{n}\right)=\mathbf{a}_{n}^{i} \tilde{V}_{n}$</p>
<p>$\mathbf{a}_{n}^{i}=\operatorname{softmax}\left(\frac{\tilde{\mathbf{q}}_{n}^{i} \tilde{K}_{n}^{T}}{\sqrt{d_{k}}}\right)​$</p>
<p>其中$\mathbf{a}_{n}^{i} \in \mathbb{R}^{1 \times J}$表示第i个source token和全部target token的关系，整个attention matrix为$A_{I \times J}$。基线模型就是Transformer的attention矩阵抽取出的对齐， 论文在这里介绍了两个前人工作，这里不做介绍了。</p>
<h1 id="本文方法"><a href="#本文方法" class="headerlink" title="本文方法"></a>本文方法</h1><h2 id="Averaging-Layer-wise-Attention-Scores"><a href="#Averaging-Layer-wise-Attention-Scores" class="headerlink" title="Averaging Layer-wise Attention Scores"></a>Averaging Layer-wise Attention Scores</h2><p>单一的attention矩阵是对称的，但是不同层、不同head的attention学习到的是不同的东西，因此我们把所有head的attention矩阵加起来做平均，这样能更好地观察对齐。并且我们发现倒数第二层的attention矩阵G更好得表达了对齐。</p>
<h2 id="Multi-task-Learning"><a href="#Multi-task-Learning" class="headerlink" title="Multi-task Learning"></a>Multi-task Learning</h2><p>由于标注alignment是个很费力的事，本文使用G来指导attention。Gij是一个0-1矩阵（可以使用layer-wise attention或giza++产生的alignment），Aij是某一个head的attention，通过最小化Gij和Aij的KL散度来进行优化：$\mathcal{L}_{a}(A)=-\frac{1}{I} \sum_{i=1}^{I} \sum_{j=1}^{J} G_{i, j}^{p} \log \left(A_{i, j}\right)$。整个模型的损失函数为：$\mathcal{L}=\mathcal{L}_{t}+\lambda \mathcal{L}_{a}(A)$，其中Lt是翻译的NLL loss。</p>
<h2 id="Providing-Full-Target-Context"><a href="#Providing-Full-Target-Context" class="headerlink" title="Providing Full Target Context"></a>Providing Full Target Context</h2><p>训练翻译模型时用的时auto-regressive进行解码，也就是生成每个target tokens时只依赖于source tokens和之前生成的target token，但对于对齐任务来说，需要指导全部的target tokens。本文使用的方法是对于不同的loss，使用不同的context计算（计算两次前向）：</p>
<p>$\mathcal{L}_{t}=-\frac{1}{I} \sum_{i=1}^{I} \log \left(p\left(e_{i} | f_{1}^{J}, e_{1}^{i-1}\right)\right)$</p>
<p>$\mathcal{L}_{a}^{\prime}=\mathcal{L}_{a}\left(A | f_{1}^{J}, e_{1}^{I}\right)$</p>
<h1 id="实验部分"><a href="#实验部分" class="headerlink" title="实验部分"></a>实验部分</h1><p>评价指标选择alignment error rate（AER）。Transformer选择base model参数设置如下：</p>
<ul>
<li>embed_size=512</li>
<li>6 encoder + 6 decoder</li>
<li>8 attention head</li>
<li>share input and output embedding</li>
<li>relu activation</li>
<li>sinusoidal positional embedding（参考：<a href="https://www.zhihu.com/question/307293465" target="_blank" rel="noopener">https://www.zhihu.com/question/307293465</a>）</li>
<li>validation translation loss for early stopping</li>
<li>Adam, learning rate=3e-4, beta1=0.9, beta2=0.98</li>
<li>warmup step=4000</li>
<li>learning rate scheduler = inverse square root</li>
<li>dropout=0.1</li>
<li>label smooth=0.1</li>
</ul>
<p>本文选择的Statistical Baseline设置如下：</p>
<ul>
<li>5次迭代IBM1 + HMM + IBM3 + IBM4</li>
<li>使用grow-diagonal将两个方向的对齐合并</li>
</ul>
<p>最终实验结果如下：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/temp1.png" alt="图片"></p>
<p>在实验中我们发现，模型更容易将代词和名词对齐，提示我们可以将对齐分为sure和possible两种。在基于统计的GIZA++对齐中possible对齐较少出现，这可能是因为giza是通过统计共现来实现的。可以认为，将context进行更好得建模有助于对齐。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="majsunflower.cn/2020/03/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E3%80%8AJointly-Learning-to-Align-and-Translate-with-Transformer%E3%80%8B/" data-id="cklf9zmps001mjtx91qjfdmt6"
         class="article-share-link">分享</a>
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/LaserTagger/" rel="tag">LaserTagger</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" rel="tag">论文阅读</a></li></ul>

    </footer>

  </div>

  
    
  <nav class="article-nav">
    
      <a href="/2020/03/29/%E4%BB%A4%E4%BA%BA%E5%A4%B4%E5%A4%A7%E4%B9%8BIBM-Model/" class="article-nav-link">
        <strong class="article-nav-caption">前一篇</strong>
        <div class="article-nav-title">
          
            令人头大之IBM Model
          
        </div>
      </a>
    
    
      <a href="/2020/03/28/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E3%80%8AEncode-Tag-Realize-High-Precision-Text-Editing%E3%80%8B/" class="article-nav-link">
        <strong class="article-nav-caption">后一篇</strong>
        <div class="article-nav-title">论文阅读：《Encode, Tag, Realize_ High-Precision Text Editing》</div>
      </a>
    
  </nav>


  

  
    
  <div class="gitalk" id="gitalk-container"></div>
  
<link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">

  
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

  
<script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>

  <script type="text/javascript">
    var gitalk = new Gitalk({
      clientID: 'a0115c330d8e2a88dc59',
      clientSecret: '2e456ec13123a898d7b34ad8e117f543a6f379ea',
      repo: 'majing2019.github.io',
      owner: 'majing2019',
      admin: ['majing2019'],
      // id: location.pathname,      // Ensure uniqueness and length less than 50
      id: md5(location.pathname),
      distractionFreeMode: false,  // Facebook-like distraction free mode
      pagerDirection: 'last'
    })

  gitalk.render('gitalk-container')
  </script>

  

</article>



</section>
  <footer class="footer">
  <div class="outer">
    <div class="float-right">
      <ul class="list-inline">
  
    <li><i class="fe fe-smile-alt"></i> <span id="busuanzi_value_site_uv"></span></li>
  
    <li><i class="fe fe-bookmark"></i> <span id="busuanzi_value_page_pv"></span></li>
  
</ul>
    </div>
    <ul class="list-inline">
      <li>&copy; 2021 大嘴怪的小世界</li>
      <li>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></li>
      <li>Theme  <a href="https://github.com/zhwangart/hexo-theme-ocean" target="_blank" rel="noopener">Ocean</a></li>
    </ul>
  </div>
</footer>

</main>
<aside class="sidebar">
  <button class="navbar-toggle"></button>
<nav class="navbar">
  
    <div class="logo">
      <a href="/"><img src="/images/shark.svg" alt="大嘴怪的小世界"></a>
    </div>
  
  <ul class="nav nav-main">
    
      <li class="nav-item">
        <a class="nav-item-link" href="/">主页</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/archives">归档</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/gallery">相册</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/about">关于</a>
      </li>
    
    <li class="nav-item">
      <a class="nav-item-link nav-item-search" title="搜索">
        <i class="fe fe-search"></i>
        搜索
      </a>
    </li>
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      <div class="totop" id="totop">
  <i class="fe fe-rocket"></i>
</div>
    </li>
    <li class="nav-item">
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
</aside>

<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>


  
<script src="/fancybox/jquery.fancybox.min.js"></script>




  
<script src="/js/tocbot.min.js"></script>

  <script>
    // Tocbot_v4.7.0  http://tscanlin.github.io/tocbot/
    tocbot.init({
      tocSelector: '.tocbot',
      contentSelector: '.article-entry',
      headingSelector: 'h1, h2, h3, h4, h5, h6',
      hasInnerContainers: true,
      scrollSmooth: true,
      positionFixedSelector: '.tocbot',
      positionFixedClass: 'is-position-fixed',
      fixedSidebarOffset: 'auto',
    });
  </script>




<script src="/js/ocean.js"></script>


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
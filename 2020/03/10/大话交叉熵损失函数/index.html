<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  
  
  
    <meta name="description" content="记录生活">
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <title>
    大话交叉熵损失函数 |
    
    大嘴怪的小世界</title>
  
    <link rel="shortcut icon" href="/favicon.ico">
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<script src="/js/pace.min.js"></script>

<meta name="generator" content="Hexo 4.2.1"></head>

<body>
<main class="content">
  <section class="outer">
  

<article id="post-大话交叉熵损失函数" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      大话交叉熵损失函数
    </h1>
  
  




      </header>
    

    
      <div class="article-meta">
        <a href="/2020/03/10/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" class="article-date">
  <time datetime="2020-03-10T15:25:13.000Z" itemprop="datePublished">2020-03-10</time>
</a>
        
      </div>
    

    
      
    <div class="tocbot"></div>





    

    <div class="article-entry" itemprop="articleBody">
      


      

      
        <p>使用keras进行二分类时，常使用binary_crossentropy作为损失函数。那么它的原理是什么，跟categorical_crossentropy、sparse_categorical_crossentropy有什么区别？在进行文本分类时，如何选择损失函数，有哪些优化损失函数的方式？本文将从原理到实现进行一一介绍。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a" target="_blank" rel="noopener">https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a</a></p>
</blockquote>
<h1 id="binary-crossentropy"><a href="#binary-crossentropy" class="headerlink" title="binary_crossentropy"></a>binary_crossentropy</h1><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>假设我们想做一个二分类，输入有10个点：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x &#x3D; [-2.2, -1.4, -0.8, 0.2, 0.4, 0.8, 1.2, 2.2, 2.9, 4.6]</span><br></pre></td></tr></table></figure>
<p>输出有两类，分别为红色、绿色：<br><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5/1.png" alt="图片"></p>
<p>我们可以将问题描述成“这个点是绿色的吗?”，或者“这个点是绿色的概率是多少?”。理想情况下，绿点的概率是1.0，而红点的（是绿色的）概率是0.0。从而，绿色就是正样本，红色就是负样本。</p>
<p>如果我们拟合一个模型来执行这种分类，它将预测我们每个点的绿色概率。那么我们如何评估预测概率的好坏呢？这就是损失函数的意义，</p>
<p>Binary CrossEntorpy的计算如下：</p>
<p>$H_{p}(q)=-\frac{1}{N} \sum_{i=1}^{N} y_{i} \cdot \log \left(p\left(y_{i}\right)\right)+\left(1-y_{i}\right) \cdot \log \left(1-p\left(y_{i}\right)\right)$</p>
<p>其中y是标签(1代表绿色点，0代表红色点)，p(y)是所有N个点都是绿色的预测概率。看到这个计算式，发现对于每一个绿点(y=1)它增加了log(p(y))的损失（概率越大，增加的越小），也就是它是绿色的概率。下面我们可视化地看一下这个损失函数。</p>
<p>假设我们训练一个逻辑回归模型来进行分类，那么训练出的函数趋近于一个sigmoid曲线，曲线上每个点表示对于每个x是绿色点的概率：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5/2.png" alt="图片"></p>
<p>那么对于这些绿色的点，他们预测为绿色的概率是多少呢？实际下面图片中绿色的bar：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5/3.png" alt="图片"></p>
<p>那么红色点预测为红色的概率是多少呢？实际就是下面图片中红色的bar：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5/4.png" alt="图片"></p>
<p>我们把图片绘制得更好看一下，如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5/5.png" alt="图片"></p>
<p>因为我们要计算损失，我们需要惩罚错误的预测。如果与正例相关的概率是1.0，我们需要它的损失为零。相反，如果概率很低，比如0.01，我们需要它的损失是巨大的！取概率的(负)对数非常适合我们的目的(由于0.0和1.0之间的值的对数是负的，我们取负对数来获得正的损失值)。下面这个图展示了当正例的概率逐渐趋近于0时loss的变化：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5/6.png" alt="图片"></p>
<p>下面这个图表示了，我们使用负对数时每个点的损失，我们计算其平均值，就是binary cross entropy了！</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5/7.png" alt="图片"></p>
<h2 id="keras实现"><a href="#keras实现" class="headerlink" title="keras实现"></a>keras实现</h2><p>tf2.1的bce用法如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bce &#x3D; tf.keras.losses.BinaryCrossentropy()</span><br><span class="line">loss &#x3D; bce([0., 0., 1., 1.], [1., 1., 1., 0.])</span><br><span class="line">print(&#39;Loss: &#39;, loss.numpy())  # Loss: 11.522857</span><br></pre></td></tr></table></figure>
<p>或者：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model &#x3D; tf.keras.Model(inputs, outputs)</span><br><span class="line">model.compile(&#39;sgd&#39;, loss&#x3D;tf.keras.losses.BinaryCrossentropy())</span><br></pre></td></tr></table></figure>
<p>具体实现如下（tensorflow.python.keras/losses）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">class BinaryCrossentropy(LossFunctionWrapper):</span><br><span class="line">    def __init__(self, from_logits&#x3D;False,</span><br><span class="line">                  label_smoothing&#x3D;0,</span><br><span class="line">                  reduction&#x3D;losses_utils.ReductionV2.AUTO,</span><br><span class="line">                  name&#x3D;&#39;binary_crossentropy&#39;):</span><br><span class="line">        super(BinaryCrossentropy, self).__init__(</span><br><span class="line">              binary_crossentropy,</span><br><span class="line">              name&#x3D;name,</span><br><span class="line">              reduction&#x3D;reduction,</span><br><span class="line">              from_logits&#x3D;from_logits,</span><br><span class="line">              label_smoothing&#x3D;label_smoothing)</span><br><span class="line">        self.from_logits &#x3D; from_logits</span><br><span class="line"></span><br><span class="line">def binary_crossentropy(y_true, y_pred, from_logits&#x3D;False, label_smoothing&#x3D;0):</span><br><span class="line">    y_pred &#x3D; ops.convert_to_tensor_v2(y_pred)</span><br><span class="line">    y_true &#x3D; math_ops.cast(y_true, y_pred.dtype)</span><br><span class="line">    label_smoothing &#x3D; ops.convert_to_tensor_v2(label_smoothing, dtype&#x3D;K.floatx())</span><br><span class="line">    </span><br><span class="line">    def _smooth_labels():</span><br><span class="line">      return y_true * (1.0 - label_smoothing) + 0.5 * label_smoothing</span><br><span class="line">    </span><br><span class="line">    y_true &#x3D; smart_cond.smart_cond(label_smoothing, _smooth_labels, lambda: y_true)</span><br><span class="line">    return K.mean(K.binary_crossentropy(y_true, y_pred, from_logits&#x3D;from_logits), axis&#x3D;-1)</span><br></pre></td></tr></table></figure>
<p>在上面代码中，如果from_logits=True，则认为y_predit是tensor（可以认为是[0,1]之间的概率值），使用from_logits=True可以更稳定一些。label_smoothing在[0,1]之间。reduction的默认值是AUTO，表示根据上下文确定；如果是SUM_OVER_BATCH_SIZE表示整个batch的结果相加。<br>其中K.binary_crossentropy实现如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def binary_crossentropy(target, output, from_logits&#x3D;False):</span><br><span class="line">  if from_logits:</span><br><span class="line">    return nn.sigmoid_cross_entropy_with_logits(labels&#x3D;target, logits&#x3D;output)</span><br><span class="line"></span><br><span class="line">  if not isinstance(output, (ops.EagerTensor, variables_module.Variable)):</span><br><span class="line">    output &#x3D; _backtrack_identity(output)</span><br><span class="line">    if output.op.type &#x3D;&#x3D; &#39;Sigmoid&#39;:</span><br><span class="line">      assert len(output.op.inputs) &#x3D;&#x3D; 1</span><br><span class="line">      output &#x3D; output.op.inputs[0]</span><br><span class="line">      return nn.sigmoid_cross_entropy_with_logits(labels&#x3D;target, logits&#x3D;output)</span><br><span class="line"></span><br><span class="line">  epsilon_ &#x3D; _constant_to_tensor(epsilon(), output.dtype.base_dtype)</span><br><span class="line">  output &#x3D; clip_ops.clip_by_value(output, epsilon_, 1. - epsilon_)</span><br><span class="line"></span><br><span class="line">  bce &#x3D; target * math_ops.log(output + epsilon())</span><br><span class="line">  bce +&#x3D; (1 - target) * math_ops.log(1 - output + epsilon())</span><br><span class="line">  return -bce</span><br></pre></td></tr></table></figure>
<p>sigmoid_cross_entropy_with_logits实现如下：（该函数适用于不同类标签之间相互独立的情况，例如一个图片可以既包含大象也包含狗）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def sigmoid_cross_entropy_with_logits(_sentinel&#x3D;None, labels&#x3D;None, logits&#x3D;None, name&#x3D;None):</span><br><span class="line">    zeros &#x3D; array_ops.zeros_like(logits, dtype&#x3D;logits.dtype)</span><br><span class="line">    cond &#x3D; (logits &gt;&#x3D; zeros)</span><br><span class="line">    relu_logits &#x3D; array_ops.where(cond, logits, zeros)</span><br><span class="line">    neg_abs_logits &#x3D; array_ops.where(cond, -logits, logits)</span><br><span class="line">    return math_ops.add(relu_logits - logits * labels, math_ops.log1p(math_ops.exp(neg_abs_logits)), name&#x3D;name)</span><br></pre></td></tr></table></figure>
<p>对于上面代码，解释如下：<br>对于x=logits, z=labels，logistic损失定义为</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))</span><br><span class="line">&#x3D; z * -log(1 &#x2F; (1 + exp(-x))) + (1 - z) * -log(exp(-x) &#x2F; (1 + exp(-x)))</span><br><span class="line">&#x3D; z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))</span><br><span class="line">&#x3D; z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))</span><br><span class="line">&#x3D; (1 - z) * x + log(1 + exp(-x))</span><br><span class="line">&#x3D; x - x * z + log(1 + exp(-x))</span><br></pre></td></tr></table></figure>
<p>对于x&lt;0，为了防止exp(-x)溢出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x - x * z + log(1 + exp(-x))</span><br><span class="line">&#x3D; log(exp(x)) - x * z + log(1 + exp(-x))</span><br><span class="line">&#x3D; - x * z + log(1 + exp(x))</span><br></pre></td></tr></table></figure>
<p>为了保证稳定和不溢出，在实现过程中使用了如下等式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">max(x, 0) - x * z + log(1 + exp(-abs(x)))</span><br></pre></td></tr></table></figure>
<h1 id="categorical-crossentropy"><a href="#categorical-crossentropy" class="headerlink" title="categorical_crossentropy"></a>categorical_crossentropy</h1><h2 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h2><p>CrossEntropy可用于多分类任务，且label且one-hot形式。它的计算式如下：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5/8.png" alt="图片"></p>
<h2 id="keras实现-1"><a href="#keras实现-1" class="headerlink" title="keras实现"></a>keras实现</h2><p>tf2.1的ce用法如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">y_true &#x3D; [[0, 1, 0], [0, 0, 1]]</span><br><span class="line">y_pred &#x3D; [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]</span><br><span class="line">cce &#x3D; tf.keras.losses.CategoricalCrossentropy()</span><br><span class="line"># Using &#39;auto&#39;&#x2F;&#39;sum_over_batch_size&#39; reduction type.</span><br><span class="line">cce(y_true, y_pred).numpy()</span><br><span class="line"># Calling with &#39;sample_weight&#39;.</span><br><span class="line">cce(y_true, y_pred, sample_weight&#x3D;tf.constant([0.3, 0.7])).numpy()</span><br><span class="line"># Using &#39;sum&#39; reduction type.</span><br><span class="line">cce &#x3D; tf.keras.losses.CategoricalCrossentropy(reduction&#x3D;tf.keras.losses.Reduction.NONE)</span><br><span class="line">cce(y_true, y_pred).numpy()</span><br><span class="line"># Usage with the &#96;compile&#96; API</span><br><span class="line">model &#x3D; tf.keras.Model(inputs, outputs)</span><br><span class="line">model.compile(&#39;sgd&#39;, loss&#x3D;tf.keras.losses.CategoricalCrossentropy())</span><br></pre></td></tr></table></figure>
<p>具体实现如下（tensorflow.python.keras/losses）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">class CategoricalCrossentropy(LossFunctionWrapper):</span><br><span class="line">    def __init__(self,</span><br><span class="line">                   from_logits&#x3D;False,</span><br><span class="line">                   label_smoothing&#x3D;0,</span><br><span class="line">                   reduction&#x3D;losses_utils.ReductionV2.AUTO,</span><br><span class="line">                   name&#x3D;&#39;categorical_crossentropy&#39;):</span><br><span class="line">        super(CategoricalCrossentropy, self).__init__(</span><br><span class="line">                categorical_crossentropy,</span><br><span class="line">                name&#x3D;name,</span><br><span class="line">                reduction&#x3D;reduction,</span><br><span class="line">                from_logits&#x3D;from_logits,</span><br><span class="line">                label_smoothing&#x3D;label_smoothing)</span><br><span class="line"></span><br><span class="line">    def categorical_crossentropy(y_true,</span><br><span class="line">                                 y_pred,</span><br><span class="line">                                 from_logits&#x3D;False,</span><br><span class="line">                                 label_smoothing&#x3D;0):</span><br><span class="line">        y_pred &#x3D; ops.convert_to_tensor_v2(y_pred)</span><br><span class="line">        y_true &#x3D; math_ops.cast(y_true, y_pred.dtype)</span><br><span class="line">        label_smoothing &#x3D; ops.convert_to_tensor_v2(label_smoothing, dtype&#x3D;K.floatx())</span><br><span class="line">        def _smooth_labels():</span><br><span class="line">            num_classes &#x3D; math_ops.cast(array_ops.shape(y_true)[1], y_pred.dtype)</span><br><span class="line">            return y_true * (1.0 - label_smoothing) + (label_smoothing &#x2F; num_classes)</span><br><span class="line">        y_true &#x3D; smart_cond.smart_cond(label_smoothing, _smooth_labels, lambda: y_true)</span><br><span class="line">        return K.categorical_crossentropy(y_true, y_pred, from_logits&#x3D;from_logits)</span><br></pre></td></tr></table></figure>
<p>其中K.categorical_crossentropy实现如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">def categorical_crossentropy(target, output, from_logits&#x3D;False, axis&#x3D;-1):</span><br><span class="line">    if from_logits:</span><br><span class="line">        return nn.softmax_cross_entropy_with_logits_v2(</span><br><span class="line">            labels&#x3D;target, logits&#x3D;output, axis&#x3D;axis)</span><br><span class="line">    if not isinstance(output, (ops.EagerTensor, variables_module.Variable)):</span><br><span class="line">        output &#x3D; _backtrack_identity(output)</span><br><span class="line">        if output.op.type &#x3D;&#x3D; &#39;Softmax&#39;:</span><br><span class="line">            output &#x3D; output.op.inputs[0]</span><br><span class="line">            return nn.softmax_cross_entropy_with_logits_v2(</span><br><span class="line">                labels&#x3D;target, logits&#x3D;output, axis&#x3D;axis)</span><br><span class="line">    # scale preds so that the class probas of each sample sum to 1</span><br><span class="line">    output &#x3D; output &#x2F; math_ops.reduce_sum(output, axis, True)</span><br><span class="line">    # Compute cross entropy from probabilities.</span><br><span class="line">    epsilon_ &#x3D; _constant_to_tensor(epsilon(), output.dtype.base_dtype)</span><br><span class="line">    output &#x3D; clip_ops.clip_by_value(output, epsilon_, 1. - epsilon_)</span><br><span class="line">    return -math_ops.reduce_sum(target * math_ops.log(output), axis)</span><br></pre></td></tr></table></figure>
<h1 id="sparse-categorical-crossentropy"><a href="#sparse-categorical-crossentropy" class="headerlink" title="sparse_categorical_crossentropy"></a>sparse_categorical_crossentropy</h1><h2 id="原理-2"><a href="#原理-2" class="headerlink" title="原理"></a>原理</h2><p>跟categorical_crossentropy的区别是其标签不是one-hot，而是integer。比如在categorical_crossentropy是[1,0,0]，在sparse_categorical_crossentropy中是3.</p>
<h2 id="keras实现-2"><a href="#keras实现-2" class="headerlink" title="keras实现"></a>keras实现</h2><p>tf2.1中使用方法如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y_true &#x3D; [1, 2]</span><br><span class="line">y_pred &#x3D; [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]</span><br><span class="line">loss &#x3D; tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)</span><br></pre></td></tr></table></figure>
<h1 id="其他技巧"><a href="#其他技巧" class="headerlink" title="其他技巧"></a>其他技巧</h1><h2 id="focal-loss"><a href="#focal-loss" class="headerlink" title="focal loss"></a>focal loss</h2><blockquote>
<p>参考：<br><a href="https://ldzhangyx.github.io/2018/11/16/focal-loss/" target="_blank" rel="noopener">https://ldzhangyx.github.io/2018/11/16/focal-loss/</a></p>
</blockquote>
<p>Focal Loss的出现是为了解决训练集正负样本极度不平衡的情况，通过reshape标准交叉熵损失解决类别不均衡（Class Imbalance）,这样它就能降低容易分类的样例的比重（Well-classified Examples）。这个方法专注训练在Hard Examples的稀疏集合上，能够防止大量的Easy Negatives在训练中压倒训练器。其公式为：</p>
<p>$F L\left(p_{t}\right)=-\left(1-p_{t}\right)^{\gamma} \log \left(p_{t}\right)​$</p>
<p>其中参数为0的时候，Focal Loss退化为交叉熵CE。当这个参数不同时，对loss的影响如下：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5/0.png" alt="图片"></p>
<p>p_t越大，FL越小，其对总体loss所做的贡献就越小；反过来说，p_t越小（小于0.5的情况也就是被误分类），越能反映在总体loss上。</p>
<p>Focal Loss的tensorflow api：<a href="https://www.tensorflow.org/addons/api_docs/python/tfa/losses/SigmoidFocalCrossEntropy" target="_blank" rel="noopener">https://www.tensorflow.org/addons/api_docs/python/tfa/losses/SigmoidFocalCrossEntropy</a></p>
<h2 id="label-smooth"><a href="#label-smooth" class="headerlink" title="label smooth"></a>label smooth</h2><blockquote>
<p>参考：<br><a href="https://towardsdatascience.com/what-is-label-smoothing-108debd7ef06" target="_blank" rel="noopener">https://towardsdatascience.com/what-is-label-smoothing-108debd7ef06</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/76587755" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/76587755</a></p>
</blockquote>
<p>在使用深度学习模型进行分类任务时，我们通常会遇到以下问题：overfit和over confidence。Overfit问题得到了很好的研究，可以通过earlystop、dropout、正则化等方法来解决。另一方面，我们over confidence的工具较少。标签平滑是一种正则化技术，解决了这两个问题。</p>
<p>Label Smooth将y_hot和均匀分布的混合来代替一个hot编码的标签向量y_hot:</p>
<p>$y_{-} l s=(1-\alpha) * y_{-} h o t+\alpha / K$</p>
<p>K是标签类的数目，α是一个决定平滑的超参数。如果α= 0，我们获得最初的一个原始的y_hot编码。如果α= 1，我们得到均匀分布。</p>
<p>当损失函数为交叉熵时，使用标签平滑，模型将softmax函数应用于倒数第二层的logit向量z，计算其输出概率p。在这种情况下，交叉熵损失函数相对于logit的梯度为：</p>
<p>$\nabla C E=p-y=\operatorname{softmax}(z)-y$</p>
<p>其中y是标签分布，并且：</p>
<ul>
<li>梯度下降会使p尽可能接近y</li>
<li>梯度在-1和1之间有界</li>
</ul>
<p>一个标准的ont-hot希望有更大的logit gaps输入到里面。直观地说，较大的logit gap加上有界的梯度会使模型的自适应性降低，并且对其预测过于自信。相反，平滑的标签鼓励小的logit差距，可以得到更好的模型校准，并防止过度自信的预测。</p>
<p>下面我们使用一个例子说明：假设我们有K = 3类，我们的标签属于第一类。令[a, b, c]为logit向量。如果我们不使用标签平滑，那么标签向量就是一个one-hot向量[1,0,0]。我们的模型将a≫b和a≫c。例如,应用softmax分对数向量(10,0,0)给(0.9999,0,0)的4位小数。</p>
<p>如果我们使用标签的平滑与α= 0.1,平滑标签向量≈(0.9333,0.0333,0.0333)。logit向量[3.3322,0,0]在softmax之后将经过平滑处理的标签向量近似为小数点后4位，并且它的差距更小。这就是为什么我们称平滑标签为一种正则化技术，因为它可以防止最大的logit变得比其他的更大。</p>
<p>更形象地说，对于label_smoothing=0.2，则意味着标签0的概率是0.1，标签1的概率是0.9。</p>
<p>另一种解释（来自<a href="https://zhuanlan.zhihu.com/p/76587755" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/76587755</a>）：</p>
<p>​    在常见的多分类问题中，先经过softmax处理后进行交叉熵计算，原理很简单可以将计算loss理解为，为了使得网络对测试集预测的概率分布和其真实分布接近，常用的做法是使用one-hot对真实标签进行编码，作者认为这种将标签强制one-hot的方式使网络过于自信会导致过拟合，因此软化这种编码方式：$q^{\prime}(k | x)=(1-\epsilon) \delta_{k, y}+\epsilon u(k)​$</p>
<p>​    等号左侧：是一种新的预测的分布；等号右侧：前半部分是对原分布乘一个权重，$\epsilon$是一个超参，需要自己设定，取值在0到1范围内。后半部分u是一个均匀分布，k表示模型的类别数。</p>
<p>​    由以上公式可以看出，这种方式使label有$\epsilon$概率来自于均匀分布， $1-\epsilon$概率来自于原分布。这就相当于在原label上增加噪声，让模型的预测值不要过度集中于概率较高的类别，把一些概率放在概率较低的类别。因此，交叉熵可以替换为：$H\left(q^{\prime}, p\right)=-\sum_{k=1}^{K} \log p(k) q^{\prime}(k)=(1-\epsilon) H(q, p)+\epsilon H(u, p)$</p>
<p>​    可以理解为：loss为对<strong>“预测的分布与真实分布”</strong>及<strong>“预测分布与先验分布（均匀分布）”</strong>的惩罚。个人理解：label smooth的思路“做软化、防止过拟合、增加扰动”是好的，个人认为用均匀分布做先验分布有待商榷。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="majsunflower.cn/2020/03/10/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" data-id="ckmklrgwl000o27wv1q376npw"
         class="article-share-link">分享</a>
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li></ul>

    </footer>

  </div>

  
    
  <nav class="article-nav">
    
      <a href="/2020/03/11/%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%A8%A1%E7%B3%8A%E5%8C%B9%E9%85%8D%E7%9A%84%E6%96%B9%E6%B3%95%E9%83%BD%E6%9C%89%E5%93%AA%E4%BA%9B/" class="article-nav-link">
        <strong class="article-nav-caption">前一篇</strong>
        <div class="article-nav-title">
          
            字符串模糊匹配的方法都有哪些
          
        </div>
      </a>
    
    
      <a href="/2020/03/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E3%80%8AReformer-The-Efficient-Transformer%E3%80%8B/" class="article-nav-link">
        <strong class="article-nav-caption">后一篇</strong>
        <div class="article-nav-title">论文阅读：《Reformer: The Efficient Transformer》</div>
      </a>
    
  </nav>


  

  
    
  <div class="gitalk" id="gitalk-container"></div>
  
<link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">

  
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

  
<script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>

  <script type="text/javascript">
    var gitalk = new Gitalk({
      clientID: 'a0115c330d8e2a88dc59',
      clientSecret: '2e456ec13123a898d7b34ad8e117f543a6f379ea',
      repo: 'majing2019.github.io',
      owner: 'majing2019',
      admin: ['majing2019'],
      // id: location.pathname,      // Ensure uniqueness and length less than 50
      id: md5(location.pathname),
      distractionFreeMode: false,  // Facebook-like distraction free mode
      pagerDirection: 'last'
    })

  gitalk.render('gitalk-container')
  </script>

  

</article>



</section>
  <footer class="footer">
  <div class="outer">
    <div class="float-right">
      <ul class="list-inline">
  
    <li><i class="fe fe-smile-alt"></i> <span id="busuanzi_value_site_uv"></span></li>
  
    <li><i class="fe fe-bookmark"></i> <span id="busuanzi_value_page_pv"></span></li>
  
</ul>
    </div>
    <ul class="list-inline">
      <li>&copy; 2021 大嘴怪的小世界</li>
      <li>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></li>
      <li>Theme  <a href="https://github.com/zhwangart/hexo-theme-ocean" target="_blank" rel="noopener">Ocean</a></li>
    </ul>
  </div>
</footer>

</main>
<aside class="sidebar">
  <button class="navbar-toggle"></button>
<nav class="navbar">
  
    <div class="logo">
      <a href="/"><img src="/images/shark.svg" alt="大嘴怪的小世界"></a>
    </div>
  
  <ul class="nav nav-main">
    
      <li class="nav-item">
        <a class="nav-item-link" href="/">主页</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/archives">归档</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/gallery">相册</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/about">关于</a>
      </li>
    
    <li class="nav-item">
      <a class="nav-item-link nav-item-search" title="搜索">
        <i class="fe fe-search"></i>
        搜索
      </a>
    </li>
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      <div class="totop" id="totop">
  <i class="fe fe-rocket"></i>
</div>
    </li>
    <li class="nav-item">
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
</aside>

<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>


  
<script src="/fancybox/jquery.fancybox.min.js"></script>




  
<script src="/js/tocbot.min.js"></script>

  <script>
    // Tocbot_v4.7.0  http://tscanlin.github.io/tocbot/
    tocbot.init({
      tocSelector: '.tocbot',
      contentSelector: '.article-entry',
      headingSelector: 'h1, h2, h3, h4, h5, h6',
      hasInnerContainers: true,
      scrollSmooth: true,
      positionFixedSelector: '.tocbot',
      positionFixedClass: 'is-position-fixed',
      fixedSidebarOffset: 'auto',
    });
  </script>




<script src="/js/ocean.js"></script>


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
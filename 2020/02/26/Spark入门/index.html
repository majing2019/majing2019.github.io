<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  
  
  
    <meta name="description" content="记录生活">
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <title>
    Spark入门 |
    
    大嘴怪的小世界</title>
  
    <link rel="shortcut icon" href="/favicon.ico">
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<script src="/js/pace.min.js"></script>

<meta name="generator" content="Hexo 4.2.1"></head>

<body>
<main class="content">
  <section class="outer">
  

<article id="post-Spark入门" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Spark入门
    </h1>
  
  




      </header>
    

    
      <div class="article-meta">
        <a href="/2020/02/26/Spark%E5%85%A5%E9%97%A8/" class="article-date">
  <time datetime="2020-02-26T14:47:44.000Z" itemprop="datePublished">2020-02-26</time>
</a>
        
      </div>
    

    
      
    <div class="tocbot"></div>





    

    <div class="article-entry" itemprop="articleBody">
      


      

      
        <p>最近需要用spark比较多，重新学习一下。今天先学习一些基础。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://classroom.udacity.com/courses/ud2002" target="_blank" rel="noopener">https://classroom.udacity.com/courses/ud2002</a></p>
</blockquote>
<h1 id="Spark处理数据"><a href="#Spark处理数据" class="headerlink" title="Spark处理数据"></a>Spark处理数据</h1><h2 id="函数式编程"><a href="#函数式编程" class="headerlink" title="函数式编程"></a>函数式编程</h2><p>首先用下图来看一下，函数式编程和过程式编程的区别。<img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/maj1.png" alt="图片"></p>
<p>函数式编程非常适合分布式系统。Python并不是函数编程语言，但使用PySparkAPI 可以让你编写Spark程序，并确保你的代码使用了函数式编程。在底层，Python 代码使用 py4j 来调用 Java 虚拟机(JVM)。</p>
<p>假设有下面一段代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">log_of_songs &#x3D; [</span><br><span class="line">        &quot;Despacito&quot;,</span><br><span class="line">        &quot;Nice for what&quot;,</span><br><span class="line">        &quot;No tears left to cry&quot;,</span><br><span class="line">        &quot;Despacito&quot;,</span><br><span class="line">        &quot;Havana&quot;,</span><br><span class="line">        &quot;In my feelings&quot;,</span><br><span class="line">        &quot;Nice for what&quot;,</span><br><span class="line">        &quot;Despacito&quot;,</span><br><span class="line">        &quot;All the stars&quot;</span><br><span class="line">]</span><br><span class="line">play_count &#x3D; 0</span><br><span class="line">def count_plays(song_title):</span><br><span class="line">    global play_count</span><br><span class="line">    for song in log_of_songs:</span><br><span class="line">        if song &#x3D;&#x3D; song_title:</span><br><span class="line">            play_count &#x3D; play_count + 1</span><br><span class="line">    return play_count</span><br></pre></td></tr></table></figure>
<p>调用两次count_plays(“Despacito”)会得到不同的结果，这是因为play_count是作为全局变量，在函数内部进行了修改。解决这个问题可以采用如下代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def count_plays(song_title, play_count):</span><br><span class="line">    for song in log_of_songs:</span><br><span class="line">        if song &#x3D;&#x3D; song_title:</span><br><span class="line">            play_count &#x3D; play_count + 1</span><br><span class="line">    return play_count</span><br></pre></td></tr></table></figure>
<p>这就是Spark解决问题的方式。<br>在Spark中我们使用Pure Function（纯函数），就像面包制造厂，不同的面包机器之间是互不干扰的，且不会损坏原材料。Spark会在函数执行前，将数据复制多分，以输入到不同函数中。为了防止内存溢出，Spark会在代码中建立一个数据的有向无环图，在运行前检查是否有必要对某一分数据进行复制。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/maj2.png" alt="图片"></p>
<h2 id="运行时参数设置"><a href="#运行时参数设置" class="headerlink" title="运行时参数设置"></a>运行时参数设置</h2><blockquote>
<p>参考：<br><a href="https://tech.meituan.com/2016/04/29/spark-tuning-basic.html" target="_blank" rel="noopener">https://tech.meituan.com/2016/04/29/spark-tuning-basic.html</a><br><a href="https://spark.apache.org/docs/1.6.1/running-on-yarn.html" target="_blank" rel="noopener">https://spark.apache.org/docs/1.6.1/running-on-yarn.html</a></p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line">    --master yarn \</span><br><span class="line">    --deploy-mode cluster  \</span><br><span class="line">    --num-executors 100 \</span><br><span class="line">    --driver-memory 2g \</span><br><span class="line">    --executor-memory 14g \</span><br><span class="line">    --executor-cores 6 \</span><br><span class="line">    --conf spark.default.parallelism&#x3D;1000 \</span><br><span class="line">    --conf spark.storage.memoryFraction&#x3D;0.2 \</span><br><span class="line">    --conf spark.shuffle.memoryFraction&#x3D;0.6 \</span><br><span class="line">    --conf spark.executor.extraJavaOptions&#x3D;&#39;-Dlog4j.configuration&#x3D;log4j.properties&#39; \</span><br><span class="line">    --driver-java-options -Dlog4j.configuration&#x3D;log4j.properties \</span><br><span class="line">    python文件  \</span><br></pre></td></tr></table></figure>
<ul>
<li>spark-submit: which spark-submit 查看该命令是 spark 系统的还是 pyspark 包自带的，应该使用 spark 系统的<ul>
<li>master:</li>
<li>standaloone: spark 自带的集群资源管理器</li>
<li>yarn</li>
<li>local: 本地运行</li>
</ul>
</li>
<li>deploy-mode：<ul>
<li>client: driver 在本机上，能够直接使用本机文件系统</li>
<li>cluster: driver 指不定在哪台机器上，不能读取本机文件系统</li>
</ul>
</li>
<li>spark 运行时配置：主要的有运行内存和节点数量:<ul>
<li>num_executors</li>
<li>spark_driver_memory</li>
<li>spark_executor_memory</li>
</ul>
</li>
<li>addFiles 与 —files（将需要使用的文件分发到每台机器上）：<ul>
<li>addFiles()：能够分发到每台机器上,包括 driver 上</li>
<li>—files: 只能分发到 executor 上</li>
</ul>
</li>
<li>引用其他模块的问题：<ul>
<li>第三方库：需要将第三方库打包上传供使用</li>
<li>自己的模块：也需要打包上传,以供使用</li>
</ul>
</li>
<li>运行下面前请确认<ul>
<li>export SPARK_HOME=…../spark-1.6.2-bin-hadoop2.6</li>
<li>export PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH</li>
<li>export JAVA_HOME=…/jdk1.8.0_60</li>
<li>PYSPARK_PYTHON=./NLTK/conda-env/bin/python spark-submit —conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./NLTK/conda-env/bin/python —master yarn-cluster —archives conda-env.zip#NLTK clean_step_two.py</li>
</ul>
</li>
</ul>
<h2 id="Maps和Lambda"><a href="#Maps和Lambda" class="headerlink" title="Maps和Lambda"></a>Maps和Lambda</h2><blockquote>
<p>lambda函数起源：<br><a href="http://palmstroem.blogspot.com/2012/05/lambda-calculus-for-absolute-dummies.html" target="_blank" rel="noopener">http://palmstroem.blogspot.com/2012/05/lambda-calculus-for-absolute-dummies.html</a></p>
</blockquote>
<p>Maps会复制原始数据，并把副本数据按照Maps中的函数进行转换。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">from pyspark import SparkContext</span><br><span class="line">log_of_songs &#x3D; [</span><br><span class="line">    &quot;Despacito&quot;,</span><br><span class="line">    &quot;Nice for what&quot;,</span><br><span class="line">    &quot;No tears left to cry&quot;,</span><br><span class="line">    &quot;Despacito&quot;,</span><br><span class="line">    &quot;Havana&quot;,</span><br><span class="line">    &quot;In my feelings&quot;,</span><br><span class="line">    &quot;Nice for what&quot;,</span><br><span class="line">    &quot;Despacito&quot;,</span><br><span class="line">    &quot;All the stars&quot;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">def convert_song_to_lowercase(song):</span><br><span class="line">    return song.lower()</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">    conf &#x3D; SparkConf()</span><br><span class="line">    conf.setAppName(&quot;Testing&quot;).setMaster(&quot;local&quot;)</span><br><span class="line">    sc &#x3D; SparkContext(conf&#x3D;conf)</span><br><span class="line">    sc.setLogLevel(&quot;WARN&quot;)</span><br><span class="line"></span><br><span class="line">    # parallelize将对象分配到不同节点上</span><br><span class="line">    distributed_song_log &#x3D; sc.parallelize(log_of_songs)</span><br><span class="line">    # 定义不同节点的所有数据执行convert_song_to_lowercase的操作</span><br><span class="line">    # 但此时spark还未执行，它在等待所有定义结束后，看是否可以优化某些操作</span><br><span class="line">    distributed_song_log.map(convert_song_to_lowercase)</span><br><span class="line">    # 如果想强制spark执行，则可以使用collect，则会将所有数据汇总</span><br><span class="line">    # 注意此时spark并没有改变原始数据的大小写，它将原始数据进行了拷贝，再做的处理</span><br><span class="line">    distributed_song_log.collect()</span><br><span class="line">    # 也可以使用python的匿名函数进行map</span><br><span class="line">    distributed_song_log.map(lambda song: song.lower()).collect()</span><br></pre></td></tr></table></figure>
<h2 id="Data-Frame"><a href="#Data-Frame" class="headerlink" title="Data Frame"></a>Data Frame</h2><p>数据处理有两种方式，一种使用Data Frame和Python进行命令式编程，另一种使用SQL进行声明式编程。命令式编程关注的是”How”，声明式编程关注的是”What”。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/maj3.png" alt="图片"></p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/maj4.png" alt="图片"></p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/maj5.png" alt="图片"></p>
<h3 id="Data-Frame的读取和写入"><a href="#Data-Frame的读取和写入" class="headerlink" title="Data Frame的读取和写入"></a>Data Frame的读取和写入</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import pyspark</span><br><span class="line">from pyspark import SparkConf</span><br><span class="line">from pyspark.sql import SparkSession</span><br><span class="line"></span><br><span class="line">spark &#x3D; SparkSession \</span><br><span class="line">    .builder \</span><br><span class="line">    .appName(&quot;Our first Python Spark SQL example&quot;) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"># 检查一下是否生效了。</span><br><span class="line">spark.sparkContext.getConf().getAll()</span><br><span class="line">path &#x3D; &quot;data&#x2F;sparkify_log_small.json&quot;</span><br><span class="line">user_log &#x3D; spark.read.json(path)</span><br><span class="line">user_log.printSchema()</span><br><span class="line">user_log.describe()</span><br><span class="line">user_log.show(n&#x3D;1)</span><br><span class="line"># 取数据的前5条</span><br><span class="line">user_log.take(5)</span><br><span class="line">out_path &#x3D; &quot;data&#x2F;sparkify_log_small.csv&quot;</span><br><span class="line">user_log.write.save(out_path, format&#x3D;&quot;csv&quot;, header&#x3D;True)</span><br><span class="line"># 读取另一个daraframe</span><br><span class="line">user_log_2 &#x3D; spark.read.csv(out_path, header&#x3D;True)</span><br><span class="line">user_log_2.printSchema()</span><br><span class="line">user_log_2.take(2)</span><br><span class="line">user_log_2.select(&quot;userID&quot;).show()</span><br></pre></td></tr></table></figure>
<h3 id="Data-Frame数据处理"><a href="#Data-Frame数据处理" class="headerlink" title="Data Frame数据处理"></a>Data Frame数据处理</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.sql import SparkSession</span><br><span class="line">from pyspark.sql.functions import udf</span><br><span class="line">from pyspark.sql.types import StringType</span><br><span class="line">from pyspark.sql.types import IntegerType</span><br><span class="line">from pyspark.sql.functions import desc</span><br><span class="line">from pyspark.sql.functions import asc</span><br><span class="line">from pyspark.sql.functions import sum as Fsum</span><br><span class="line"></span><br><span class="line">import datetime</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">%matplotlib inline</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">spark &#x3D; SparkSession \</span><br><span class="line">    .builder \</span><br><span class="line">    .appName(&quot;Wrangling Data&quot;) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line">path &#x3D; &quot;data&#x2F;sparkify_log_small.json&quot;</span><br><span class="line">user_log &#x3D; spark.read.json(path)</span><br><span class="line"># 数据搜索</span><br><span class="line">user_log.take(5)</span><br><span class="line">user_log.printSchema()</span><br><span class="line">user_log.describe().show()</span><br><span class="line">user_log.describe(&quot;artist&quot;).show()</span><br><span class="line">user_log.describe(&quot;sessionId&quot;).show()</span><br><span class="line">user_log.count()</span><br><span class="line">user_log.select(&quot;page&quot;).dropDuplicates().sort(&quot;page&quot;).show()</span><br><span class="line">user_log.select([&quot;userId&quot;, &quot;firstname&quot;, &quot;page&quot;, &quot;song&quot;]).where(user_log.userId &#x3D;&#x3D; &quot;1046&quot;).collect()</span><br><span class="line"># 按小时统计数据</span><br><span class="line">get_hour &#x3D; udf(lambda x: datetime.datetime.fromtimestamp(x &#x2F; 1000.0). hour)</span><br><span class="line">user_log &#x3D; user_log.withColumn(&quot;hour&quot;, get_hour(user_log.ts))</span><br><span class="line">user_log.head()</span><br><span class="line">songs_in_hour &#x3D; user_log.filter(user_log.page &#x3D;&#x3D; &quot;NextSong&quot;).groupby(user_log.hour).count().orderBy(user_log.hour.cast(&quot;float&quot;))</span><br><span class="line">songs_in_hour.show()</span><br><span class="line">songs_in_hour_pd &#x3D; songs_in_hour.toPandas()</span><br><span class="line">songs_in_hour_pd.hour &#x3D; pd.to_numeric(songs_in_hour_pd.hour)</span><br><span class="line">plt.scatter(songs_in_hour_pd[&quot;hour&quot;], songs_in_hour_pd[&quot;count&quot;])</span><br><span class="line">plt.xlim(-1, 24);</span><br><span class="line">plt.ylim(0, 1.2 * max(songs_in_hour_pd[&quot;count&quot;]))</span><br><span class="line">plt.xlabel(&quot;Hour&quot;)</span><br><span class="line">plt.ylabel(&quot;Songs played&quot;);</span><br><span class="line"></span><br><span class="line"># 删除空值的行</span><br><span class="line">user_log_valid &#x3D; user_log.dropna(how &#x3D; &quot;any&quot;, subset &#x3D; [&quot;userId&quot;, &quot;sessionId&quot;])</span><br><span class="line">user_log_valid.count()</span><br><span class="line">user_log.select(&quot;userId&quot;).dropDuplicates().sort(&quot;userId&quot;).show()</span><br><span class="line">user_log_valid &#x3D; user_log_valid.filter(user_log_valid[&quot;userId&quot;] !&#x3D; &quot;&quot;)</span><br><span class="line">user_log_valid.count()</span><br><span class="line"># 降级服务的用户</span><br><span class="line">user_log_valid.filter(&quot;page &#x3D; &#39;Submit Downgrade&#39;&quot;).show()</span><br><span class="line">user_log.select([&quot;userId&quot;, &quot;firstname&quot;, &quot;page&quot;, &quot;level&quot;, &quot;song&quot;]).where(user_log.userId &#x3D;&#x3D; &quot;1138&quot;).collect()</span><br><span class="line">flag_downgrade_event &#x3D; udf(lambda x: 1 if x &#x3D;&#x3D; &quot;Submit Downgrade&quot; else 0, IntegerType())</span><br><span class="line">user_log_valid &#x3D; user_log_valid.withColumn(&quot;downgraded&quot;, flag_downgrade_event(&quot;page&quot;))</span><br><span class="line">user_log_valid.head()</span><br><span class="line">from pyspark.sql import Window</span><br><span class="line">windowval &#x3D; Window.partitionBy(&quot;userId&quot;).orderBy(desc(&quot;ts&quot;)).rangeBetween(Window.unboundedPreceding, 0)</span><br><span class="line">user_log_valid &#x3D; user_log_valid.withColumn(&quot;phase&quot;, Fsum(&quot;downgraded&quot;).over(windowval))</span><br><span class="line">user_log_valid.select([&quot;userId&quot;, &quot;firstname&quot;, &quot;ts&quot;, &quot;page&quot;, &quot;level&quot;, &quot;phase&quot;]).where(user_log.userId &#x3D;&#x3D; &quot;1138&quot;).sort(&quot;ts&quot;).collect()</span><br></pre></td></tr></table></figure>
<h2 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.sql import SparkSession</span><br><span class="line">from pyspark.sql.functions import udf</span><br><span class="line">from pyspark.sql.types import StringType</span><br><span class="line">from pyspark.sql.types import IntegerType</span><br><span class="line">from pyspark.sql.functions import desc</span><br><span class="line">from pyspark.sql.functions import asc</span><br><span class="line">from pyspark.sql.functions import sum as Fsum</span><br><span class="line"></span><br><span class="line">import datetime</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">%matplotlib inline</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">spark &#x3D; SparkSession \</span><br><span class="line">    .builder \</span><br><span class="line">    .appName(&quot;Data wrangling with Spark SQL&quot;) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line">path &#x3D; &quot;data&#x2F;sparkify_log_small.json&quot;</span><br><span class="line">user_log &#x3D; spark.read.json(path)</span><br><span class="line"></span><br><span class="line">user_log.take(1)</span><br><span class="line"># 下面的代码创建了一个临时视图，你可以使用该视图运行 SQL 查询</span><br><span class="line">user_log.createOrReplaceTempView(&quot;user_log_table&quot;)</span><br><span class="line">spark.sql(&quot;SELECT * FROM user_log_table LIMIT 2&quot;).show()</span><br><span class="line">spark.sql(&#39;&#39;&#39;</span><br><span class="line">          SELECT * </span><br><span class="line">          FROM user_log_table </span><br><span class="line">          LIMIT 2</span><br><span class="line">          &#39;&#39;&#39;</span><br><span class="line">          ).show()</span><br><span class="line">spark.sql(&#39;&#39;&#39;</span><br><span class="line">          SELECT COUNT(*) </span><br><span class="line">          FROM user_log_table </span><br><span class="line">          &#39;&#39;&#39;</span><br><span class="line">          ).show()</span><br><span class="line">spark.sql(&#39;&#39;&#39;</span><br><span class="line">          SELECT userID, firstname, page, song</span><br><span class="line">          FROM user_log_table </span><br><span class="line">          WHERE userID &#x3D;&#x3D; &#39;1046&#39;</span><br><span class="line">          &#39;&#39;&#39;</span><br><span class="line">          ).collect()</span><br><span class="line">spark.sql(&#39;&#39;&#39;</span><br><span class="line">          SELECT DISTINCT page</span><br><span class="line">          FROM user_log_table </span><br><span class="line">          ORDER BY page ASC</span><br><span class="line">          &#39;&#39;&#39;</span><br><span class="line">          ).show()</span><br><span class="line"></span><br><span class="line"># 自定义函数</span><br><span class="line">spark.udf.register(&quot;get_hour&quot;, lambda x: int(datetime.datetime.fromtimestamp(x &#x2F; 1000.0).hour))</span><br><span class="line">spark.sql(&#39;&#39;&#39;</span><br><span class="line">          SELECT *, get_hour(ts) AS hour</span><br><span class="line">          FROM user_log_table </span><br><span class="line">          LIMIT 1</span><br><span class="line">          &#39;&#39;&#39;</span><br><span class="line">          ).collect()</span><br><span class="line">songs_in_hour &#x3D; spark.sql(&#39;&#39;&#39;</span><br><span class="line">          SELECT get_hour(ts) AS hour, COUNT(*) as plays_per_hour</span><br><span class="line">          FROM user_log_table</span><br><span class="line">          WHERE page &#x3D; &quot;NextSong&quot;</span><br><span class="line">          GROUP BY hour</span><br><span class="line">          ORDER BY cast(hour as int) ASC</span><br><span class="line">          &#39;&#39;&#39;</span><br><span class="line">          )</span><br><span class="line">songs_in_hour.show()</span><br><span class="line"># 用 Pandas 转换数据</span><br><span class="line">songs_in_hour_pd &#x3D; songs_in_hour.toPandas()</span><br><span class="line">print(songs_in_hour_pd)</span><br></pre></td></tr></table></figure>
<h2 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h2><blockquote>
<p>参考：<br><a href="https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html" target="_blank" rel="noopener">https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html</a><br><a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/rdd-programming-guide.html</a></p>
</blockquote>
<p>不管使用Pyspark还是其他语言，Spark的底层都会通过Catalyst转成执行DAG序列：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/spark1.png" alt="图片"></p>
<p>DAG在底层使用RDD对象进行操作。</p>
<h1 id="Spark中的机器学习"><a href="#Spark中的机器学习" class="headerlink" title="Spark中的机器学习"></a>Spark中的机器学习</h1><h2 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.sql import SparkSession</span><br><span class="line">from pyspark.ml.feature import RegexTokenizer, VectorAssembler, Normalizer, StandardScaler</span><br><span class="line">from pyspark.sql.functions import udf</span><br><span class="line">from pyspark.sql.types import IntegerType</span><br><span class="line"></span><br><span class="line">import re</span><br><span class="line"># create a SparkSession: note this step was left out of the screencast</span><br><span class="line">spark &#x3D; SparkSession.builder \</span><br><span class="line">    .master(&quot;local&quot;) \</span><br><span class="line">    .appName(&quot;Word Count&quot;) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line"># 如何读取数据集</span><br><span class="line">stack_overflow_data &#x3D; &#39;Train_onetag_small.json&#39;</span><br><span class="line">df &#x3D; spark.read.json(stack_overflow_data)</span><br><span class="line">df.head()</span><br><span class="line"># 把字符串分为单独的单词。Spark有一个[Tokenizer]（https:&#x2F;&#x2F;spark.apache.org&#x2F;docs&#x2F;latest&#x2F;ml-features.html#tokenizer） 类以及RegexTokenizer。 后者在分词时有更大的自由度。</span><br><span class="line"># split the body text into separate words</span><br><span class="line">regexTokenizer &#x3D; RegexTokenizer(inputCol&#x3D;&quot;Body&quot;, outputCol&#x3D;&quot;words&quot;, pattern&#x3D;&quot;\\W&quot;)</span><br><span class="line">df &#x3D; regexTokenizer.transform(df)</span><br><span class="line">df.head()</span><br><span class="line"># count the number of words in each body tag</span><br><span class="line">body_length &#x3D; udf(lambda x: len(x), IntegerType())</span><br><span class="line">df &#x3D; df.withColumn(&quot;BodyLength&quot;, body_length(df.words))</span><br><span class="line"># count the number of paragraphs and links in each body tag</span><br><span class="line">number_of_paragraphs &#x3D; udf(lambda x: len(re.findall(&quot;&lt;&#x2F;p&gt;&quot;, x)), IntegerType())</span><br><span class="line">number_of_links &#x3D; udf(lambda x: len(re.findall(&quot;&lt;&#x2F;a&gt;&quot;, x)), IntegerType())</span><br><span class="line">df &#x3D; df.withColumn(&quot;NumParagraphs&quot;, number_of_paragraphs(df.Body))</span><br><span class="line">df &#x3D; df.withColumn(&quot;NumLinks&quot;, number_of_links(df.Body))</span><br><span class="line">df.head(2)</span><br><span class="line"># 将内容长度，段落数和内容中的链接数合并为一个向量</span><br><span class="line">assembler &#x3D; VectorAssembler(inputCols&#x3D;[&quot;BodyLength&quot;, &quot;NumParagraphs&quot;, &quot;NumLinks&quot;], outputCol&#x3D;&quot;NumFeatures&quot;)</span><br><span class="line">df &#x3D; assembler.transform(df)</span><br><span class="line">df.head()</span><br><span class="line"># 归一化向量</span><br><span class="line">scaler &#x3D; Normalizer(inputCol&#x3D;&quot;NumFeatures&quot;, outputCol&#x3D;&quot;ScaledNumFeatures&quot;)</span><br><span class="line">df &#x3D; scaler.transform(df)</span><br><span class="line">df.head(2)</span><br><span class="line"># 缩放向量</span><br><span class="line">scaler2 &#x3D; StandardScaler(inputCol&#x3D;&quot;NumFeatures&quot;, outputCol&#x3D;&quot;ScaledNumFeatures2&quot;, withStd&#x3D;True)</span><br><span class="line">scalerModel &#x3D; scaler2.fit(df)</span><br><span class="line">df &#x3D; scalerModel.transform(df)</span><br><span class="line">df.head(2)</span><br></pre></td></tr></table></figure>
<h2 id="文本特征"><a href="#文本特征" class="headerlink" title="文本特征"></a>文本特征</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.sql import SparkSession</span><br><span class="line">from pyspark.ml.feature import RegexTokenizer, CountVectorizer, \</span><br><span class="line">    IDF, StringIndexer</span><br><span class="line">from pyspark.sql.functions import udf</span><br><span class="line">from pyspark.sql.types import IntegerType</span><br><span class="line"></span><br><span class="line">import re</span><br><span class="line"># create a SparkSession: note this step was left out of the screencast</span><br><span class="line">spark &#x3D; SparkSession.builder \</span><br><span class="line">    .master(&quot;local&quot;) \</span><br><span class="line">    .appName(&quot;Word Count&quot;) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line"># 如何读取数据集</span><br><span class="line">stack_overflow_data &#x3D; &#39;Train_onetag_small.json&#39;</span><br><span class="line">df &#x3D; spark.read.json(stack_overflow_data)</span><br><span class="line">df.head()</span><br><span class="line"></span><br><span class="line"># 分词将字符串拆分为单独的单词。Spark 有一个[Tokenizer] （https:&#x2F;&#x2F;spark.apache.org&#x2F;docs&#x2F;latest&#x2F;ml-features.html#tokenizer） 类和RegexTokenizer。后者在分词时有更大的自由度 。</span><br><span class="line"># split the body text into separate words</span><br><span class="line">regexTokenizer &#x3D; RegexTokenizer(inputCol&#x3D;&quot;Body&quot;, outputCol&#x3D;&quot;words&quot;, pattern&#x3D;&quot;\\W&quot;)</span><br><span class="line">df &#x3D; regexTokenizer.transform(df)</span><br><span class="line">df.head()</span><br><span class="line"></span><br><span class="line"># CountVectorizer</span><br><span class="line"># find the term frequencies of the words</span><br><span class="line">cv &#x3D; CountVectorizer(inputCol&#x3D;&quot;words&quot;, outputCol&#x3D;&quot;TF&quot;, vocabSize&#x3D;1000)</span><br><span class="line">cvmodel &#x3D; cv.fit(df)</span><br><span class="line">df &#x3D; cvmodel.transform(df)</span><br><span class="line">df.take(1)</span><br><span class="line"># show the vocabulary in order of </span><br><span class="line">cvmodel.vocabulary</span><br><span class="line"># show the last 10 terms in the vocabulary</span><br><span class="line">cvmodel.vocabulary[-10:]</span><br><span class="line"></span><br><span class="line"># 逆文本频率指数（Inter-document Frequency ）</span><br><span class="line">idf &#x3D; IDF(inputCol&#x3D;&quot;TF&quot;, outputCol&#x3D;&quot;TFIDF&quot;)</span><br><span class="line">idfModel &#x3D; idf.fit(df)</span><br><span class="line">df &#x3D; idfModel.transform(df)</span><br><span class="line">df.head()</span><br><span class="line"></span><br><span class="line"># StringIndexer</span><br><span class="line">indexer &#x3D; StringIndexer(inputCol&#x3D;&quot;oneTag&quot;, outputCol&#x3D;&quot;label&quot;)</span><br><span class="line">df &#x3D; indexer.fit(df).transform(df)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="majsunflower.cn/2020/02/26/Spark%E5%85%A5%E9%97%A8/" data-id="ckab1p2n80009jhwv9fi9cuh9"
         class="article-share-link">分享</a>
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/" rel="tag">spark</a></li></ul>

    </footer>

  </div>

  
    
  <nav class="article-nav">
    
      <a href="/2020/03/03/PLY%E6%95%99%E7%A8%8B%E5%8F%8A%E4%BE%8B%E5%AD%90/" class="article-nav-link">
        <strong class="article-nav-caption">前一篇</strong>
        <div class="article-nav-title">
          
            PLY教程及例子
          
        </div>
      </a>
    
    
      <a href="/2020/02/16/%E9%9A%8F%E6%84%9F%E4%B8%80%E7%AF%87/" class="article-nav-link">
        <strong class="article-nav-caption">后一篇</strong>
        <div class="article-nav-title">一首小诗：做最好的自己</div>
      </a>
    
  </nav>


  

  
    
  <div class="gitalk" id="gitalk-container"></div>
  
<link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">

  
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

  
<script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>

  <script type="text/javascript">
    var gitalk = new Gitalk({
      clientID: 'a0115c330d8e2a88dc59',
      clientSecret: '2e456ec13123a898d7b34ad8e117f543a6f379ea',
      repo: 'majing2019.github.io',
      owner: 'majing2019',
      admin: ['majing2019'],
      // id: location.pathname,      // Ensure uniqueness and length less than 50
      id: md5(location.pathname),
      distractionFreeMode: false,  // Facebook-like distraction free mode
      pagerDirection: 'last'
    })

  gitalk.render('gitalk-container')
  </script>

  

</article>



</section>
  <footer class="footer">
  <div class="outer">
    <div class="float-right">
      <ul class="list-inline">
  
    <li><i class="fe fe-smile-alt"></i> <span id="busuanzi_value_site_uv"></span></li>
  
    <li><i class="fe fe-bookmark"></i> <span id="busuanzi_value_page_pv"></span></li>
  
</ul>
    </div>
    <ul class="list-inline">
      <li>&copy; 2020 大嘴怪的小世界</li>
      <li>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></li>
      <li>Theme  <a href="https://github.com/zhwangart/hexo-theme-ocean" target="_blank" rel="noopener">Ocean</a></li>
    </ul>
  </div>
</footer>

</main>
<aside class="sidebar">
  <button class="navbar-toggle"></button>
<nav class="navbar">
  
    <div class="logo">
      <a href="/"><img src="/images/shark.svg" alt="大嘴怪的小世界"></a>
    </div>
  
  <ul class="nav nav-main">
    
      <li class="nav-item">
        <a class="nav-item-link" href="/">主页</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/archives">归档</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/gallery">相册</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/about">关于</a>
      </li>
    
    <li class="nav-item">
      <a class="nav-item-link nav-item-search" title="搜索">
        <i class="fe fe-search"></i>
        搜索
      </a>
    </li>
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      <div class="totop" id="totop">
  <i class="fe fe-rocket"></i>
</div>
    </li>
    <li class="nav-item">
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
</aside>

<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>


  
<script src="/fancybox/jquery.fancybox.min.js"></script>




  
<script src="/js/tocbot.min.js"></script>

  <script>
    // Tocbot_v4.7.0  http://tscanlin.github.io/tocbot/
    tocbot.init({
      tocSelector: '.tocbot',
      contentSelector: '.article-entry',
      headingSelector: 'h1, h2, h3, h4, h5, h6',
      hasInnerContainers: true,
      scrollSmooth: true,
      positionFixedSelector: '.tocbot',
      positionFixedClass: 'is-position-fixed',
      fixedSidebarOffset: 'auto',
    });
  </script>




<script src="/js/ocean.js"></script>


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
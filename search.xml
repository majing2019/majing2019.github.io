<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>字符串模糊匹配的方法都有哪些</title>
    <url>/2020/03/11/%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%A8%A1%E7%B3%8A%E5%8C%B9%E9%85%8D%E7%9A%84%E6%96%B9%E6%B3%95%E9%83%BD%E6%9C%89%E5%93%AA%E4%BA%9B/</url>
    <content><![CDATA[<p>工作中经常遇到文本处理上的两个问题，一个是如何在长的文本串中找到跟短文本串最像的子串；另一个是如何将两个文本串进行对齐，忽略掉其中不同的部分。准备专门写一个工具来解决这些问题，因此先调研了模糊匹配和字符串对齐的工具。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://zhuanlan.zhihu.com/p/53135935" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/53135935</a><br><a href="https://www.thinbug.com/q/17740833" target="_blank" rel="noopener">https://www.thinbug.com/q/17740833</a><br><a href="https://github.com/eseraygun/python-alignment" target="_blank" rel="noopener">https://github.com/eseraygun/python-alignment</a><br><a href="https://pypi.org/project/StringDist/" target="_blank" rel="noopener">https://pypi.org/project/StringDist/</a><br><a href="https://pypi.org/project/edlib/" target="_blank" rel="noopener">https://pypi.org/project/edlib/</a><br><a href="https://pypi.org/project/strsimpy/" target="_blank" rel="noopener">https://pypi.org/project/strsimpy/</a><br><a href="https://github.com/gfairchild/pyxDamerauLevenshtein" target="_blank" rel="noopener">https://github.com/gfairchild/pyxDamerauLevenshtein</a><br><a href="https://github.com/mbreese/swalign/" target="_blank" rel="noopener">https://github.com/mbreese/swalign/</a><br>打印表格：<a href="https://pypi.org/project/tabulate/" target="_blank" rel="noopener">https://pypi.org/project/tabulate/</a><br><a href="https://pypi.org/project/weighted-levenshtein/" target="_blank" rel="noopener">https://pypi.org/project/weighted-levenshtein/</a><br><a href="https://pypi.org/project/nwalign/" target="_blank" rel="noopener">https://pypi.org/project/nwalign/</a><br><a href="https://pypi.org/project/pyhacrf-datamade/" target="_blank" rel="noopener">https://pypi.org/project/pyhacrf-datamade/</a><br>打印表格：<a href="https://pypi.org/project/Frmt/" target="_blank" rel="noopener">https://pypi.org/project/Frmt/</a></p>
</blockquote>
<h1 id="difflib"><a href="#difflib" class="headerlink" title="difflib"></a>difflib</h1><blockquote>
<p>参考：<br><a href="https://blog.csdn.net/Lockey23/article/details/77913855" target="_blank" rel="noopener">https://blog.csdn.net/Lockey23/article/details/77913855</a><br><a href="https://blog.csdn.net/gavin_john/article/details/78951698" target="_blank" rel="noopener">https://blog.csdn.net/gavin_john/article/details/78951698</a><br><a href="https://docs.python.org/3.5/library/difflib.html" target="_blank" rel="noopener">https://docs.python.org/3.5/library/difflib.html</a></p>
</blockquote>
<p>difflib模块提供的类和方法用来进行序列的差异化比较，它能够比对文件并生成差异结果文本或者html格式的差异化比较页面。</p>
<h2 id="SequenceMatcher"><a href="#SequenceMatcher" class="headerlink" title="SequenceMatcher"></a>SequenceMatcher</h2><p>SequenceMatcher类可以用来比较两个任意类型的数据，只要是可以哈希的。它使用一个算法来计算序列的最长连续子序列，并且忽略没有意义的“无用数据”。下面代码计算了模糊匹配的相似度：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; import difflib</span><br><span class="line">&gt;&gt;&gt; difflib.SequenceMatcher(None,&quot;amazing&quot;,&quot;amaging&quot;).ratio()</span><br><span class="line">0.8571428571428571</span><br></pre></td></tr></table></figure>
<p>其基本算法比Ratcliff和Obershelp在20世纪80年代末发表的“格式塔模式匹配”(gestalt pattern matching)算法更早，也更新奇。其思想是寻找不包含“垃圾”元素的最长连续匹配子序列;这些“垃圾”元素在某种意义上是无趣的，比如空白行或空白(垃圾信息处理是Ratcliff和Obershelp算法的扩展)。然后，将相同的思想递归地应用到匹配子序列的左子序列和右子序列。这不会产生最小的编辑序列，但是会产生人们“看起来正确”的匹配。<br>在时间复杂度上，基本的Ratcliff-Obershelp算法在最坏情况下是三次时间，在期望情况下是二次时间。SequenceMatcher是最坏情况下的二次时间，它的期望情况行为以一种复杂的方式依赖于序列有多少个公共元素;最好的情况是时间是线性的。</p>
<p>SequenceMatcher支持一种自动将某些序列项视为垃圾的启发式方法。启发式计算每个单独的项目在序列中出现的次数。如果一个项目的重复项(在第一个之后)占序列的1%以上，并且序列至少有200个项目长，则该项目将被标记为“popular”，并被视为垃圾，以便进行序列匹配。在创建SequenceMatcher时，可以通过将autojunk参数设置为False来关闭这种启发式。下面代码可以得到编辑距离的所有操作：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import difflib</span><br><span class="line">s1 &#x3D; [1, 2, 3, 5, 6, 4]</span><br><span class="line">s2 &#x3D; [2, 3, 5, 4, 6, 1]</span><br><span class="line"># 忽略所有空格</span><br><span class="line"># SequenceMatcher(lambda x: x &#x3D;&#x3D; &#39; &#39;, A, B)</span><br><span class="line">matcher &#x3D; difflib.SequenceMatcher(None, s1, s2)</span><br><span class="line">for tag, i1, i2, j1, j2 in reversed(matcher.get_opcodes()):</span><br><span class="line">    if tag &#x3D;&#x3D; &#39;delete&#39;:</span><br><span class="line">        print(&#39;Remove &#123;&#125; from positions [&#123;&#125;:&#123;&#125;]&#39;.format(</span><br><span class="line">            s1[i1:i2], i1, i2))</span><br><span class="line">        print(&#39;  before &#x3D;&#39;, s1)</span><br><span class="line">        del s1[i1:i2]</span><br><span class="line">    elif tag &#x3D;&#x3D; &#39;equal&#39;:</span><br><span class="line">        print(&#39;s1[&#123;&#125;:&#123;&#125;] and s2[&#123;&#125;:&#123;&#125;] are the same&#39;.format(</span><br><span class="line">            i1, i2, j1, j2))</span><br><span class="line">    elif tag &#x3D;&#x3D; &#39;insert&#39;:</span><br><span class="line">        print(&#39;Insert &#123;&#125; from s2[&#123;&#125;:&#123;&#125;] into s1 at &#123;&#125;&#39;.format(</span><br><span class="line">            s2[j1:j2], j1, j2, i1))</span><br><span class="line">        print(&#39;  before &#x3D;&#39;, s1)</span><br><span class="line">        s1[i1:i2] &#x3D; s2[j1:j2]</span><br><span class="line">    elif tag &#x3D;&#x3D; &#39;replace&#39;:</span><br><span class="line">        print((&#39;Replace &#123;&#125; from s1[&#123;&#125;:&#123;&#125;] &#39;</span><br><span class="line">               &#39;with &#123;&#125; from s2[&#123;&#125;:&#123;&#125;]&#39;).format(</span><br><span class="line">                   s1[i1:i2], i1, i2, s2[j1:j2], j1, j2))</span><br><span class="line">        print(&#39;  before &#x3D;&#39;, s1)</span><br><span class="line">        s1[i1:i2] &#x3D; s2[j1:j2]</span><br><span class="line">    print(&#39;   after &#x3D;&#39;, s1, &#39;\n&#39;)</span><br><span class="line">print(&#39;s1 &#x3D;&#x3D; s2:&#39;, s1 &#x3D;&#x3D; s2)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Replace [4] from s1[5:6] with [1] from s2[5:6]</span><br><span class="line">  before &#x3D; [1, 2, 3, 5, 6, 4]</span><br><span class="line">   after &#x3D; [1, 2, 3, 5, 6, 1] </span><br><span class="line"></span><br><span class="line">s1[4:5] and s2[4:5] are the same</span><br><span class="line">   after &#x3D; [1, 2, 3, 5, 6, 1] </span><br><span class="line"></span><br><span class="line">Insert [4] from s2[3:4] into s1 at 4</span><br><span class="line">  before &#x3D; [1, 2, 3, 5, 6, 1]</span><br><span class="line">   after &#x3D; [1, 2, 3, 5, 4, 6, 1] </span><br><span class="line"></span><br><span class="line">s1[1:4] and s2[0:3] are the same</span><br><span class="line">   after &#x3D; [1, 2, 3, 5, 4, 6, 1] </span><br><span class="line"></span><br><span class="line">Remove [1] from positions [0:1]</span><br><span class="line">  before &#x3D; [1, 2, 3, 5, 4, 6, 1]</span><br><span class="line">   after &#x3D; [2, 3, 5, 4, 6, 1]</span><br></pre></td></tr></table></figure>
<h2 id="get-matching-blocks"><a href="#get-matching-blocks" class="headerlink" title="get_matching_blocks"></a>get_matching_blocks</h2><p>返回匹配子序列的三元组列表。每个三元组的形式是(i, j, n)，表示a[i:i+n] == b[j:j+n]。在i和j中，三元组是单调递增的。最后一个三元组是一个哑元，它的值是(len(a)， len(b)， 0)，它是唯一一个n == 0的三元组。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; s &#x3D; SequenceMatcher(None, &quot;abxcd&quot;, &quot;abcd&quot;)</span><br><span class="line">&gt;&gt;&gt; s.get_matching_blocks()</span><br><span class="line">[Match(a&#x3D;0, b&#x3D;0, size&#x3D;2), Match(a&#x3D;3, b&#x3D;2, size&#x3D;2), Match(a&#x3D;5, b&#x3D;4, size&#x3D;0)]</span><br></pre></td></tr></table></figure>
<p>如果想获得所有match的子串，可以这样写：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import difflib</span><br><span class="line">def matches(large_string, query_string, threshold):</span><br><span class="line">    words &#x3D; large_string.split()</span><br><span class="line">    for word in words:</span><br><span class="line">        s &#x3D; difflib.SequenceMatcher(None, word, query_string)</span><br><span class="line">        match &#x3D; &#39;&#39;.join(word[i:i+n] for i, j, n in s.get_matching_blocks() if n)</span><br><span class="line">        if len(match) &#x2F; float(len(query_string)) &gt;&#x3D; threshold:</span><br><span class="line">            yield match</span><br><span class="line">large_string &#x3D; &quot;thelargemanhatanproject is a great project in themanhattincity&quot;</span><br><span class="line">query_string &#x3D; &quot;manhattan&quot;</span><br></pre></td></tr></table></figure>
<p>结果：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; print(list(matches(large_string, query_string, 0.8)))</span><br><span class="line">[&#39;manhatan&#39;, &#39;manhattn&#39;]</span><br></pre></td></tr></table></figure>
<h1 id="fuzzywuzzy"><a href="#fuzzywuzzy" class="headerlink" title="fuzzywuzzy"></a>fuzzywuzzy</h1><blockquote>
<p>参考：<br><a href="https://github.com/seatgeek/fuzzywuzzy" target="_blank" rel="noopener">https://github.com/seatgeek/fuzzywuzzy</a><br><a href="https://zhuanlan.zhihu.com/p/77166627" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/77166627</a><br><a href="https://blog.csdn.net/laobai1015/article/details/80451371" target="_blank" rel="noopener">https://blog.csdn.net/laobai1015/article/details/80451371</a><br><a href="https://stackoverflow.com/questions/48671270/use-sklearn-tfidfvectorizer-with-already-tokenized-inputs" target="_blank" rel="noopener">https://stackoverflow.com/questions/48671270/use-sklearn-tfidfvectorizer-with-already-tokenized-inputs</a><br><a href="https://github.com/ing-bank/sparse_dot_topn" target="_blank" rel="noopener">https://github.com/ing-bank/sparse_dot_topn</a></p>
</blockquote>
<p>FuzzyWuzzy 是一个简单易用的模糊字符串匹配工具包。它依据 Levenshtein Distance 算法 计算两个序列之间的差异。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from fuzzywuzzy import fuzz</span><br><span class="line">from fuzzywuzzy import process</span><br><span class="line"></span><br><span class="line"># 简单匹配</span><br><span class="line">fuzz.ratio(&quot;this is a test&quot;, &quot;this is a test!&quot;)</span><br><span class="line"></span><br><span class="line"># 非完全匹配</span><br><span class="line">fuzz.partial_ratio(&quot;this is a test&quot;, &quot;this is a test!&quot;)</span><br><span class="line"></span><br><span class="line"># 忽略顺序匹配</span><br><span class="line">fuzz.ratio(&quot;fuzzy wuzzy was a bear&quot;, &quot;wuzzy fuzzy was a bear&quot;)</span><br><span class="line">fuzz.token_sort_ratio(&quot;fuzzy wuzzy was a bear&quot;, &quot;wuzzy fuzzy was a bear&quot;)</span><br><span class="line"></span><br><span class="line"># 去重子集匹配</span><br><span class="line">fuzz.token_sort_ratio(&quot;fuzzy was a bear&quot;, &quot;fuzzy fuzzy was a bear&quot;)</span><br><span class="line">fuzz.token_set_ratio(&quot;fuzzy was a bear&quot;, &quot;fuzzy fuzzy was a bear&quot;)</span><br><span class="line"></span><br><span class="line"># 返回模糊匹配的字符串和相似度</span><br><span class="line">choices &#x3D; [&quot;Atlanta Falcons&quot;, &quot;New York Jets&quot;, &quot;New York Giants&quot;, &quot;Dallas Cowboys&quot;]</span><br><span class="line">process.extract(&quot;new york jets&quot;, choices, limit&#x3D;2)</span><br><span class="line"># 返回：[(&#39;New York Jets&#39;, 100), (&#39;New York Giants&#39;, 78)]</span><br><span class="line">process.extractOne(&quot;cowboys&quot;, choices)</span><br><span class="line"># 返回：(&quot;Dallas Cowboys&quot;, 90)</span><br><span class="line"># 可以传入附加参数到 extractOne 方法来设置使用特定的匹配模式，一个典型的用法是来匹配文件路径</span><br><span class="line">process.extractOne(&quot;System of a down - Hypnotize - Heroin&quot;, songs)</span><br><span class="line"># 返回：(&#39;&#x2F;music&#x2F;library&#x2F;good&#x2F;System of a Down&#x2F;2005 - Hypnotize&#x2F;01 - Attack.mp3&#39;, 86)</span><br><span class="line">process.extractOne(&quot;System of a down - Hypnotize - Heroin&quot;, songs, scorer&#x3D;fuzz.token_sort_ratio)</span><br><span class="line"># 返回：(&quot;&#x2F;music&#x2F;library&#x2F;good&#x2F;System of a Down&#x2F;2005 - Hypnotize&#x2F;10 - She&#39;s Like Heroin.mp3&quot;, 61)</span><br></pre></td></tr></table></figure>
<p>上面方法可以用于在候选answers中找到最接近query的answer，但在面临大数据时，会遇到速度慢的问题。我们可以通过先确定一个候选answers的子集，再进行fuzzywuzzy的方式缩短运行时间。<br>首先，我们先将候选answers转换成tf-idf向量：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from sklearn.feature_extraction.text import TfidfVectorizer</span><br><span class="line">import nltk</span><br><span class="line">nltk.download(&#39;stopwords&#39;)</span><br><span class="line">from nltk.corpus import stopwords</span><br><span class="line">stop_words &#x3D; set(stopwords.words(&#39;english&#39;))</span><br><span class="line">choices &#x3D; [[&quot;candle&quot;], [&quot;Don&#39;t&quot;, &quot;trouble&quot;, &quot;trouble&quot;, &quot;until&quot;, &quot;trouble&quot;, &quot;troubles&quot;, &quot;you.&quot;], [&quot;A&quot;, &quot;bad&quot;, &quot;excuse&quot;, &quot;is&quot;, &quot;better&quot;, &quot;than&quot;, &quot;none&quot;, &quot;at&quot;, &quot;all.&quot;], [&quot;Bad&quot;, &quot;excuses&quot;, &quot;are&quot;, &quot;worse&quot;, &quot;than&quot;, &quot;none.&quot;], [&quot;A&quot;, &quot;bribe&quot;, &quot;in&quot;, &quot;hand&quot;, &quot;betrays&quot;, &quot;mischief&quot;, &quot;at&quot;, &quot;heart.&quot;], [&quot;A&quot;, &quot;candle&quot;, &quot;lights&quot;, &quot;others&quot;, &quot;and&quot;, &quot;consumes&quot;, &quot;itself.&quot;], [&quot;Don&#39;t&quot;, &quot;teach&quot;, &quot;your&quot;, &quot;grandmother&quot;, &quot;to&quot;, &quot;suck&quot;, &quot;eggs.&quot;], [&quot;A&quot;, &quot;teacher&quot;, &quot;is&quot;, &quot;just&quot;, &quot;a&quot;, &quot;candle&quot;, &quot;,&quot;, &quot;lights&quot;, &quot;others&quot;, &quot;and&quot;, &quot;consumes&quot;, &quot;itself.&quot;], [&quot;A&quot;, &quot;a&quot;, &quot;candle&quot;, &quot;lights&quot;, &quot;others&quot;, &quot;and&quot;, &quot;consumes&quot;, &quot;itself.&quot;]]</span><br><span class="line"># 按word分，还可以按char、char_wb处理：</span><br><span class="line"># vectorizer &#x3D; TfidfVectorizer(min_df&#x3D;1, analyzer&#x3D;&#39;word&#39;)</span><br><span class="line"># 也可以使用自定义的分词</span><br><span class="line">vectorizer &#x3D; TfidfVectorizer(analyzer&#x3D;lambda x:[w for w in x if w not in stop_words])</span><br><span class="line">tf_idf_matrix_candidates &#x3D; vectorizer.fit_transform(choices)</span><br><span class="line">tf_idf_matrix_queries &#x3D; tf_idf_matrix_candidates[-1]</span><br><span class="line">tf_idf_matrix_candidates &#x3D; tf_idf_matrix_candidates[:-1]</span><br><span class="line"># vectorizer.get_feature_names()可以看到所有的token</span><br></pre></td></tr></table></figure>
<p>其次使用sparse_dot_topn找到相似的字符串：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from scipy.sparse import csr_matrix</span><br><span class="line">import sparse_dot_topn.sparse_dot_topn as ct</span><br><span class="line"></span><br><span class="line">def awesome_cossim_top(A, B, ntop, lower_bound&#x3D;0):</span><br><span class="line">    # force A and B as a CSR matrix.</span><br><span class="line">    # If they have already been CSR,there is no overhead</span><br><span class="line">    A &#x3D; A.tocsr()</span><br><span class="line">    B &#x3D; B.tocsr()</span><br><span class="line">    M, _ &#x3D; A.shape</span><br><span class="line">    _, N &#x3D; B.shape</span><br><span class="line">    idx_dtype &#x3D; np.int32</span><br><span class="line">    nnz_max &#x3D; M*ntop</span><br><span class="line">    indptr &#x3D; np.zeros(M+1,dtype&#x3D;idx_dtype)</span><br><span class="line">    indices &#x3D; np.zeros(nnz_max,dtype&#x3D;idx_dtype)</span><br><span class="line">    data &#x3D; np.zeros(nnz_max,dtype&#x3D;A.dtype)</span><br><span class="line">    ct.sparse_dot_topn(M, N, np.asarray(A.indptr,dtype&#x3D;idx_dtype), np.asarray(A.indices,dtype&#x3D;idx_dtype), A.data, np.asarray(B.indptr,dtype&#x3D;idx_dtype), np.asarray(B.indices,dtype&#x3D;idx_dtype), B.data, ntop, lower_bound, indptr, indices, data)</span><br><span class="line">    return csr_matrix((data,indices,indptr),shape&#x3D;(M,N))</span><br><span class="line">    </span><br><span class="line">matches &#x3D; awesome_cossim_top(tf_idf_matrix_candidates, tf_idf_matrix_queries.transpose(),1,0.0).todense()</span><br><span class="line">matches &#x3D; np.squeeze(matches)</span><br><span class="line">match_score_index &#x3D; np.argsort(-matches)</span><br></pre></td></tr></table></figure>
<h1 id="alignment"><a href="#alignment" class="headerlink" title="alignment"></a>alignment</h1><p>alignment主要用于字符串之间对齐，其使用方法如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from alignment.sequence import Sequence</span><br><span class="line">from alignment.vocabulary import Vocabulary</span><br><span class="line">from alignment.sequencealigner import SimpleScoring, GlobalSequenceAligner</span><br><span class="line"># Create sequences to be aligned.</span><br><span class="line">a &#x3D; Sequence(&#39;what a beautiful day&#39;.split())</span><br><span class="line">b &#x3D; Sequence(&#39;what a disappointingly bad day&#39;.split())</span><br><span class="line"># Create a vocabulary and encode the sequences.</span><br><span class="line">v &#x3D; Vocabulary()</span><br><span class="line">aEncoded &#x3D; v.encodeSequence(a)</span><br><span class="line">bEncoded &#x3D; v.encodeSequence(b)</span><br><span class="line"># Create a scoring and align the sequences using global aligner.</span><br><span class="line">scoring &#x3D; SimpleScoring(1, -1)</span><br><span class="line">aligner &#x3D; GlobalSequenceAligner(scoring, -1)</span><br><span class="line">score, encodeds &#x3D; aligner.align(aEncoded, bEncoded, backtrace&#x3D;True)</span><br><span class="line"># Iterate over optimal alignments and print them.</span><br><span class="line">for encoded in encodeds:</span><br><span class="line">    alignment &#x3D; v.decodeSequenceAlignment(encoded)</span><br><span class="line">    print(alignment)</span><br><span class="line">    print(&#39;Alignment score:&#39;, alignment.score)</span><br><span class="line">    print(&#39;Percent identity:&#39;, alignment.percentIdentity())</span><br></pre></td></tr></table></figure>
<h1 id="strsimpy"><a href="#strsimpy" class="headerlink" title="strsimpy"></a>strsimpy</h1><p>这是一个用于计算各种字符串距离的包。其使用方法如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from strsimpy.levenshtein import Levenshtein</span><br><span class="line">levenshtein &#x3D; Levenshtein()</span><br><span class="line">print(levenshtein.distance(&#39;My string&#39;, &#39;My $string&#39;))</span><br><span class="line">from strsimpy.normalized_levenshtein import NormalizedLevenshtein</span><br><span class="line">normalized_levenshtein &#x3D; NormalizedLevenshtein()</span><br><span class="line">print(normalized_levenshtein.distance(&#39;My string&#39;, &#39;My $string&#39;))</span><br><span class="line">print(normalized_levenshtein.similarity(&#39;My string&#39;, &#39;My $string&#39;))</span><br><span class="line"></span><br><span class="line"># 带权重的编辑距离</span><br><span class="line">from strsimpy.weighted_levenshtein import WeightedLevenshtein</span><br><span class="line">from strsimpy.weighted_levenshtein import CharacterSubstitutionInterface</span><br><span class="line">class CharacterSubstitution(CharacterSubstitutionInterface):</span><br><span class="line">    def cost(self, c0, c1):</span><br><span class="line">        if c0&#x3D;&#x3D;&#39;t&#39; and c1&#x3D;&#x3D;&#39;r&#39;:</span><br><span class="line">            return 0.5</span><br><span class="line">        return 1.0</span><br><span class="line">weighted_levenshtein &#x3D; WeightedLevenshtein(CharacterSubstitution())</span><br><span class="line">print(weighted_levenshtein.distance(&#39;String1&#39;, &#39;String2&#39;))</span><br><span class="line">from strsimpy.damerau import Damerau</span><br><span class="line">damerau &#x3D; Damerau()</span><br><span class="line">print(damerau.distance(&#39;ABCDEF&#39;, &#39;ABDCEF&#39;))</span><br><span class="line"></span><br><span class="line"># 最优化对齐后的编辑距离</span><br><span class="line">from strsimpy.optimal_string_alignment import OptimalStringAlignment</span><br><span class="line">optimal_string_alignment &#x3D; OptimalStringAlignment()</span><br><span class="line">print(optimal_string_alignment.distance(&#39;CA&#39;, &#39;ABC&#39;))</span><br><span class="line"></span><br><span class="line">from strsimpy.jaro_winkler import JaroWinkler</span><br><span class="line">jarowinkler &#x3D; JaroWinkler()</span><br><span class="line">print(jarowinkler.similarity(&#39;My string&#39;, &#39;My tsring&#39;))</span><br><span class="line"></span><br><span class="line"># 最长公共子序列</span><br><span class="line">from strsimpy.longest_common_subsequence import LongestCommonSubsequence</span><br><span class="line">lcs &#x3D; LongestCommonSubsequence()</span><br><span class="line">print(lcs.distance(&#39;AGCAT&#39;, &#39;GAC&#39;))</span><br><span class="line"></span><br><span class="line">from strsimpy.metric_lcs import MetricLCS</span><br><span class="line">metric_lcs &#x3D; MetricLCS()</span><br><span class="line">s1 &#x3D; &#39;ABCDEFG&#39;</span><br><span class="line">s2 &#x3D; &#39;ABCDEFHJKL&#39;</span><br><span class="line">print(metric_lcs.distance(s1, s2))</span><br><span class="line"></span><br><span class="line"># ngram</span><br><span class="line">from strsimpy.ngram import NGram</span><br><span class="line">twogram &#x3D; NGram(2)</span><br><span class="line">print(twogram.distance(&#39;ABCD&#39;, &#39;ABTUIO&#39;))</span><br><span class="line">s1 &#x3D; &#39;Adobe CreativeSuite 5 Master Collection from cheap 4zp&#39;</span><br><span class="line">s2 &#x3D; &#39;Adobe CreativeSuite 5 Master Collection from cheap d1x&#39;</span><br><span class="line">fourgram &#x3D; NGram(4)</span><br><span class="line">print(fourgram.distance(s1, s2))</span><br><span class="line"></span><br><span class="line">from strsimpy.qgram import QGram</span><br><span class="line">qgram &#x3D; QGram(2)</span><br><span class="line">print(qgram.distance(&#39;ABCD&#39;, &#39;ABCE&#39;))</span><br><span class="line"></span><br><span class="line">from strsimpy.cosine import Cosine</span><br><span class="line">cosine &#x3D; Cosine(2)</span><br><span class="line">s0 &#x3D; &#39;My first string&#39;</span><br><span class="line">s1 &#x3D; &#39;My other string...&#39;</span><br><span class="line">p0 &#x3D; cosine.get_profile(s0)</span><br><span class="line">p1 &#x3D; cosine.get_profile(s1)</span><br><span class="line">print(cosine.similarity_profiles(p0, p1))</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title>大话交叉熵损失函数</title>
    <url>/2020/03/10/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<p>使用keras进行二分类时，常使用binary_crossentropy作为损失函数。那么它的原理是什么，跟categorical_crossentropy、sparse_categorical_crossentropy有什么区别？在进行文本分类时，如何选择损失函数，有哪些优化损失函数的方式？本文将从原理到实现进行一一介绍。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a" target="_blank" rel="noopener">https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a</a></p>
</blockquote>
<h1 id="binary-crossentropy"><a href="#binary-crossentropy" class="headerlink" title="binary_crossentropy"></a>binary_crossentropy</h1><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>假设我们想做一个二分类，输入有10个点：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x &#x3D; [-2.2, -1.4, -0.8, 0.2, 0.4, 0.8, 1.2, 2.2, 2.9, 4.6]</span><br></pre></td></tr></table></figure>
<p>输出有两类，分别为红色、绿色：<br><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5/1.png" alt="图片"></p>
<p>我们可以将问题描述成“这个点是绿色的吗?”，或者“这个点是绿色的概率是多少?”。理想情况下，绿点的概率是1.0，而红点的（是绿色的）概率是0.0。从而，绿色就是正样本，红色就是负样本。</p>
<p>如果我们拟合一个模型来执行这种分类，它将预测我们每个点的绿色概率。那么我们如何评估预测概率的好坏呢？这就是损失函数的意义，</p>
<p>Binary CrossEntorpy的计算如下：</p>
<p>$H_{p}(q)=-\frac{1}{N} \sum_{i=1}^{N} y_{i} \cdot \log \left(p\left(y_{i}\right)\right)+\left(1-y_{i}\right) \cdot \log \left(1-p\left(y_{i}\right)\right)$</p>
<p>其中y是标签(1代表绿色点，0代表红色点)，p(y)是所有N个点都是绿色的预测概率。看到这个计算式，发现对于每一个绿点(y=1)它增加了log(p(y))的损失（概率越大，增加的越小），也就是它是绿色的概率。下面我们可视化地看一下这个损失函数。</p>
<p>假设我们训练一个逻辑回归模型来进行分类，那么训练出的函数趋近于一个sigmoid曲线，曲线上每个点表示对于每个x是绿色点的概率：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5/2.png" alt="图片"></p>
<p>那么对于这些绿色的点，他们预测为绿色的概率是多少呢？实际下面图片中绿色的bar：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5/3.png" alt="图片"></p>
<p>那么红色点预测为红色的概率是多少呢？实际就是下面图片中红色的bar：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5/4.png" alt="图片"></p>
<p>我们把图片绘制得更好看一下，如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5/5.png" alt="图片"></p>
<p>因为我们要计算损失，我们需要惩罚错误的预测。如果与正例相关的概率是1.0，我们需要它的损失为零。相反，如果概率很低，比如0.01，我们需要它的损失是巨大的！取概率的(负)对数非常适合我们的目的(由于0.0和1.0之间的值的对数是负的，我们取负对数来获得正的损失值)。下面这个图展示了当正例的概率逐渐趋近于0时loss的变化：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5/6.png" alt="图片"></p>
<p>下面这个图表示了，我们使用负对数时每个点的损失，我们计算其平均值，就是binary cross entropy了！</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5/7.png" alt="图片"></p>
<h2 id="keras实现"><a href="#keras实现" class="headerlink" title="keras实现"></a>keras实现</h2><p>tf2.1的bce用法如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bce &#x3D; tf.keras.losses.BinaryCrossentropy()</span><br><span class="line">loss &#x3D; bce([0., 0., 1., 1.], [1., 1., 1., 0.])</span><br><span class="line">print(&#39;Loss: &#39;, loss.numpy())  # Loss: 11.522857</span><br></pre></td></tr></table></figure>
<p>或者：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">model &#x3D; tf.keras.Model(inputs, outputs)</span><br><span class="line">model.compile(&#39;sgd&#39;, loss&#x3D;tf.keras.losses.BinaryCrossentropy())</span><br></pre></td></tr></table></figure>
<p>具体实现如下（tensorflow.python.keras/losses）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class BinaryCrossentropy(LossFunctionWrapper):</span><br><span class="line">    def __init__(self, from_logits&#x3D;False,</span><br><span class="line">                  label_smoothing&#x3D;0,</span><br><span class="line">                  reduction&#x3D;losses_utils.ReductionV2.AUTO,</span><br><span class="line">                  name&#x3D;&#39;binary_crossentropy&#39;):</span><br><span class="line">        super(BinaryCrossentropy, self).__init__(</span><br><span class="line">              binary_crossentropy,</span><br><span class="line">              name&#x3D;name,</span><br><span class="line">              reduction&#x3D;reduction,</span><br><span class="line">              from_logits&#x3D;from_logits,</span><br><span class="line">              label_smoothing&#x3D;label_smoothing)</span><br><span class="line">        self.from_logits &#x3D; from_logits</span><br><span class="line"></span><br><span class="line">def binary_crossentropy(y_true, y_pred, from_logits&#x3D;False, label_smoothing&#x3D;0):</span><br><span class="line">    y_pred &#x3D; ops.convert_to_tensor_v2(y_pred)</span><br><span class="line">    y_true &#x3D; math_ops.cast(y_true, y_pred.dtype)</span><br><span class="line">    label_smoothing &#x3D; ops.convert_to_tensor_v2(label_smoothing, dtype&#x3D;K.floatx())</span><br><span class="line">    </span><br><span class="line">    def _smooth_labels():</span><br><span class="line">      return y_true * (1.0 - label_smoothing) + 0.5 * label_smoothing</span><br><span class="line">    </span><br><span class="line">    y_true &#x3D; smart_cond.smart_cond(label_smoothing, _smooth_labels, lambda: y_true)</span><br><span class="line">    return K.mean(K.binary_crossentropy(y_true, y_pred, from_logits&#x3D;from_logits), axis&#x3D;-1)</span><br></pre></td></tr></table></figure>
<p>在上面代码中，如果from_logits=True，则认为y_predit是tensor（可以认为是[0,1]之间的概率值），使用from_logits=True可以更稳定一些。label_smoothing在[0,1]之间。reduction的默认值是AUTO，表示根据上下文确定；如果是SUM_OVER_BATCH_SIZE表示整个batch的结果相加。<br>其中K.binary_crossentropy实现如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def binary_crossentropy(target, output, from_logits&#x3D;False):</span><br><span class="line">  if from_logits:</span><br><span class="line">    return nn.sigmoid_cross_entropy_with_logits(labels&#x3D;target, logits&#x3D;output)</span><br><span class="line"></span><br><span class="line">  if not isinstance(output, (ops.EagerTensor, variables_module.Variable)):</span><br><span class="line">    output &#x3D; _backtrack_identity(output)</span><br><span class="line">    if output.op.type &#x3D;&#x3D; &#39;Sigmoid&#39;:</span><br><span class="line">      assert len(output.op.inputs) &#x3D;&#x3D; 1</span><br><span class="line">      output &#x3D; output.op.inputs[0]</span><br><span class="line">      return nn.sigmoid_cross_entropy_with_logits(labels&#x3D;target, logits&#x3D;output)</span><br><span class="line"></span><br><span class="line">  epsilon_ &#x3D; _constant_to_tensor(epsilon(), output.dtype.base_dtype)</span><br><span class="line">  output &#x3D; clip_ops.clip_by_value(output, epsilon_, 1. - epsilon_)</span><br><span class="line"></span><br><span class="line">  bce &#x3D; target * math_ops.log(output + epsilon())</span><br><span class="line">  bce +&#x3D; (1 - target) * math_ops.log(1 - output + epsilon())</span><br><span class="line">  return -bce</span><br></pre></td></tr></table></figure>
<p>sigmoid_cross_entropy_with_logits实现如下：（该函数适用于不同类标签之间相互独立的情况，例如一个图片可以既包含大象也包含狗）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def sigmoid_cross_entropy_with_logits(_sentinel&#x3D;None, labels&#x3D;None, logits&#x3D;None, name&#x3D;None):</span><br><span class="line">    zeros &#x3D; array_ops.zeros_like(logits, dtype&#x3D;logits.dtype)</span><br><span class="line">    cond &#x3D; (logits &gt;&#x3D; zeros)</span><br><span class="line">    relu_logits &#x3D; array_ops.where(cond, logits, zeros)</span><br><span class="line">    neg_abs_logits &#x3D; array_ops.where(cond, -logits, logits)</span><br><span class="line">    return math_ops.add(relu_logits - logits * labels, math_ops.log1p(math_ops.exp(neg_abs_logits)), name&#x3D;name)</span><br></pre></td></tr></table></figure>
<p>对于上面代码，解释如下：<br>对于x=logits, z=labels，logistic损失定义为</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))</span><br><span class="line">&#x3D; z * -log(1 &#x2F; (1 + exp(-x))) + (1 - z) * -log(exp(-x) &#x2F; (1 + exp(-x)))</span><br><span class="line">&#x3D; z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))</span><br><span class="line">&#x3D; z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))</span><br><span class="line">&#x3D; (1 - z) * x + log(1 + exp(-x))</span><br><span class="line">&#x3D; x - x * z + log(1 + exp(-x))</span><br></pre></td></tr></table></figure>
<p>对于x&lt;0，为了防止exp(-x)溢出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x - x * z + log(1 + exp(-x))</span><br><span class="line">&#x3D; log(exp(x)) - x * z + log(1 + exp(-x))</span><br><span class="line">&#x3D; - x * z + log(1 + exp(x))</span><br></pre></td></tr></table></figure>
<p>为了保证稳定和不溢出，在实现过程中使用了如下等式：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">max(x, 0) - x * z + log(1 + exp(-abs(x)))</span><br></pre></td></tr></table></figure>
<h1 id="categorical-crossentropy"><a href="#categorical-crossentropy" class="headerlink" title="categorical_crossentropy"></a>categorical_crossentropy</h1><h2 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h2><p>CrossEntropy可用于多分类任务，且label且one-hot形式。它的计算式如下：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5/8.png" alt="图片"></p>
<h2 id="keras实现-1"><a href="#keras实现-1" class="headerlink" title="keras实现"></a>keras实现</h2><p>tf2.1的ce用法如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">y_true &#x3D; [[0, 1, 0], [0, 0, 1]]</span><br><span class="line">y_pred &#x3D; [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]</span><br><span class="line">cce &#x3D; tf.keras.losses.CategoricalCrossentropy()</span><br><span class="line"># Using &#39;auto&#39;&#x2F;&#39;sum_over_batch_size&#39; reduction type.</span><br><span class="line">cce(y_true, y_pred).numpy()</span><br><span class="line"># Calling with &#39;sample_weight&#39;.</span><br><span class="line">cce(y_true, y_pred, sample_weight&#x3D;tf.constant([0.3, 0.7])).numpy()</span><br><span class="line"># Using &#39;sum&#39; reduction type.</span><br><span class="line">cce &#x3D; tf.keras.losses.CategoricalCrossentropy(reduction&#x3D;tf.keras.losses.Reduction.NONE)</span><br><span class="line">cce(y_true, y_pred).numpy()</span><br><span class="line"># Usage with the &#96;compile&#96; API</span><br><span class="line">model &#x3D; tf.keras.Model(inputs, outputs)</span><br><span class="line">model.compile(&#39;sgd&#39;, loss&#x3D;tf.keras.losses.CategoricalCrossentropy())</span><br></pre></td></tr></table></figure>
<p>具体实现如下（tensorflow.python.keras/losses）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class CategoricalCrossentropy(LossFunctionWrapper):</span><br><span class="line">    def __init__(self,</span><br><span class="line">                   from_logits&#x3D;False,</span><br><span class="line">                   label_smoothing&#x3D;0,</span><br><span class="line">                   reduction&#x3D;losses_utils.ReductionV2.AUTO,</span><br><span class="line">                   name&#x3D;&#39;categorical_crossentropy&#39;):</span><br><span class="line">        super(CategoricalCrossentropy, self).__init__(</span><br><span class="line">                categorical_crossentropy,</span><br><span class="line">                name&#x3D;name,</span><br><span class="line">                reduction&#x3D;reduction,</span><br><span class="line">                from_logits&#x3D;from_logits,</span><br><span class="line">                label_smoothing&#x3D;label_smoothing)</span><br><span class="line"></span><br><span class="line">    def categorical_crossentropy(y_true,</span><br><span class="line">                                 y_pred,</span><br><span class="line">                                 from_logits&#x3D;False,</span><br><span class="line">                                 label_smoothing&#x3D;0):</span><br><span class="line">        y_pred &#x3D; ops.convert_to_tensor_v2(y_pred)</span><br><span class="line">        y_true &#x3D; math_ops.cast(y_true, y_pred.dtype)</span><br><span class="line">        label_smoothing &#x3D; ops.convert_to_tensor_v2(label_smoothing, dtype&#x3D;K.floatx())</span><br><span class="line">        def _smooth_labels():</span><br><span class="line">            num_classes &#x3D; math_ops.cast(array_ops.shape(y_true)[1], y_pred.dtype)</span><br><span class="line">            return y_true * (1.0 - label_smoothing) + (label_smoothing &#x2F; num_classes)</span><br><span class="line">        y_true &#x3D; smart_cond.smart_cond(label_smoothing, _smooth_labels, lambda: y_true)</span><br><span class="line">        return K.categorical_crossentropy(y_true, y_pred, from_logits&#x3D;from_logits)</span><br></pre></td></tr></table></figure>
<p>其中K.categorical_crossentropy实现如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def categorical_crossentropy(target, output, from_logits&#x3D;False, axis&#x3D;-1):</span><br><span class="line">    if from_logits:</span><br><span class="line">        return nn.softmax_cross_entropy_with_logits_v2(</span><br><span class="line">            labels&#x3D;target, logits&#x3D;output, axis&#x3D;axis)</span><br><span class="line">    if not isinstance(output, (ops.EagerTensor, variables_module.Variable)):</span><br><span class="line">        output &#x3D; _backtrack_identity(output)</span><br><span class="line">        if output.op.type &#x3D;&#x3D; &#39;Softmax&#39;:</span><br><span class="line">            output &#x3D; output.op.inputs[0]</span><br><span class="line">            return nn.softmax_cross_entropy_with_logits_v2(</span><br><span class="line">                labels&#x3D;target, logits&#x3D;output, axis&#x3D;axis)</span><br><span class="line">    # scale preds so that the class probas of each sample sum to 1</span><br><span class="line">    output &#x3D; output &#x2F; math_ops.reduce_sum(output, axis, True)</span><br><span class="line">    # Compute cross entropy from probabilities.</span><br><span class="line">    epsilon_ &#x3D; _constant_to_tensor(epsilon(), output.dtype.base_dtype)</span><br><span class="line">    output &#x3D; clip_ops.clip_by_value(output, epsilon_, 1. - epsilon_)</span><br><span class="line">    return -math_ops.reduce_sum(target * math_ops.log(output), axis)</span><br></pre></td></tr></table></figure>
<h1 id="sparse-categorical-crossentropy"><a href="#sparse-categorical-crossentropy" class="headerlink" title="sparse_categorical_crossentropy"></a>sparse_categorical_crossentropy</h1><h2 id="原理-2"><a href="#原理-2" class="headerlink" title="原理"></a>原理</h2><p>跟categorical_crossentropy的区别是其标签不是one-hot，而是integer。比如在categorical_crossentropy是[1,0,0]，在sparse_categorical_crossentropy中是3.</p>
<h2 id="keras实现-2"><a href="#keras实现-2" class="headerlink" title="keras实现"></a>keras实现</h2><p>tf2.1中使用方法如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">y_true &#x3D; [1, 2]</span><br><span class="line">y_pred &#x3D; [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]</span><br><span class="line">loss &#x3D; tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)</span><br></pre></td></tr></table></figure>
<h1 id="其他技巧"><a href="#其他技巧" class="headerlink" title="其他技巧"></a>其他技巧</h1><h2 id="focal-loss"><a href="#focal-loss" class="headerlink" title="focal loss"></a>focal loss</h2><blockquote>
<p>参考：<br><a href="https://ldzhangyx.github.io/2018/11/16/focal-loss/" target="_blank" rel="noopener">https://ldzhangyx.github.io/2018/11/16/focal-loss/</a></p>
</blockquote>
<p>Focal Loss的出现是为了解决训练集正负样本极度不平衡的情况，通过reshape标准交叉熵损失解决类别不均衡（Class Imbalance）,这样它就能降低容易分类的样例的比重（Well-classified Examples）。这个方法专注训练在Hard Examples的稀疏集合上，能够防止大量的Easy Negatives在训练中压倒训练器。其公式为：</p>
<p>$F L\left(p_{t}\right)=-\left(1-p_{t}\right)^{\gamma} \log \left(p_{t}\right)$</p>
<p>其中参数为0的时候，Focal Loss退化为交叉熵CE。当这个参数不同时，对loss的影响如下：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5/0.png" alt="图片"></p>
<p>p_t越大，FL越小，其对总体loss所做的贡献就越小；反过来说，p_t越小（小于0.5的情况也就是被误分类），越能反映在总体loss上。</p>
<p>Focal Loss的tensorflow api：<a href="https://www.tensorflow.org/addons/api_docs/python/tfa/losses/SigmoidFocalCrossEntropy" target="_blank" rel="noopener">https://www.tensorflow.org/addons/api_docs/python/tfa/losses/SigmoidFocalCrossEntropy</a></p>
<h2 id="label-smooth"><a href="#label-smooth" class="headerlink" title="label smooth"></a>label smooth</h2><blockquote>
<p>参考：<br><a href="https://towardsdatascience.com/what-is-label-smoothing-108debd7ef06" target="_blank" rel="noopener">https://towardsdatascience.com/what-is-label-smoothing-108debd7ef06</a></p>
</blockquote>
<p>在使用深度学习模型进行分类任务时，我们通常会遇到以下问题：overfit和over confidence。Overfit问题得到了很好的研究，可以通过earlystop、dropout、正则化等方法来解决。另一方面，我们over confidence的工具较少。标签平滑是一种正则化技术，解决了这两个问题。</p>
<p>Label Smooth将y_hot和均匀分布的混合来代替一个hot编码的标签向量y_hot:</p>
<p>$y_{-} l s=(1-\alpha) * y_{-} h o t+\alpha / K$</p>
<p>K是标签类的数目，α是一个决定平滑的超参数。如果α= 0，我们获得最初的一个原始的y_hot编码。如果α= 1，我们得到均匀分布。</p>
<p>当损失函数为交叉熵时，使用标签平滑，模型将softmax函数应用于倒数第二层的logit向量z，计算其输出概率p。在这种情况下，交叉熵损失函数相对于logit的梯度为：</p>
<p>$\nabla C E=p-y=\operatorname{softmax}(z)-y$</p>
<p>其中y是标签分布，并且：</p>
<ul>
<li>梯度下降会使p尽可能接近y</li>
<li>梯度在-1和1之间有界</li>
</ul>
<p>一个标准的ont-hot希望有更大的logit gaps输入到里面。直观地说，较大的logit gap加上有界的梯度会使模型的自适应性降低，并且对其预测过于自信。相反，平滑的标签鼓励小的logit差距，可以得到更好的模型校准，并防止过度自信的预测。</p>
<p>下面我们使用一个例子说明：假设我们有K = 3类，我们的标签属于第一类。令[a, b, c]为logit向量。如果我们不使用标签平滑，那么标签向量就是一个one-hot向量[1,0,0]。我们的模型将a≫b和a≫c。例如,应用softmax分对数向量(10,0,0)给(0.9999,0,0)的4位小数。</p>
<p>如果我们使用标签的平滑与α= 0.1,平滑标签向量≈(0.9333,0.0333,0.0333)。logit向量[3.3322,0,0]在softmax之后将经过平滑处理的标签向量近似为小数点后4位，并且它的差距更小。这就是为什么我们称平滑标签为一种正则化技术，因为它可以防止最大的logit变得比其他的更大。</p>
<p>更形象地说，对于label_smoothing=0.2，则意味着标签0的概率是0.1，标签1的概率是0.9</p>
]]></content>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：《Reformer: The Efficient Transformer》</title>
    <url>/2020/03/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E3%80%8AReformer-The-Efficient-Transformer%E3%80%8B/</url>
    <content><![CDATA[<p>本论文为谷歌近期发表的对Transformer改进的一篇论文，论文名字中的Efficient Transformer解释了论文的主要目的。过去一些基于Transformer结构的论文，一看到模型的总参数量就让人望而生畏，有些模型在我们的单卡GPU上根本跑不起来，因此就看了一下这篇论文。论文感觉比较偏工程，了解下它的大致思想就好。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://arxiv.org/pdf/2001.04451.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2001.04451.pdf</a><br><a href="https://github.com/google/trax/blob/master/trax/models/reformer/reformer.py" target="_blank" rel="noopener">https://github.com/google/trax/blob/master/trax/models/reformer/reformer.py</a><br><a href="https://zhuanlan.zhihu.com/p/92153420" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/92153420</a></p>
</blockquote>
<h1 id="论文摘要"><a href="#论文摘要" class="headerlink" title="论文摘要"></a>论文摘要</h1><p>为了解决Transformer模型在处理长序列时的GPU资源消耗问题，提出了更省内存和更快的Transformer模型结构。其改进主要有两点：</p>
<ul>
<li>使用locality-sensitive hashing代替dot-product attention，使得计算复杂度由 $O(L^2)$直接降为$O(LlogL)$，其中L为序列长度</li>
<li>使用reversible residual layers来代替传统的残差层，使得训练过程中对激活函数的值的存储由N次降低为1次，其中N是层数。</li>
</ul>
<h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h1><p>比较大的Transformer模型里的每一层有0.5Billion的参数，最多可达到64层。并且随着序列长度增加，单个文本train example需要能处理11k左右的token。对于音乐、图像等数据，序列可能会更长，因此有些模型只能在大型GPU集群中进行并行训练。受GPU显存限制，有的模型也很难在单个GPU机器上进行微调。</p>
<p>因此会有这样一个疑问，这么大的Transformer模型到底在哪里消耗了这么多资源？我们不妨计算一下：</p>
<ul>
<li>每层0.5Billion的参数需要2GB的存储</li>
<li>使用1024 embedding size和8 batch size训练的64k token的激活函数值需要64K <em> 1K </em> 8 = 0.5Billion的参数，即2GB的存储</li>
<li>N层网络需要将激活值存储N次(为了back-propagation时进行计算)</li>
<li>Feed-forward层的维度通常要比d_model大很多</li>
<li>对于序列长度L来说计算attention所需要的时空复杂度为O(L^2)</li>
</ul>
<p>具体计算过程如下：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/reformer1.png" alt="图片"></p>
<ul>
<li><p>Transformer Block</p>
<p>$h_{m i d}=\text { LayerNorm }\left(h_{i n}+\text { MultiHead }\left(h_{i n}\right)\right)$</p>
<p>$h_{\text {out}}=\text {LayerNorm }\left(h_{\text {mid}}+\mathrm{FFN}\left(h_{\text {mid}}\right)\right)$</p>
</li>
<li><p>Multi-head Attention</p>
</li>
</ul>
<p>$\begin{array}{l}\text {head}_{i}=\text { Attention }\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right) \ \text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V \ \text { MultiHead }(Q, K, V)=\text {Concat}\left(\text {head}_{1}, \ldots, \text {head}_{h}\right) W^{O}\end{array}$</p>
<ul>
<li>Attention输入：<ul>
<li>Q: (batch_size, seq_q, d_model)</li>
<li>K: (batch_size, seq_k, d_model)</li>
<li>V: (batch_size, seq_k, d_model)</li>
</ul>
</li>
<li>Attention输出：<ul>
<li>(batch_size, q_seq_len, d_model)</li>
</ul>
</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/reformer2.png" alt="图片"></p>
<ul>
<li><p>Feed-Forward</p>
<p>$\operatorname{FFN}(h)=\operatorname{ReLU}\left(h W_{1}+b_{1}\right) W_{2}+b_{2}$</p>
</li>
<li><p>FFN输入：</p>
<ul>
<li>(batch_size, q_seq_len, d_model)</li>
</ul>
</li>
<li>FFN输出：<ul>
<li>(batch_size, q_seq_len, d_ff)</li>
</ul>
</li>
</ul>
<p>本论文提出了Reformer模型，使用了下面方法解决了内存和速度的问题：</p>
<ul>
<li>Reversible layers：整个模型只需保存一次activations，使因层数导致的内存问题解决</li>
<li>FFN层分块并行处理：降低d_ff产生的内存消耗</li>
<li>局部敏感哈希(locality-sensitive hashing)：代替dot-product attention带来的O(L^2)计算和内存复杂度，使得能处理更长的序列</li>
</ul>
<h1 id="局部敏感哈希"><a href="#局部敏感哈希" class="headerlink" title="局部敏感哈希"></a>局部敏感哈希</h1><blockquote>
<p>参考：<br><a href="https://arxiv.org/abs/1509.02897" target="_blank" rel="noopener">https://arxiv.org/abs/1509.02897</a><br><a href="https://www.cnblogs.com/maybe2030/p/4953039.html" target="_blank" rel="noopener">https://www.cnblogs.com/maybe2030/p/4953039.html</a></p>
</blockquote>
<p>Attention计算中最耗时和消耗内存的是QK^T([batch size, length, length])。我们其实关注的是softmax(QK^T)，而softmax的取值主要被其中较大的元素主导，因此对Q的每个向量qi，只需要关注K中哪个向量最接近qi。比如说如果K的长度是64K，对于每个qi，我们只需要关注其中跟qi距离最近的32或64个kj。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/reformer3.png" alt="图片"></p>
<p>我们首先想到的是 locality-sensitive hashing，其特点是对于每个向量x，在经过哈希函数h(x)后，在原来的空间中挨的近的向量有更大的概率获得相同的哈希值。就像上面这张图，经过旋转(映射)后，距离远得点(第一行)有很大概率分到不同得桶中，而距离近得点(第二行)很大概率分到相同得桶中。</p>
<p>在实现时我们使用了一个随机产生的大小为(dk, b/2)的矩阵R，定义$h(x)=\arg{\max }([x R ;-x R])​$为哈希函数，这样所有x，可以把它们分配到b个哈希桶里。具体的计算和证明在另一片论文(Practical and Optimal LSH for Angular Distance)中。</p>
<p>下面这张图说明了LSH具体的计算流程：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/reformer4.png" alt="图片"></p>
<p>在上图中，不同的颜色表示不同的哈希值，相似的词则具有相同的颜色。分配哈希值后，序列重新排列，将具有相同哈希值的元素放在一起，再分为多个片段（或多个区块）以实现并行处理。然后在这些短得多的区块（及其相近邻块以覆盖溢出）内应用注意力，从而大大降低计算负载。</p>
<p>上图右侧（a-b）是和传统注意力的比较。(a)表明传统的注意力是很稀疏的，也就是说大多数的字符其实都不用关注；(b) k和q根据它们的哈希桶（注意随机的那个矩阵R是共享的）排序好，然后再使用。</p>
<p>由于哈希桶的大小很可能不均匀，所以我们首先令$k_{j}=\frac{q_{i}}{\left|q_{i}\right|}​$来保证$h\left(k_{j}\right)=h\left(q_{j}\right)​$，然后再从小到大给Q的哈希桶排序，在每个桶内部，按照位置先后排序。这实际上定义了一个置换$i \mapsto s_{i}​$。在排序后的注意力矩阵中，来自同一个哈希桶的(q,k)对会聚集在矩阵的对角(上图右c)。最后，把它们分组，每组m个，在各组内相互关注。</p>
<p>为了进一步减小桶分布不均的情况，可以用不同的哈希函数进行多轮哈希。下表是几种注意力方式的时空复杂度：(l: 序列长度，b: batch_size, $n_h$: num of heads, $n_c$: num of LSH chunk, $n_r$: num of hash repetition)<img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/reformer5.png" alt="图片"></p>
<h1 id="可逆层"><a href="#可逆层" class="headerlink" title="可逆层"></a>可逆层</h1><blockquote>
<p>参考：<br><a href="https://arxiv.org/pdf/1707.04585.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1707.04585.pdf</a><br><a href="https://zhuanlan.zhihu.com/p/60479586" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/60479586</a><br><a href="https://www.cnblogs.com/gczr/p/12181354.html" target="_blank" rel="noopener">https://www.cnblogs.com/gczr/p/12181354.html</a></p>
</blockquote>
<p>通过LSH可以将attention的复杂度减少为序列长度的线性级，但是参数量占的复杂度依旧很高，我们想要进一步减少。在上面表中我们看出，每一层的输入前都至少有$b \cdot l \cdot d_{\text {model}}$的激活输出值，$n_l$层则至少有个$b \cdot l \cdot d_{\bmod e l} \cdot n_{l}$。而且光是FFN层就会产生$b \cdot l \cdot d_{f f} \cdot n_{l}$的激活输出，对于一些大模型，这个$d_ff$会比较大(4K甚至64K)，甚至消耗掉16GB的内存。因此采用可逆层来解决$n_l$和$d_ff$的问题。</p>
<h2 id="可逆Transformer"><a href="#可逆Transformer" class="headerlink" title="可逆Transformer"></a>可逆Transformer</h2><p>可逆残差网络的前向传播和反向计算过程如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/reformer6.png" alt="图片"></p>
<p>前向：</p>
<p>$\begin{array}{l}y_{1}=x_{1}+\mathcal{F}\left(x_{2}\right) \ y_{2}=x_{2}+\mathcal{G}\left(y_{1}\right)\end{array}​$</p>
<p>逆向：</p>
<p>$\begin{array}{l}x_{2}=y_{2}-\mathcal{G}\left(y_{1}\right) \ x_{1}=y_{1}-\mathcal{F}\left(x_{2}\right)\end{array}$</p>
<p>在典型的残差网络中，通过网络传递的输入将会向堆栈中的每一层不断添加至向量。相反，可逆层中每个层有两组激活。一组遵循刚才描述的标准过程，从一层逐步更新到下一层，但是另一组仅捕获第一层的变更。因此，若要反向运行网络，只需简单地减去每一层应用的激活。</p>
<p>简单来说，可逆层将输入分成两部分，使得每一层的值可以由它下一层的输出推导出来。因此整个网络只需要存储最后一层的值即可。</p>
<p>具体的解释可参考论文：The Reversible Residual Network: Backpropagation Without Storing Activations.</p>
<p>在Transformer中我们这样应用可逆层：</p>
<p>$Y_{1}=X_{1}+\text { Attention }\left(X_{2}\right)​$</p>
<p>$Y_{2}=X_{2}+\text { FeedForward }\left(Y_{1}\right)$</p>
<h2 id="FF层分组"><a href="#FF层分组" class="headerlink" title="FF层分组"></a>FF层分组</h2><p>由于FFN层的计算不依赖于位置信息，可以将计算进行分块处理：$Y_{2}=\left[Y_{2}^{(1)} ; \ldots ; Y_{2}^{(c)}\right]=\left[X_{2}^{(1)}+\text { FeedForward }\left(Y_{1}^{(1)}\right) ; \ldots ; X_{2}^{(c)}+\text { FeedForward }\left(Y_{1}^{(c)}\right)\right]$</p>
<p>论文中特别强调，虽然通过分块和可逆层使得激活值是独立于层数的，但是对参数来说可不是这样，参数会随着层的增长而增长。好在我们可以利用CPU的内存，在逐层计算时将暂不使用的参数存储到CPU内存中，当需要时再交换回来。虽说从GPU到CPU的传输是比较慢的，但这对于Reformer来说，其batch_size * lenth已经达到可以忽略到这种参数传输的成本。</p>
<p>下表是所有变体的复杂度：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/reformer7.png" alt="图片"></p>
<h1 id="实验效果"><a href="#实验效果" class="headerlink" title="实验效果"></a>实验效果</h1><blockquote>
<p>参考：<br><a href="https://colab.research.google.com/github/google/trax/blob/master/trax/models/reformer/image_generation.ipynb" target="_blank" rel="noopener">https://colab.research.google.com/github/google/trax/blob/master/trax/models/reformer/image_generation.ipynb</a><br><a href="https://colab.research.google.com/github/google/trax/blob/master/trax/models/reformer/text_generation.ipynb" target="_blank" rel="noopener">https://colab.research.google.com/github/google/trax/blob/master/trax/models/reformer/text_generation.ipynb</a><br><a href="https://colab.research.google.com/github/google/trax/blob/master/trax/intro.ipynb" target="_blank" rel="noopener">https://colab.research.google.com/github/google/trax/blob/master/trax/intro.ipynb</a></p>
</blockquote>
<p>论文中的实验结论主要是为了证实Reformer可以更高效，且对精度几乎没有损失。这里贴一张Colab上对Reformer应用的效果图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/reformer8.png" alt="图片"></p>
<p>上图使用Reformer逐像素生成全画幅图像。</p>
]]></content>
      <tags>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>PLY教程及例子</title>
    <url>/2020/03/03/PLY%E6%95%99%E7%A8%8B%E5%8F%8A%E4%BE%8B%E5%AD%90/</url>
    <content><![CDATA[<p>最近需要重改语音助手中的计算器模块，打算用yacc&amp;lex实现，在这里记录一下学习和使用过程。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://github.com/PChou/python-lex-yacc" target="_blank" rel="noopener">https://github.com/PChou/python-lex-yacc</a></p>
</blockquote>
<h1 id="PLY教程"><a href="#PLY教程" class="headerlink" title="PLY教程"></a>PLY教程</h1><h2 id="PLY简介"><a href="#PLY简介" class="headerlink" title="PLY简介"></a>PLY简介</h2><p>PLY 是纯粹由 Python 实现的 Lex 和 yacc（流行的编译器构建工具）。PLY 的设计目标是尽可能的沿袭传统 lex 和 yacc 工具的工作方式，包括支持 LALR(1)分析法、提供丰富的输入验证、错误报告和诊断。因此，如果你曾经在其他编程语言下使用过 yacc，你应该能够很容易的迁移到 PLY 上。</p>
<p>PLY 包含两个独立的模块：lex.py 和 yacc.py，都定义在 ply 包下。lex.py 模块用来将输入字符通过一系列的正则表达式分解成标记序列，yacc.py 通过一些上下文无关的文法来识别编程语言语法。yacc.py 使用 LR 解析法，并使用 LALR(1)算法（默认）或者 SLR 算法生成分析表。</p>
<p>这两个工具是为了一起工作的。lex.py 通过向外部提供token()方法作为接口，方法每次会从输入中返回下一个有效的标记。yacc.py 将会不断的调用这个方法来获取标记并匹配语法规则。yacc.py 的功能通常是生成抽象语法树(AST)，不过，这完全取决于用户，如果需要，yacc.py 可以直接用来完成简单的翻译。</p>
<p>就像相应的 unix 工具，yacc.py 提供了大多数你期望的特性，其中包括：丰富的错误检查、语法验证、支持空产生式、错误的标记、通过优先级规则解决二义性。事实上，传统 yacc 能够做到的 PLY 都应该支持。</p>
<p>yacc.py 与 Unix 下的 yacc 的主要不同之处在于，yacc.py 没有包含一个独立的代码生成器，而是在 PLY 中依赖反射来构建词法分析器和语法解析器。不像传统的 lex/yacc 工具需要一个独立的输入文件，并将之转化成一个源文件，Python 程序必须是一个可直接可用的程序，这意味着不能有额外的源文件和特殊的创建步骤（像是那种执行 yacc 命令来生成 Python 代码）。又由于生成分析表开销较大，PLY 会缓存生成的分析表，并将它们保存在独立的文件中，除非源文件有变化，会重新生成分析表，否则将从缓存中直接读取。</p>
<h2 id="LEX简介"><a href="#LEX简介" class="headerlink" title="LEX简介"></a>LEX简介</h2><p>lex.py是用来将输入字符串标记化。例如，假设你正在设计一个编程语言，用户的输入字符串如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x &#x3D; 3 + 42 * (s - t)</span><br></pre></td></tr></table></figure>
<p>标记器将字符串分割成独立的标记：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#39;x&#39;,&#39;&#x3D;&#39;, &#39;3&#39;, &#39;+&#39;, &#39;42&#39;, &#39;*&#39;, &#39;(&#39;, &#39;s&#39;, &#39;-&#39;, &#39;t&#39;, &#39;)&#39;</span><br></pre></td></tr></table></figure><br>标记通常用一组名字来命名和表示：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#39;ID&#39;,&#39;EQUALS&#39;,&#39;NUMBER&#39;,&#39;PLUS&#39;,&#39;NUMBER&#39;,&#39;TIMES&#39;,&#39;LPAREN&#39;,&#39;ID&#39;,&#39;MINUS&#39;,&#39;ID&#39;,&#39;RPAREN&#39;</span><br></pre></td></tr></table></figure><br>将标记名和标记值本身组合起来：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(&#39;ID&#39;,&#39;x&#39;), (&#39;EQUALS&#39;,&#39;&#x3D;&#39;), (&#39;NUMBER&#39;,&#39;3&#39;),(&#39;PLUS&#39;,&#39;+&#39;), (&#39;NUMBER&#39;,&#39;42), (&#39;TIMES&#39;,&#39;*&#39;),(&#39;LPAREN&#39;,&#39;(&#39;), (&#39;ID&#39;,&#39;s&#39;),(&#39;MINUS&#39;,&#39;-&#39;),(&#39;ID&#39;,&#39;t&#39;), (&#39;RPAREN&#39;,&#39;)</span><br></pre></td></tr></table></figure></p>
<h3 id="LEX例子"><a href="#LEX例子" class="headerlink" title="LEX例子"></a>LEX例子</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import ply.lex as lex</span><br><span class="line"></span><br><span class="line"># List of token names.   This is always required</span><br><span class="line">tokens &#x3D; (</span><br><span class="line">   &#39;NUMBER&#39;,</span><br><span class="line">   &#39;PLUS&#39;,</span><br><span class="line">   &#39;MINUS&#39;,</span><br><span class="line">   &#39;TIMES&#39;,</span><br><span class="line">   &#39;DIVIDE&#39;,</span><br><span class="line">   &#39;LPAREN&#39;,</span><br><span class="line">   &#39;RPAREN&#39;,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># Regular expression rules for simple tokens</span><br><span class="line">t_PLUS    &#x3D; r&#39;\+&#39;</span><br><span class="line">t_MINUS   &#x3D; r&#39;-&#39;</span><br><span class="line">t_TIMES   &#x3D; r&#39;\*&#39;</span><br><span class="line">t_DIVIDE  &#x3D; r&#39;&#x2F;&#39;</span><br><span class="line">t_LPAREN  &#x3D; r&#39;\(&#39;</span><br><span class="line">t_RPAREN  &#x3D; r&#39;\)&#39;</span><br><span class="line"></span><br><span class="line"># A regular expression rule with some action code</span><br><span class="line">def t_NUMBER(t):</span><br><span class="line">    r&#39;\d+&#39;</span><br><span class="line">    t.value &#x3D; int(t.value)    </span><br><span class="line">    return t</span><br><span class="line"></span><br><span class="line"># Define a rule so we can track line numbers</span><br><span class="line">def t_newline(t):</span><br><span class="line">    r&#39;\n+&#39;</span><br><span class="line">    t.lexer.lineno +&#x3D; len(t.value)</span><br><span class="line"></span><br><span class="line"># A string containing ignored characters (spaces and tabs)</span><br><span class="line">t_ignore  &#x3D; &#39; \t&#39;</span><br><span class="line"></span><br><span class="line"># Error handling rule</span><br><span class="line">def t_error(t):</span><br><span class="line">    print(&quot;Illegal character &#39;%s&#39;&quot; % t.value[0])</span><br><span class="line">    t.lexer.skip(1)</span><br><span class="line"></span><br><span class="line"># Build the lexer</span><br><span class="line">lexer &#x3D; lex.lex()</span><br></pre></td></tr></table></figure>
<p>为了使 lexer 工作，你需要给定一个输入，并传递给input()方法。然后，重复调用token()方法来获取标记序列：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Test it out</span><br><span class="line">data &#x3D; &#39;&#39;&#39;</span><br><span class="line">3 + 4 * 10</span><br><span class="line">  + -20 *2</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line"></span><br><span class="line"># Give the lexer some input</span><br><span class="line">lexer.input(data)</span><br><span class="line"></span><br><span class="line"># Tokenize</span><br><span class="line">for tok in lexer:</span><br><span class="line">    if not tok: break      # No more input</span><br><span class="line">    print(tok.type, tok.value, tok.lineno, tok.lexpos)</span><br></pre></td></tr></table></figure><br>程序执行，将给出如下输出：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">LexToken(NUMBER,3,2,1)</span><br><span class="line">LexToken(PLUS,&#39;+&#39;,2,3)</span><br><span class="line">LexToken(NUMBER,4,2,5)</span><br><span class="line">LexToken(TIMES,&#39;*&#39;,2,7)</span><br><span class="line">LexToken(NUMBER,10,2,10)</span><br><span class="line">LexToken(PLUS,&#39;+&#39;,3,14)</span><br><span class="line">LexToken(MINUS,&#39;-&#39;,3,16)</span><br><span class="line">LexToken(NUMBER,20,3,18)</span><br><span class="line">LexToken(TIMES,&#39;*&#39;,3,20)</span><br><span class="line">LexToken(NUMBER,2,3,21)</span><br></pre></td></tr></table></figure><br>tok.type和tok.value属性表示标记本身的类型和值。tok.line和tok.lexpos属性包含了标记的位置信息，tok.lexpos表示标记相对于输入串起始位置的偏移。</p>
<h3 id="标记列表"><a href="#标记列表" class="headerlink" title="标记列表"></a>标记列表</h3><p>词法分析器必须提供一个标记的列表，这个列表将所有可能的标记告诉分析器，用来执行各种验证，同时也提供给 yacc.py 作为终结符。在上面的例子中，标记列表是由tokens指定的。</p>
<h3 id="标记的规则"><a href="#标记的规则" class="headerlink" title="标记的规则"></a>标记的规则</h3><p>每种标记用一个正则表达式规则来表示，每个规则需要以”t_”开头声明，表示该声明是对标记的规则定义。对于简单的标记，可以定义成这样（在 Python 中使用 raw string 能比较方便的书写正则表达式）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">t_PLUS &#x3D; r&#39;\+&#39;</span><br></pre></td></tr></table></figure>
<p>这里，紧跟在 t_ 后面的单词，必须跟标记列表中的某个标记名称对应。如果需要执行动作的话，规则可以写成一个方法。例如，下面的规则匹配数字字串，并且将匹配的字符串转化成 Python 的整型：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def t_NUMBER(t):</span><br><span class="line">    r&#39;\d+&#39;</span><br><span class="line">    t.value &#x3D; int(t.value)</span><br><span class="line">    return t</span><br></pre></td></tr></table></figure><br>如果使用方法的话，正则表达式写成方法的文档字符串。方法总是需要接受一个 LexToken 实例的参数，该实例有一个 t.type 的属性（字符串表示）来表示标记的类型名称，t.value 是标记值（匹配的实际的字符串），t.lineno 表示当前在源输入串中的作业行，t.lexpos 表示标记相对于输入串起始位置的偏移。默认情况下，t.type 是以t_开头的变量或方法的后面部分。方法可以在方法体里面修改这些属性。但是，如果这样做，应该返回结果 token，否则，标记将被丢弃。<br>在 lex 内部，lex.py 用re模块处理模式匹配，在构造最终的完整的正则式的时候，用户提供的规则按照下面的顺序加入：</p>
<ul>
<li>所有由方法定义的标记规则，按照他们的出现顺序依次加入</li>
<li>由字符串变量定义的标记规则按照其正则式长度倒序后，依次加入（长的先入）</li>
<li>顺序的约定对于精确匹配是必要的。比如，如果你想区分‘=’和‘==’，你需要确保‘==’优先检查。如果用字符串来定义这样的表达式的话，通过将较长的正则式先加入，可以帮助解决这个问题。用方法定义标记，可以显示地控制哪个规则优先检查。</li>
</ul>
<p>为了处理保留字，你应该写一个单一的规则来匹配这些标识，并在方法里面作特殊的查询：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">reserved &#x3D; &#123;</span><br><span class="line">   &#39;if&#39; : &#39;IF&#39;,</span><br><span class="line">   &#39;then&#39; : &#39;THEN&#39;,</span><br><span class="line">   &#39;else&#39; : &#39;ELSE&#39;,</span><br><span class="line">   &#39;while&#39; : &#39;WHILE&#39;,</span><br><span class="line">   ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">tokens &#x3D; [&#39;LPAREN&#39;,&#39;RPAREN&#39;,...,&#39;ID&#39;] + list(reserved.values())</span><br><span class="line"></span><br><span class="line">def t_ID(t):</span><br><span class="line">    r&#39;[a-zA-Z_][a-zA-Z_0-9]*&#39;</span><br><span class="line">    t.type &#x3D; reserved.get(t.value,&#39;ID&#39;)    # Check for reserved words</span><br></pre></td></tr></table></figure>
<pre><code>return t
</code></pre><p>这样做可以大大减少正则式的个数，并稍稍加快处理速度。注意：你应该避免为保留字编写单独的规则，例如，如果你像下面这样写：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">t_FOR   &#x3D; r&#39;for&#39;</span><br><span class="line">t_PRINT &#x3D; r&#39;print&#39;</span><br></pre></td></tr></table></figure><br>但是，这些规则照样也能够匹配以这些字符开头的单词，比如’forget’或者’printed’，这通常不是你想要的。</p>
<h3 id="标记的值"><a href="#标记的值" class="headerlink" title="标记的值"></a>标记的值</h3><p>标记被 lex 返回后，它们的值被保存在value属性中。正常情况下，value 是匹配的实际文本。事实上，value 可以被赋为任何 Python 支持的类型。例如，当扫描到标识符的时候，你可能不仅需要返回标识符的名字，还需要返回其在符号表中的位置，可以像下面这样写：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def t_ID(t):</span><br><span class="line">    ...</span><br><span class="line">    # Look up symbol table information and return a tuple</span><br><span class="line">    t.value &#x3D; (t.value, symbol_lookup(t.value))</span><br><span class="line">    ...</span><br><span class="line">    return t</span><br></pre></td></tr></table></figure>
<p>需要注意的是，不推荐用其他属性来保存值，因为 yacc.py 模块只会暴露出标记的 value属 性，访问其他属性会变得不自然。如果想保存多种属性，可以将元组、字典、或者对象实例赋给 value。</p>
<h3 id="丢弃标记"><a href="#丢弃标记" class="headerlink" title="丢弃标记"></a>丢弃标记</h3><p>想丢弃像注释之类的标记，只要不返回 value 就行了，像这样：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def t_COMMENT(t):</span><br><span class="line">    r&#39;\#.*&#39;</span><br><span class="line">    pass</span><br><span class="line">    # No return value. Token discarded</span><br></pre></td></tr></table></figure>
<p>为标记声明添加”ignore_”前缀同样可以达到目的：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">t_ignore_COMMENT &#x3D; r&#39;\#.*&#39;</span><br></pre></td></tr></table></figure><br>如果有多种文本需要丢弃，建议使用方法来定义规则，因为方法能够提供更精确的匹配优先级控制（方法根据出现的顺序，而字符串的正则表达式依据正则表达式的长度）</p>
<h3 id="行号和位置信息"><a href="#行号和位置信息" class="headerlink" title="行号和位置信息"></a>行号和位置信息</h3><p>默认情况下，lex.py 对行号一无所知。因为 lex.py 根本不知道何为”行”的概念（换行符本身也作为文本的一部分）。不过，可以通过写一个特殊的规则来记录行号：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Define a rule so we can track line numbers</span><br><span class="line">def t_newline(t):</span><br><span class="line">    r&#39;\n+&#39;</span><br><span class="line">    t.lexer.lineno +&#x3D; len(t.value)</span><br></pre></td></tr></table></figure>
<p>在这个规则中，当前 lexer 对象 t.lexer 的 lineno 属性被修改了，而且空行被简单的丢弃了，因为没有任何的返回。<br>lex.py 也不自动做列跟踪。但是，位置信息被记录在了每个标记对象的lexpos属性中，这样，就有可能来计算列信息了。例如：每当遇到新行的时候就重置列值：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Compute column. input is the input text string token is a token instance</span><br><span class="line">def find_column(input,token):</span><br><span class="line">    last_cr &#x3D; input.rfind(&#39;\n&#39;,0,token.lexpos)</span><br><span class="line">    if last_cr &lt; 0:</span><br><span class="line">        last_cr &#x3D; 0</span><br><span class="line">    column &#x3D; (token.lexpos - last_cr) + 1</span><br><span class="line">    return column</span><br></pre></td></tr></table></figure>
<p>通常，计算列的信息是为了指示上下文的错误位置，所以只在必要时有用。</p>
<h3 id="忽略字符"><a href="#忽略字符" class="headerlink" title="忽略字符"></a>忽略字符</h3><p>t_ignore规则比较特殊，是lex.py所保留用来忽略字符的，通常用来跳过空白或者不需要的字符。虽然可以通过定义像t_newline()这样的规则来完成相同的事情，不过使用t_ignore能够提供较好的词法分析性能，因为相比普通的正则式，它被特殊化处理了。用PLY写一个简单计算器</p>
<h3 id="字面字符"><a href="#字面字符" class="headerlink" title="字面字符"></a>字面字符</h3><p>字面字符可以通过在词法模块中定义一个literals变量做到，例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">literals &#x3D; [ &#39;+&#39;,&#39;-&#39;,&#39;*&#39;,&#39;&#x2F;&#39; ]</span><br></pre></td></tr></table></figure>
<p>或者<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">literals &#x3D; &quot;+-*&#x2F;&quot;</span><br></pre></td></tr></table></figure><br>字面字符是指单个字符，表示把字符本身作为标记，标记的type和value都是字符本身。不过，字面字符是在其他正则式之后被检查的，因此如果有规则是以这些字符开头的，那么这些规则的优先级较高。</p>
<h3 id="错误处理"><a href="#错误处理" class="headerlink" title="错误处理"></a>错误处理</h3><p>最后，在词法分析中遇到非法字符时，t_error()用来处理这类错误。这种情况下，t.value包含了余下还未被处理的输入字串，在之前的例子中，错误处理方法是这样的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Error handling rule</span><br><span class="line">def t_error(t):</span><br><span class="line">    print &quot;Illegal character &#39;%s&#39;&quot; % t.value[0]</span><br><span class="line">    t.lexer.skip(1)</span><br></pre></td></tr></table></figure>
<p>这个例子中，我们只是简单的输出不合法的字符，并且通过调用t.lexer.skip(1)跳过一个字符。</p>
<h3 id="构建和使用-lexer"><a href="#构建和使用-lexer" class="headerlink" title="构建和使用 lexer"></a>构建和使用 lexer</h3><p>函数lex.lex()使用 Python 的反射机制读取调用上下文中的正则表达式，来创建 lexer。lexer 一旦创建好，有两个方法可以用来控制 lexer 对象：</p>
<ul>
<li>lexer.input(data) 重置 lexer 和输入字串</li>
<li>lexer.token() 返回下一个 LexToken 类型的标记实例，如果进行到输入字串的尾部时将返回None</li>
</ul>
<p>推荐直接在 lex() 函数返回的 lexer 对象上调用上述接口，尽管也可以向下面这样用模块级别的 lex.input() 和 lex.token()：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lex.lex()</span><br><span class="line">lex.input(sometext)</span><br><span class="line">while 1:</span><br><span class="line">    tok &#x3D; lex.token()</span><br><span class="line">    if not tok: break</span><br><span class="line">    print tok</span><br></pre></td></tr></table></figure>
<p>在这个例子中，lex.input() 和 lex.token() 是模块级别的方法，在 lex 模块中，input() 和 token() 方法绑定到最新创建的 lexer 对象的对应方法上。最好不要这样用，因为这种接口可能不知道在什么时候就失效（例如垃圾回收）。</p>
<h3 id="TOKEN-装饰器"><a href="#TOKEN-装饰器" class="headerlink" title="@TOKEN 装饰器"></a>@TOKEN 装饰器</h3><p>在一些应用中，你可能需要定义一系列辅助的记号来构建复杂的正则表达式，例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">digit            &#x3D; r&#39;([0-9])&#39;</span><br><span class="line">nondigit         &#x3D; r&#39;([_A-Za-z])&#39;</span><br><span class="line">identifier       &#x3D; r&#39;(&#39; + nondigit + r&#39;(&#39; + digit + r&#39;|&#39; + nondigit + r&#39;)*)&#39;        </span><br><span class="line"></span><br><span class="line">def t_ID(t):</span><br><span class="line">    # want docstring to be identifier above. ?????</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p>在这个例子中，我们希望 ID 的规则引用上面的已有的变量。然而，使用文档字符串无法做到，为了解决这个问题，你可以使用@TOKEN装饰器：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from ply.lex import TOKEN</span><br><span class="line"></span><br><span class="line">@TOKEN(identifier)</span><br><span class="line">def t_ID(t):</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><br>装饰器可以将 identifier 关联到 t_ID() 的文档字符串上以使 lex.py 正常工作，一种等价的做法是直接给文档字符串赋值：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def t_ID(t):</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">t_ID.__doc__ &#x3D; identifier</span><br></pre></td></tr></table></figure></p>
<h3 id="优化模式"><a href="#优化模式" class="headerlink" title="优化模式"></a>优化模式</h3><p>为了提高性能，你可能希望使用 Python 的优化模式（比如，使用 -o 选项执行 Python）。然而，这样的话，Python 会忽略文档字串，这是 lex.py 的特殊问题，可以通过在创建 lexer 的时候使用 optimize 选项：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lexer &#x3D; lex.lex(optimize&#x3D;1)</span><br></pre></td></tr></table></figure>
<p>接着，用 Python 常规的模式运行，这样，lex.py 会在当前目录下创建一个 lextab.py 文件，这个文件会包含所有的正则表达式规则和词法分析阶段的分析表。然后，lextab.py 可以被导入用来构建 lexer。这种方法大大改善了词法分析程序的启动时间，而且可以在 Python 的优化模式下工作。<br>想要更改生成的文件名，使用如下参数：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lexer &#x3D; lex.lex(optimize&#x3D;1,lextab&#x3D;&quot;footab&quot;)</span><br></pre></td></tr></table></figure>
<p>在优化模式下执行，需要注意的是 lex 会被禁用大多数的错误检查。因此，建议只在确保万事俱备准备发布最终代码时使用。</p>
<h3 id="调试"><a href="#调试" class="headerlink" title="调试"></a>调试</h3><p>如果想要调试，可以使 lex() 运行在调试模式：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lexer &#x3D; lex.lex(debug&#x3D;1)</span><br></pre></td></tr></table></figure>
<p>这将打出一些调试信息，包括添加的规则、最终的正则表达式和词法分析过程中得到的标记。除此之外，lex.py 有一个简单的主函数，不但支持对命令行参数输入的字串进行扫描，还支持命令行参数指定的文件名：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;</span><br><span class="line">     lex.runmain()</span><br></pre></td></tr></table></figure></p>
<h3 id="其他方式定义词法规则"><a href="#其他方式定义词法规则" class="headerlink" title="其他方式定义词法规则"></a>其他方式定义词法规则</h3><p>上面的例子，词法分析器都是在单个的 Python 模块中指定的。如果你想将标记的规则放到不同的模块，使用 module 关键字参数。例如，你可能有一个专有的模块，包含了标记的规则：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># module: tokrules.py</span><br><span class="line"># This module just contains the lexing rules</span><br><span class="line"></span><br><span class="line"># List of token names.   This is always required</span><br><span class="line">tokens &#x3D; (</span><br><span class="line">   &#39;NUMBER&#39;,</span><br><span class="line">   &#39;PLUS&#39;,</span><br><span class="line">   &#39;MINUS&#39;,</span><br><span class="line">   &#39;TIMES&#39;,</span><br><span class="line">   &#39;DIVIDE&#39;,</span><br><span class="line">   &#39;LPAREN&#39;,</span><br><span class="line">   &#39;RPAREN&#39;,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># Regular expression rules for simple tokens</span><br><span class="line">t_PLUS    &#x3D; r&#39;\+&#39;</span><br><span class="line">t_MINUS   &#x3D; r&#39;-&#39;</span><br><span class="line">t_TIMES   &#x3D; r&#39;\*&#39;</span><br><span class="line">t_DIVIDE  &#x3D; r&#39;&#x2F;&#39;</span><br><span class="line">t_LPAREN  &#x3D; r&#39;\(&#39;</span><br><span class="line">t_RPAREN  &#x3D; r&#39;\)&#39;</span><br><span class="line"></span><br><span class="line"># A regular expression rule with some action code</span><br><span class="line">def t_NUMBER(t):</span><br><span class="line">    r&#39;\d+&#39;</span><br><span class="line">    t.value &#x3D; int(t.value)    </span><br><span class="line">    return t</span><br><span class="line"></span><br><span class="line"># Define a rule so we can track line numbers</span><br><span class="line">def t_newline(t):</span><br><span class="line">    r&#39;\n+&#39;</span><br><span class="line">    t.lexer.lineno +&#x3D; len(t.value)</span><br><span class="line"></span><br><span class="line"># A string containing ignored characters (spaces and tabs)</span><br><span class="line">t_ignore  &#x3D; &#39; \t&#39;</span><br><span class="line"></span><br><span class="line"># Error handling rule</span><br><span class="line">def t_error(t):</span><br><span class="line">    print &quot;Illegal character &#39;%s&#39;&quot; % t.value[0]</span><br></pre></td></tr></table></figure>
<pre><code>t.lexer.skip(1)
</code></pre><p>现在，如果你想要从不同的模块中构建分析器，应该这样：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import tokrules</span><br><span class="line">import ply.lex as lex</span><br><span class="line"></span><br><span class="line">lexer &#x3D; lex.lex(module&#x3D;tokrules)</span><br><span class="line">lexer.input(&quot;3 + 4&quot;)</span><br><span class="line">for tok in lexer:</span><br><span class="line">    if not tok: break      # No more input</span><br><span class="line">    print(tok.type, tok.value, tok.lineno, tok.lexpos)</span><br></pre></td></tr></table></figure><br>module选项也可以指定类型的实例，例如：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import ply.lex as lex</span><br><span class="line"></span><br><span class="line">class MyLexer:</span><br><span class="line">    # List of token names.   This is always required</span><br><span class="line">    tokens &#x3D; (</span><br><span class="line">       &#39;NUMBER&#39;,</span><br><span class="line">       &#39;PLUS&#39;,</span><br><span class="line">       &#39;MINUS&#39;,</span><br><span class="line">       &#39;TIMES&#39;,</span><br><span class="line">       &#39;DIVIDE&#39;,</span><br><span class="line">       &#39;LPAREN&#39;,</span><br><span class="line">       &#39;RPAREN&#39;,</span><br><span class="line">    )</span><br><span class="line">    # Regular expression rules for simple tokens</span><br><span class="line">    t_PLUS    &#x3D; r&#39;\+&#39;</span><br><span class="line">    t_MINUS   &#x3D; r&#39;-&#39;</span><br><span class="line">    t_TIMES   &#x3D; r&#39;\*&#39;</span><br><span class="line">    t_DIVIDE  &#x3D; r&#39;&#x2F;&#39;</span><br><span class="line">    t_LPAREN  &#x3D; r&#39;\(&#39;</span><br><span class="line">    t_RPAREN  &#x3D; r&#39;\)&#39;</span><br><span class="line">    # A regular expression rule with some action code</span><br><span class="line">    # Note addition of self parameter since we&#39;re in a class</span><br><span class="line">    def t_NUMBER(self,t):</span><br><span class="line">        r&#39;\d+&#39;</span><br><span class="line">        t.value &#x3D; int(t.value)    </span><br><span class="line">        return t</span><br><span class="line">    # Define a rule so we can track line numbers</span><br><span class="line">    def t_newline(self,t):</span><br><span class="line">        r&#39;\n+&#39;</span><br><span class="line">        t.lexer.lineno +&#x3D; len(t.value)</span><br><span class="line">    # A string containing ignored characters (spaces and tabs)</span><br><span class="line">    t_ignore  &#x3D; &#39; \t&#39;</span><br><span class="line">    # Error handling rule</span><br><span class="line">    def t_error(self,t):</span><br><span class="line">        print(&quot;Illegal character &#39;%s&#39;&quot; % t.value[0])</span><br><span class="line">        t.lexer.skip(1)</span><br><span class="line">    # Build the lexer</span><br><span class="line">    def build(self,**kwargs):</span><br><span class="line">        self.lexer &#x3D; lex.lex(module&#x3D;self, **kwargs)</span><br><span class="line">    </span><br><span class="line">    # Test it output</span><br><span class="line">    def test(self,data):</span><br><span class="line">        self.lexer.input(data)</span><br><span class="line">        while True:</span><br><span class="line">             tok &#x3D; self.lexer.token()</span><br><span class="line">             if not tok: break</span><br><span class="line">             print(tok)</span><br><span class="line"># Build the lexer and try it out</span><br><span class="line">m &#x3D; MyLexer()</span><br><span class="line">m.build()           # Build the lexer</span><br><span class="line">m.test(&quot;3 + 4&quot;)     # Test it</span><br></pre></td></tr></table></figure><br>当从类中定义 lexer，你需要创建类的实例，而不是类本身。这是因为，lexer 的方法只有被绑定（bound-methods）对象后才能使 PLY 正常工作。<br>当给 lex() 方法使用 module 选项时，PLY 使用dir()方法，从对象中获取符号信息，因为不能直接访问对象的<strong>dict</strong>属性。（译者注：可能是因为兼容性原因，<strong>dict</strong>这个方法可能不存在）</p>
<p>最后，如果你希望保持较好的封装性，但不希望什么东西都写在类里面，lexers 可以在闭包中定义，例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import ply.lex as lex</span><br><span class="line"></span><br><span class="line"># List of token names.   This is always required</span><br><span class="line">tokens &#x3D; (</span><br><span class="line">  &#39;NUMBER&#39;,</span><br><span class="line">  &#39;PLUS&#39;,</span><br><span class="line">  &#39;MINUS&#39;,</span><br><span class="line">  &#39;TIMES&#39;,</span><br><span class="line">  &#39;DIVIDE&#39;,</span><br><span class="line">  &#39;LPAREN&#39;,</span><br><span class="line">  &#39;RPAREN&#39;,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">def MyLexer():</span><br><span class="line">    # Regular expression rules for simple tokens</span><br><span class="line">    t_PLUS    &#x3D; r&#39;\+&#39;</span><br><span class="line">    t_MINUS   &#x3D; r&#39;-&#39;</span><br><span class="line">    t_TIMES   &#x3D; r&#39;\*&#39;</span><br><span class="line">    t_DIVIDE  &#x3D; r&#39;&#x2F;&#39;</span><br><span class="line">    t_LPAREN  &#x3D; r&#39;\(&#39;</span><br><span class="line">    t_RPAREN  &#x3D; r&#39;\)&#39;</span><br><span class="line"></span><br><span class="line">    # A regular expression rule with some action code</span><br><span class="line">    def t_NUMBER(t):</span><br><span class="line">        r&#39;\d+&#39;</span><br><span class="line">        t.value &#x3D; int(t.value)    </span><br><span class="line">        return t</span><br><span class="line"></span><br><span class="line">    # Define a rule so we can track line numbers</span><br><span class="line">    def t_newline(t):</span><br><span class="line">        r&#39;\n+&#39;</span><br><span class="line">        t.lexer.lineno +&#x3D; len(t.value)</span><br><span class="line"></span><br><span class="line">    # A string containing ignored characters (spaces and tabs)</span><br><span class="line">    t_ignore  &#x3D; &#39; \t&#39;</span><br><span class="line"></span><br><span class="line">    # Error handling rule</span><br><span class="line">    def t_error(t):</span><br><span class="line">        print &quot;Illegal character &#39;%s&#39;&quot; % t.value[0]</span><br><span class="line">        t.lexer.skip(1)</span><br><span class="line"></span><br><span class="line">    # Build the lexer from my environment and return it    </span><br><span class="line">    return lex.lex()</span><br></pre></td></tr></table></figure>
<h3 id="额外状态维护"><a href="#额外状态维护" class="headerlink" title="额外状态维护"></a>额外状态维护</h3><p>在你的词法分析器中，你可能想要维护一些状态。这可能包括模式设置，符号表和其他细节。例如，假设你想要跟踪NUMBER标记的出现个数。</p>
<p>一种方法是维护一个全局变量：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">num_count &#x3D; 0</span><br><span class="line">def t_NUMBER(t):</span><br><span class="line">    r&#39;\d+&#39;</span><br><span class="line">    global num_count</span><br><span class="line">    num_count +&#x3D; 1</span><br><span class="line">    t.value &#x3D; int(t.value)    </span><br><span class="line">    return t</span><br></pre></td></tr></table></figure>
<p>如果你不喜欢全局变量，另一个记录信息的地方是 lexer 对象内部。可以通过当前标记的 lexer 属性访问：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def t_NUMBER(t):</span><br><span class="line">    r&#39;\d+&#39;</span><br><span class="line">    t.lexer.num_count +&#x3D; 1     # Note use of lexer attribute</span><br><span class="line">    t.value &#x3D; int(t.value)    </span><br><span class="line">    return t</span><br><span class="line"></span><br><span class="line">lexer &#x3D; lex.lex()</span><br><span class="line">lexer.num_count &#x3D; 0            # Set the initial count</span><br></pre></td></tr></table></figure><br>上面这样做的优点是当同时存在多个 lexer 实例的情况下，简单易行。不过这看上去似乎是严重违反了面向对象的封装原则。lexer 的内部属性（除了 lineno ）都是以 lex 开头命名的（lexdata、lexpos）。因此，只要不以 lex 开头来命名属性就很安全的。<br>如果你不喜欢给 lexer 对象赋值，你可以自定义你的 lexer 类型，就像前面看到的那样：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class MyLexer:</span><br><span class="line">    ...</span><br><span class="line">    def t_NUMBER(self,t):</span><br><span class="line">        r&#39;\d+&#39;</span><br><span class="line">        self.num_count +&#x3D; 1</span><br><span class="line">        t.value &#x3D; int(t.value)    </span><br><span class="line">        return t</span><br><span class="line"></span><br><span class="line">    def build(self, **kwargs):</span><br><span class="line">        self.lexer &#x3D; lex.lex(object&#x3D;self,**kwargs)</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.num_count &#x3D; 0</span><br></pre></td></tr></table></figure>
<p>如果你的应用会创建很多 lexer 的实例，并且需要维护很多状态，上面的类可能是最容易管理的。<br>状态也可以用闭包来管理，比如，在 Python3 中：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def MyLexer():</span><br><span class="line">    num_count &#x3D; 0</span><br><span class="line">    ...</span><br><span class="line">    def t_NUMBER(t):</span><br><span class="line">        r&#39;\d+&#39;</span><br><span class="line">        nonlocal num_count</span><br><span class="line">        num_count +&#x3D; 1</span><br><span class="line">        t.value &#x3D; int(t.value)    </span><br><span class="line">        return t</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<h3 id="Lexer-克隆"><a href="#Lexer-克隆" class="headerlink" title="Lexer 克隆"></a>Lexer 克隆</h3><p>如果有必要的话，lexer 对象可以通过clone()方法来复制：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lexer &#x3D; lex.lex()</span><br><span class="line">...</span><br><span class="line">newlexer &#x3D; lexer.clone()</span><br></pre></td></tr></table></figure>
<p>当 lexer 被克隆后，复制品能够精确的保留输入串和内部状态，不过，新的 lexer 可以接受一个不同的输出字串，并独立运作起来。这在几种情况下也许有用：当你在编写的解析器或编译器涉及到递归或者回退处理时，你需要扫描先前的部分，你可以clone并使用复制品，或者你在实现某种预编译处理，可以 clone 一些 lexer 来处理不同的输入文件。<br>创建克隆跟重新调用 lex.lex() 的不同点在于，PLY 不会重新构建任何的内部分析表或者正则式。当 lexer 是用类或者闭包创建的，需要注意类或闭包本身的的状态。换句话说你要注意新创建的 lexer 会共享原始 lexer 的这些状态，比如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">m &#x3D; MyLexer()</span><br><span class="line">a &#x3D; lex.lex(object&#x3D;m)      # Create a lexer</span><br><span class="line"></span><br><span class="line">b &#x3D; a.clone()              # Clone the lexer</span><br></pre></td></tr></table></figure>
<h3 id="Lexer-的内部状态"><a href="#Lexer-的内部状态" class="headerlink" title="Lexer 的内部状态"></a>Lexer 的内部状态</h3><p>lexer 有一些内部属性在特定情况下有用：</p>
<ul>
<li>lexer.lexpos。这是一个表示当前分析点的位置的整型值。如果你修改这个值的话，这会改变下一个 token() 的调用行为。在标记的规则方法里面，这个值表示紧跟匹配字串后面的第一个字符的位置，如果这个值在规则中修改，下一个返回的标记将从新的位置开始匹配</li>
<li>lexer.lineno。表示当前行号。PLY 只是声明这个属性的存在，却永远不更新这个值。如果你想要跟踪行号的话，你需要自己添加代码（ 4.6 行号和位置信息）</li>
<li>lexer.lexdata。当前 lexer 的输入字串，这个字符串就是 input() 方法的输入字串，更改它可能是个糟糕的做法，除非你知道自己在干什么。</li>
<li>lexer.lexmatch。PLY 内部调用 Python 的 re.match() 方法得到的当前标记的原始的 Match 对象，该对象被保存在这个属性中。如果你的正则式中包含分组的话，你可以通过这个对象获得这些分组的值。注意：这个属性只在有标记规则定义的方法中才有效。<h3 id="基于条件的扫描和启动条件"><a href="#基于条件的扫描和启动条件" class="headerlink" title="基于条件的扫描和启动条件"></a>基于条件的扫描和启动条件</h3>在高级的分析器应用程序中，使用状态化的词法扫描是很有用的。比如，你想在出现特定标记或句子结构的时候触发开始一个不同的词法分析逻辑。PLY 允许 lexer 在不同的状态之间转换。每个状态可以包含一些自己独特的标记和规则等。这是基于 GNU flex 的“启动条件”来实现的，关于 flex 详见 <a href="http://flex.sourceforge.net/manual/Start-Conditions.html#Start-Conditions" target="_blank" rel="noopener">http://flex.sourceforge.net/manual/Start-Conditions.html#Start-Conditions</a></li>
</ul>
<p>要使用 lex 的状态，你必须首先声明。通过在 lex 模块中声明”states”来做到：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">states &#x3D; (</span><br><span class="line">   (&#39;foo&#39;,&#39;exclusive&#39;),</span><br><span class="line">   (&#39;bar&#39;,&#39;inclusive&#39;),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>这个声明中包含有两个状态：’foo’和’bar’。状态可以有两种类型：’排他型’和’包容型’。排他型的状态会使得 lexer 的行为发生完全的改变：只有能够匹配在这个状态下定义的规则的标记才会返回；包容型状态会将定义在这个状态下的规则添加到默认的规则集中，进而，只要能匹配这个规则集的标记都会返回。<br>一旦声明好之后，标记规则的命名需要包含状态名：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">t_foo_NUMBER &#x3D; r&#39;\d+&#39;                      # Token &#39;NUMBER&#39; in state &#39;foo&#39;        </span><br><span class="line">t_bar_ID     &#x3D; r&#39;[a-zA-Z_][a-zA-Z0-9_]*&#39;   # Token &#39;ID&#39; in state &#39;bar&#39;</span><br><span class="line"></span><br><span class="line">def t_foo_newline(t):</span><br><span class="line">    r&#39;\n&#39;</span><br><span class="line">    t.lexer.lineno +&#x3D; 1</span><br></pre></td></tr></table></figure>
<p>一个标记可以用在多个状态中，只要将多个状态名包含在声明中：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">t_foo_bar_NUMBER &#x3D; r&#39;\d+&#39;         # Defines token &#39;NUMBER&#39; in both state &#39;foo&#39; and &#39;bar&#39;</span><br></pre></td></tr></table></figure><br>同样的，在任何状态下都生效的声明可以在命名中使用ANY：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">t_ANY_NUMBER &#x3D; r&#39;\d+&#39;         # Defines a token &#39;NUMBER&#39; in all states</span><br></pre></td></tr></table></figure><br>不包含状态名的情况下，标记被关联到一个特殊的状态INITIAL，比如，下面两个声明是等价的：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">t_NUMBER &#x3D; r&#39;\d+&#39;</span><br><span class="line">t_INITIAL_NUMBER &#x3D; r&#39;\d+&#39;</span><br></pre></td></tr></table></figure><br>特殊的t_ignore()和t_error()也可以用状态关联：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">t_foo_ignore &#x3D; &quot; \t\n&quot;       # Ignored characters for state &#39;foo&#39;</span><br><span class="line">def t_bar_error(t):          # Special error handler for state &#39;bar&#39;</span><br><span class="line">    pass</span><br></pre></td></tr></table></figure><br>词法分析默认在INITIAL状态下工作，这个状态下包含了所有默认的标记规则定义。对于不希望使用“状态”的用户来说，这是完全透明的。在分析过程中，如果你想要改变词法分析器的这种的状态，使用begin()方法：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def t_begin_foo(t):</span><br><span class="line">    r&#39;start_foo&#39;</span><br><span class="line">    t.lexer.begin(&#39;foo&#39;)             # Starts &#39;foo&#39; state</span><br></pre></td></tr></table></figure><br>使用 begin() 切换回初始状态：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def t_foo_end(t):</span><br><span class="line">    r&#39;end_foo&#39;</span><br><span class="line">    t.lexer.begin(&#39;INITIAL&#39;)        # Back to the initial state</span><br></pre></td></tr></table></figure><br>状态的切换可以使用栈：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def t_begin_foo(t):</span><br><span class="line">    r&#39;start_foo&#39;</span><br><span class="line">    t.lexer.push_state(&#39;foo&#39;)             # Starts &#39;foo&#39; state</span><br><span class="line"></span><br><span class="line">def t_foo_end(t):</span><br><span class="line">    r&#39;end_foo&#39;</span><br><span class="line">    t.lexer.pop_state()                   # Back to the previous state</span><br></pre></td></tr></table></figure><br>当你在面临很多状态可以选择进入，而又仅仅想要回到之前的状态时，状态栈比较有用。<br>举个例子会更清晰。假设你在写一个分析器想要从一堆 C 代码中获取任意匹配的闭合的大括号里面的部分：这意味着，当遇到起始括号’{‘，你需要读取与之匹配的’}’以上的所有部分。并返回字符串。使用通常的正则表达式几乎不可能，这是因为大括号可以嵌套，而且可以有注释，字符串等干扰。因此，试图简单的匹配第一个出现的’}’是不行的。这里你可以用lex的状态来做到：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Declare the state</span><br><span class="line">states &#x3D; (</span><br><span class="line">  (&#39;ccode&#39;,&#39;exclusive&#39;),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># Match the first &#123;. Enter ccode state.</span><br><span class="line">def t_ccode(t):</span><br><span class="line">    r&#39;\&#123;&#39;</span><br><span class="line">    t.lexer.code_start &#x3D; t.lexer.lexpos        # Record the starting position</span><br><span class="line">    t.lexer.level &#x3D; 1                          # Initial brace level</span><br><span class="line">    t.lexer.begin(&#39;ccode&#39;)                     # Enter &#39;ccode&#39; state</span><br><span class="line"></span><br><span class="line"># Rules for the ccode state</span><br><span class="line">def t_ccode_lbrace(t):     </span><br><span class="line">    r&#39;\&#123;&#39;</span><br><span class="line">    t.lexer.level +&#x3D;1                </span><br><span class="line"></span><br><span class="line">def t_ccode_rbrace(t):</span><br><span class="line">    r&#39;\&#125;&#39;</span><br><span class="line">    t.lexer.level -&#x3D;1</span><br><span class="line"></span><br><span class="line">    # If closing brace, return the code fragment</span><br><span class="line">    if t.lexer.level &#x3D;&#x3D; 0:</span><br><span class="line">         t.value &#x3D; t.lexer.lexdata[t.lexer.code_start:t.lexer.lexpos+1]</span><br><span class="line">         t.type &#x3D; &quot;CCODE&quot;</span><br><span class="line">         t.lexer.lineno +&#x3D; t.value.count(&#39;\n&#39;)</span><br><span class="line">         t.lexer.begin(&#39;INITIAL&#39;)           </span><br><span class="line">         return t</span><br><span class="line"></span><br><span class="line"># C or C++ comment (ignore)    </span><br><span class="line">def t_ccode_comment(t):</span><br><span class="line">    r&#39;(&#x2F;\*(.|\n)*?*&#x2F;)|(&#x2F;&#x2F;.*)&#39;</span><br><span class="line">    pass</span><br><span class="line"></span><br><span class="line"># C string</span><br><span class="line">def t_ccode_string(t):</span><br><span class="line">   r&#39;\&quot;([^\\\n]|(\\.))*?\&quot;&#39;</span><br><span class="line"></span><br><span class="line"># C character literal</span><br><span class="line">def t_ccode_char(t):</span><br><span class="line">   r&#39;\&#39;([^\\\n]|(\\.))*?\&#39;&#39;</span><br><span class="line"></span><br><span class="line"># Any sequence of non-whitespace characters (not braces, strings)</span><br><span class="line">def t_ccode_nonspace(t):</span><br><span class="line">   r&#39;[^\s\&#123;\&#125;\&#39;\&quot;]+&#39;</span><br><span class="line"></span><br><span class="line"># Ignored characters (whitespace)</span><br><span class="line">t_ccode_ignore &#x3D; &quot; \t\n&quot;</span><br><span class="line"></span><br><span class="line"># For bad characters, we just skip over it</span><br><span class="line">def t_ccode_error(t):</span><br></pre></td></tr></table></figure>
<pre><code>t.lexer.skip(1)
</code></pre><p>这个例子中，第一个’{‘使得 lexer 记录了起始位置，并且进入新的状态’ccode’。一系列规则用来匹配接下来的输入，这些规则只是丢弃掉标记（不返回值），如果遇到闭合右括号，t_ccode_rbrace 规则收集其中所有的代码（利用先前记录的开始位置），并保存，返回的标记类型为’CCODE’，与此同时，词法分析的状态退回到初始状态。</p>
<h3 id="其他问题"><a href="#其他问题" class="headerlink" title="其他问题"></a>其他问题</h3><ul>
<li>lexer 需要输入的是一个字符串。好在大多数机器都有足够的内存，这很少导致性能的问题。这意味着，lexer 现在还不能用来处理文件流或者 socket 流。这主要是受到 re 模块的限制。</li>
<li>lexer 支持用 Unicode 字符描述标记的匹配规则，也支持输入字串包含 Unicode</li>
<li>如果你想要向re.compile()方法提供 flag，使用 reflags 选项：lex.lex(reflags=re.UNICODE)</li>
<li>由于 lexer 是全部用 Python 写的，性能很大程度上取决于 Python 的 re 模块，即使已经尽可能的高效了。当接收极其大量的输入文件时表现并不尽人意。如果担忧性能，你可以升级到最新的 Python，或者手工创建分析器，或者用 C 语言写 lexer 并做成扩展模块。</li>
</ul>
<p>如果你要创建一个手写的词法分析器并计划用在 yacc.py 中，只需要满足下面的要求：</p>
<ul>
<li>需要提供一个 token() 方法来返回下一个标记，如果没有可用的标记了，则返回 None。</li>
<li>token() 方法必须返回一个 tok 对象，具有 type 和 valu e属性。如果行号需要跟踪的话，标记还需要定义 lineno 属性。<h2 id="语法分析基础"><a href="#语法分析基础" class="headerlink" title="语法分析基础"></a>语法分析基础</h2>‘语法’通常用 BNF 范式来表达。例如，如果想要分析简单的算术表达式，你应该首先写下无二义的文法：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">expression : expression + term</span><br><span class="line">           | expression - term</span><br><span class="line">           | term</span><br><span class="line"></span><br><span class="line">term       : term * factor</span><br><span class="line">           | term &#x2F; factor</span><br><span class="line">           | factor</span><br><span class="line"></span><br><span class="line">factor     : NUMBER</span><br><span class="line">           | ( expression )</span><br></pre></td></tr></table></figure>
<p>在这个文法中，像NUMBER,+,-,*,/的符号被称为终结符，对应原始的输入。类似term，factor等称为非终结符，它们由一系列终结符或其他规则的符号组成，用来指代语法规则。<br>通常使用一种叫语法制导翻译的技术来指定某种语言的语义。在语法制导翻译中，符号及其属性出现在每个语法规则后面的动作中。每当一个语法被识别，动作就能够描述需要做什么。比如，对于上面给定的文法，想要实现一个简单的计算器，应该写成下面这样：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Grammar                             Action</span><br><span class="line">--------------------------------    -------------------------------------------- </span><br><span class="line">expression0 : expression1 + term    expression0.val &#x3D; expression1.val + term.val</span><br><span class="line">            | expression1 - term    expression0.val &#x3D; expression1.val - term.val</span><br><span class="line">            | term                  expression0.val &#x3D; term.val</span><br><span class="line"></span><br><span class="line">term0       : term1 * factor        term0.val &#x3D; term1.val * factor.val</span><br><span class="line">            | term1 &#x2F; factor        term0.val &#x3D; term1.val &#x2F; factor.val</span><br><span class="line">            | factor                term0.val &#x3D; factor.val</span><br><span class="line"></span><br><span class="line">factor      : NUMBER                factor.val &#x3D; int(NUMBER.lexval)</span><br></pre></td></tr></table></figure>
<pre><code>        | ( expression )        factor.val = expression.val
</code></pre><p>一种理解语法指导翻译的好方法是将符号看成对象。与符号相关的值代表了符号的“状态”（比如上面的 val 属性），语义行为用一组操作符号及符号值的函数或者方法来表达。<br>Yacc 用的分析技术是著名的 LR 分析法或者叫移进-归约分析法。LR 分析法是一种自下而上的技术：首先尝试识别右部的语法规则，每当右部得到满足，相应的行为代码将被触发执行，当前右边的语法符号将被替换为左边的语法符号。（归约）</p>
<p>LR 分析法一般这样实现：将下一个符号进栈，然后结合栈顶的符号和后继符号（译者注：下一个将要输入符号），与文法中的某种规则相比较。具体的算法可以在编译器的手册中查到，下面的例子展现了如果通过上面定义的文法，来分析 3 + 5 * ( 10 - 20 ) 这个表达式，$ 用来表示输入结束，action 里面的 Shift 就是进栈动作，简称移进；Reduce 是归约：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Step Symbol Stack           Input Tokens            Action</span><br><span class="line">---- ---------------------  ---------------------   -------------------------------</span><br><span class="line">1                           3 + 5 * ( 10 - 20 )$    Shift 3</span><br><span class="line">2    3                        + 5 * ( 10 - 20 )$    Reduce factor : NUMBER</span><br><span class="line">3    factor                   + 5 * ( 10 - 20 )$    Reduce term   : factor</span><br><span class="line">4    term                     + 5 * ( 10 - 20 )$    Reduce expr : term</span><br><span class="line">5    expr                     + 5 * ( 10 - 20 )$    Shift +</span><br><span class="line">6    expr +                     5 * ( 10 - 20 )$    Shift 5</span><br><span class="line">7    expr + 5                     * ( 10 - 20 )$    Reduce factor : NUMBER</span><br><span class="line">8    expr + factor                * ( 10 - 20 )$    Reduce term   : factor</span><br><span class="line">9    expr + term                  * ( 10 - 20 )$    Shift *</span><br><span class="line">10   expr + term *                  ( 10 - 20 )$    Shift (</span><br><span class="line">11   expr + term * (                  10 - 20 )$    Shift 10</span><br><span class="line">12   expr + term * ( 10                  - 20 )$    Reduce factor : NUMBER</span><br><span class="line">13   expr + term * ( factor              - 20 )$    Reduce term : factor</span><br><span class="line">14   expr + term * ( term                - 20 )$    Reduce expr : term</span><br><span class="line">15   expr + term * ( expr                - 20 )$    Shift -</span><br><span class="line">16   expr + term * ( expr -                20 )$    Shift 20</span><br><span class="line">17   expr + term * ( expr - 20                )$    Reduce factor : NUMBER</span><br><span class="line">18   expr + term * ( expr - factor            )$    Reduce term : factor</span><br><span class="line">19   expr + term * ( expr - term              )$    Reduce expr : expr - term</span><br><span class="line">20   expr + term * ( expr                     )$    Shift )</span><br><span class="line">21   expr + term * ( expr )                    $    Reduce factor : (expr)</span><br><span class="line">22   expr + term * factor                      $    Reduce term : term * factor</span><br><span class="line">23   expr + term                               $    Reduce expr : expr + term</span><br><span class="line">24   expr                                      $    Reduce expr</span><br></pre></td></tr></table></figure>
<p>25                                             $    Success!</p>
<p>在分析表达式的过程中，一个相关的自动状态机和后继符号决定了下一步应该做什么。如果下一个标记看起来是一个有效语法（产生式）的一部分（通过栈上的其他项判断这一点），那么这个标记应该进栈。如果栈顶的项可以组成一个完整的右部语法规则，一般就可以进行“归约”，用产生式左边的符号代替这一组符号。当归约发生时，相应的行为动作就会执行。如果输入标记既不能移进也不能归约的话，就会发生语法错误，分析器必须进行相应的错误恢复。分析器直到栈空并且没有另外的输入标记时，才算成功。 需要注意的是，这是基于一个有限自动机实现的，有限自动器被转化成分析表。分析表的构建比较复杂，超出了本文的讨论范围。不过，这构建过程的微妙细节能够解释为什么在上面的例子中，解析器选择在步骤 9 将标记转移到堆栈中，而不是按照规则 expr : expr + term 做归约。</p>
<h2 id="Yacc简介"><a href="#Yacc简介" class="headerlink" title="Yacc简介"></a>Yacc简介</h2><p>ply.yacc 模块实现了 PLY 的分析功能，‘yacc’是‘Yet Another Compiler Compiler’的缩写并保留了其作为 Unix 工具的名字。</p>
<h3 id="一个例子"><a href="#一个例子" class="headerlink" title="一个例子"></a>一个例子</h3><p>假设你希望实现上面的简单算术表达式的语法分析：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Yacc example</span><br><span class="line"></span><br><span class="line">import ply.yacc as yacc</span><br><span class="line"></span><br><span class="line"># Get the token map from the lexer.  This is required.</span><br><span class="line">from calclex import tokens</span><br><span class="line"></span><br><span class="line">def p_expression_plus(p):</span><br><span class="line">    &#39;expression : expression PLUS term&#39;</span><br><span class="line">    p[0] &#x3D; p[1] + p[3]</span><br><span class="line"></span><br><span class="line">def p_expression_minus(p):</span><br><span class="line">    &#39;expression : expression MINUS term&#39;</span><br><span class="line">    p[0] &#x3D; p[1] - p[3]</span><br><span class="line"></span><br><span class="line">def p_expression_term(p):</span><br><span class="line">    &#39;expression : term&#39;</span><br><span class="line">    p[0] &#x3D; p[1]</span><br><span class="line"></span><br><span class="line">def p_term_times(p):</span><br><span class="line">    &#39;term : term TIMES factor&#39;</span><br><span class="line">    p[0] &#x3D; p[1] * p[3]</span><br><span class="line"></span><br><span class="line">def p_term_div(p):</span><br><span class="line">    &#39;term : term DIVIDE factor&#39;</span><br><span class="line">    p[0] &#x3D; p[1] &#x2F; p[3]</span><br><span class="line"></span><br><span class="line">def p_term_factor(p):</span><br><span class="line">    &#39;term : factor&#39;</span><br><span class="line">    p[0] &#x3D; p[1]</span><br><span class="line"></span><br><span class="line">def p_factor_num(p):</span><br><span class="line">    &#39;factor : NUMBER&#39;</span><br><span class="line">    p[0] &#x3D; p[1]</span><br><span class="line"></span><br><span class="line">def p_factor_expr(p):</span><br><span class="line">    &#39;factor : LPAREN expression RPAREN&#39;</span><br><span class="line">    p[0] &#x3D; p[2]</span><br><span class="line"></span><br><span class="line"># Error rule for syntax errors</span><br><span class="line">def p_error(p):</span><br><span class="line">    print &quot;Syntax error in input!&quot;</span><br><span class="line"></span><br><span class="line"># Build the parser</span><br><span class="line">parser &#x3D; yacc.yacc()</span><br><span class="line"></span><br><span class="line">while True:</span><br><span class="line">   try:</span><br><span class="line">       s &#x3D; raw_input(&#39;calc &gt; &#39;)</span><br><span class="line">   except EOFError:</span><br><span class="line">       break</span><br><span class="line">   if not s: continue</span><br><span class="line">   result &#x3D; parser.parse(s)</span><br></pre></td></tr></table></figure>
<p>   print result</p>
<p>在这个例子中，每个语法规则被定义成一个 Python 的方法，方法的文档字符串描述了相应的上下文无关文法，方法的语句实现了对应规则的语义行为。每个方法接受一个单独的 p 参数，p 是一个包含有当前匹配语法的符号的序列，p[i] 与语法符号的对应关系如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_expression_plus(p):</span><br><span class="line">    &#39;expression : expression PLUS term&#39;</span><br><span class="line">    #   ^            ^        ^    ^</span><br><span class="line">    #  p[0]         p[1]     p[2] p[3]</span><br><span class="line"></span><br><span class="line">    p[0] &#x3D; p[1] + p[3]</span><br></pre></td></tr></table></figure><br>其中，p[i] 的值相当于词法分析模块中对 p.value 属性赋的值，对于非终结符的值，将在归约时由 p[0] 的赋值决定，这里的值可以是任何类型，当然，大多数情况下只是 Python 的简单类型、元组或者类的实例。在这个例子中，我们依赖这样一个事实：NUMBER 标记的值保存的是整型值，所有规则的行为都是得到这些整型值的算术运算结果，并传递结果。<br>在 yacc 中定义的第一个语法规则被默认为起始规则（这个例子中的第一个出现的 expression 规则）。一旦起始规则被分析器归约，而且再无其他输入，分析器终止，最后的值将返回（这个值将是起始规则的p[0]）。注意：也可以通过在 yacc() 中使用 start 关键字参数来指定起始规则。</p>
<p>p_error(p) 规则用于捕获语法错误。详见处理语法错误部分。</p>
<p>为了构建分析器，需要调用 yacc.yacc() 方法。这个方法查看整个当前模块，然后试图根据你提供的文法构建 LR 分析表。由于分析表的得出相对开销较大（尤其包含大量的语法的情况下），分析表被写入当前目录的一个叫 parsetab.py 的文件中。除此之外，会生成一个调试文件 parser.out。在接下来的执行中，yacc 直到发现文法发生变化，才会重新生成分析表和 parsetab.py 文件，否则 yacc 会从 parsetab.py 中加载分析表。注：如果有必要的话这里输出的文件名是可以改的。</p>
<p>如果在你的文法中有任何错误的话，yacc.py 会产生调试信息，而且可能抛出异常。一些可以被检测到的错误如下：</p>
<ul>
<li>方法重复定义（在语法文件中具有相同名字的方法）</li>
<li>二义文法产生的移进-归约和归约-归约冲突</li>
<li>指定了错误的文法</li>
<li>不可终止的递归（规则永远无法终结）</li>
<li>未使用的规则或标记</li>
<li>未定义的规则或标记</li>
</ul>
<p>这个例子的最后部分展示了如何执行由 yacc() 方法创建的分析器。你只需要简单的调用 parse()，并将输入字符串作为参数就能运行分析器。它将运行所有的语法规则，并返回整个分析的结果，这个结果就是在起始规则中赋给 p[0] 的值。</p>
<h3 id="将语法规则合并"><a href="#将语法规则合并" class="headerlink" title="将语法规则合并"></a>将语法规则合并</h3><p>如果语法规则类似的话，可以合并到一个方法中。例如，考虑前面例子中的两个规则：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_expression_plus(p):</span><br><span class="line">    &#39;expression : expression PLUS term&#39;</span><br><span class="line">    p[0] &#x3D; p[1] + p[3]</span><br><span class="line"></span><br><span class="line">def p_expression_minus(t):</span><br><span class="line">    &#39;expression : expression MINUS term&#39;</span><br><span class="line">    p[0] &#x3D; p[1] - p[3]</span><br></pre></td></tr></table></figure>
<p>比起写两个方法，你可以像下面这样写在一个方法里面：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_expression(p):</span><br><span class="line">    &#39;&#39;&#39;expression : expression PLUS term</span><br><span class="line">                  | expression MINUS term&#39;&#39;&#39;</span><br><span class="line">    if p[2] &#x3D;&#x3D; &#39;+&#39;:</span><br><span class="line">        p[0] &#x3D; p[1] + p[3]</span><br><span class="line">    elif p[2] &#x3D;&#x3D; &#39;-&#39;:</span><br><span class="line">        p[0] &#x3D; p[1] - p[3]</span><br></pre></td></tr></table></figure><br>总之，方法的文档字符串可以包含多个语法规则。所以，像这样写也是合法的（尽管可能会引起困惑）：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_binary_operators(p):</span><br><span class="line">    &#39;&#39;&#39;expression : expression PLUS term</span><br><span class="line">                  | expression MINUS term</span><br><span class="line">       term       : term TIMES factor</span><br><span class="line">                  | term DIVIDE factor&#39;&#39;&#39;</span><br><span class="line">    if p[2] &#x3D;&#x3D; &#39;+&#39;:</span><br><span class="line">        p[0] &#x3D; p[1] + p[3]</span><br><span class="line">    elif p[2] &#x3D;&#x3D; &#39;-&#39;:</span><br><span class="line">        p[0] &#x3D; p[1] - p[3]</span><br><span class="line">    elif p[2] &#x3D;&#x3D; &#39;*&#39;:</span><br><span class="line">        p[0] &#x3D; p[1] * p[3]</span><br><span class="line">    elif p[2] &#x3D;&#x3D; &#39;&#x2F;&#39;:</span><br><span class="line">        p[0] &#x3D; p[1] &#x2F; p[3]</span><br></pre></td></tr></table></figure><br>如果所有的规则都有相似的结构，那么将语法规则合并才是个不错的注意（比如，产生式的项数相同）。不然，语义动作可能会变得复杂。不过，简单情况下，可以使用len()方法区分，比如：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_expressions(p):</span><br><span class="line">    &#39;&#39;&#39;expression : expression MINUS expression</span><br><span class="line">                  | MINUS expression&#39;&#39;&#39;</span><br><span class="line">    if (len(p) &#x3D;&#x3D; 4):</span><br><span class="line">        p[0] &#x3D; p[1] - p[3]</span><br><span class="line">    elif (len(p) &#x3D;&#x3D; 3):</span><br><span class="line">        p[0] &#x3D; -p[2]</span><br></pre></td></tr></table></figure><br>如果考虑解析的性能，你应该避免像这些例子一样在一个语法规则里面用很多条件来处理。因为，每次检查当前究竟匹配的是哪个语法规则的时候，实际上重复做了分析器已经做过的事（分析器已经准确的知道哪个规则被匹配了）。为每个规则定义单独的方法，可以消除这点开销。</p>
<h3 id="字面字符-1"><a href="#字面字符-1" class="headerlink" title="字面字符"></a>字面字符</h3><p>如果愿意，可以在语法规则里面使用单个的字面字符，例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_binary_operators(p):</span><br><span class="line">    &#39;&#39;&#39;expression : expression &#39;+&#39; term</span><br><span class="line">                  | expression &#39;-&#39; term</span><br><span class="line">       term       : term &#39;*&#39; factor</span><br><span class="line">                  | term &#39;&#x2F;&#39; factor&#39;&#39;&#39;</span><br><span class="line">    if p[2] &#x3D;&#x3D; &#39;+&#39;:</span><br><span class="line">        p[0] &#x3D; p[1] + p[3]</span><br><span class="line">    elif p[2] &#x3D;&#x3D; &#39;-&#39;:</span><br><span class="line">        p[0] &#x3D; p[1] - p[3]</span><br><span class="line">    elif p[2] &#x3D;&#x3D; &#39;*&#39;:</span><br><span class="line">        p[0] &#x3D; p[1] * p[3]</span><br><span class="line">    elif p[2] &#x3D;&#x3D; &#39;&#x2F;&#39;:</span><br><span class="line">        p[0] &#x3D; p[1] &#x2F; p[3]</span><br></pre></td></tr></table></figure>
<p>字符必须像’+’那样使用单引号。除此之外，需要将用到的字符定义单独定义在 lex 文件的literals列表里：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Literals.  Should be placed in module given to lex()</span><br><span class="line">literals &#x3D; [&#39;+&#39;,&#39;-&#39;,&#39;*&#39;,&#39;&#x2F;&#39; ]</span><br></pre></td></tr></table></figure><br>字面的字符只能是单个字符。因此，像’&lt;=’或者’==’都是不合法的，只能使用一般的词法规则（例如 t_EQ = r’==’)。</p>
<h3 id="空产生式"><a href="#空产生式" class="headerlink" title="空产生式"></a>空产生式</h3><p>yacc.py 可以处理空产生式，像下面这样做：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_empty(p):</span><br><span class="line">    &#39;empty :&#39;</span><br><span class="line">    pass</span><br></pre></td></tr></table></figure>
<p>现在可以使用空匹配，只要将’empty’当成一个符号使用：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_optitem(p):</span><br><span class="line">    &#39;optitem : item&#39;</span><br><span class="line">    &#39;        | empty&#39;</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><br>注意：你可以将产生式保持’空’，来表示空匹配。然而，我发现用一个’empty’规则并用其来替代’空’，更容易表达意图，并有较好的可读性。</p>
<h3 id="改变起始符号"><a href="#改变起始符号" class="headerlink" title="改变起始符号"></a>改变起始符号</h3><p>默认情况下，在 yacc 中的第一条规则是起始语法规则（顶层规则）。可以用 start 标识来改变这种行为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">start &#x3D; &#39;foo&#39;</span><br><span class="line">def p_bar(p):</span><br><span class="line">    &#39;bar : A B&#39;</span><br><span class="line"></span><br><span class="line"># This is the starting rule due to the start specifier above</span><br><span class="line">def p_foo(p):</span><br><span class="line">    &#39;foo : bar X&#39;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>用 start 标识有助于在调试的时候将大型的语法规则分成小部分来分析。也可把 start 符号作为yacc的参数：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yacc.yacc(start&#x3D;&#39;foo&#39;)</span><br></pre></td></tr></table></figure></p>
<h3 id="处理二义文法"><a href="#处理二义文法" class="headerlink" title="处理二义文法"></a>处理二义文法</h3><p>上面例子中，对表达式的文法描述用一种特别的形式规避了二义文法。然而，在很多情况下，这样的特殊文法很难写，或者很别扭。一个更为自然和舒服的语法表达应该是这样的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">expression : expression PLUS expression</span><br><span class="line">           | expression MINUS expression</span><br><span class="line">           | expression TIMES expression</span><br><span class="line">           | expression DIVIDE expression</span><br><span class="line">           | LPAREN expression RPAREN</span><br><span class="line">           | NUMBER</span><br></pre></td></tr></table></figure>
<p>不幸的是，这样的文法是存在二义性的。举个例子，如果你要解析字符串”3 <em> 4 + 5”，操作符如何分组并没有指明，究竟是表示”(3 </em> 4) + 5”还是”3 <em> (4 + 5)”呢？<br>如果在 yacc.py 中存在二义文法，会输出”移进归约冲突”或者”归约归约冲突”。在分析器无法确定是将下一个符号移进栈还是将当前栈中的符号归约时会产生移进归约冲突。例如，对于”3 </em> 4 + 5”，分析器内部栈是这样工作的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Step Symbol Stack           Input Tokens            Action</span><br><span class="line">---- ---------------------  ---------------------   -------------------------------</span><br><span class="line">1    $                                3 * 4 + 5$    Shift 3</span><br><span class="line">2    $ 3                                * 4 + 5$    Reduce : expression : NUMBER</span><br><span class="line">3    $ expr                             * 4 + 5$    Shift *</span><br><span class="line">4    $ expr *                             4 + 5$    Shift 4</span><br><span class="line">5    $ expr * 4                             + 5$    Reduce: expression : NUMBER</span><br><span class="line">6    $ expr * expr                          + 5$    SHIFT&#x2F;REDUCE CONFLICT ????</span><br></pre></td></tr></table></figure>
<p>两种选择对于上面的上下文无关文法而言都是合法的。<br>默认情况下，所有的移进归约冲突会倾向于使用移进来处理。因此，对于上面的例子，分析器总是会将’+’进栈，而不是做归约。虽然在很多情况下，这个策略是合适的（像”if-then”和”if-then-else”），但这对于算术表达式是不够的。事实上，对于上面的例子，将’+’进栈是完全错误的，应当先将expr * expr归约，因为乘法的优先级要高于加法。</p>
<p>为了解决二义文法，尤其是对表达式文法，yacc.py 允许为标记单独指定优先级和结合性。需要像下面这样增加一个 precedence 变量：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">precedence &#x3D; (</span><br><span class="line">    (&#39;left&#39;, &#39;PLUS&#39;, &#39;MINUS&#39;),</span><br><span class="line">    (&#39;left&#39;, &#39;TIMES&#39;, &#39;DIVIDE&#39;),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>这样的定义说明 PLUS/MINUS 标记具有相同的优先级和左结合性，TIMES/DIVIDE 具有相同的优先级和左结合性。在 precedence 声明中，标记的优先级从低到高。因此，这个声明表明 TIMES/DIVIDE（他们较晚加入 precedence）的优先级高于 PLUS/MINUS。<br>由于为标记添加了数字表示的优先级和结合性的属性，所以，对于上面的例子，将会得到：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PLUS      : level &#x3D; 1,  assoc &#x3D; &#39;left&#39;</span><br><span class="line">MINUS     : level &#x3D; 1,  assoc &#x3D; &#39;left&#39;</span><br><span class="line">TIMES     : level &#x3D; 2,  assoc &#x3D; &#39;left&#39;</span><br><span class="line">DIVIDE    : level &#x3D; 2,  assoc &#x3D; &#39;left&#39;</span><br></pre></td></tr></table></figure>
<p>随后这些值被附加到语法规则的优先级和结合性属性上，这些值由最右边的终结符的优先级和结合性决定：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">expression : expression PLUS expression                 # level &#x3D; 1, left</span><br><span class="line">           | expression MINUS expression                # level &#x3D; 1, left</span><br><span class="line">           | expression TIMES expression                # level &#x3D; 2, left</span><br><span class="line">           | expression DIVIDE expression               # level &#x3D; 2, left</span><br><span class="line">           | LPAREN expression RPAREN                   # level &#x3D; None (not specified)</span><br></pre></td></tr></table></figure><br>           | NUMBER                                     # level = None (not specified)</p>
<p>当出现移进归约冲突时，分析器生成器根据下面的规则解决二义文法：</p>
<ul>
<li>如果当前的标记的优先级高于栈顶规则的优先级，移进当前标记</li>
<li>如果栈顶规则的优先级更高，进行归约</li>
<li>如果当前的标记与栈顶规则的优先级相同，如果标记是左结合的，则归约，否则，如果是右结合的则移进</li>
<li>如果没有优先级可以参考，默认对于移进归约冲突执行移进</li>
</ul>
<p>比如，当解析到”expression PLUS expression”这个语法时，下一个标记是 TIMES，此时将执行移进，因为 TIMES 具有比 PLUS 更高的优先级；当解析到”expression TIMES expression”，下一个标记是 PLUS，此时将执行归约，因为 PLUS 的优先级低于 TIMES。</p>
<p>如果在使用前三种技术解决已经归约冲突后，yacc.py 将不会报告语法中的冲突或者错误（不过，会在 parser.out 这个调试文件中输出一些信息）。</p>
<p>使用 precedence 指定优先级的技术会带来一个问题，有时运算符的优先级需要基于上下文。例如，考虑”3 + 4 * -5”中的一元的’-‘。数学上讲，一元运算符应当拥有较高的优先级。然而，在我们的 precedence 定义中，MINUS 的优先级却低于 TIMES。为了解决这个问题，precedene 规则中可以包含”虚拟标记”：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">precedence &#x3D; (</span><br><span class="line">    (&#39;left&#39;, &#39;PLUS&#39;, &#39;MINUS&#39;),</span><br><span class="line">    (&#39;left&#39;, &#39;TIMES&#39;, &#39;DIVIDE&#39;),</span><br><span class="line">    (&#39;right&#39;, &#39;UMINUS&#39;),            # Unary minus operator</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>在语法文件中，我们可以这么表示一元算符：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_expr_uminus(p):</span><br><span class="line">    &#39;expression : MINUS expression %prec UMINUS&#39;</span><br><span class="line">    p[0] &#x3D; -p[2]</span><br></pre></td></tr></table></figure><br>在这个例子中，%prec UMINUS 覆盖了默认的优先级（MINUS 的优先级），将 UMINUS 指代的优先级应用在该语法规则上。<br>起初，UMINUS 标记的例子会让人感到困惑。UMINUS 既不是输入的标记也不是语法规则，你应当将其看成 precedence 表中的特殊的占位符。当你使用 %prec 宏时，你是在告诉 yacc，你希望表达式使用这个占位符所表示的优先级，而不是正常的优先级。</p>
<p>还可以在 precedence 表中指定”非关联”。这表明你不希望链式运算符。比如，假如你希望支持比较运算符’&lt;’和’&gt;’，但是你不希望支持 a &lt; b &lt; c，只要简单指定规则如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">precedence &#x3D; (</span><br><span class="line">    (&#39;nonassoc&#39;, &#39;LESSTHAN&#39;, &#39;GREATERTHAN&#39;),  # Nonassociative operators</span><br><span class="line">    (&#39;left&#39;, &#39;PLUS&#39;, &#39;MINUS&#39;),</span><br><span class="line">    (&#39;left&#39;, &#39;TIMES&#39;, &#39;DIVIDE&#39;),</span><br><span class="line">    (&#39;right&#39;, &#39;UMINUS&#39;),            # Unary minus operator</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>此时，当输入形如 a &lt; b &lt; c 时，将产生语法错误，却不影响形如 a &lt; b 的表达式。<br>对于给定的符号集，存在多种语法规则可以匹配时会产生归约/归约冲突。这样的冲突往往很严重，而且总是通过匹配最早出现的语法规则来解决。归约/归约冲突几乎总是相同的符号集合具有不同的规则可以匹配，而在这一点上无法抉择，比如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">assignment :  ID EQUALS NUMBER</span><br><span class="line">           |  ID EQUALS expression</span><br><span class="line">           </span><br><span class="line">expression : expression PLUS expression</span><br><span class="line">           | expression MINUS expression</span><br><span class="line">           | expression TIMES expression</span><br><span class="line">           | expression DIVIDE expression</span><br><span class="line">           | LPAREN expression RPAREN</span><br><span class="line">           | NUMBER</span><br></pre></td></tr></table></figure>
<p>这个例子中，对于下面这两条规则将产生归约/归约冲突：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">assignment  : ID EQUALS NUMBER</span><br><span class="line">expression  : NUMBER</span><br></pre></td></tr></table></figure><br>比如，对于”a = 5”，分析器不知道应当按照 assignment : ID EQUALS NUMBER 归约，还是先将 5 归约成 expression，再归约成 assignment : ID EQUALS expression。<br>应当指出的是，只是简单的查看语法规则是很难减少归约/归约冲突。如果出现归约/归约冲突，yacc()会帮助打印出警告信息：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">WARNING: 1 reduce&#x2F;reduce conflict</span><br><span class="line">WARNING: reduce&#x2F;reduce conflict in state 15 resolved using rule (assignment -&gt; ID EQUALS NUMBER)</span><br><span class="line">WARNING: rejected rule (expression -&gt; NUMBER)</span><br></pre></td></tr></table></figure>
<p>上面的信息标识出了冲突的两条规则，但是，并无法指出究竟在什么情况下会出现这样的状态。想要发现问题，你可能需要结合语法规则和parser.out调试文件的内容。</p>
<h3 id="parser-out调试文件"><a href="#parser-out调试文件" class="headerlink" title="parser.out调试文件"></a>parser.out调试文件</h3><p>使用 LR 分析算法跟踪移进/归约冲突和归约/归约冲突是件乐在其中的事。为了辅助调试，yacc.py 在生成分析表时会创建出一个调试文件叫 parser.out：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Unused terminals:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Grammar</span><br><span class="line"></span><br><span class="line">Rule 1     expression -&gt; expression PLUS expression</span><br><span class="line">Rule 2     expression -&gt; expression MINUS expression</span><br><span class="line">Rule 3     expression -&gt; expression TIMES expression</span><br><span class="line">Rule 4     expression -&gt; expression DIVIDE expression</span><br><span class="line">Rule 5     expression -&gt; NUMBER</span><br><span class="line">Rule 6     expression -&gt; LPAREN expression RPAREN</span><br><span class="line"></span><br><span class="line">Terminals, with rules where they appear</span><br><span class="line"></span><br><span class="line">TIMES                : 3</span><br><span class="line">error                : </span><br><span class="line">MINUS                : 2</span><br><span class="line">RPAREN               : 6</span><br><span class="line">LPAREN               : 6</span><br><span class="line">DIVIDE               : 4</span><br><span class="line">PLUS                 : 1</span><br><span class="line">NUMBER               : 5</span><br><span class="line"></span><br><span class="line">Nonterminals, with rules where they appear</span><br><span class="line"></span><br><span class="line">expression           : 1 1 2 2 3 3 4 4 6 0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Parsing method: LALR</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">state 0</span><br><span class="line"></span><br><span class="line">    S&#39; -&gt; . expression</span><br><span class="line">    expression -&gt; . expression PLUS expression</span><br><span class="line">    expression -&gt; . expression MINUS expression</span><br><span class="line">    expression -&gt; . expression TIMES expression</span><br><span class="line">    expression -&gt; . expression DIVIDE expression</span><br><span class="line">    expression -&gt; . NUMBER</span><br><span class="line">    expression -&gt; . LPAREN expression RPAREN</span><br><span class="line"></span><br><span class="line">    NUMBER          shift and go to state 3</span><br><span class="line">    LPAREN          shift and go to state 2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">state 1</span><br><span class="line"></span><br><span class="line">    S&#39; -&gt; expression .</span><br><span class="line">    expression -&gt; expression . PLUS expression</span><br><span class="line">    expression -&gt; expression . MINUS expression</span><br><span class="line">    expression -&gt; expression . TIMES expression</span><br><span class="line">    expression -&gt; expression . DIVIDE expression</span><br><span class="line"></span><br><span class="line">    PLUS            shift and go to state 6</span><br><span class="line">    MINUS           shift and go to state 5</span><br><span class="line">    TIMES           shift and go to state 4</span><br><span class="line">    DIVIDE          shift and go to state 7</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">state 2</span><br><span class="line"></span><br><span class="line">    expression -&gt; LPAREN . expression RPAREN</span><br><span class="line">    expression -&gt; . expression PLUS expression</span><br><span class="line">    expression -&gt; . expression MINUS expression</span><br><span class="line">    expression -&gt; . expression TIMES expression</span><br><span class="line">    expression -&gt; . expression DIVIDE expression</span><br><span class="line">    expression -&gt; . NUMBER</span><br><span class="line">    expression -&gt; . LPAREN expression RPAREN</span><br><span class="line"></span><br><span class="line">    NUMBER          shift and go to state 3</span><br><span class="line">    LPAREN          shift and go to state 2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">state 3</span><br><span class="line"></span><br><span class="line">    expression -&gt; NUMBER .</span><br><span class="line"></span><br><span class="line">    $               reduce using rule 5</span><br><span class="line">    PLUS            reduce using rule 5</span><br><span class="line">    MINUS           reduce using rule 5</span><br><span class="line">    TIMES           reduce using rule 5</span><br><span class="line">    DIVIDE          reduce using rule 5</span><br><span class="line">    RPAREN          reduce using rule 5</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">state 4</span><br><span class="line"></span><br><span class="line">    expression -&gt; expression TIMES . expression</span><br><span class="line">    expression -&gt; . expression PLUS expression</span><br><span class="line">    expression -&gt; . expression MINUS expression</span><br><span class="line">    expression -&gt; . expression TIMES expression</span><br><span class="line">    expression -&gt; . expression DIVIDE expression</span><br><span class="line">    expression -&gt; . NUMBER</span><br><span class="line">    expression -&gt; . LPAREN expression RPAREN</span><br><span class="line"></span><br><span class="line">    NUMBER          shift and go to state 3</span><br><span class="line">    LPAREN          shift and go to state 2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">state 5</span><br><span class="line"></span><br><span class="line">    expression -&gt; expression MINUS . expression</span><br><span class="line">    expression -&gt; . expression PLUS expression</span><br><span class="line">    expression -&gt; . expression MINUS expression</span><br><span class="line">    expression -&gt; . expression TIMES expression</span><br><span class="line">    expression -&gt; . expression DIVIDE expression</span><br><span class="line">    expression -&gt; . NUMBER</span><br><span class="line">    expression -&gt; . LPAREN expression RPAREN</span><br><span class="line"></span><br><span class="line">    NUMBER          shift and go to state 3</span><br><span class="line">    LPAREN          shift and go to state 2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">state 6</span><br><span class="line"></span><br><span class="line">    expression -&gt; expression PLUS . expression</span><br><span class="line">    expression -&gt; . expression PLUS expression</span><br><span class="line">    expression -&gt; . expression MINUS expression</span><br><span class="line">    expression -&gt; . expression TIMES expression</span><br><span class="line">    expression -&gt; . expression DIVIDE expression</span><br><span class="line">    expression -&gt; . NUMBER</span><br><span class="line">    expression -&gt; . LPAREN expression RPAREN</span><br><span class="line"></span><br><span class="line">    NUMBER          shift and go to state 3</span><br><span class="line">    LPAREN          shift and go to state 2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">state 7</span><br><span class="line"></span><br><span class="line">    expression -&gt; expression DIVIDE . expression</span><br><span class="line">    expression -&gt; . expression PLUS expression</span><br><span class="line">    expression -&gt; . expression MINUS expression</span><br><span class="line">    expression -&gt; . expression TIMES expression</span><br><span class="line">    expression -&gt; . expression DIVIDE expression</span><br><span class="line">    expression -&gt; . NUMBER</span><br><span class="line">    expression -&gt; . LPAREN expression RPAREN</span><br><span class="line"></span><br><span class="line">    NUMBER          shift and go to state 3</span><br><span class="line">    LPAREN          shift and go to state 2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">state 8</span><br><span class="line"></span><br><span class="line">    expression -&gt; LPAREN expression . RPAREN</span><br><span class="line">    expression -&gt; expression . PLUS expression</span><br><span class="line">    expression -&gt; expression . MINUS expression</span><br><span class="line">    expression -&gt; expression . TIMES expression</span><br><span class="line">    expression -&gt; expression . DIVIDE expression</span><br><span class="line"></span><br><span class="line">    RPAREN          shift and go to state 13</span><br><span class="line">    PLUS            shift and go to state 6</span><br><span class="line">    MINUS           shift and go to state 5</span><br><span class="line">    TIMES           shift and go to state 4</span><br><span class="line">    DIVIDE          shift and go to state 7</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">state 9</span><br><span class="line"></span><br><span class="line">    expression -&gt; expression TIMES expression .</span><br><span class="line">    expression -&gt; expression . PLUS expression</span><br><span class="line">    expression -&gt; expression . MINUS expression</span><br><span class="line">    expression -&gt; expression . TIMES expression</span><br><span class="line">    expression -&gt; expression . DIVIDE expression</span><br><span class="line"></span><br><span class="line">    $               reduce using rule 3</span><br><span class="line">    PLUS            reduce using rule 3</span><br><span class="line">    MINUS           reduce using rule 3</span><br><span class="line">    TIMES           reduce using rule 3</span><br><span class="line">    DIVIDE          reduce using rule 3</span><br><span class="line">    RPAREN          reduce using rule 3</span><br><span class="line"></span><br><span class="line">  ! PLUS            [ shift and go to state 6 ]</span><br><span class="line">  ! MINUS           [ shift and go to state 5 ]</span><br><span class="line">  ! TIMES           [ shift and go to state 4 ]</span><br><span class="line">  ! DIVIDE          [ shift and go to state 7 ]</span><br><span class="line"></span><br><span class="line">state 10</span><br><span class="line"></span><br><span class="line">    expression -&gt; expression MINUS expression .</span><br><span class="line">    expression -&gt; expression . PLUS expression</span><br><span class="line">    expression -&gt; expression . MINUS expression</span><br><span class="line">    expression -&gt; expression . TIMES expression</span><br><span class="line">    expression -&gt; expression . DIVIDE expression</span><br><span class="line"></span><br><span class="line">    $               reduce using rule 2</span><br><span class="line">    PLUS            reduce using rule 2</span><br><span class="line">    MINUS           reduce using rule 2</span><br><span class="line">    RPAREN          reduce using rule 2</span><br><span class="line">    TIMES           shift and go to state 4</span><br><span class="line">    DIVIDE          shift and go to state 7</span><br><span class="line"></span><br><span class="line">  ! TIMES           [ reduce using rule 2 ]</span><br><span class="line">  ! DIVIDE          [ reduce using rule 2 ]</span><br><span class="line">  ! PLUS            [ shift and go to state 6 ]</span><br><span class="line">  ! MINUS           [ shift and go to state 5 ]</span><br><span class="line"></span><br><span class="line">state 11</span><br><span class="line"></span><br><span class="line">    expression -&gt; expression PLUS expression .</span><br><span class="line">    expression -&gt; expression . PLUS expression</span><br><span class="line">    expression -&gt; expression . MINUS expression</span><br><span class="line">    expression -&gt; expression . TIMES expression</span><br><span class="line">    expression -&gt; expression . DIVIDE expression</span><br><span class="line"></span><br><span class="line">    $               reduce using rule 1</span><br><span class="line">    PLUS            reduce using rule 1</span><br><span class="line">    MINUS           reduce using rule 1</span><br><span class="line">    RPAREN          reduce using rule 1</span><br><span class="line">    TIMES           shift and go to state 4</span><br><span class="line">    DIVIDE          shift and go to state 7</span><br><span class="line"></span><br><span class="line">  ! TIMES           [ reduce using rule 1 ]</span><br><span class="line">  ! DIVIDE          [ reduce using rule 1 ]</span><br><span class="line">  ! PLUS            [ shift and go to state 6 ]</span><br><span class="line">  ! MINUS           [ shift and go to state 5 ]</span><br><span class="line"></span><br><span class="line">state 12</span><br><span class="line"></span><br><span class="line">    expression -&gt; expression DIVIDE expression .</span><br><span class="line">    expression -&gt; expression . PLUS expression</span><br><span class="line">    expression -&gt; expression . MINUS expression</span><br><span class="line">    expression -&gt; expression . TIMES expression</span><br><span class="line">    expression -&gt; expression . DIVIDE expression</span><br><span class="line"></span><br><span class="line">    $               reduce using rule 4</span><br><span class="line">    PLUS            reduce using rule 4</span><br><span class="line">    MINUS           reduce using rule 4</span><br><span class="line">    TIMES           reduce using rule 4</span><br><span class="line">    DIVIDE          reduce using rule 4</span><br><span class="line">    RPAREN          reduce using rule 4</span><br><span class="line"></span><br><span class="line">  ! PLUS            [ shift and go to state 6 ]</span><br><span class="line">  ! MINUS           [ shift and go to state 5 ]</span><br><span class="line">  ! TIMES           [ shift and go to state 4 ]</span><br><span class="line">  ! DIVIDE          [ shift and go to state 7 ]</span><br><span class="line"></span><br><span class="line">state 13</span><br><span class="line"></span><br><span class="line">    expression -&gt; LPAREN expression RPAREN .</span><br><span class="line"></span><br><span class="line">    $               reduce using rule 6</span><br><span class="line">    PLUS            reduce using rule 6</span><br><span class="line">    MINUS           reduce using rule 6</span><br><span class="line">    TIMES           reduce using rule 6</span><br><span class="line">    DIVIDE          reduce using rule 6</span><br></pre></td></tr></table></figure>
<pre><code>RPAREN          reduce using rule 6
</code></pre><p>文件中出现的不同状态，代表了有效输入标记的所有可能的组合，这是依据文法规则得到的。当得到输入标记时，分析器将构造一个栈，并找到匹配的规则。每个状态跟踪了当前输入进行到语法规则中的哪个位置，在每个规则中，’.’表示当前分析到规则的哪个位置，而且，对于在当前状态下，输入的每个有效标记导致的动作也被罗列出来。当出现移进/归约或归约/归约冲突时，被忽略的规则前面会添加!，就像这样：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">! TIMES           [ reduce using rule 2 ]</span><br><span class="line">  ! DIVIDE          [ reduce using rule 2 ]</span><br><span class="line">  ! PLUS            [ shift and go to state 6 ]</span><br><span class="line">  ! MINUS           [ shift and go to state 5 ]</span><br></pre></td></tr></table></figure>
<p>通过查看这些规则并结合一些实例，通常能够找到大部分冲突的根源。应该强调的是，不是所有的移进归约冲突都是不好的，想要确定解决方法是否正确，唯一的办法就是查看 parser.out。</p>
<h3 id="处理语法错误"><a href="#处理语法错误" class="headerlink" title="处理语法错误"></a>处理语法错误</h3><p>如果你创建的分析器用于产品，处理语法错误是很重要的。一般而言，你不希望分析器在遇到错误的时候就抛出异常并终止，相反，你需要它报告错误，尽可能的恢复并继续分析，一次性的将输入中所有的错误报告给用户。这是一些已知语言编译器的标准行为，例如 C,C++,Java。在 PLY 中，在语法分析过程中出现错误，错误会被立即检测到（分析器不会继续读取源文件中错误点后面的标记）。然而，这时，分析器会进入恢复模式，这个模式能够用来尝试继续向下分析。LR 分析器的错误恢复是个理论与技巧兼备的问题，yacc.py 提供的错误机制与 Unix 下的 yacc 类似，所以你可以从诸如 O’Reilly 出版的《Lex and yacc》的书中找到更多的细节。</p>
<p>当错误发生时，yacc.py 按照如下步骤进行：</p>
<ul>
<li>第一次错误产生时，用户定义的 p_error()方法会被调用，出错的标记会作为参数传入；如果错误是因为到达文件结尾造成的，传入的参数将为 None。随后，分析器进入到“错误恢复”模式，该模式下不会在产生p_error()调用，直到它成功的移进 3 个标记，然后回归到正常模式。</li>
<li>如果在 p_error() 中没有指定恢复动作的话，这个导致错误的标记会被替换成一个特殊的 error 标记。</li>
<li>如果导致错误的标记已经是 error 的话，原先的栈顶的标记将被移除。</li>
<li>如果整个分析栈被放弃，分析器会进入重置状态，并从他的初始状态开始分析。</li>
<li>如果此时的语法规则接受 error 标记，error 标记会移进栈。</li>
<li>如果当前栈顶是 error 标记，之后的标记将被忽略，直到有标记能够导致 error 的归约。<ul>
<li>根据 error 规则恢复和再同步</li>
</ul>
</li>
</ul>
<p>最佳的处理语法错误的做法是在语法规则中包含 error 标记。例如，假设你的语言有一个关于 print 的语句的语法规则：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_statement_print(p):</span><br><span class="line">     &#39;statement : PRINT expr SEMI&#39;</span><br><span class="line">     ...</span><br></pre></td></tr></table></figure>
<p>为了处理可能的错误表达式，你可以添加一条额外的语法规则：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_statement_print_error(p):</span><br><span class="line">     &#39;statement : PRINT error SEMI&#39;</span><br><span class="line">     print &quot;Syntax error in print statement. Bad expression&quot;</span><br></pre></td></tr></table></figure><br>这样（expr 错误时），error 标记会匹配任意多个分号之前的标记（分号是SEMI指代的字符）。一旦找到分号，规则将被匹配，这样 error 标记就被归约了。<br>这种类型的恢复有时称为”分析器再同步”。error 标记扮演了表示所有错误标记的通配符的角色，而紧随其后的标记扮演了同步标记的角色。</p>
<p>重要的一个说明是，通常 error 不会作为语法规则的最后一个标记，像这样：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_statement_print_error(p):</span><br><span class="line">    &#39;statement : PRINT error&#39;</span><br><span class="line">    print &quot;Syntax error in print statement. Bad expression&quot;</span><br></pre></td></tr></table></figure>
<p>这是因为，第一个导致错误的标记会使得该规则立刻归约，进而使得在后面还有错误标记的情况下，恢复变得困难。</p>
<ul>
<li>悲观恢复模式</li>
</ul>
<p>另一个错误恢复方法是采用“悲观模式”：该模式下，开始放弃剩余的标记，直到能够达到一个合适的恢复机会。</p>
<p>悲观恢复模式都是在 p_error() 方法中做到的。例如，这个方法在开始丢弃标记后，直到找到闭合的’}’，才重置分析器到初始化状态：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_error(p):</span><br><span class="line">    print &quot;Whoa. You are seriously hosed.&quot;</span><br><span class="line">    # Read ahead looking for a closing &#39;&#125;&#39;</span><br><span class="line">    while 1:</span><br><span class="line">        tok &#x3D; yacc.token()             # Get the next token</span><br><span class="line">        if not tok or tok.type &#x3D;&#x3D; &#39;RBRACE&#39;: break</span><br><span class="line">    yacc.restart()</span><br></pre></td></tr></table></figure>
<p>下面这个方法简单的抛弃错误的标记，并告知分析器错误被接受了：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_error(p):</span><br><span class="line">    print &quot;Syntax error at token&quot;, p.type</span><br><span class="line">    # Just discard the token and tell the parser it&#39;s okay.</span><br><span class="line">    yacc.errok()</span><br></pre></td></tr></table></figure><br>在p_error()方法中，有三个可用的方法来控制分析器的行为：</p>
<ul>
<li>yacc.errok() 这个方法将分析器从恢复模式切换回正常模式。这会使得不会产生 error 标记，并重置内部的 error 计数器，而且下一个语法错误会再次产生 p_error() 调用</li>
<li>yacc.token() 这个方法用于得到下一个标记</li>
<li>yacc.restart() 这个方法抛弃当前整个分析栈，并重置分析器为起始状态</li>
</ul>
<p>注意：这三个方法只能在p_error()中使用，不能用在其他任何地方。</p>
<p>p_error()方法也可以返回标记，这样能够控制将哪个标记作为下一个标记返回给分析器。这对于需要同步一些特殊标记的时候有用，比如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_error(p):</span><br><span class="line">    # Read ahead looking for a terminating &quot;;&quot;</span><br><span class="line">    while 1:</span><br><span class="line">        tok &#x3D; yacc.token()             # Get the next token</span><br><span class="line">        if not tok or tok.type &#x3D;&#x3D; &#39;SEMI&#39;: break</span><br><span class="line">    yacc.errok()</span><br><span class="line"></span><br><span class="line">    # Return SEMI to the parser as the next lookahead token</span><br><span class="line">    return tok</span><br></pre></td></tr></table></figure>
<ul>
<li>从产生式中抛出错误</li>
</ul>
<p>如果有需要的话，产生式规则可以主动的使分析器进入恢复模式。这是通过抛出SyntaxError异常做到的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_production(p):</span><br><span class="line">    &#39;production : some production ...&#39;</span><br><span class="line">    raise SyntaxError</span><br></pre></td></tr></table></figure>
<p>raise SyntaxError 错误的效果就如同当前的标记是错误标记一样。因此，当你这么做的话，最后一个标记将被弹出栈，当前的下一个标记将是 error 标记，分析器进入恢复模式，试图归约满足 error 标记的规则。此后的步骤与检测到语法错误的情况是完全一样的，p_error() 也会被调用。<br>手动设置错误有个重要的方面，就是 p_error() 方法在这种情况下不会调用。如果你希望记录错误，确保在抛出 SyntaxError 错误的产生式中实现。</p>
<p>注：这个功能是为了模仿 yacc 中的YYERROR宏的行为</p>
<ul>
<li>错误恢复总结</li>
</ul>
<p>对于通常的语言，使用 error 规则和再同步标记可能是最合理的手段。这是因为你可以将语法设计成在一个相对容易恢复和继续分析的点捕获错误。悲观恢复模式只在一些十分特殊的应用中有用，这些应用往往需要丢弃掉大量输入，再寻找合理的同步点。</p>
<h3 id="行号和位置的跟踪"><a href="#行号和位置的跟踪" class="headerlink" title="行号和位置的跟踪"></a>行号和位置的跟踪</h3><p>位置跟踪通常是个设计编译器时的技巧性玩意儿。默认情况下，PLY 跟踪所有标记的行号和位置，这些信息可以这样得到：</p>
<ul>
<li>p.lineno(num) 返回第 num 个符号的行号</li>
<li>p.lexpos(num) 返回第 num 个符号的词法位置偏移<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_expression(p):</span><br><span class="line">    &#39;expression : expression PLUS expression&#39;</span><br><span class="line">    p.lineno(1)        # Line number of the left expression</span><br><span class="line">    p.lineno(2)        # line number of the PLUS operator</span><br><span class="line">    p.lineno(3)        # line number of the right expression</span><br><span class="line">    ...</span><br><span class="line">    start,end &#x3D; p.linespan(3)    # Start,end lines of the right expression</span><br><span class="line">    starti,endi &#x3D; p.lexspan(3)   # Start,end positions of right expression</span><br></pre></td></tr></table></figure>
注意：lexspan() 方法只会返回的结束位置是最后一个符号的起始位置。<br>虽然，PLY 对所有符号的行号和位置的跟踪很管用，但经常是不必要的。例如，你仅仅是在错误信息中使用行号，你通常可以仅仅使用关键标记的信息，比如：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_bad_func(p):</span><br><span class="line">    &#39;funccall : fname LPAREN error RPAREN&#39;</span><br><span class="line">    # Line number reported from LPAREN token</span><br><span class="line">    print &quot;Bad function call at line&quot;, p.lineno(2)</span><br></pre></td></tr></table></figure>
<p>类似的，为了改善性能，你可以有选择性的将行号信息在必要的时候进行传递，这是通过 p.set_lineno() 实现的，例如：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_fname(p):</span><br><span class="line">    &#39;fname : ID&#39;</span><br><span class="line">    p[0] &#x3D; p[1]</span><br><span class="line">    p.set_lineno(0,p.lineno(1))</span><br></pre></td></tr></table></figure><br>对于已经完成分析的规则，PLY 不会保留行号信息，如果你是在构建抽象语法树而且需要行号，你应该确保行号保留在树上。</p>
<h3 id="构造抽象语法树"><a href="#构造抽象语法树" class="headerlink" title="构造抽象语法树"></a>构造抽象语法树</h3><p>yacc.py 没有构造抽像语法树的特殊方法。不过，你可以自己很简单的构造出来。</p>
<p>一个最为简单的构造方法是为每个语法规则创建元组或者字典，并传递它们。有很多中可行的方案，下面是一个例子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_expression_binop(p):</span><br><span class="line">    &#39;&#39;&#39;expression : expression PLUS expression</span><br><span class="line">                  | expression MINUS expression</span><br><span class="line">                  | expression TIMES expression</span><br><span class="line">                  | expression DIVIDE expression&#39;&#39;&#39;</span><br><span class="line"></span><br><span class="line">    p[0] &#x3D; (&#39;binary-expression&#39;,p[2],p[1],p[3])</span><br><span class="line"></span><br><span class="line">def p_expression_group(p):</span><br><span class="line">    &#39;expression : LPAREN expression RPAREN&#39;</span><br><span class="line">    p[0] &#x3D; (&#39;group-expression&#39;,p[2])</span><br><span class="line"></span><br><span class="line">def p_expression_number(p):</span><br><span class="line">    &#39;expression : NUMBER&#39;</span><br><span class="line">    p[0] &#x3D; (&#39;number-expression&#39;,p[1])</span><br></pre></td></tr></table></figure>
<p>另一种方法可以是为不同的抽象树节点创建一系列的数据结构，并赋值给 p[0]：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Expr: pass</span><br><span class="line"></span><br><span class="line">class BinOp(Expr):</span><br><span class="line">    def __init__(self,left,op,right):</span><br><span class="line">        self.type &#x3D; &quot;binop&quot;</span><br><span class="line">        self.left &#x3D; left</span><br><span class="line">        self.right &#x3D; right</span><br><span class="line">        self.op &#x3D; op</span><br><span class="line"></span><br><span class="line">class Number(Expr):</span><br><span class="line">    def __init__(self,value):</span><br><span class="line">        self.type &#x3D; &quot;number&quot;</span><br><span class="line">        self.value &#x3D; value</span><br><span class="line"></span><br><span class="line">def p_expression_binop(p):</span><br><span class="line">    &#39;&#39;&#39;expression : expression PLUS expression</span><br><span class="line">                  | expression MINUS expression</span><br><span class="line">                  | expression TIMES expression</span><br><span class="line">                  | expression DIVIDE expression&#39;&#39;&#39;</span><br><span class="line"></span><br><span class="line">    p[0] &#x3D; BinOp(p[1],p[2],p[3])</span><br><span class="line"></span><br><span class="line">def p_expression_group(p):</span><br><span class="line">    &#39;expression : LPAREN expression RPAREN&#39;</span><br><span class="line">    p[0] &#x3D; p[2]</span><br><span class="line"></span><br><span class="line">def p_expression_number(p):</span><br><span class="line">    &#39;expression : NUMBER&#39;</span><br><span class="line">    p[0] &#x3D; Number(p[1])</span><br></pre></td></tr></table></figure><br>这种方式的好处是在处理复杂语义时比较简单：类型检查、代码生成、以及其他针对树节点的功能。<br>为了简化树的遍历，可以创建一个通用的树节点结构，例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Node:</span><br><span class="line">    def __init__(self,type,children&#x3D;None,leaf&#x3D;None):</span><br><span class="line">         self.type &#x3D; type</span><br><span class="line">         if children:</span><br><span class="line">              self.children &#x3D; children</span><br><span class="line">         else:</span><br><span class="line">              self.children &#x3D; [ ]</span><br><span class="line">         self.leaf &#x3D; leaf</span><br><span class="line"> </span><br><span class="line">def p_expression_binop(p):</span><br><span class="line">    &#39;&#39;&#39;expression : expression PLUS expression</span><br><span class="line">                  | expression MINUS expression</span><br><span class="line">                  | expression TIMES expression</span><br><span class="line">                  | expression DIVIDE expression&#39;&#39;&#39;</span><br><span class="line">    p[0] &#x3D; Node(&quot;binop&quot;, [p[1],p[3]], p[2])</span><br></pre></td></tr></table></figure>
<h3 id="嵌入式动作"><a href="#嵌入式动作" class="headerlink" title="嵌入式动作"></a>嵌入式动作</h3><p>yacc 使用的分析技术只允许在规则规约后执行动作。假设有如下规则：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_foo(p):</span><br><span class="line">    &quot;foo : A B C D&quot;</span><br><span class="line">    print &quot;Parsed a foo&quot;, p[1],p[2],p[3],p[4]</span><br></pre></td></tr></table></figure>
<p>方法只会在符号 A,B,C和D 都完成后才能执行。可是有的时候，在中间阶段执行一小段代码是有用的。假如，你想在 A 完成后立即执行一些动作，像下面这样用空规则：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_foo(p):</span><br><span class="line">    &quot;foo : A seen_A B C D&quot;</span><br><span class="line">    print &quot;Parsed a foo&quot;, p[1],p[3],p[4],p[5]</span><br><span class="line">    print &quot;seen_A returned&quot;, p[2]</span><br><span class="line">def p_seen_A(p):</span><br><span class="line">    &quot;seen_A :&quot;</span><br><span class="line">    print &quot;Saw an A &#x3D; &quot;, p[-1]   # Access grammar symbol to left</span><br><span class="line">    p[0] &#x3D; some_value            # Assign value to seen_A</span><br></pre></td></tr></table></figure><br>在这个例子中，空规则 seen_A 将在 A 移进分析栈后立即执行。p[-1] 指代的是在分析栈上紧跟在 seen_A 左侧的符号。在这个例子中，是 A 符号。像其他普通的规则一样，在嵌入式行为中也可以通过为 p[0] 赋值来返回某些值。<br>使用嵌入式动作可能会导致移进归约冲突，比如，下面的语法是没有冲突的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_foo(p):</span><br><span class="line">    &quot;&quot;&quot;foo : abcd</span><br><span class="line">           | abcx&quot;&quot;&quot;</span><br><span class="line">def p_abcd(p):</span><br><span class="line">    &quot;abcd : A B C D&quot;</span><br><span class="line">def p_abcx(p):</span><br><span class="line">    &quot;abcx : A B C X&quot;</span><br></pre></td></tr></table></figure>
<p>可是，如果像这样插入一个嵌入式动作：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_foo(p):</span><br><span class="line">    &quot;&quot;&quot;foo : abcd</span><br><span class="line">           | abcx&quot;&quot;&quot;</span><br><span class="line">def p_abcd(p):</span><br><span class="line">    &quot;abcd : A B C D&quot;</span><br><span class="line">def p_abcx(p):</span><br><span class="line">    &quot;abcx : A B seen_AB C X&quot;</span><br><span class="line">def p_seen_AB(p):</span><br><span class="line">    &quot;seen_AB :&quot;</span><br></pre></td></tr></table></figure><br>会产生移进归约冲，只是由于对于两个规则 abcd 和 abcx 中的 C，分析器既可以根据 abcd 规则移进，也可以根据 abcx 规则先将空的 seen_AB 归约。<br>嵌入动作的一般用于分析以外的控制，比如为本地变量定义作用于。对于 C 语言：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_statements_block(p):</span><br><span class="line">    &quot;statements: LBRACE new_scope statements RBRACE&quot;&quot;&quot;</span><br><span class="line">    # Action code</span><br><span class="line">    ...</span><br><span class="line">    pop_scope()        # Return to previous scope</span><br><span class="line"></span><br><span class="line">def p_new_scope(p):</span><br><span class="line">    &quot;new_scope :&quot;</span><br><span class="line">    # Create a new scope for local variables</span><br><span class="line">    s &#x3D; new_scope()</span><br><span class="line">    push_scope(s)</span><br></pre></td></tr></table></figure>
<p>在这个例子中，new_scope 作为嵌入式行为，在左大括号{之后立即执行。可以是调正内部符号表或者其他方面。statements_block 一完成，代码可能会撤销在嵌入动作时的操作（比如，pop_scope())</p>
<h3 id="Yacc-的其他"><a href="#Yacc-的其他" class="headerlink" title="Yacc 的其他"></a>Yacc 的其他</h3><ul>
<li>默认的分析方法是 LALR，使用 SLR 请像这样运行 yacc()：yacc.yacc(method=”SLR”) 注意：LRLR 生成的分析表大约要比 SLR 的大两倍。解析的性能没有本质的区别，因为代码是一样的。由于 LALR 能力稍强，所以更多的用于复杂的语法。</li>
<li>默认情况下，yacc.py 依赖 lex.py 产生的标记。不过，可以用一个等价的词法标记生成器代替： yacc.parse(lexer=x) 这个例子中，x 必须是一个 Lexer 对象，至少拥有 x.token() 方法用来获取标记。如果将输入字串提供给 yacc.parse()，lexer 还必须具有 x.input() 方法。</li>
<li>默认情况下，yacc 在调试模式下生成分析表（会生成 parser.out 文件和其他东西），使用 yacc.yacc(debug=0) 禁用调试模式。</li>
<li>改变 parsetab.py 的文件名：yacc.yacc(tabmodule=”foo”)</li>
<li>改变 parsetab.py 的生成目录：yacc.yacc(tabmodule=”foo”,outputdir=”somedirectory”)</li>
<li>不生成分析表：yacc.yacc(write_tables=0)。注意：如果禁用分析表生成，yacc()将在每次运行的时候重新构建分析表（这里耗费的时候取决于语法文件的规模）</li>
<li>想在分析过程中输出丰富的调试信息，使用：yacc.parse(debug=1)</li>
<li>yacc.yacc()方法会返回分析器对象，如果你想在一个程序中支持多个分析器：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">p &#x3D; yacc.yacc()</span><br><span class="line">...</span><br><span class="line">p.parse()</span><br></pre></td></tr></table></figure>
注意：yacc.parse() 方法只绑定到最新创建的分析器对象上。</li>
<li>由于生成生成 LALR 分析表相对开销较大，先前生成的分析表会被缓存和重用。判断是否重新生成的依据是对所有的语法规则和优先级规则进行 MD5 校验，只有不匹配时才会重新生成。生成分析表是合理有效的办法，即使是面对上百个规则和状态的语法。对于复杂的编程语言，像 C 语言，在一些慢的机器上生成分析表可能要花费 30-60 秒，请耐心。</li>
<li>由于 LR 分析过程是基于分析表的，分析器的性能很大程度上取决于语法的规模。最大的瓶颈可能是词法分析器和语法规则的复杂度。<h2 id="多个语法和词法分析器"><a href="#多个语法和词法分析器" class="headerlink" title="多个语法和词法分析器"></a>多个语法和词法分析器</h2>在高级的分析器程序中，你可能同时需要多个语法和词法分析器。依照规则行事不会有问题。不过，你需要小心确定所有东西都正确的绑定(hooked up)了。首先，保证将 lex() 和 yacc() 返回的对象保存起来：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lexer  &#x3D; lex.lex()       # Return lexer object</span><br><span class="line">parser &#x3D; yacc.yacc()     # Return parser object</span><br></pre></td></tr></table></figure>
<p>接着，在解析时，确保给 parse() 方法一个正确的 lexer 引用：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">parser.parse(text,lexer&#x3D;lexer)</span><br></pre></td></tr></table></figure><br>如果遗漏这一步，分析器会使用最新创建的 lexer 对象，这可能不是你希望的。<br>词法器和语法器的方法中也可以访问这些对象。在词法器中，标记的 lexer 属性指代的是当前触发规则的词法器对象：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def t_NUMBER(t):</span><br><span class="line">   r&#39;\d+&#39;</span><br><span class="line">   ...</span><br><span class="line">   print t.lexer           # Show lexer object</span><br></pre></td></tr></table></figure>
<p>在语法器中，lexer 和 parser 属性指代的是对应的词法器对象和语法器对象<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_expr_plus(p):</span><br><span class="line">   &#39;expr : expr PLUS expr&#39;</span><br><span class="line">   ...</span><br><span class="line">   print p.parser          # Show parser object</span><br><span class="line">   print p.lexer           # Show lexer object</span><br></pre></td></tr></table></figure><br>如果有必要，lexe r对象和 parser 对象都可以附加其他属性。例如，你想要有不同的解析器状态，可以为 parser 对象附加更多的属性，并在后面用到它们。</p>
<h2 id="使用Python的优化模式"><a href="#使用Python的优化模式" class="headerlink" title="使用Python的优化模式"></a>使用Python的优化模式</h2><p>由于 PLY 从文档字串中获取信息，语法解析和词法分析信息必须通过正常模式下的 Python 解释器得到（不带 有-O 或者 -OO 选项）。不过，如果你像这样指定 optimize 模式：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lex.lex(optimize&#x3D;1)</span><br><span class="line">yacc.yacc(optimize&#x3D;1)</span><br></pre></td></tr></table></figure>
<p>PLY 可以在下次执行，在 Python 的优化模式下执行。但你必须确保第一次执行是在 Python 的正常模式下进行，一旦词法分析表和语法分析表生成一次后，在 Python 优化模式下执行，PLY 会使用生成好的分析表而不再需要文档字串。</p>
<h2 id="高级调试"><a href="#高级调试" class="headerlink" title="高级调试"></a>高级调试</h2><p>调试一个编译器不是件容易的事情。PLY 提供了一些高级的调试能力，这是通过 Python 的l ogging 模块实现的，下面两节介绍这一主题：</p>
<h3 id="调试-lex-和-yacc-命令"><a href="#调试-lex-和-yacc-命令" class="headerlink" title="调试 lex() 和 yacc() 命令"></a>调试 lex() 和 yacc() 命令</h3><p>lex() 和 yacc() 命令都有调试模式，可以通过 debug 标识实现：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lex.lex(debug&#x3D;True)</span><br><span class="line">yacc.yacc(debug&#x3D;True)</span><br></pre></td></tr></table></figure>
<p>正常情况下，调试不仅输出标准错误，对于 yacc()，还会给出 parser.out 文件。这些输出可以通过提供 logging 对象来精细的控制。下面这个例子增加了对调试信息来源的输出：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Set up a logging object</span><br><span class="line">import logging</span><br><span class="line">logging.basicConfig(</span><br><span class="line">    level &#x3D; logging.DEBUG,</span><br><span class="line">    filename &#x3D; &quot;parselog.txt&quot;,</span><br><span class="line">    filemode &#x3D; &quot;w&quot;,</span><br><span class="line">    format &#x3D; &quot;%(filename)10s:%(lineno)4d:%(message)s&quot;</span><br><span class="line">)</span><br><span class="line">log &#x3D; logging.getLogger()</span><br><span class="line"></span><br><span class="line">lex.lex(debug&#x3D;True,debuglog&#x3D;log)</span><br><span class="line">yacc.yacc(debug&#x3D;True,debuglog&#x3D;log)</span><br></pre></td></tr></table></figure><br>如果你提供一个自定义的 logger，大量的调试信息可以通过分级来控制。典型的是将调试信息分为 DEBUG,INFO,或者 WARNING 三个级别。<br>PLY 的错误和警告信息通过日志接口提供，可以从 errorlog 参数中传入日志对象</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lex.lex(errorlog&#x3D;log)</span><br><span class="line">yacc.yacc(errorlog&#x3D;log)</span><br></pre></td></tr></table></figure>
<p>如果想完全过滤掉警告信息，你除了可以使用带级别过滤功能的日志对象，也可以使用 lex 和 yacc 模块都内建的 Nulllogger 对象。例如：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yacc.yacc(errorlog&#x3D;yacc.NullLogger())</span><br></pre></td></tr></table></figure></p>
<h3 id="运行时调试"><a href="#运行时调试" class="headerlink" title="运行时调试"></a>运行时调试</h3><p>为分析器指定 debug 选项，可以激活语法分析器的运行时调试功能。这个选项可以是整数（表示对调试功能是开还是关），也可以是 logger 对象。例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">log &#x3D; logging.getLogger()</span><br><span class="line">parser.parse(input,debug&#x3D;log)</span><br></pre></td></tr></table></figure>
<p>如果传入日志对象的话，你可以使用其级别过滤功能来控制内容的输出。INFO 级别用来产生归约信息；DEBUG 级别会显示分析栈的信息、移进的标记和其他详细信息。ERROR 级别显示分析过程中的错误相关信息。<br>对于每个复杂的问题，你应该用日志对象，以便输出重定向到文件中，进而方便在执行结束后检查。</p>
<h1 id="写一个计算器"><a href="#写一个计算器" class="headerlink" title="写一个计算器"></a>写一个计算器</h1><h3 id="Lex文件"><a href="#Lex文件" class="headerlink" title="Lex文件"></a>Lex文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from ply import lex</span><br><span class="line"></span><br><span class="line"># Define &#96;tokens&#96;, a list of token names.</span><br><span class="line">tokens &#x3D; ( &#39;PLUS&#39;, &#39;MINUS&#39;, &#39;MULT&#39;, &#39;DIV&#39;, &#39;EXPONENT&#39;, \</span><br><span class="line">        &#39;LPAREN&#39;, &#39;RPAREN&#39;, &#39;AB&#39;, &#39;NUMBER&#39;, \</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"># Define &#96;t_ignore&#96; to ignore unnecessary characters between tokens, such as whitespaces.</span><br><span class="line">t_ignore &#x3D; &quot; \t&quot;</span><br><span class="line"></span><br><span class="line"># Define functions representing regular expression rules for each token.</span><br><span class="line"># The name of functions must be like &#96;t_&lt;token_name&gt;&#96;.</span><br><span class="line"># Functions accept one argument, which is a parsed token.</span><br><span class="line">#    t.type  : name of token</span><br><span class="line">#    t.value : string of parsed token </span><br><span class="line">#    t.lineno: line number of token</span><br><span class="line">#    t.lexpos: position of token from the beginning of input string</span><br><span class="line"></span><br><span class="line">def t_PLUS(t):</span><br><span class="line">    r&#39;\+&#39; # regular expression for the token</span><br><span class="line">    return t</span><br><span class="line"></span><br><span class="line">def t_MINUS(t):</span><br><span class="line">    r&#39;\-&#39;</span><br><span class="line">    return t</span><br><span class="line"></span><br><span class="line"># The order of declaration is also the order of rules the lexer uses.</span><br><span class="line"># That is why &#96;t_EXPONENT&#96; must be before &#96;t_MULT&#96;.</span><br><span class="line">def t_EXPONENT(t):</span><br><span class="line">    r&#39;\*\*&#39;</span><br><span class="line">    return t</span><br><span class="line"></span><br><span class="line">def t_MULT(t):</span><br><span class="line">    r&#39;\*&#39;</span><br><span class="line">    return t</span><br><span class="line"></span><br><span class="line">def t_DIV(t):</span><br><span class="line">    r&#39;&#x2F;&#39;</span><br><span class="line">    return t</span><br><span class="line"></span><br><span class="line">def t_LPAREN(t):</span><br><span class="line">    r&#39;\(&#39;</span><br><span class="line">    return t</span><br><span class="line"></span><br><span class="line">def t_RPAREN(t):</span><br><span class="line">    r&#39;\)&#39;</span><br><span class="line">    return t</span><br><span class="line"></span><br><span class="line">def t_AB(t):</span><br><span class="line">    r&#39;ab&#39;</span><br><span class="line">    return t</span><br><span class="line"></span><br><span class="line">def t_NUMBER(t):</span><br><span class="line">    r&#39;[0-9]+&#39;</span><br><span class="line">    t.value &#x3D; int(t.value)</span><br><span class="line">    return t</span><br><span class="line"></span><br><span class="line"># To count correct line number</span><br><span class="line">def t_newline(t):</span><br><span class="line">    r&#39;\n+&#39;</span><br><span class="line">    t.lexer.lineno +&#x3D; t.value.count(&quot;\n&quot;)</span><br><span class="line">    # return None, so this newlines will not be in the parsed token list.</span><br><span class="line"></span><br><span class="line"># Special function for error handling</span><br><span class="line">def t_error(t):</span><br><span class="line">    print(&quot;illegal character &#39;%s&#39;&quot; % (t.value[0]))</span><br><span class="line">    t.lexer.skip(1)</span><br><span class="line"></span><br><span class="line"># Generate a lexer by &#96;lex.lex()&#96;</span><br><span class="line">lexer &#x3D; lex.lex()</span><br><span class="line"></span><br><span class="line">def test_lexer(input_string):</span><br><span class="line">    lexer.input(input_string)</span><br><span class="line">    result &#x3D; []</span><br><span class="line">    while True:</span><br><span class="line">        tok &#x3D; lexer.token()</span><br><span class="line">        if not tok:</span><br><span class="line">            break</span><br><span class="line">        result &#x3D; result + [(tok.type, tok.value)]</span><br><span class="line">    return result</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    print(test_lexer(&#39;1 + 2&#39;))</span><br><span class="line">    print(test_lexer(&#39;1 + 20 * 3 - 10 &#x2F; -2 * (1 + 3)&#39;))</span><br><span class="line">    print(test_lexer(&#39;1 ** 2&#39;))</span><br><span class="line">    print(test_lexer(&#39;ab 5 + ab -2 * ab (1 - 2)&#39;))</span><br></pre></td></tr></table></figure>
<h3 id="Yacc文件"><a href="#Yacc文件" class="headerlink" title="Yacc文件"></a>Yacc文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from ply import yacc</span><br><span class="line">from calclexer import tokens, lexer</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">Grammars:</span><br><span class="line">S -&gt; E</span><br><span class="line">E -&gt; E + E</span><br><span class="line">E -&gt; E - E</span><br><span class="line">E -&gt; E * E</span><br><span class="line">E -&gt; E &#x2F; E</span><br><span class="line">E -&gt; E ** E</span><br><span class="line">E -&gt; N</span><br><span class="line">E -&gt; +N</span><br><span class="line">E -&gt; -N</span><br><span class="line">E -&gt; ab E</span><br><span class="line">E -&gt; (E)</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"># (optional) Define precedence and associativity of operators.</span><br><span class="line"># The format is ( (&#39;left&#39; or &#39;right&#39;, &lt;token name&gt;, ...), (...) ).</span><br><span class="line"># &lt;token name&gt; is expected to be defined in the lexer definition.</span><br><span class="line"># The latter has the the higher precedence (e.g. &#39;MULT&#39; and &#39;DIV&#39; have the higher precedence than &#39;PLUS&#39; and &#39;MINUS&#39;).</span><br><span class="line"># &#39;UPLUS&#39; and &#39;UMINUS&#39; are defined as aliases to override precedence (see &#96;p_expr_um_num&#96;)</span><br><span class="line">precedence &#x3D; ( \</span><br><span class="line">        (&#39;left&#39;, &#39;PLUS&#39;, &#39;MINUS&#39;), \</span><br><span class="line">        (&#39;left&#39;, &#39;MULT&#39;, &#39;DIV&#39;), \</span><br><span class="line">        (&#39;right&#39;, &#39;EXPONENT&#39;), \</span><br><span class="line">        (&#39;right&#39;, &#39;UPLUS&#39;, &#39;UMINUS&#39;, &#39;AB&#39;), \</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"># Parsing rules</span><br><span class="line"># Functions should be start with &#96;p_&#96;.</span><br><span class="line"># The first rule will be the starting rule of parsing (?).</span><br><span class="line"></span><br><span class="line"># S -&gt; E</span><br><span class="line">def p_statement(p):</span><br><span class="line">    &#39;statement : expr&#39;</span><br><span class="line">    p[0] &#x3D; p[1]</span><br><span class="line"></span><br><span class="line"># E -&gt; E + E</span><br><span class="line">def p_expr_plus(p):</span><br><span class="line">    &#39;expr : expr PLUS expr&#39;</span><br><span class="line">    p[0] &#x3D; p[1] + p[3]</span><br><span class="line"></span><br><span class="line"># E -&gt; E - E</span><br><span class="line">def p_expr_minus(p):</span><br><span class="line">    &#39;expr : expr MINUS expr&#39;</span><br><span class="line">    p[0] &#x3D; p[1] - p[3]</span><br><span class="line"></span><br><span class="line"># E -&gt; E * E</span><br><span class="line">def p_expr_mult(p):</span><br><span class="line">    &#39;expr : expr MULT expr&#39;</span><br><span class="line">    p[0] &#x3D; p[1] * p[3]</span><br><span class="line"></span><br><span class="line"># E -&gt; E &#x2F; E</span><br><span class="line">def p_expr_div(p):</span><br><span class="line">    &#39;expr : expr DIV expr&#39;</span><br><span class="line">    p[0] &#x3D; p[1] &#x2F; p[3]</span><br><span class="line"></span><br><span class="line"># E -&gt; E ** E</span><br><span class="line">def p_expr_exponent(p):</span><br><span class="line">    &#39;expr : expr EXPONENT expr&#39;</span><br><span class="line">    p[0] &#x3D; p[1] ** p[3]</span><br><span class="line"></span><br><span class="line"># E -&gt; N</span><br><span class="line">def p_expr_num(p):</span><br><span class="line">    &#39;expr : NUMBER&#39;</span><br><span class="line">    p[0] &#x3D; p[1]</span><br><span class="line"></span><br><span class="line"># E -&gt; +N</span><br><span class="line">def p_expr_up_num(p):</span><br><span class="line">    &#39;expr : PLUS NUMBER %prec UPLUS&#39; # override precedence of PLUS by &#96;%prec UPLUS&#96;</span><br><span class="line">    p[0] &#x3D; p[2]</span><br><span class="line"></span><br><span class="line"># E -&gt; -N</span><br><span class="line">def p_expr_um_num(p):</span><br><span class="line">    &#39;expr : MINUS NUMBER %prec UMINUS&#39; # override precedence of MINUS by &#96;%prec UMINUS&#96;</span><br><span class="line">    p[0] &#x3D; -p[2]</span><br><span class="line"></span><br><span class="line"># E -&gt; ab E</span><br><span class="line">def p_expr_ab(p):</span><br><span class="line">    &#39;expr : AB expr&#39;</span><br><span class="line">    p[0] &#x3D; abs(p[2])</span><br><span class="line"></span><br><span class="line"># E -&gt; ( E )</span><br><span class="line">def p_expr_paren(p):</span><br><span class="line">    &#39;expr : LPAREN expr RPAREN&#39;</span><br><span class="line">    p[0] &#x3D; p[2]</span><br><span class="line"></span><br><span class="line"># Rule for error handling</span><br><span class="line">def p_error(t):</span><br><span class="line">    print(&quot;syntax error at &#39;%s&#39;&quot; % (t.value))</span><br><span class="line"></span><br><span class="line"># Generate a LALR parser</span><br><span class="line">parser &#x3D; yacc.yacc()</span><br><span class="line"></span><br><span class="line">def parse(input_string):</span><br><span class="line">    lexer.input(input_string)</span><br><span class="line">    parse_tree &#x3D; parser.parse(input_string, lexer&#x3D;lexer)</span><br><span class="line">    return parse_tree</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    assert parse(&#39;1 + 2 + 3&#39;) &#x3D;&#x3D; 6</span><br><span class="line">    assert parse(&#39;1 + 2 * 3 * 4&#39;) &#x3D;&#x3D; 25</span><br><span class="line">    assert parse(&#39;3 * 4 - 10 &#x2F; 2 + 5&#39;) &#x3D;&#x3D; 12</span><br><span class="line">    assert parse(&#39;-3 * (+4 - 10) &#x2F; -2 + 5&#39;) &#x3D;&#x3D; -4</span><br><span class="line">    assert parse(&#39;1 + 2 ** 3 ** 2&#39;) &#x3D;&#x3D; 513</span><br><span class="line">    assert parse(&#39;ab (1 - 2  -3)&#39;) &#x3D;&#x3D; 4</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>工具使用</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark入门</title>
    <url>/2020/02/26/Spark%E5%85%A5%E9%97%A8/</url>
    <content><![CDATA[<p>最近需要用spark比较多，重新学习一下。今天先学习一些基础。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://classroom.udacity.com/courses/ud2002" target="_blank" rel="noopener">https://classroom.udacity.com/courses/ud2002</a></p>
</blockquote>
<h1 id="Spark处理数据"><a href="#Spark处理数据" class="headerlink" title="Spark处理数据"></a>Spark处理数据</h1><h2 id="函数式编程"><a href="#函数式编程" class="headerlink" title="函数式编程"></a>函数式编程</h2><p>首先用下图来看一下，函数式编程和过程式编程的区别。<img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/maj1.png" alt="图片"></p>
<p>函数式编程非常适合分布式系统。Python并不是函数编程语言，但使用PySparkAPI 可以让你编写Spark程序，并确保你的代码使用了函数式编程。在底层，Python 代码使用 py4j 来调用 Java 虚拟机(JVM)。</p>
<p>假设有下面一段代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">log_of_songs &#x3D; [</span><br><span class="line">        &quot;Despacito&quot;,</span><br><span class="line">        &quot;Nice for what&quot;,</span><br><span class="line">        &quot;No tears left to cry&quot;,</span><br><span class="line">        &quot;Despacito&quot;,</span><br><span class="line">        &quot;Havana&quot;,</span><br><span class="line">        &quot;In my feelings&quot;,</span><br><span class="line">        &quot;Nice for what&quot;,</span><br><span class="line">        &quot;Despacito&quot;,</span><br><span class="line">        &quot;All the stars&quot;</span><br><span class="line">]</span><br><span class="line">play_count &#x3D; 0</span><br><span class="line">def count_plays(song_title):</span><br><span class="line">    global play_count</span><br><span class="line">    for song in log_of_songs:</span><br><span class="line">        if song &#x3D;&#x3D; song_title:</span><br><span class="line">            play_count &#x3D; play_count + 1</span><br><span class="line">    return play_count</span><br></pre></td></tr></table></figure>
<p>调用两次count_plays(“Despacito”)会得到不同的结果，这是因为play_count是作为全局变量，在函数内部进行了修改。解决这个问题可以采用如下代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def count_plays(song_title, play_count):</span><br><span class="line">    for song in log_of_songs:</span><br><span class="line">        if song &#x3D;&#x3D; song_title:</span><br><span class="line">            play_count &#x3D; play_count + 1</span><br><span class="line">    return play_count</span><br></pre></td></tr></table></figure>
<p>这就是Spark解决问题的方式。<br>在Spark中我们使用Pure Function（纯函数），就像面包制造厂，不同的面包机器之间是互不干扰的，且不会损坏原材料。Spark会在函数执行前，将数据复制多分，以输入到不同函数中。为了防止内存溢出，Spark会在代码中建立一个数据的有向无环图，在运行前检查是否有必要对某一分数据进行复制。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/maj2.png" alt="图片"></p>
<h2 id="运行时参数设置"><a href="#运行时参数设置" class="headerlink" title="运行时参数设置"></a>运行时参数设置</h2><blockquote>
<p>参考：<br><a href="https://tech.meituan.com/2016/04/29/spark-tuning-basic.html" target="_blank" rel="noopener">https://tech.meituan.com/2016/04/29/spark-tuning-basic.html</a><br><a href="https://spark.apache.org/docs/1.6.1/running-on-yarn.html" target="_blank" rel="noopener">https://spark.apache.org/docs/1.6.1/running-on-yarn.html</a></p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line">    --master yarn \</span><br><span class="line">    --deploy-mode cluster  \</span><br><span class="line">    --num-executors 100 \</span><br><span class="line">    --driver-memory 2g \</span><br><span class="line">    --executor-memory 14g \</span><br><span class="line">    --executor-cores 6 \</span><br><span class="line">    --conf spark.default.parallelism&#x3D;1000 \</span><br><span class="line">    --conf spark.storage.memoryFraction&#x3D;0.2 \</span><br><span class="line">    --conf spark.shuffle.memoryFraction&#x3D;0.6 \</span><br><span class="line">    --conf spark.executor.extraJavaOptions&#x3D;&#39;-Dlog4j.configuration&#x3D;log4j.properties&#39; \</span><br><span class="line">    --driver-java-options -Dlog4j.configuration&#x3D;log4j.properties \</span><br><span class="line">    python文件  \</span><br></pre></td></tr></table></figure>
<ul>
<li>spark-submit: which spark-submit 查看该命令是 spark 系统的还是 pyspark 包自带的，应该使用 spark 系统的<ul>
<li>master:</li>
<li>standaloone: spark 自带的集群资源管理器</li>
<li>yarn</li>
<li>local: 本地运行</li>
</ul>
</li>
<li>deploy-mode：<ul>
<li>client: driver 在本机上，能够直接使用本机文件系统</li>
<li>cluster: driver 指不定在哪台机器上，不能读取本机文件系统</li>
</ul>
</li>
<li>spark 运行时配置：主要的有运行内存和节点数量:<ul>
<li>num_executors</li>
<li>spark_driver_memory</li>
<li>spark_executor_memory</li>
</ul>
</li>
<li>addFiles 与 —files（将需要使用的文件分发到每台机器上）：<ul>
<li>addFiles()：能够分发到每台机器上,包括 driver 上</li>
<li>—files: 只能分发到 executor 上</li>
</ul>
</li>
<li>引用其他模块的问题：<ul>
<li>第三方库：需要将第三方库打包上传供使用</li>
<li>自己的模块：也需要打包上传,以供使用</li>
</ul>
</li>
<li>运行下面前请确认<ul>
<li>export SPARK_HOME=…../spark-1.6.2-bin-hadoop2.6</li>
<li>export PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH</li>
<li>export JAVA_HOME=…/jdk1.8.0_60</li>
<li>PYSPARK_PYTHON=./NLTK/conda-env/bin/python spark-submit —conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./NLTK/conda-env/bin/python —master yarn-cluster —archives conda-env.zip#NLTK clean_step_two.py</li>
</ul>
</li>
</ul>
<h2 id="Maps和Lambda"><a href="#Maps和Lambda" class="headerlink" title="Maps和Lambda"></a>Maps和Lambda</h2><blockquote>
<p>lambda函数起源：<br><a href="http://palmstroem.blogspot.com/2012/05/lambda-calculus-for-absolute-dummies.html" target="_blank" rel="noopener">http://palmstroem.blogspot.com/2012/05/lambda-calculus-for-absolute-dummies.html</a></p>
</blockquote>
<p>Maps会复制原始数据，并把副本数据按照Maps中的函数进行转换。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from pyspark import SparkContext</span><br><span class="line">log_of_songs &#x3D; [</span><br><span class="line">    &quot;Despacito&quot;,</span><br><span class="line">    &quot;Nice for what&quot;,</span><br><span class="line">    &quot;No tears left to cry&quot;,</span><br><span class="line">    &quot;Despacito&quot;,</span><br><span class="line">    &quot;Havana&quot;,</span><br><span class="line">    &quot;In my feelings&quot;,</span><br><span class="line">    &quot;Nice for what&quot;,</span><br><span class="line">    &quot;Despacito&quot;,</span><br><span class="line">    &quot;All the stars&quot;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">def convert_song_to_lowercase(song):</span><br><span class="line">    return song.lower()</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">    conf &#x3D; SparkConf()</span><br><span class="line">    conf.setAppName(&quot;Testing&quot;).setMaster(&quot;local&quot;)</span><br><span class="line">    sc &#x3D; SparkContext(conf&#x3D;conf)</span><br><span class="line">    sc.setLogLevel(&quot;WARN&quot;)</span><br><span class="line"></span><br><span class="line">    # parallelize将对象分配到不同节点上</span><br><span class="line">    distributed_song_log &#x3D; sc.parallelize(log_of_songs)</span><br><span class="line">    # 定义不同节点的所有数据执行convert_song_to_lowercase的操作</span><br><span class="line">    # 但此时spark还未执行，它在等待所有定义结束后，看是否可以优化某些操作</span><br><span class="line">    distributed_song_log.map(convert_song_to_lowercase)</span><br><span class="line">    # 如果想强制spark执行，则可以使用collect，则会将所有数据汇总</span><br><span class="line">    # 注意此时spark并没有改变原始数据的大小写，它将原始数据进行了拷贝，再做的处理</span><br><span class="line">    distributed_song_log.collect()</span><br><span class="line">    # 也可以使用python的匿名函数进行map</span><br><span class="line">    distributed_song_log.map(lambda song: song.lower()).collect()</span><br></pre></td></tr></table></figure>
<h2 id="Data-Frame"><a href="#Data-Frame" class="headerlink" title="Data Frame"></a>Data Frame</h2><p>数据处理有两种方式，一种使用Data Frame和Python进行命令式编程，另一种使用SQL进行声明式编程。命令式编程关注的是”How”，声明式编程关注的是”What”。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/maj3.png" alt="图片"></p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/maj4.png" alt="图片"></p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/maj5.png" alt="图片"></p>
<h3 id="Data-Frame的读取和写入"><a href="#Data-Frame的读取和写入" class="headerlink" title="Data Frame的读取和写入"></a>Data Frame的读取和写入</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import pyspark</span><br><span class="line">from pyspark import SparkConf</span><br><span class="line">from pyspark.sql import SparkSession</span><br><span class="line"></span><br><span class="line">spark &#x3D; SparkSession \</span><br><span class="line">    .builder \</span><br><span class="line">    .appName(&quot;Our first Python Spark SQL example&quot;) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"># 检查一下是否生效了。</span><br><span class="line">spark.sparkContext.getConf().getAll()</span><br><span class="line">path &#x3D; &quot;data&#x2F;sparkify_log_small.json&quot;</span><br><span class="line">user_log &#x3D; spark.read.json(path)</span><br><span class="line">user_log.printSchema()</span><br><span class="line">user_log.describe()</span><br><span class="line">user_log.show(n&#x3D;1)</span><br><span class="line"># 取数据的前5条</span><br><span class="line">user_log.take(5)</span><br><span class="line">out_path &#x3D; &quot;data&#x2F;sparkify_log_small.csv&quot;</span><br><span class="line">user_log.write.save(out_path, format&#x3D;&quot;csv&quot;, header&#x3D;True)</span><br><span class="line"># 读取另一个daraframe</span><br><span class="line">user_log_2 &#x3D; spark.read.csv(out_path, header&#x3D;True)</span><br><span class="line">user_log_2.printSchema()</span><br><span class="line">user_log_2.take(2)</span><br><span class="line">user_log_2.select(&quot;userID&quot;).show()</span><br></pre></td></tr></table></figure>
<h3 id="Data-Frame数据处理"><a href="#Data-Frame数据处理" class="headerlink" title="Data Frame数据处理"></a>Data Frame数据处理</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from pyspark.sql import SparkSession</span><br><span class="line">from pyspark.sql.functions import udf</span><br><span class="line">from pyspark.sql.types import StringType</span><br><span class="line">from pyspark.sql.types import IntegerType</span><br><span class="line">from pyspark.sql.functions import desc</span><br><span class="line">from pyspark.sql.functions import asc</span><br><span class="line">from pyspark.sql.functions import sum as Fsum</span><br><span class="line"></span><br><span class="line">import datetime</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">%matplotlib inline</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">spark &#x3D; SparkSession \</span><br><span class="line">    .builder \</span><br><span class="line">    .appName(&quot;Wrangling Data&quot;) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line">path &#x3D; &quot;data&#x2F;sparkify_log_small.json&quot;</span><br><span class="line">user_log &#x3D; spark.read.json(path)</span><br><span class="line"># 数据搜索</span><br><span class="line">user_log.take(5)</span><br><span class="line">user_log.printSchema()</span><br><span class="line">user_log.describe().show()</span><br><span class="line">user_log.describe(&quot;artist&quot;).show()</span><br><span class="line">user_log.describe(&quot;sessionId&quot;).show()</span><br><span class="line">user_log.count()</span><br><span class="line">user_log.select(&quot;page&quot;).dropDuplicates().sort(&quot;page&quot;).show()</span><br><span class="line">user_log.select([&quot;userId&quot;, &quot;firstname&quot;, &quot;page&quot;, &quot;song&quot;]).where(user_log.userId &#x3D;&#x3D; &quot;1046&quot;).collect()</span><br><span class="line"># 按小时统计数据</span><br><span class="line">get_hour &#x3D; udf(lambda x: datetime.datetime.fromtimestamp(x &#x2F; 1000.0). hour)</span><br><span class="line">user_log &#x3D; user_log.withColumn(&quot;hour&quot;, get_hour(user_log.ts))</span><br><span class="line">user_log.head()</span><br><span class="line">songs_in_hour &#x3D; user_log.filter(user_log.page &#x3D;&#x3D; &quot;NextSong&quot;).groupby(user_log.hour).count().orderBy(user_log.hour.cast(&quot;float&quot;))</span><br><span class="line">songs_in_hour.show()</span><br><span class="line">songs_in_hour_pd &#x3D; songs_in_hour.toPandas()</span><br><span class="line">songs_in_hour_pd.hour &#x3D; pd.to_numeric(songs_in_hour_pd.hour)</span><br><span class="line">plt.scatter(songs_in_hour_pd[&quot;hour&quot;], songs_in_hour_pd[&quot;count&quot;])</span><br><span class="line">plt.xlim(-1, 24);</span><br><span class="line">plt.ylim(0, 1.2 * max(songs_in_hour_pd[&quot;count&quot;]))</span><br><span class="line">plt.xlabel(&quot;Hour&quot;)</span><br><span class="line">plt.ylabel(&quot;Songs played&quot;);</span><br><span class="line"></span><br><span class="line"># 删除空值的行</span><br><span class="line">user_log_valid &#x3D; user_log.dropna(how &#x3D; &quot;any&quot;, subset &#x3D; [&quot;userId&quot;, &quot;sessionId&quot;])</span><br><span class="line">user_log_valid.count()</span><br><span class="line">user_log.select(&quot;userId&quot;).dropDuplicates().sort(&quot;userId&quot;).show()</span><br><span class="line">user_log_valid &#x3D; user_log_valid.filter(user_log_valid[&quot;userId&quot;] !&#x3D; &quot;&quot;)</span><br><span class="line">user_log_valid.count()</span><br><span class="line"># 降级服务的用户</span><br><span class="line">user_log_valid.filter(&quot;page &#x3D; &#39;Submit Downgrade&#39;&quot;).show()</span><br><span class="line">user_log.select([&quot;userId&quot;, &quot;firstname&quot;, &quot;page&quot;, &quot;level&quot;, &quot;song&quot;]).where(user_log.userId &#x3D;&#x3D; &quot;1138&quot;).collect()</span><br><span class="line">flag_downgrade_event &#x3D; udf(lambda x: 1 if x &#x3D;&#x3D; &quot;Submit Downgrade&quot; else 0, IntegerType())</span><br><span class="line">user_log_valid &#x3D; user_log_valid.withColumn(&quot;downgraded&quot;, flag_downgrade_event(&quot;page&quot;))</span><br><span class="line">user_log_valid.head()</span><br><span class="line">from pyspark.sql import Window</span><br><span class="line">windowval &#x3D; Window.partitionBy(&quot;userId&quot;).orderBy(desc(&quot;ts&quot;)).rangeBetween(Window.unboundedPreceding, 0)</span><br><span class="line">user_log_valid &#x3D; user_log_valid.withColumn(&quot;phase&quot;, Fsum(&quot;downgraded&quot;).over(windowval))</span><br><span class="line">user_log_valid.select([&quot;userId&quot;, &quot;firstname&quot;, &quot;ts&quot;, &quot;page&quot;, &quot;level&quot;, &quot;phase&quot;]).where(user_log.userId &#x3D;&#x3D; &quot;1138&quot;).sort(&quot;ts&quot;).collect()</span><br></pre></td></tr></table></figure>
<h2 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from pyspark.sql import SparkSession</span><br><span class="line">from pyspark.sql.functions import udf</span><br><span class="line">from pyspark.sql.types import StringType</span><br><span class="line">from pyspark.sql.types import IntegerType</span><br><span class="line">from pyspark.sql.functions import desc</span><br><span class="line">from pyspark.sql.functions import asc</span><br><span class="line">from pyspark.sql.functions import sum as Fsum</span><br><span class="line"></span><br><span class="line">import datetime</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">%matplotlib inline</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">spark &#x3D; SparkSession \</span><br><span class="line">    .builder \</span><br><span class="line">    .appName(&quot;Data wrangling with Spark SQL&quot;) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line">path &#x3D; &quot;data&#x2F;sparkify_log_small.json&quot;</span><br><span class="line">user_log &#x3D; spark.read.json(path)</span><br><span class="line"></span><br><span class="line">user_log.take(1)</span><br><span class="line"># 下面的代码创建了一个临时视图，你可以使用该视图运行 SQL 查询</span><br><span class="line">user_log.createOrReplaceTempView(&quot;user_log_table&quot;)</span><br><span class="line">spark.sql(&quot;SELECT * FROM user_log_table LIMIT 2&quot;).show()</span><br><span class="line">spark.sql(&#39;&#39;&#39;</span><br><span class="line">          SELECT * </span><br><span class="line">          FROM user_log_table </span><br><span class="line">          LIMIT 2</span><br><span class="line">          &#39;&#39;&#39;</span><br><span class="line">          ).show()</span><br><span class="line">spark.sql(&#39;&#39;&#39;</span><br><span class="line">          SELECT COUNT(*) </span><br><span class="line">          FROM user_log_table </span><br><span class="line">          &#39;&#39;&#39;</span><br><span class="line">          ).show()</span><br><span class="line">spark.sql(&#39;&#39;&#39;</span><br><span class="line">          SELECT userID, firstname, page, song</span><br><span class="line">          FROM user_log_table </span><br><span class="line">          WHERE userID &#x3D;&#x3D; &#39;1046&#39;</span><br><span class="line">          &#39;&#39;&#39;</span><br><span class="line">          ).collect()</span><br><span class="line">spark.sql(&#39;&#39;&#39;</span><br><span class="line">          SELECT DISTINCT page</span><br><span class="line">          FROM user_log_table </span><br><span class="line">          ORDER BY page ASC</span><br><span class="line">          &#39;&#39;&#39;</span><br><span class="line">          ).show()</span><br><span class="line"></span><br><span class="line"># 自定义函数</span><br><span class="line">spark.udf.register(&quot;get_hour&quot;, lambda x: int(datetime.datetime.fromtimestamp(x &#x2F; 1000.0).hour))</span><br><span class="line">spark.sql(&#39;&#39;&#39;</span><br><span class="line">          SELECT *, get_hour(ts) AS hour</span><br><span class="line">          FROM user_log_table </span><br><span class="line">          LIMIT 1</span><br><span class="line">          &#39;&#39;&#39;</span><br><span class="line">          ).collect()</span><br><span class="line">songs_in_hour &#x3D; spark.sql(&#39;&#39;&#39;</span><br><span class="line">          SELECT get_hour(ts) AS hour, COUNT(*) as plays_per_hour</span><br><span class="line">          FROM user_log_table</span><br><span class="line">          WHERE page &#x3D; &quot;NextSong&quot;</span><br><span class="line">          GROUP BY hour</span><br><span class="line">          ORDER BY cast(hour as int) ASC</span><br><span class="line">          &#39;&#39;&#39;</span><br><span class="line">          )</span><br><span class="line">songs_in_hour.show()</span><br><span class="line"># 用 Pandas 转换数据</span><br><span class="line">songs_in_hour_pd &#x3D; songs_in_hour.toPandas()</span><br><span class="line">print(songs_in_hour_pd)</span><br></pre></td></tr></table></figure>
<h2 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h2><blockquote>
<p>参考：<br><a href="https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html" target="_blank" rel="noopener">https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html</a><br><a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/rdd-programming-guide.html</a></p>
</blockquote>
<p>不管使用Pyspark还是其他语言，Spark的底层都会通过Catalyst转成执行DAG序列：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/spark1.png" alt="图片"></p>
<p>DAG在底层使用RDD对象进行操作。</p>
<h1 id="Spark中的机器学习"><a href="#Spark中的机器学习" class="headerlink" title="Spark中的机器学习"></a>Spark中的机器学习</h1><h2 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from pyspark.sql import SparkSession</span><br><span class="line">from pyspark.ml.feature import RegexTokenizer, VectorAssembler, Normalizer, StandardScaler</span><br><span class="line">from pyspark.sql.functions import udf</span><br><span class="line">from pyspark.sql.types import IntegerType</span><br><span class="line"></span><br><span class="line">import re</span><br><span class="line"># create a SparkSession: note this step was left out of the screencast</span><br><span class="line">spark &#x3D; SparkSession.builder \</span><br><span class="line">    .master(&quot;local&quot;) \</span><br><span class="line">    .appName(&quot;Word Count&quot;) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line"># 如何读取数据集</span><br><span class="line">stack_overflow_data &#x3D; &#39;Train_onetag_small.json&#39;</span><br><span class="line">df &#x3D; spark.read.json(stack_overflow_data)</span><br><span class="line">df.head()</span><br><span class="line"># 把字符串分为单独的单词。Spark有一个[Tokenizer]（https:&#x2F;&#x2F;spark.apache.org&#x2F;docs&#x2F;latest&#x2F;ml-features.html#tokenizer） 类以及RegexTokenizer。 后者在分词时有更大的自由度。</span><br><span class="line"># split the body text into separate words</span><br><span class="line">regexTokenizer &#x3D; RegexTokenizer(inputCol&#x3D;&quot;Body&quot;, outputCol&#x3D;&quot;words&quot;, pattern&#x3D;&quot;\\W&quot;)</span><br><span class="line">df &#x3D; regexTokenizer.transform(df)</span><br><span class="line">df.head()</span><br><span class="line"># count the number of words in each body tag</span><br><span class="line">body_length &#x3D; udf(lambda x: len(x), IntegerType())</span><br><span class="line">df &#x3D; df.withColumn(&quot;BodyLength&quot;, body_length(df.words))</span><br><span class="line"># count the number of paragraphs and links in each body tag</span><br><span class="line">number_of_paragraphs &#x3D; udf(lambda x: len(re.findall(&quot;&lt;&#x2F;p&gt;&quot;, x)), IntegerType())</span><br><span class="line">number_of_links &#x3D; udf(lambda x: len(re.findall(&quot;&lt;&#x2F;a&gt;&quot;, x)), IntegerType())</span><br><span class="line">df &#x3D; df.withColumn(&quot;NumParagraphs&quot;, number_of_paragraphs(df.Body))</span><br><span class="line">df &#x3D; df.withColumn(&quot;NumLinks&quot;, number_of_links(df.Body))</span><br><span class="line">df.head(2)</span><br><span class="line"># 将内容长度，段落数和内容中的链接数合并为一个向量</span><br><span class="line">assembler &#x3D; VectorAssembler(inputCols&#x3D;[&quot;BodyLength&quot;, &quot;NumParagraphs&quot;, &quot;NumLinks&quot;], outputCol&#x3D;&quot;NumFeatures&quot;)</span><br><span class="line">df &#x3D; assembler.transform(df)</span><br><span class="line">df.head()</span><br><span class="line"># 归一化向量</span><br><span class="line">scaler &#x3D; Normalizer(inputCol&#x3D;&quot;NumFeatures&quot;, outputCol&#x3D;&quot;ScaledNumFeatures&quot;)</span><br><span class="line">df &#x3D; scaler.transform(df)</span><br><span class="line">df.head(2)</span><br><span class="line"># 缩放向量</span><br><span class="line">scaler2 &#x3D; StandardScaler(inputCol&#x3D;&quot;NumFeatures&quot;, outputCol&#x3D;&quot;ScaledNumFeatures2&quot;, withStd&#x3D;True)</span><br><span class="line">scalerModel &#x3D; scaler2.fit(df)</span><br><span class="line">df &#x3D; scalerModel.transform(df)</span><br><span class="line">df.head(2)</span><br></pre></td></tr></table></figure>
<h2 id="文本特征"><a href="#文本特征" class="headerlink" title="文本特征"></a>文本特征</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from pyspark.sql import SparkSession</span><br><span class="line">from pyspark.ml.feature import RegexTokenizer, CountVectorizer, \</span><br><span class="line">    IDF, StringIndexer</span><br><span class="line">from pyspark.sql.functions import udf</span><br><span class="line">from pyspark.sql.types import IntegerType</span><br><span class="line"></span><br><span class="line">import re</span><br><span class="line"># create a SparkSession: note this step was left out of the screencast</span><br><span class="line">spark &#x3D; SparkSession.builder \</span><br><span class="line">    .master(&quot;local&quot;) \</span><br><span class="line">    .appName(&quot;Word Count&quot;) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line"># 如何读取数据集</span><br><span class="line">stack_overflow_data &#x3D; &#39;Train_onetag_small.json&#39;</span><br><span class="line">df &#x3D; spark.read.json(stack_overflow_data)</span><br><span class="line">df.head()</span><br><span class="line"></span><br><span class="line"># 分词将字符串拆分为单独的单词。Spark 有一个[Tokenizer] （https:&#x2F;&#x2F;spark.apache.org&#x2F;docs&#x2F;latest&#x2F;ml-features.html#tokenizer） 类和RegexTokenizer。后者在分词时有更大的自由度 。</span><br><span class="line"># split the body text into separate words</span><br><span class="line">regexTokenizer &#x3D; RegexTokenizer(inputCol&#x3D;&quot;Body&quot;, outputCol&#x3D;&quot;words&quot;, pattern&#x3D;&quot;\\W&quot;)</span><br><span class="line">df &#x3D; regexTokenizer.transform(df)</span><br><span class="line">df.head()</span><br><span class="line"></span><br><span class="line"># CountVectorizer</span><br><span class="line"># find the term frequencies of the words</span><br><span class="line">cv &#x3D; CountVectorizer(inputCol&#x3D;&quot;words&quot;, outputCol&#x3D;&quot;TF&quot;, vocabSize&#x3D;1000)</span><br><span class="line">cvmodel &#x3D; cv.fit(df)</span><br><span class="line">df &#x3D; cvmodel.transform(df)</span><br><span class="line">df.take(1)</span><br><span class="line"># show the vocabulary in order of </span><br><span class="line">cvmodel.vocabulary</span><br><span class="line"># show the last 10 terms in the vocabulary</span><br><span class="line">cvmodel.vocabulary[-10:]</span><br><span class="line"></span><br><span class="line"># 逆文本频率指数（Inter-document Frequency ）</span><br><span class="line">idf &#x3D; IDF(inputCol&#x3D;&quot;TF&quot;, outputCol&#x3D;&quot;TFIDF&quot;)</span><br><span class="line">idfModel &#x3D; idf.fit(df)</span><br><span class="line">df &#x3D; idfModel.transform(df)</span><br><span class="line">df.head()</span><br><span class="line"></span><br><span class="line"># StringIndexer</span><br><span class="line">indexer &#x3D; StringIndexer(inputCol&#x3D;&quot;oneTag&quot;, outputCol&#x3D;&quot;label&quot;)</span><br><span class="line">df &#x3D; indexer.fit(df).transform(df)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>一首小诗：做最好的自己</title>
    <url>/2020/02/16/%E9%9A%8F%E6%84%9F%E4%B8%80%E7%AF%87/</url>
    <content><![CDATA[<p>今天看一个纪录片《人生第一次》时听到的小诗，来自美国诗人、短片小说作家——道格拉斯·马拉赫。</p>
<a id="more"></a>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=475218187&auto=1&height=66"></iframe>

<h4 id="如果你不能成为山顶上的高松，"><a href="#如果你不能成为山顶上的高松，" class="headerlink" title="如果你不能成为山顶上的高松，"></a>如果你不能成为山顶上的高松，</h4><h4 id="那就当棵山谷里的小树吧，"><a href="#那就当棵山谷里的小树吧，" class="headerlink" title="那就当棵山谷里的小树吧，"></a>那就当棵山谷里的小树吧，</h4><h4 id="但要当棵溪边最好的小树。"><a href="#但要当棵溪边最好的小树。" class="headerlink" title="但要当棵溪边最好的小树。"></a>但要当棵溪边最好的小树。</h4><p><br/></p>
<h4 id="如果你不能成为一棵大树，"><a href="#如果你不能成为一棵大树，" class="headerlink" title="如果你不能成为一棵大树，"></a>如果你不能成为一棵大树，</h4><h4 id="那就当丛小灌木；"><a href="#那就当丛小灌木；" class="headerlink" title="那就当丛小灌木；"></a>那就当丛小灌木；</h4><h4 id="如果你不能成为一丛小灌木，"><a href="#如果你不能成为一丛小灌木，" class="headerlink" title="如果你不能成为一丛小灌木，"></a>如果你不能成为一丛小灌木，</h4><h4 id="那就当一片小草地。"><a href="#那就当一片小草地。" class="headerlink" title="那就当一片小草地。"></a>那就当一片小草地。</h4><p><br/></p>
<h4 id="如果你不能是一只香獐，"><a href="#如果你不能是一只香獐，" class="headerlink" title="如果你不能是一只香獐，"></a>如果你不能是一只香獐，</h4><h4 id="那就当尾小鲈鱼，"><a href="#那就当尾小鲈鱼，" class="headerlink" title="那就当尾小鲈鱼，"></a>那就当尾小鲈鱼，</h4><h4 id="但要当湖里最活泼的小鲈鱼。"><a href="#但要当湖里最活泼的小鲈鱼。" class="headerlink" title="但要当湖里最活泼的小鲈鱼。"></a>但要当湖里最活泼的小鲈鱼。</h4><p><br/></p>
<h4 id="我们不能全是船长，"><a href="#我们不能全是船长，" class="headerlink" title="我们不能全是船长，"></a>我们不能全是船长，</h4><h4 id="必须有人也是水手。"><a href="#必须有人也是水手。" class="headerlink" title="必须有人也是水手。"></a>必须有人也是水手。</h4><p><br/></p>
<h4 id="这里有许多事让我们去做，"><a href="#这里有许多事让我们去做，" class="headerlink" title="这里有许多事让我们去做，"></a>这里有许多事让我们去做，</h4><h4 id="有大事，有小事，"><a href="#有大事，有小事，" class="headerlink" title="有大事，有小事，"></a>有大事，有小事，</h4><h4 id="但最重要的是我们身旁的事。"><a href="#但最重要的是我们身旁的事。" class="headerlink" title="但最重要的是我们身旁的事。"></a>但最重要的是我们身旁的事。</h4><p><br/></p>
<h4 id="如果你不能成为大道，"><a href="#如果你不能成为大道，" class="headerlink" title="如果你不能成为大道，"></a>如果你不能成为大道，</h4><h4 id="那就当一条小路，"><a href="#那就当一条小路，" class="headerlink" title="那就当一条小路，"></a>那就当一条小路，</h4><h4 id="如果你不能成为太阳，"><a href="#如果你不能成为太阳，" class="headerlink" title="如果你不能成为太阳，"></a>如果你不能成为太阳，</h4><h4 id="那就当一颗星星。"><a href="#那就当一颗星星。" class="headerlink" title="那就当一颗星星。"></a>那就当一颗星星。</h4><p><br/></p>
<h4 id="决定成败的不是你的身材"><a href="#决定成败的不是你的身材" class="headerlink" title="决定成败的不是你的身材"></a>决定成败的不是你的身材</h4><h4 id="而是做一个最好的你。"><a href="#而是做一个最好的你。" class="headerlink" title="而是做一个最好的你。"></a>而是做一个最好的你。</h4><p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/suigan1.png" alt="图片"></p>
]]></content>
      <tags>
        <tag>小诗</tag>
      </tags>
  </entry>
  <entry>
    <title>机器翻译检测</title>
    <url>/2020/02/13/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A3%80%E6%B5%8B/</url>
    <content><![CDATA[<p>因为本周要做一个机器翻译检测的任务，因此搜到了几篇论文，看一下大概有哪些思路。论文基本上只简单扫了一眼，简单介绍一下其中的3篇。</p>
<a id="more"></a>
<blockquote>
<p>参考：</p>
<p>Machine Translation Detection from Monolingual Web-Text</p>
<p>Automatic Detection of Machine Translated Text and Translation Quality</p>
<p>Detecting Machine-Translated Paragraphs by Matching Similar Words</p>
<p>Automatic Detection of Translated Text and its Impact on Machine Translation</p>
<p>BLEU: a Method for Automatic Evaluation of Machine Translation</p>
<p>Building a Web-based parallel corpus and filtering out machinetranslated text</p>
<p>Machine Translation Detection from MonolingualWeb-Text</p>
<p>Machine Translationness: a Concept for Machine Translation Evaluation and Detection</p>
<p>MT Detection in Web-Scraped Parallel Corpora</p>
<p>On the Features of Translationese</p>
<p>Translationese and Its Dialects</p>
</blockquote>
<h2 id="Machine-Translation-Detection-from-Monolingual-Web-Text"><a href="#Machine-Translation-Detection-from-Monolingual-Web-Text" class="headerlink" title="Machine Translation Detection from Monolingual Web-Text"></a>Machine Translation Detection from Monolingual Web-Text</h2><p>首先强调的是，这篇论文检测的是SMT机器翻译。看到论文摘要时我想到，针对不同的机器翻译模型，检测的机制也是不一样的，要有这点意识。这篇论文关注到的是SMT系统中“phrase salad”现象，并使用单语语料就可以达到95.8%的准确率。</p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul>
<li>SMT翻译中的‘phrase salad’现象，指的是翻译结果的每个短语单独拿出来是对的，但组合到一起是错的</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mtcheck1.png" alt="图片"></p>
<ul>
<li>比如上面这个例子，not only后面应该有but also，但这个短语在SMT翻译系统里只有一半被翻译了</li>
<li>使用了一个分类器对句子是否是‘phrase salad’进行检测，使用到的特征包括两个，一个是语言模型，另外是一些人们常用但对SMT来说难以生成的短语</li>
</ul>
<h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><p>基于SMT翻译的特点，在特征选择时主要考虑3点：句子流畅度、语法正确度、短语完整度。从人工翻译提取到的特征表达了它和人工产生句子的相似性，从机器翻译中提取到的特征表达了它和机器翻译句子的相似性。</p>
<h3 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h3><ul>
<li>Fluency Feature<ul>
<li>使用两个语言模型，f(w,H)和f(w,MT)，前者表示人工翻译的语言模型，后者是机器翻译的语言模型</li>
</ul>
</li>
<li>Grammaticality Feature<ul>
<li>使用POS语言模型，f(pos,H)和f(pos, MT)，前者表示人工翻译的POS序列的语言模型，后者是机器翻译的</li>
<li>对提取出的function word的POS语言模型，f(fw, H)和f(fw, MT)<ul>
<li>function word: <ul>
<li>Prepositions: of, at, in, without, between</li>
<li>Pro<a href="https://www.baidu.com/s?wd=no&amp;tn=SE_PcZhidaonwhc_ngpagmjz&amp;rsv_dl=gh_pc_zhidao" target="_blank" rel="noopener">no</a>uns: he, they, anybody, it, one</li>
<li>Determiners: the, a, that, my, more, much, either,neither</li>
<li>Conjunctions: and, that, when, while, although, or</li>
<li>Auxiliary verbs: be (is, am, are), have, got, do</li>
<li>Particles: <a href="https://www.baidu.com/s?wd=no&amp;tn=SE_PcZhidaonwhc_ngpagmjz&amp;rsv_dl=gh_pc_zhidao" target="_blank" rel="noopener">no</a>, <a href="https://www.baidu.com/s?wd=no&amp;tn=SE_PcZhidaonwhc_ngpagmjz&amp;rsv_dl=gh_pc_zhidao" target="_blank" rel="noopener">no</a>t, nor, as</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Gappy-Phrase Feature<ul>
<li>中间有间隔的短语：如not only * but also</li>
<li>使用character级别的LM衡量</li>
<li>Sequential Pattern Mining<ul>
<li>使用sequential pattern挖掘的方法找到所有Gappy-Phrase</li>
</ul>
</li>
<li>使用的信息增益进行的短语选择，但是没看懂是如何计算特征的</li>
</ul>
</li>
<li>最后使用SVM进行分类</li>
<li>其他可考虑的feature：<ul>
<li>Translationese and its dialects论文</li>
<li>On the features of translationese论文（比较学术）</li>
<li>average token length</li>
<li>type-token ratio</li>
</ul>
</li>
</ul>
<h2 id="Automatic-Detection-of-Machine-Translated-Text-and-Translation-Quality"><a href="#Automatic-Detection-of-Machine-Translated-Text-and-Translation-Quality" class="headerlink" title="Automatic Detection of Machine Translated Text and Translation Quality"></a>Automatic Detection of Machine Translated Text and Translation Quality</h2><p>这篇论文也是使用的单语语料训练的分类器，用来进行机器翻译的检测和翻译质量的评估。这篇文章的重点强调的是，通过分类器检测的方法，只能对质量特别差的翻译系统起作用，如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mtcheck2.png" alt="图片"></p>
<p>可以看出来，翻译系统越好，检测的准确率就越低。</p>
<h3 id="特征选择-1"><a href="#特征选择-1" class="headerlink" title="特征选择"></a>特征选择</h3><ul>
<li>本文使用的特征都是二元特征，即是否存在POS ngram和467个function word。</li>
<li>从4个方面构建特征：word、lemma、pos、mixed</li>
</ul>
<h2 id="Detecting-Machine-Translated-Paragraphs-by-Matching-Similar-Words"><a href="#Detecting-Machine-Translated-Paragraphs-by-Matching-Similar-Words" class="headerlink" title="Detecting Machine-Translated Paragraphs by Matching Similar Words"></a>Detecting Machine-Translated Paragraphs by Matching Similar Words</h2><p>这篇文章主要是检查段落级别的机器翻译，方法是通过计算word的match情况，和段落的coherence来检测机器翻译，整体流程如下：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mtcheck3.png" alt="图片"></p>
<h3 id="计算相似词"><a href="#计算相似词" class="headerlink" title="计算相似词"></a>计算相似词</h3><p>先把段落内所有词打上POS标签，依次计算和其他词的相似度（如果POS相同则保留）。能够看出人工翻译的整体相似度比较低，机器翻译的相似度高一些，如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mtcheck4.png" alt="图片"></p>
<h3 id="计算Coherence"><a href="#计算Coherence" class="headerlink" title="计算Coherence"></a>计算Coherence</h3><p>基于POS对统计根据的均值和方差。如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mtcheck5.png" alt="图片"></p>
<h3 id="进行分类"><a href="#进行分类" class="headerlink" title="进行分类"></a>进行分类</h3><p>使用SVM分类。</p>
]]></content>
      <tags>
        <tag>机器翻译</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：《Solving and Generating Chinese Character Riddles》</title>
    <url>/2020/02/11/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E3%80%8ASolving-and-Generating-Chinese-Character-Riddles%E3%80%8B/</url>
    <content><![CDATA[<p>之前想给语音助手增加一个猜字谜的功能，这两天不忙就读了一下这篇机器解谜语的论文。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://www.aclweb.org/anthology/D16-1081.pdf" target="_blank" rel="noopener">https://www.aclweb.org/anthology/D16-1081.pdf</a></p>
</blockquote>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>解谜语类似于下面的过程：</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/solgene1.png" alt="图片"></p>
<ul>
<li>解谜的pipeline<ul>
<li>解题过程<ul>
<li>学习谜语中的短语和部首的对齐关系</li>
<li>学习谜语和rule的关系</li>
<li>使用上面两个关系，用算法得到候选答案</li>
<li>使用Ranking-SVM来对candidate进行排序</li>
</ul>
</li>
<li>生成谜语过程<ul>
<li>使用模版方法</li>
<li>使用替代的方法</li>
<li>使用Ranking-SVM来对candidate进行排序</li>
</ul>
</li>
<li>整体过程如下图：</li>
</ul>
</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/solgene2.png" alt="图片"></p>
<h2 id="Phrase-Radical-Alignments-and-Rules"><a href="#Phrase-Radical-Alignments-and-Rules" class="headerlink" title="Phrase-Radical Alignments and Rules"></a>Phrase-Radical Alignments and Rules</h2><h3 id="Alignment"><a href="#Alignment" class="headerlink" title="Alignment"></a>Alignment</h3><ul>
<li>希望将“千里”和“马”进行对齐</li>
<li>方法一<ul>
<li>将谜语分词$\left(w_{1}, w_{2}, \ldots, w_{n}\right)$</li>
<li>将答案分成不同部首$\left(r_{1}, r_{2}, \ldots, r_{m}\right)$</li>
<li>统计对齐$\left(\left[w_{i}, w_{j}\right], r_{k}\right)(i, j \in[1, n], k \in[1, m])$</li>
</ul>
</li>
<li>方法二<ul>
<li>谜语中两个连续字符$\left(w_{1}, w_{2}\right)$，如果w1是w2的部首，且w2的其余部分r出现在答案q中，则$\left(\left(w_{1}, w_{2}\right), r\right)$是一个对齐</li>
</ul>
</li>
<li>统计所有的对齐，并过滤掉出现频次小于3的</li>
<li>特别常见的对齐如下图：</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/solgne3.png" alt="图片"></p>
<h3 id="Rules"><a href="#Rules" class="headerlink" title="Rules"></a>Rules</h3><ul>
<li>总结了6类规则</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/solgne4.png" alt="图片"></p>
<ul>
<li>对于$\left(\left[w_{1}, w_{n}\right], r\right)$，如果r是wi的部首，则$\left(w_{1}, \dots, w_{i-1},(.), w_{i+1}, \dots, w_{n}\right)$就是一个潜在的规则，我们从数据中最终总结193条规则，归纳为上面6类</li>
<li>1000个汉字有至少1个alignment，27个汉字有至少100个alignment</li>
</ul>
<h2 id="Riddle-Solving-and-Generation"><a href="#Riddle-Solving-and-Generation" class="headerlink" title="Riddle Solving and Generation"></a>Riddle Solving and Generation</h2><h3 id="Solving-Chinese-Character-Riddles"><a href="#Solving-Chinese-Character-Riddles" class="headerlink" title="Solving Chinese Character Riddles"></a>Solving Chinese Character Riddles</h3><ul>
<li>解谜算法的伪代码如下</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/solgne5.png" alt="图片"></p>
<ul>
<li>以“上岗必戴安全帽”为例，“上岗”通过规则“上(up) (.)”和山对齐，“必” 和 “戴”跟自己对齐，“安全帽”因为analogical shape和“宀”对齐，最终得到结果“密”</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/solgene6.png" alt="图片"></p>
<ul>
<li>对答案进行排序，排序时使用的特征如下：</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/solgene7.png" alt="图片"></p>
<h3 id="Generating-Chinese-Character-Riddles"><a href="#Generating-Chinese-Character-Riddles" class="headerlink" title="Generating Chinese Character Riddles"></a>Generating Chinese Character Riddles</h3><ul>
<li>基于模板的方法</li>
<li>基于替换的方法</li>
<li>对候选的description进行排序，排序使用的特征如下：</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/solgene8.png" alt="图片"></p>
<h3 id="Ranking-Model"><a href="#Ranking-Model" class="headerlink" title="Ranking Model"></a>Ranking Model</h3><ul>
<li>score的计算：$\text { Score }(c)=\sum_{i=1}^{m} \lambda_{i} * g_{i}(c)$，其中c表示一个候选，gi(c)表示c的第i个特征，m是特征的总数，$\lambda_{i}​$表示特征的权重</li>
<li>使用Ranking SVM算法求解特征权重参数</li>
</ul>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><ul>
<li>数据：从网络上爬取的7w+谜语，3k+的笔画，古代诗词和对联等用于训练语言模型</li>
<li>使用准确率评价解谜的效果，使用人工评测来评价生成谜题</li>
</ul>
]]></content>
      <tags>
        <tag>论文阅读</tag>
        <tag>机器猜字谜</tag>
      </tags>
  </entry>
  <entry>
    <title>论文粗读:《Generating Sentences by Editing Prototypes》</title>
    <url>/2020/02/11/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94%E3%80%8AGenerating-Sentences-by-Editing-Prototypes%E3%80%8B/</url>
    <content><![CDATA[<p>因为想做一个根据不同年龄段的人生成不同故事内容的demo，所以阅读了这篇文本风格转换的论文。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://arxiv.org/abs/1709.08878" target="_blank" rel="noopener">https://arxiv.org/abs/1709.08878</a><br><a href="https://github.com/kelvinguu/neural-editor" target="_blank" rel="noopener">https://github.com/kelvinguu/neural-editor</a></p>
</blockquote>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h3><ul>
<li>通过从训练集中挑选一个prototype sentence，产生一个能捕捉到句子相似度等句子级别信息的latent edit vector，并通过这个向量生成句子</li>
<li>模型的整体框架如下图，这个模型的由来是基于人们的一个经验，通常人们写一个复杂的句子时，都是根据一个简单的句子，逐步修改而来的</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/gsep1.png" alt="图片"></p>
<ul>
<li>目标函数：最大化生成模型的log likelihood</li>
<li>使用locality sensitive hashing寻找相似的句子</li>
</ul>
<h2 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h2><h3 id="解决问题分两步"><a href="#解决问题分两步" class="headerlink" title="解决问题分两步"></a>解决问题分两步</h3><ul>
<li>从语料库里选择一句话<ul>
<li>prototype distribution: p(x’) （uniform over X）</li>
<li>以p(x’)的概率从语料库中随机选择一个prototype sentence</li>
</ul>
</li>
<li>把这句话进行修改<ul>
<li>以edit prior: p(z)的概率sample出一个edit vector: z （实际是对edit type进行编码）</li>
<li>将z和x’送入到$p_{\text {edit }}\left(x | x^{\prime}, z\right)$的神经网络中，产生新的句子x</li>
</ul>
</li>
</ul>
<p>整体公式如下：</p>
<p>$p(x)=\sum_{x^{\prime} \in \mathcal{X}} p\left(x | x^{\prime}\right) p\left(x^{\prime}\right)$</p>
<p>$p\left(x | x^{\prime}\right)=\mathbb{E}_{z \sim p(z)}\left[p_{\mathrm{edit}}\left(x | x^{\prime}, z\right)\right]$</p>
<h3 id="模型满足两个条件"><a href="#模型满足两个条件" class="headerlink" title="模型满足两个条件"></a>模型满足两个条件</h3><ul>
<li>Semantic smoothness：一次edit只能对文本进行小改动；多次edit可以产生大的改动</li>
<li>Consistent edit behavior：对edit有类型的控制，对不同句子同一类型的edit应该产生相似的效果</li>
</ul>
<h2 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h2><h3 id="对p-x-进行近似"><a href="#对p-x-进行近似" class="headerlink" title="对p(x)进行近似"></a>对p(x)进行近似</h3><ul>
<li>大多数的prototype x都是不相关的，即p(x|x’)非常小；因此我们只考虑和x有非常高的lexical overlap的prototype x’</li>
<li>定义一个lexical similarity neighborhor $\mathcal{N}(x) \stackrel{\text { def }}{=}\left\{x^{\prime} \in \mathcal{X}: d_{J}\left(x, x^{\prime}\right)&lt;0.5\right\}$，其中dj是x和x’的Jaccard距离</li>
<li>利用neighborhood prototypes和Jensen不等式求解</li>
</ul>
<p>$\begin{aligned} \log p(x) &amp;=\log \left[\sum_{x^{\prime} \in \mathcal{X}} p\left(x | x^{\prime}\right) p\left(x^{\prime}\right)\right] \ &amp; \geq \log \left[\sum_{x^{\prime} \in \mathcal{N}(x)} p\left(x | x^{\prime}\right) p\left(x^{\prime}\right)\right] \end{aligned}$</p>
<p>$\begin{array}{l}{=\log \left[|\mathcal{N}(x)|^{-1} \sum_{x^{\prime} \in \mathcal{N}(x)} p\left(x | x^{\prime}\right)\right]+R(x)} \ {\geq|\mathcal{N}(x)|^{-1} \sum_{x^{\prime} \in \mathcal{N}(x)} \log p\left(x | x^{\prime}\right)+R(x)} \ {\underbrace{x^{\prime} \in \mathcal{N}(x)}_{\text {def }_{\mathrm{LEX}}(x)}+R(x)}\end{array}$</p>
<ul>
<li>$\begin{array}{l}{p\left(x^{\prime}\right)=1 /|\mathcal{X}|} \ {\mathrm{R}(\mathrm{x})=\log (|\mathcal{N}(x)| /|\mathcal{X}|)}\end{array}$</li>
<li>$|\mathcal{N}(x)|​$是跟x相关的常数，x的邻居使用locality sensitive hashing (LSH) and minhashing进行预先的计算</li>
</ul>
<h3 id="对log-p-x-x’-进行近似"><a href="#对log-p-x-x’-进行近似" class="headerlink" title="对log p(x|x’)进行近似"></a>对log p(x|x’)进行近似</h3><ul>
<li><p>使用蒙特卡洛对$z \sim p(z)​$进行采样时可能会产生比较高的方差，因为$p_{\text {edit }}\left(x | x^{\prime}, z\right)​$对于大部分从p(z) sample出来的z来说都输出0，只对一部分不常见的值输出较大的值</p>
</li>
<li><p>使用inverse neural editor：$q\left(z | x^{\prime}, x\right)$</p>
<ul>
<li>对prototype x’和修正后的句子x，生成一个x’到x的转换的edit vector z，这个z在重要的值上会有较大的概率</li>
</ul>
</li>
<li><p>使用evidence lower bound（ELBO）来计算log p(x|x’)</p>
<p>$\begin{aligned} \log p\left(x | x^{\prime}\right) \geq &amp; \underbrace{\mathbb{E}_{z \sim q\left(z | x^{\prime}, x\right)}\left[\log p_{\text {edit }}\left(x | x^{\prime}, z\right)\right]}_{\mathcal{L}_{\text {gen }}} \ &amp;-\underbrace{\operatorname{KL}\left(q\left(z | x^{\prime}, x\right) | p(z)\right)}_{\mathcal{E}_{\mathrm{KL}}^{L}} \ \stackrel{\text { def }}{=} \operatorname{ELBO}\left(x, x^{\prime}\right) \end{aligned}$</p>
</li>
<li><p>q(z|x’,x)可以看成是VAE的encoder，pedit(x|x’,z)可以看成是VAE的decoder</p>
</li>
</ul>
<h3 id="目标函数-sum-x-prime-in-mathcal-N-x-operatorname-ELBO-left-x-x-prime-right"><a href="#目标函数-sum-x-prime-in-mathcal-N-x-operatorname-ELBO-left-x-x-prime-right" class="headerlink" title="目标函数 $\sum_{x^{\prime} \in \mathcal{N}(x)} \operatorname{ELBO}\left(x, x^{\prime}\right)$"></a>目标函数 $\sum_{x^{\prime} \in \mathcal{N}(x)} \operatorname{ELBO}\left(x, x^{\prime}\right)$</h3><ul>
<li>参数：$\Theta=\left(\Theta_{p}, \Theta_{q}\right)$，包含neural editor的参数和inverse neural editor的参数</li>
</ul>
<h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><h3 id="Neural-editor-p-text-edit-left-x-x-prime-z-right"><a href="#Neural-editor-p-text-edit-left-x-x-prime-z-right" class="headerlink" title="Neural editor $p_{\text {edit }}\left(x | x^{\prime}, z\right)$"></a>Neural editor $p_{\text {edit }}\left(x | x^{\prime}, z\right)$</h3><ul>
<li>input: prototype x’</li>
<li>output: revised sentence x</li>
<li>seq2seq<ul>
<li>encoder: 3层双向LSTM，使用Glove词向量初始化</li>
<li>decoder: 3层包含attention的LSTM<ul>
<li>最上面一层的hidden state用来和encoder输出的hidden state一起算attention</li>
<li>将attention向量和z向量concate一起，再送入softmax中</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Edit-prior-p-z"><a href="#Edit-prior-p-z" class="headerlink" title="Edit prior $p(z)$"></a>Edit prior $p(z)$</h3><ul>
<li>edit vector z的sample方法<ul>
<li>先采样其scalar length：$z_{\text {norm }} \sim  \operatorname{Unif}(0,10)​$</li>
<li>再采样其direction:  在uniform distribution中采样一个zdir向量</li>
<li>$z=z_{\text {norm }} \cdot z_{\text {dir }}$</li>
<li>这样做是为了方便计算KL散度</li>
</ul>
</li>
</ul>
<h3 id="Inverse-neural-editor-q-left-z-x-prime-x-right"><a href="#Inverse-neural-editor-q-left-z-x-prime-x-right" class="headerlink" title="Inverse neural editor $q\left(z | x^{\prime}, x\right)$"></a>Inverse neural editor $q\left(z | x^{\prime}, x\right)$</h3><ul>
<li>假设x’和x只差了一个word，那么edit vector z跟word的词向量应该是一样的，那么多个word的插入就相当于多个word的词向量的和，删除同理</li>
<li>加入到x’的词的集合：$I=x \backslash x^{\prime}​$</li>
<li>从x’中删除的词的集合：$D=x^{\prime}\backslash  x$</li>
<li>x’和x的差异：$f\left(x, x^{\prime}\right)=\sum_{w \in I} \Phi(w) \oplus \sum_{w \in D} \Phi(w)$<ul>
<li>$\Phi(w)$表示w的词向量，它同时也是inverse neural editor q的参数，使用300维的Glove向量初始化</li>
<li>$\oplus$表示concate操作</li>
</ul>
</li>
<li>认为q是在f的基础上加入噪声获得的（先旋转，在rescale）：</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/gsep2.png" alt="图片"></p>
<ul>
<li>$\begin{array}{l}{f_{\text {norm }}=|f|} \ {f_{\text {dir }}=f / f_{\text {norm }}}\end{array}$</li>
<li>$\operatorname{vMF}(v ; \mu, \kappa)​$表示点v的unit空间中的vMF分布，参数包含mean vector$\mu​$和concentration parameter $ \kappa​$</li>
<li><p>因此可得：$\begin{aligned} q\left(z_{\text {dir }} | x^{\prime}, x\right) &amp;=\operatorname{vMF}\left(z_{\text {dir }} ; f_{\text {dir }}, \kappa\right) \ q\left(z_{\text {norm }} | x^{\prime}, x\right) &amp;=\text { Unif }\left(z_{\text {norn }} ;\left[\tilde{f}_{\text {norn }}, \tilde{f}_{\text {nom }}+\epsilon\right]\right) \end{aligned}$</p>
</li>
<li><p>其中 $\tilde{f}_{\text {norm }}=\min \left(f_{\text {norm }}, 10-\epsilon\right)$</p>
</li>
<li>最终 $z=z_{\mathrm{dir}} \cdot z_{\mathrm{norm}}$</li>
</ul>
<h2 id="Details"><a href="#Details" class="headerlink" title="Details"></a>Details</h2><h3 id="计算目标函数的梯度-nabla-Theta-q-operatorname-ELBO-left-x-x-prime-right-nabla-Theta-q-mathcal-L-mathrm-gen-nabla-Theta-q-mathcal-L-mathrm-KL"><a href="#计算目标函数的梯度-nabla-Theta-q-operatorname-ELBO-left-x-x-prime-right-nabla-Theta-q-mathcal-L-mathrm-gen-nabla-Theta-q-mathcal-L-mathrm-KL" class="headerlink" title="计算目标函数的梯度$\nabla_{\Theta_{q}} \operatorname{ELBO}\left(x, x^{\prime}\right)=\nabla_{\Theta_{q}} \mathcal{L}_{\mathrm{gen}}-\nabla_{\Theta_{q}} \mathcal{L}_{\mathrm{KL}}$"></a>计算目标函数的梯度$\nabla_{\Theta_{q}} \operatorname{ELBO}\left(x, x^{\prime}\right)=\nabla_{\Theta_{q}} \mathcal{L}_{\mathrm{gen}}-\nabla_{\Theta_{q}} \mathcal{L}_{\mathrm{KL}}$</h3><ul>
<li>使用重参数计算：$\nabla_{\Theta_{q}} \mathcal{L}_{\mathrm{gen}}$<ul>
<li>将z~q(z|x’,x)重写为$z=h(\alpha)$</li>
<li>$\begin{aligned} \nabla \Theta_{q} \mathcal{L}_{\mathrm{gen}} &amp;=\nabla \Theta_{q} \mathbb{E}_{z \sim q\left(z | x^{\prime}, x\right)}\left[\log p_{\text {edit }}\left(x | x^{\prime}, z\right)\right] \ &amp;=\mathbb{E}_{\alpha \sim p(\alpha)}\left[\nabla \Theta_{q} \log p_{\text {edit }}\left(x | x^{\prime}, h(\alpha)\right)\right] \end{aligned}$</li>
</ul>
</li>
<li>计算$\nabla_{\Theta_{q}} \mathcal{L}_{\mathrm{KL}}$<ul>
<li>$\begin{aligned} \mathcal{L}_{\mathrm{KL}} &amp;=\mathrm{KL}\left(q\left(z_{\text {norm }} | x^{\prime}, x\right) | p\left(z_{\text {norm }}\right)\right) \ &amp;+\mathrm{KL}\left(q\left(z_{\text {dir }} | x^{\prime}, x\right) | p\left(z_{\text {dir }}\right)\right) \end{aligned}$</li>
<li>$\begin{array}{l}{\operatorname{KL}(\operatorname{vMF}(\mu, \kappa) | \operatorname{vMF}(\mu, 0))=\kappa \frac{I_{d / 2}(\kappa)+I_{d / 2-1}(\kappa) \frac{d-2}{2 \kappa}}{I_{d / 2-1}(\kappa)-\frac{d-2}{2 \kappa}}} \ {-\log \left(I_{d / 2-1}(\kappa)\right)-\log (\Gamma(d / 2))} \ {+\log (\kappa)(d / 2-1)-(d-2) \log (2) / 2}\end{array}$</li>
</ul>
</li>
</ul>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="Datsets"><a href="#Datsets" class="headerlink" title="Datsets"></a>Datsets</h3><ul>
<li>Yelp</li>
<li>One BillionWord Language Model Benchmark</li>
<li>使用Spacy将NER的词替换成其NER category</li>
<li>将出现频次小于10000的词用UNK替换</li>
</ul>
<h3 id="Generative-Modeling"><a href="#Generative-Modeling" class="headerlink" title="Generative Modeling"></a>Generative Modeling</h3><ul>
<li>对比几个生成模型的效果（KENLM语言模型、自回归语言模型）</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/gsep3.png" alt="图片"></p>
<ul>
<li>使用neural editor可以生成跟prototype很不一样的句子</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/gsep4.png" alt="图片"></p>
<h3 id="Human-Evaluation"><a href="#Human-Evaluation" class="headerlink" title="Human Evaluation"></a>Human Evaluation</h3><ul>
<li>降低softmax temperture有助于产生更符合语法的句子，但也会产生short and generic sentence<ul>
<li>从corpus中sample出prototype sentence可以增加生成句子的多样性，因此即便temperature设置为0，也不会影响句子多样子，这是比传统的NLM强的地方</li>
</ul>
</li>
<li>从grammaticality 和 plausibility两方面进行评测</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/gsep5.png" alt="图片"></p>
<h3 id="Semantics-of-NeuralEditor"><a href="#Semantics-of-NeuralEditor" class="headerlink" title="Semantics of NeuralEditor"></a>Semantics of NeuralEditor</h3><ul>
<li>跟sentence variational autoencoder (SVAE)模型对比，SVAE将句子映射到semantic空间向量，再从向量还原句子</li>
<li>semantic smoothness<ul>
<li>smoothness表示每一个小的edit只能对句子有一点点改变</li>
<li>我们先从corpus中选择一个prototype sentence，然后不断对它使用neural editor，并让人工对semantic的变化进行打分。</li>
<li>对比实验有两个，一个是SVAE，一个是从corpus中根据cosine相似度选择出的句子</li>
</ul>
</li>
<li>consistent edit behavior</li>
</ul>
]]></content>
      <tags>
        <tag>论文阅读</tag>
        <tag>文本风格迁移</tag>
      </tags>
  </entry>
  <entry>
    <title>Separable Convolution</title>
    <url>/2020/02/06/Separable-Convolution/</url>
    <content><![CDATA[<p>阅读论文《The Evolved Transformer》时遇到了separable convolution的概念，因此找了相关资料学习了一下。</p>
<blockquote>
<p>参考：<br><a href="https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-al" target="_blank" rel="noopener">https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-al</a></p>
</blockquote>
<a id="more"></a>
<p>在讲Separable Convolution前先了解下常用的卷积网络的定义。</p>
<h2 id="Convolutional-Neural-Networks"><a href="#Convolutional-Neural-Networks" class="headerlink" title="Convolutional Neural Networks"></a>Convolutional Neural Networks</h2><blockquote>
<p>参考：<br><a href="https://towardsdatascience.com/what-is-a-neural-network-6010edabde2b" target="_blank" rel="noopener">https://towardsdatascience.com/what-is-a-neural-network-6010edabde2b</a></p>
</blockquote>
<p>卷积网络中最重要的是卷积核，通过卷积核在图像每个区域的运算，得到图像不同的特征，如下图（可以在<a href="http://setosa.io/ev/image-kernels/" target="_blank" rel="noopener">http://setosa.io/ev/image-kernels/</a>中更好得体验）：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn1.png" alt="图片"></p>
<p>上面这张图使用outliine卷积核，实际中可以使用sharp等不同功能的卷积核以达到不同效果。</p>
<p>对于神经网络的每一层而言，可以使用多个卷积和得到不同的特征图，并将这些特征图一起输入到下一层网络。最终这些特征供给最后一层的分类器进行匹配，得到分类结果。下面的动画展示了这个过程：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn1.gif" alt="图片"></p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn2.gif" alt="图片"></p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn3.gif" alt="图片"></p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn4.gif" alt="图片"></p>
<p>Separable Convolution可以分成spatial separable convolution和depthwise separable convolution。</p>
<p>对于12x12x3的图像，5x5x3的卷积核，能产生8x8x1的输出：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn2.png" alt="图片"></p>
<p>假设我们想要8x8x256的输出，则需要使用256个卷积核来创造256个8x8x1的图像，把他们叠加在一起产生8x8x256的输出：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn3.png" alt="图片"></p>
<p>即12x12x3 — (5x5x3x256) — &gt;12x12x256</p>
<h2 id="Spatial-Separable-Convolutions"><a href="#Spatial-Separable-Convolutions" class="headerlink" title="Spatial Separable Convolutions"></a>Spatial Separable Convolutions</h2><p>Spatial separable convolution将卷积分成两部分，最常见的是把3x3的kernel分解成3x1和1x3的kernel，如：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn4.png" alt="图片"></p>
<p>通过这种方式，原本一次卷积要算9次乘法，现在只需要6次。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn5.png" alt="图片"></p>
<p>还有一个Sobel kernel（用来检测边）也是用的这种方法：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn6.png" alt="图片"></p>
<p>但spatial separable convolution存在的问题是，不是所有kernel都能转换成2个小的kernel。</p>
<h2 id="Depthwise-Separable-Convolutions"><a href="#Depthwise-Separable-Convolutions" class="headerlink" title="Depthwise Separable Convolutions"></a>Depthwise Separable Convolutions</h2><p>由于卷积并不使用矩阵相乘，为了减少计算量，可以将卷积的过程分成两部分：a depthwise convolution and a pointwise convolution. </p>
<h3 id="depthwise-convolution"><a href="#depthwise-convolution" class="headerlink" title="depthwise convolution"></a>depthwise convolution</h3><p>首先，我们使用3个5x5x1的卷积核产生8x8x3的图像：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn7.png" alt="图片"></p>
<h3 id="pointwise-convolution"><a href="#pointwise-convolution" class="headerlink" title="pointwise convolution"></a>pointwise convolution</h3><p>其次，使用1x1x3 的卷积核对每个像素计算，得到8x8x1 的图像：</p>
<p><img src="http://q503tsu73.bkt.clouddn.com/sc8.png?e=1580956603&amp;token=05Ii263bPN3Z-CT3JPRaRfWi5sXIj8pwX6V1bN2j:BxS4Xqtt2YsqJ5GJVFVlnK9D3xw=&amp;attname=" alt="图片"></p>
<p>使用256个1x1x3 的卷积核，则恶意产生8x8x256的图像：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn9.png" alt="图片"></p>
<p>可以看到，整个过程由原来的12x12x3 — (5x5x3x256) →12x12x256，变成12x12x3 — (5x5x1x1) — &gt; (1x1x3x256) — &gt;12x12x256</p>
<h2 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h2><p>主要就是减少了计算量，原先是256个5x5x3的卷积核移动8x8次，即需要256x3x5x5x8x8=1,228,800次乘法计算。使用depthwise convolution，有3个5x5x1的卷积核移动8x8次，需要3x5x5x8x8 = 4,800次乘法计算。使用pointwise convolution，有256个1x1x3的卷积核移动8x8次，需要256x1x1x3x8x8=49,152次乘法计算，加起来共有53,952次计算。</p>
<h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><ul>
<li>keras doc：<a href="https://keras.io/layers/convolutional/" target="_blank" rel="noopener">https://keras.io/layers/convolutional/</a></li>
<li><a href="https://github.com/alexandrosstergiou/keras-DepthwiseConv3D" target="_blank" rel="noopener">https://github.com/alexandrosstergiou/keras-DepthwiseConv3D</a></li>
<li>[<a href="https://github.com/sara-kassani/Depthwise-Separable-Convolutional-Neural-Network-for-Skin-Lesion-Classification](" target="_blank" rel="noopener">https://github.com/sara-kassani/Depthwise-Separable-Convolutional-Neural-Network-for-Skin-Lesion-Classification](</a></li>
</ul>
]]></content>
      <tags>
        <tag>Convolution</tag>
        <tag>卷积网络</tag>
      </tags>
  </entry>
  <entry>
    <title>论文粗读:《The Evolved Transformer》</title>
    <url>/2020/02/05/%E8%AE%BA%E6%96%87%E7%B2%97%E8%AF%BB%E2%80%94%E2%80%94%E3%80%8AThe-Evolved-Transformer%E3%80%8B/</url>
    <content><![CDATA[<p>今天阅读了论文《The Evolved Transformer》，该论文使用了神经架构搜索方法找到了一个更优的transformer结构。下面是阅读过程中的笔记。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://arxiv.org/pdf/1901.11117.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1901.11117.pdf</a><br><a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/evolved_transformer.py" target="_blank" rel="noopener">https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/evolved_transformer.py</a><br><a href="https://blog.csdn.net/jasonzhoujx/article/details/88875469" target="_blank" rel="noopener">https://blog.csdn.net/jasonzhoujx/article/details/88875469</a></p>
</blockquote>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h3><ul>
<li>神经架构搜索<ul>
<li>tournament selection architecture search </li>
<li>warm start</li>
<li>Progressive Dynamic Hurdles（PDH）</li>
</ul>
</li>
<li>搜索出了一个新的transformer架构：Evolved Transformer</li>
</ul>
<h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><h3 id="搜索空间"><a href="#搜索空间" class="headerlink" title="搜索空间"></a>搜索空间</h3><ul>
<li>encoder stackable cell<ul>
<li>6个NASNet-style block<ul>
<li>左右两个block将输入的hidden state转成左右两个hidden state再归并成为一个新的hidden state，作为self-attention的输入</li>
</ul>
</li>
</ul>
</li>
<li>decoder stackable cell<ul>
<li>8个NASNet-style block</li>
</ul>
</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/et1.png" alt="图片"></p>
<ul>
<li>搜索空间branch<ul>
<li>Input：分支可以从输入池中选择一个隐藏状态作为当前block的输入。单元中的第i个block可以从[0, i]个隐藏状态中进行选择，其中第j个隐藏状态表示该cell中第j个block的输出，第0个候选项为单元的输入。</li>
<li>Normalization：归一化项提供了两个选项， [LAYER NORMALIZATION (Ba et al., 2016), NONE]</li>
<li>Layer：构造一个神经网络层，提供的选项包括：<ul>
<li>标准卷积</li>
<li>深度可分离卷积</li>
<li>LIGHTWEIGHT 卷积</li>
<li>n头注意力层</li>
<li>GATED LINEAR UNIT</li>
<li>ATTEND TO ENCODER（decoder专用）</li>
<li>全等无操作</li>
<li>Dead Branch，切断输出</li>
</ul>
</li>
<li>Relative Output Dimension：决定神经网络层输出的维度。</li>
<li>Activation：搜索中激活函数的选项有[SWISH, RELU, LEAKY RELU, NON]</li>
<li>Combiner Function：表征的是左枝和右枝的结合方式，包括{ADDITION、CONCATENATION、MULTIPLICATION}。如果左右枝最终输出形状不同，则需要使用padding进行填充。短的向量向长的向量对齐，当使用加法进行结合时使用0填充，当使用乘法进行结合时使用1填充。</li>
<li>Number of cells：纵向叠加的cell的数量，搜索范围是[1,6]</li>
</ul>
</li>
</ul>
<h3 id="演进过程"><a href="#演进过程" class="headerlink" title="演进过程"></a>演进过程</h3><ul>
<li>锦标赛选择（Tournament Selection）：<ul>
<li>tournament selection算法是一种遗传算法，首先随机生成一批个体, 这些个体是一个个由不同组件组成的完整的模型，我们在目标任务上训练这些个体并在验证集上面计算他们的表现。</li>
<li>首先在初始种群中进行采样产生子种群，从子种群中选出适应性（fitness）最高的个体作为亲本（parent）。被选中的亲本进行突变——也就是将网络模型中的一些组件改变为其他的组件——以产生子模型，然后在对这些子模型分配适应度（fitness），在训练集和测试集上进行训练和验证。</li>
<li>对种群重新进行采样，用通过评估的子模型代替子种群中的fitness的个体以生成新的种群。</li>
<li>重复上面的步骤，直到种群中出现超过给定指标的模型。</li>
</ul>
</li>
<li>渐进式动态障碍（Progressive Dynamic Hurdle）：<ul>
<li>实验使用的训练集是WMT14英语到德语的机器翻译数据集，完整的训练和验证过程需要很长的时间，如果在所有的子模型上进行完整的训练和验证过程将会耗费很大的计算资源。因此论文中使用渐进式动态障碍的方法来提前停止一些没有前景的模型的训练，转而将更多的计算资源分配那些当前表现更好的子模型。具体来说就是让当前表现最好的一些模型多训练一些step。</li>
<li>假设当前种群经过一次锦标赛选择，生成了m个子模型并且加入到了种群中，这时候计算整个种群fitness的平均值h0，下一次锦标赛选择将会以h0作为对照，生成的另外m个fitness超过h0的子模型可以继续训练s1个step，接着进行种群中的所有的其他个体会继续训练s1个step，然后在新的种群中生成h1，以此类推知道种群中所有的个体的训练step都达到一个指定值。</li>
<li>如果一个子模型是由第iii次锦标赛选择之后的亲本生成的，那么验证的过程将会进行iii次。第一次为该模型分配s0次的训练step并且在验证集上进行验证，若验证的fitness大于h0则再分配s1次训练step，再验证，再与h1比较，只有子样本通过h0,h1,…,hi次比较才能作为新的个体加入到新的种群中。</li>
</ul>
</li>
</ul>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><ul>
<li><p>机器翻译</p>
<ul>
<li>在初始的10K step使用0.01的learning rate</li>
<li><p>Transformer</p>
<ul>
<li>inverse-square-root decay to 0 at 300K steps：$l r=s t e p^{-0.00303926^{\circ}}-.962392$</li>
</ul>
</li>
<li><p>Evolved Transformer</p>
<ul>
<li>single-cycle cosine decay</li>
</ul>
</li>
<li>every decay was paired with the same constant 0.01 warmup.</li>
<li>大模型使用高一点的dropout（0.3），小模型使用0.2 dropout</li>
<li>beam-size=6, lenth-penalty=0.6, max-output=50</li>
</ul>
</li>
<li>语言模型<ul>
<li>跟机器翻译差不多，去掉了label smooth, intra-attention dropout=0.0</li>
</ul>
</li>
<li>Search Configuration<ul>
<li>populatino=100</li>
<li>mutation=2.5%</li>
<li>fitness: negative log perplexity</li>
</ul>
</li>
</ul>
<h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><ul>
<li>最终搜索出来的模型结构</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/et2.png" alt="图片"></p>
<ul>
<li>embedding_size=768, 6 encoder, 6 decoder</li>
<li>attention_head=16</li>
<li>ET比Transformer可以在更小的模型上达到更好的效果，当模型增大时两者的差距就不大了（可能因为模型越大越容易过拟合，而且单独增加embedding_size可能不起作用，需要和depth共同增加）</li>
</ul>
]]></content>
      <tags>
        <tag>论文阅读</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读:《Towards a Human-like Open-Domain Chatbot》</title>
    <url>/2020/02/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94%E3%80%8ATowards-a-Human-like-Open-Domain-Chatbot%E3%80%8B/</url>
    <content><![CDATA[<p>今天阅读了谷歌最新出的一篇论文，《Towards a Human-like Open-Domain Chatbot》，主要提出了端到端对话机器人的一种评测方法和模型框架。下面是阅读过程中的笔记。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html" target="_blank" rel="noopener">https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html</a><br><a href="https://arxiv.org/pdf/2001.09977.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2001.09977.pdf</a><br><a href="https://github.com/google-research/google-research/tree/master/meena" target="_blank" rel="noopener">https://github.com/google-research/google-research/tree/master/meena</a></p>
</blockquote>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="开放的chatbot-API总结"><a href="#开放的chatbot-API总结" class="headerlink" title="开放的chatbot API总结"></a>开放的chatbot API总结</h3><ul>
<li>cleverbot API: <a href="https://www.cleverbot.com/api/" target="_blank" rel="noopener">https://www.cleverbot.com/api/</a><ul>
<li><a href="https://github.com/plasticuproject/cleverbotfree" target="_blank" rel="noopener">https://github.com/plasticuproject/cleverbotfree</a></li>
</ul>
</li>
<li>xiaobing: <a href="https://www.msxiaobing.com/" target="_blank" rel="noopener">https://www.msxiaobing.com/</a></li>
<li>mitsuku: <a href="https://www.pandorabots.com/mitsuku/" target="_blank" rel="noopener">https://www.pandorabots.com/mitsuku/</a><ul>
<li><a href="https://github.com/hanwenzhu/mitsuku-api" target="_blank" rel="noopener">https://github.com/hanwenzhu/mitsuku-api</a></li>
</ul>
</li>
</ul>
<h3 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h3><ul>
<li>模型架构：Evolved Transformer<ul>
<li>模型输入：多轮对话（最多7轮）</li>
<li>模型输出：回复</li>
<li>最佳模型：2.6B参数，10.2PPL，8K BPE subword vocabulary, 训练数据40B words</li>
</ul>
</li>
<li>评测指标<ul>
<li>PPL</li>
<li>SSA（Sensibleness and Specificity Average）用来评估<ul>
<li>whether make sense</li>
<li>whether specific</li>
</ul>
</li>
<li>人工评测使用static（1477个多轮对话）和interactive（想说啥就说啥）两种数据集，发现SSA和PPL在这两个数据集上高度相关</li>
<li>模型在评测集的表现：<ul>
<li>0.72的SSA</li>
<li>经过filtering mechanism 和 tuned decoding后有0.79的SSA，相比于人提供的0.86SSA的回复已经很接近了</li>
</ul>
</li>
</ul>
</li>
<li>方法的局限性<ul>
<li>评测数据集的局限性，不能解决所有领域的问题</li>
</ul>
</li>
</ul>
<h2 id="对话机器人的评价"><a href="#对话机器人的评价" class="headerlink" title="对话机器人的评价"></a>对话机器人的评价</h2><h3 id="人工进行评测时的参考标准"><a href="#人工进行评测时的参考标准" class="headerlink" title="人工进行评测时的参考标准"></a>人工进行评测时的参考标准</h3><ul>
<li>Sensibleness<ul>
<li>common sense</li>
<li>logical coherence</li>
<li>consistency</li>
<li>人工评测时对于可打的标签：confusing, illogical, out of context, factually wrong, make sense</li>
<li>缺陷：对于安全的回答，如I don’t know，无法区分</li>
</ul>
</li>
<li>Specificity<ul>
<li>A: I love tennis.   B: That’s nice 应该被标记为not specific，如果 B：Me too, I can’t get enough of Roger Federer!则被标记为specific</li>
<li>已经被标记为not sensible的直接标记为not specific</li>
</ul>
</li>
<li>SSA<ul>
<li>可以使用Sensibleness和Specificity标记在所有responses的比例来作为参考标准</li>
<li>使用SSA将Sensibleness和Specificity的比例进行了结合</li>
</ul>
</li>
</ul>
<h3 id="可进行对比的几个开源chatbot框架"><a href="#可进行对比的几个开源chatbot框架" class="headerlink" title="可进行对比的几个开源chatbot框架"></a>可进行对比的几个开源chatbot框架</h3><ul>
<li>基于RNN：<a href="https://github.com/lukalabs/cakechat" target="_blank" rel="noopener">https://github.com/lukalabs/cakechat</a></li>
<li>基于Transformer: <a href="https://github.com/microsoft/DialoGPT" target="_blank" rel="noopener">https://github.com/microsoft/DialoGPT</a><ul>
<li>762M参数的模型效果更好一些</li>
<li>dialogpt没有公开其解码和MMI-reranking的过程，gpt2bot实现了解码：<a href="https://github.com/polakowo/gpt2bot" target="_blank" rel="noopener">https://github.com/polakowo/gpt2bot</a></li>
<li>附加一个中文的基于DialoGPT开发的闲聊模型<ul>
<li><a href="https://github.com/yangjianxin1/GPT2-chitchat" target="_blank" rel="noopener">https://github.com/yangjianxin1/GPT2-chitchat</a></li>
<li><a href="https://blog.csdn.net/kingsonyoung/article/details/103803067" target="_blank" rel="noopener">https://blog.csdn.net/kingsonyoung/article/details/103803067</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="构建静态评测集"><a href="#构建静态评测集" class="headerlink" title="构建静态评测集"></a>构建静态评测集</h3><ul>
<li>从单轮开始：<a href="http://ai.stanford.edu/~quocle/QAresults.pdf" target="_blank" rel="noopener">http://ai.stanford.edu/~quocle/QAresults.pdf</a></li>
<li>增加一些个性化问题，如：Do you like cats?<ul>
<li>A: Do you like movies?; B: Yeah. I like sci-fi mostly; A: Really? Which is your favorite?期待I love Back to the Future这样的回答，对于I don’t like movies这样的回复应标记为not sensible</li>
</ul>
</li>
</ul>
<h3 id="进行动态评测"><a href="#进行动态评测" class="headerlink" title="进行动态评测"></a>进行动态评测</h3><ul>
<li>机器人以Hi开始，评测人员自由与bot对话，并对每一个bot的回复进行评测。每一个对话至少14轮，至多28轮。</li>
</ul>
<h2 id="Meena-Chatbot"><a href="#Meena-Chatbot" class="headerlink" title="Meena Chatbot"></a>Meena Chatbot</h2><h3 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h3><ul>
<li>来源于public social media</li>
<li>清洗流程<ul>
<li>去掉 subword 数目&lt;=2 或 subword 数目 &gt;= 128</li>
<li>去掉 字母比例&lt;0.7</li>
<li>去掉 包含URL</li>
<li>去掉 作者名字bot</li>
<li>去掉 出现100次以上</li>
<li>去掉 跟上文n-gram重复比例过高</li>
<li>去掉 敏感句子</li>
<li>去掉 括号中内容</li>
<li>当一个句子被删除时，则上文全部被删除</li>
</ul>
</li>
<li>共清洗出867M的(context, response)对</li>
<li>使用sentence piece进行BPE分词，得到8K的BPE vocab</li>
<li>最终语料包含341GB的语料(40B word)</li>
</ul>
<h3 id="模型框架"><a href="#模型框架" class="headerlink" title="模型框架"></a>模型框架</h3><ul>
<li>Evolved Transformer<ul>
<li>2.6B parameter</li>
<li>1 ET encoder + 13 ET decoder</li>
</ul>
</li>
<li>最大的模型可达到10.2的PPL</li>
<li>最大的传统Transformer模型（32层decoder）可达到10.7的PPL</li>
<li>hidden size: 2560</li>
<li>attention head: 32</li>
<li>共享编码、解码、softmax的embedding</li>
<li>编码、解码最长是128</li>
</ul>
<h3 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h3><ul>
<li>使用Adafactor optimizer，初始学习率0.01，在前10k step保持不变，使用inverse square root of the number of steps进行衰减</li>
<li>使用<a href="https://github.com/tensorflow/" target="_blank" rel="noopener">https://github.com/tensorflow/</a>tensor2tensor代码进行训练</li>
</ul>
<h3 id="解码细节"><a href="#解码细节" class="headerlink" title="解码细节"></a>解码细节</h3><ul>
<li><p>为了避免产生乏味的回复，可以使用多种方法进行解码</p>
<ul>
<li>reranking</li>
<li>基于profiles, topics, and styles</li>
<li>强化学习</li>
<li>变分自编吗</li>
</ul>
</li>
<li><p>当PPL足够小时，可以使用sample-and-rank策略进行解码</p>
<ul>
<li><p>使用temperature T随机产生N个独立的候选</p>
<ul>
<li><p>$p_{i}=\frac{\exp \left(z_{i} / T\right)}{\sum_{j} \exp \left(z_{j} / T\right)}$</p>
</li>
<li><p>T=1产生不经过修正的分布</p>
</li>
<li><p>T越大，越容易产生不常见的词，如相关的实体名词，但可能产生错误的词</p>
</li>
<li><p>T越小，越容易产生常见的词，如冠词或介词，虽然安全但不specific</p>
</li>
<li><p>解释1</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">温度是神经网络的超参数，用于在应用softmax之前通过缩放对数来控制预测的随机性。 例如，在TensorFlow的LSTM中，温度代表在计算softmax之前将logit除以多少。</span><br><span class="line"></span><br><span class="line">当温度为1时，我们直接在logits（较早层的未缩放输出）上计算softmax，并使用温度为0.6的模型在logits&#x2F;0.6上计算softmax，从而得出较大的值。 在更大的值上执行softmax可使LSTM 更加自信 （需要较少的输入来激活输出层），但在其样本中也更加保守 （从不太可能的候选样本中进行抽样的可能性较小）。 使用较高的温度会在各个类上产生较软的概率分布，并使RNN更容易被样本“激发”，从而导致更多的多样性和更多的错误 。</span><br><span class="line"></span><br><span class="line">softmax函数通过确保网络输出在每个时间步长都在零到一之间，基于其指数值对候选网络的每次迭代进行归一化。</span><br><span class="line"></span><br><span class="line">因此，温度增加了对低概率候选者的敏感性。</span><br></pre></td></tr></table></figure>
</li>
<li><p>解释2</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">当T很大时，即趋于正无穷时，所有的激活值对应的激活概率趋近于相同（激活概率差异性较小）；而当T很低时，即趋于0时，不同的激活值对应的激活概率差异也就越大。</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
<li><p>发现使用beam-search解码会产生重复且无趣的回复，使用sample-and-rank产生的回复会丰富一些</p>
</li>
<li>使用N=20，T=0.88</li>
<li>response score的计算：logP/T，P是response的likelihood，T是token的个数</li>
<li><p>解码时增加detect cross turn repetitions</p>
<ul>
<li>当两个turn的n-gram重复超过一定比例时，则从候选中删除</li>
</ul>
</li>
<li>增加一个分类层，用来过滤掉敏感回复</li>
</ul>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><h3 id="SSA和PPL是相关的"><a href="#SSA和PPL是相关的" class="headerlink" title="SSA和PPL是相关的"></a>SSA和PPL是相关的</h3><ul>
<li>基本呈线性关系</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/tod1.png" alt="图片"></p>
<h3 id="效果的比较"><a href="#效果的比较" class="headerlink" title="效果的比较"></a>效果的比较</h3><ul>
<li>小冰：呈现出个性化的回复，但有时也会无意义，且经常回复得太平常。小冰另一个特点就是具有同情心，可以在以后的评价指标中考虑这一点。小冰有near-human-level engagingness但not very close to human-level humanness，因此在我们的评测指标上SSA不高。</li>
<li>mitsuku：56%SSA（72%sensibility 40%specifity）, 网站上的对话并不是它参加图灵测试的版本</li>
<li>DialoGPT：48%SSA（57%sensibility 49%specifity）</li>
<li>CleverBot：在interactive评测表现比static上稍微好一些（56% interactive SSA，44% static SSA）。发现cleverbot更擅长将话题引入到它更擅长的领域中，缺少personality</li>
<li>Meena：base（72% SSA），full（79% SSA）</li>
</ul>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>论文阅读</tag>
        <tag>Chatbot</tag>
      </tags>
  </entry>
  <entry>
    <title>各类资源定期汇总</title>
    <url>/2020/02/01/%E5%90%84%E7%B1%BB%E8%B5%84%E6%BA%90%E5%AE%9A%E6%9C%9F%E6%B1%87%E6%80%BB/</url>
    <content><![CDATA[<p>一些学习等资源的总结，不定期更新。</p>
<a id="more"></a>
<h1 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h1><h2 id="视频类资源"><a href="#视频类资源" class="headerlink" title="视频类资源"></a>视频类资源</h2><h3 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h3><ul>
<li>吴恩达机器学习：<a href="https://study.163.com/course/courseMain.htm?courseId=1004570029" target="_blank" rel="noopener">https://study.163.com/course/courseMain.htm?courseId=1004570029</a></li>
<li>统计学习基础：链接: <a href="https://pan.baidu.com/s/1NWE6lEJrSgOFAEVVOW2TmQ" target="_blank" rel="noopener">https://pan.baidu.com/s/1NWE6lEJrSgOFAEVVOW2TmQ</a> 提取码: g7km</li>
<li>林轩田机器学习：<a href="https://www.tinymind.cn/articles/168" target="_blank" rel="noopener">https://www.tinymind.cn/articles/168</a></li>
</ul>
<h3 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h3><ul>
<li>花书：<a href="https://www.bilibili.com/video/av69236102?from=search&amp;seid=4183604428283884229" target="_blank" rel="noopener">https://www.bilibili.com/video/av69236102?from=search&amp;seid=4183604428283884229</a></li>
</ul>
<h3 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h3><ul>
<li>斯坦福自然语言处理：<a href="https://zhuanlan.zhihu.com/p/63199665" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/63199665</a></li>
<li>宗成庆统计自然语言处理：<ul>
<li><a href="https://space.bilibili.com/373951238" target="_blank" rel="noopener">https://space.bilibili.com/373951238</a></li>
<li><a href="https://space.bilibili.com/303667813/video" target="_blank" rel="noopener">https://space.bilibili.com/303667813/video</a></li>
</ul>
</li>
</ul>
<h2 id="CV"><a href="#CV" class="headerlink" title="CV"></a>CV</h2><ul>
<li>CS231n计算机视觉：<a href="https://www.bilibili.com/video/av77752864/" target="_blank" rel="noopener">https://www.bilibili.com/video/av77752864/</a></li>
</ul>
<h3 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h3><ul>
<li>David Silver强化学习：<a href="https://space.bilibili.com/74997410?spm_id_from=333.788.b_765f7570696e666f.2" target="_blank" rel="noopener">https://space.bilibili.com/74997410?spm_id_from=333.788.b_765f7570696e666f.2</a></li>
</ul>
<h2 id="博客类资源"><a href="#博客类资源" class="headerlink" title="博客类资源"></a>博客类资源</h2><ul>
<li>科学空间：<a href="https://kexue.fm/" target="_blank" rel="noopener">https://kexue.fm/</a></li>
</ul>
<h2 id="学习笔记类资源"><a href="#学习笔记类资源" class="headerlink" title="学习笔记类资源"></a>学习笔记类资源</h2><h3 id="机器学习-1"><a href="#机器学习-1" class="headerlink" title="机器学习"></a>机器学习</h3><ul>
<li>吴恩达机器学习笔记：<a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes" target="_blank" rel="noopener">https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes</a></li>
<li>统计学习基础笔记：<a href="https://github.com/SmirkCao/Lihang" target="_blank" rel="noopener">https://github.com/SmirkCao/Lihang</a></li>
<li>百面机器学习：<a href="https://github.com/Relph1119/QuestForMachineLearning-Camp" target="_blank" rel="noopener">https://github.com/Relph1119/QuestForMachineLearning-Camp</a></li>
</ul>
<h3 id="深度学习-1"><a href="#深度学习-1" class="headerlink" title="深度学习"></a>深度学习</h3><ul>
<li>花书笔记：<a href="https://discoverml.github.io/simplified-deeplearning/" target="_blank" rel="noopener">https://discoverml.github.io/simplified-deeplearning/</a><ul>
<li>中文版图书：<a href="https://github.com/exacity/deeplearningbook-chinese" target="_blank" rel="noopener">https://github.com/exacity/deeplearningbook-chinese</a></li>
</ul>
</li>
</ul>
<h3 id="NLP-1"><a href="#NLP-1" class="headerlink" title="NLP"></a>NLP</h3><ul>
<li>斯坦福自然语言处理：<a href="https://zhuanlan.zhihu.com/p/59011576" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/59011576</a></li>
<li>宗成庆统计自然语言处理：<ul>
<li><a href="https://github.com/aicourse/ZMC301-CAS-NLP-2019" target="_blank" rel="noopener">https://github.com/aicourse/ZMC301-CAS-NLP-2019</a></li>
<li><a href="http://www.nlpr.ia.ac.cn/cip/ZongReportandLecture/ReportandLectureIndex.htm" target="_blank" rel="noopener">http://www.nlpr.ia.ac.cn/cip/ZongReportandLecture/ReportandLectureIndex.htm</a></li>
</ul>
</li>
</ul>
<h3 id="CV-1"><a href="#CV-1" class="headerlink" title="CV"></a>CV</h3><ul>
<li>CS231n计算机视觉：<ul>
<li><a href="https://zhuanlan.zhihu.com/p/21930884" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21930884</a></li>
<li><a href="https://github.com/mbadry1/CS231n-2017-Summary" target="_blank" rel="noopener">https://github.com/mbadry1/CS231n-2017-Summary</a></li>
</ul>
</li>
</ul>
<h3 id="强化学习-1"><a href="#强化学习-1" class="headerlink" title="强化学习"></a>强化学习</h3><ul>
<li>David Silver强化学习笔记：<a href="https://zhuanlan.zhihu.com/reinforce" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/reinforce</a></li>
</ul>
<h2 id="工具汇总"><a href="#工具汇总" class="headerlink" title="工具汇总"></a>工具汇总</h2><h3 id="NLP-2"><a href="#NLP-2" class="headerlink" title="NLP"></a>NLP</h3><ul>
<li>jialu: <a href="https://github.com/ownthink/Jiagu" target="_blank" rel="noopener">https://github.com/ownthink/Jiagu</a></li>
</ul>
<h2 id="数据集汇总"><a href="#数据集汇总" class="headerlink" title="数据集汇总"></a>数据集汇总</h2><h3 id="NLP-3"><a href="#NLP-3" class="headerlink" title="NLP"></a>NLP</h3><ul>
<li>百度：<a href="http://ai.baidu.com/broad" target="_blank" rel="noopener">http://ai.baidu.com/broad</a></li>
</ul>
<h2 id="前沿追踪类资源"><a href="#前沿追踪类资源" class="headerlink" title="前沿追踪类资源"></a>前沿追踪类资源</h2><h3 id="NLP-4"><a href="#NLP-4" class="headerlink" title="NLP"></a>NLP</h3><ul>
<li><a href="https://github.com/sebastianruder/NLP-progress" target="_blank" rel="noopener">https://github.com/sebastianruder/NLP-progress</a></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>用GitHub+Hexo搭建个人网站</title>
    <url>/2020/02/01/%E7%94%A8GitHub-Hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/</url>
    <content><![CDATA[<p>假期宅在家里，研究了一下用github搭建个人网站，把里面使用到的工具和命令总结一下。相关代码可参考：<a href="https://github.com/majing2019/myblogs" target="_blank" rel="noopener">https://github.com/majing2019/myblogs</a><br><a id="more"></a></p>
<h1 id="安装相关软件"><a href="#安装相关软件" class="headerlink" title="安装相关软件"></a>安装相关软件</h1><blockquote>
<p>参考：<br><a href="https://zhuanlan.zhihu.com/p/26625249" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26625249</a><br><a href="https://zhuanlan.zhihu.com/p/62555815" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/62555815</a></p>
</blockquote>
<ul>
<li>npm install -g hexo-cli</li>
<li>hexo init blog</li>
<li>cd ~/blog</li>
<li>export CC=/usr/bin/clang</li>
<li>export CXX=/usr/bin/clang++</li>
<li>npm install</li>
<li>npm install hexo-server —save</li>
<li>hexo server</li>
<li>在<a href="http://localhost:4000" target="_blank" rel="noopener">http://localhost:4000</a>访问网站首页</li>
<li>npm install hexo-deployer-git —save</li>
</ul>
<h1 id="建立repository"><a href="#建立repository" class="headerlink" title="建立repository"></a>建立repository</h1><blockquote>
<p>参考：<br><a href="https://help.github.com/en/github/working-with-github-pages" target="_blank" rel="noopener">https://help.github.com/en/github/working-with-github-pages</a><br><a href="https://github.community/t5/GitHub-Pages/404-Error/td-p/14331" target="_blank" rel="noopener">https://github.community/t5/GitHub-Pages/404-Error/td-p/14331</a></p>
</blockquote>
<ul>
<li>创建username.github.io的repository</li>
<li>在Settings-&gt;Github Pages中升级账户</li>
</ul>
<h1 id="修改相关配置"><a href="#修改相关配置" class="headerlink" title="修改相关配置"></a>修改相关配置</h1><ul>
<li>修改_config.yml<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: &lt;repository url&gt; #git@github.com:sufaith&#x2F;sufaith.github.io.git</span><br><span class="line">  branch: [branch] #master</span><br><span class="line">  message: [message]</span><br><span class="line">url: majing2019.github.io</span><br></pre></td></tr></table></figure></li>
<li>在source文件夹下创建CNAME文件，内容为二级域名</li>
<li>在~/blog目录下运行hexo generate</li>
<li>hexo clean &amp;&amp; hexo deploy</li>
<li>访问 <a href="https://majing2019.github.io/archives/" target="_blank" rel="noopener">https://majing2019.github.io/archives/</a></li>
</ul>
<h1 id="绑定域名"><a href="#绑定域名" class="headerlink" title="绑定域名"></a>绑定域名</h1><ul>
<li>登录<a href="https://www.aliyun.com/" target="_blank" rel="noopener">https://www.aliyun.com/</a>注册了一个域名majsunflower.cn</li>
<li>添加一个域名解析<ul>
<li>类型CNAME，主机记录www，记录值majing2019.github.io</li>
<li>类型A，主机记录@，记录值是对应的ip地址，可通过ping majing2019.github.io获得</li>
</ul>
</li>
<li>在github仓库中设置custom domain</li>
<li>在blog下创建source/CNAME文件，并写入majsunflower.cn</li>
</ul>
<h1 id="编写自己的个性化网站"><a href="#编写自己的个性化网站" class="headerlink" title="编写自己的个性化网站"></a>编写自己的个性化网站</h1><blockquote>
<p>参考：<br><a href="https://zhwangart.github.io/2018/11/30/Ocean/" target="_blank" rel="noopener">https://zhwangart.github.io/2018/11/30/Ocean/</a></p>
</blockquote>
<h2 id="更换主题"><a href="#更换主题" class="headerlink" title="更换主题"></a>更换主题</h2><ul>
<li>在<a href="https://hexo.io/themes/" target="_blank" rel="noopener">https://hexo.io/themes/</a>中选择一个主题</li>
<li>git clone <a href="https://github.com/zhwangart/hexo-theme-ocean.git" target="_blank" rel="noopener">https://github.com/zhwangart/hexo-theme-ocean.git</a> themes/ocean</li>
<li>修改_config.yml中theme为ocean</li>
</ul>
<h2 id="配置语言"><a href="#配置语言" class="headerlink" title="配置语言"></a>配置语言</h2><ul>
<li>_config.yml中language改为zh-CN</li>
</ul>
<h2 id="评论功能"><a href="#评论功能" class="headerlink" title="评论功能"></a>评论功能</h2><blockquote>
<p>参考：<br><a href="https://zhwangart.github.io/2018/12/06/Gitalk/" target="_blank" rel="noopener">https://zhwangart.github.io/2018/12/06/Gitalk/</a></p>
</blockquote>
<ul>
<li>在<a href="https://github.com/settings/applications/new" target="_blank" rel="noopener">https://github.com/settings/applications/new</a>申请<ul>
<li>后续可在<a href="https://github.com/settings/developers" target="_blank" rel="noopener">https://github.com/settings/developers</a>中修改app相关内容</li>
<li>注意Authorization callback URL在网站绑定域名后需要写域名</li>
</ul>
</li>
<li>填写themes/ocean/_config.yml中gitalk相关字段</li>
</ul>
<h2 id="使用图床"><a href="#使用图床" class="headerlink" title="使用图床"></a>使用图床</h2><blockquote>
<p>参考：<br><a href="https://zhuanlan.zhihu.com/p/26625249" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26625249</a><br><a href="https://blog.csdn.net/qq_36305327/article/details/71578290" target="_blank" rel="noopener">https://blog.csdn.net/qq_36305327/article/details/71578290</a></p>
</blockquote>
<ul>
<li><p>到<a href="https://www.qiniu.com/" target="_blank" rel="noopener">https://www.qiniu.com/</a>上添加对象存储<a href="https://portal.qiniu.com/kodo/bucket/" target="_blank" rel="noopener">https://portal.qiniu.com/kodo/bucket/</a></p>
</li>
<li><p>在markdown中可直接引用图片</p>
</li>
</ul>
<h2 id="添加关于"><a href="#添加关于" class="headerlink" title="添加关于"></a>添加关于</h2><ul>
<li>hexo new page about</li>
<li>使用markdown编写source/about/index.md</li>
</ul>
<h2 id="添加标签"><a href="#添加标签" class="headerlink" title="添加标签"></a>添加标签</h2><ul>
<li>hexo new page tags // 创建标签页面</li>
<li>修改source/tags/index.md为<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: Tags</span><br><span class="line">date: 2019-04-19 17:28:54</span><br><span class="line">type: tags</span><br><span class="line">layout: &quot;tags&quot;</span><br><span class="line">---</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="添加相册"><a href="#添加相册" class="headerlink" title="添加相册"></a>添加相册</h2><ul>
<li>hexo new page gallery</li>
<li>编辑source/gallery/index.md<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: Gallery</span><br><span class="line">albums: [</span><br><span class="line">        [&quot;img_url&quot;,&quot;img_caption&quot;],</span><br><span class="line">        [&quot;img_url&quot;,&quot;img_caption&quot;]</span><br><span class="line">        ]</span><br><span class="line">---</span><br></pre></td></tr></table></figure></li>
<li>如果出现相册加载过慢的问题，可以参考<a href="https://zhwangart.github.io/2019/07/02/Ocean-Issues/" target="_blank" rel="noopener">https://zhwangart.github.io/2019/07/02/Ocean-Issues/</a>解决</li>
</ul>
<h2 id="添加分类"><a href="#添加分类" class="headerlink" title="添加分类"></a>添加分类</h2><ul>
<li>hexo new page categories</li>
</ul>
<h2 id="本地搜索"><a href="#本地搜索" class="headerlink" title="本地搜索"></a>本地搜索</h2><blockquote>
<p>参考：<br><a href="https://github.com/zhwangart/gitalk/issues/7#issuecomment-451877736" target="_blank" rel="noopener">https://github.com/zhwangart/gitalk/issues/7#issuecomment-451877736</a></p>
</blockquote>
<ul>
<li>npm install hexo-generator-searchdb —save</li>
<li>在blog/_config.yml中添加配置<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">search:</span><br><span class="line">  path: search.xml</span><br><span class="line">  field: post</span><br><span class="line">  content: true</span><br></pre></td></tr></table></figure></li>
<li>hexo g</li>
<li><del>修改themes/ocean/layout/_partial/after-footer.ejs中修改如下内容</del><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;% if (theme.local_search.enable)&#123; %&gt;</span><br><span class="line">  &lt;%- js(&#39;&#x2F;js&#x2F;search&#39;) %&gt;</span><br><span class="line">&lt;% &#125; %&gt;</span><br><span class="line"></span><br><span class="line">&lt;%- js(&#39;&#x2F;js&#x2F;ocean&#39;) %&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="用Markdown写文章"><a href="#用Markdown写文章" class="headerlink" title="用Markdown写文章"></a>用Markdown写文章</h2><h3 id="创建文章"><a href="#创建文章" class="headerlink" title="创建文章"></a>创建文章</h3><blockquote>
<p>参考：<br><a href="https://chaxiaoniu.oschina.io/2017/07/10/Markdown-Grammar/" target="_blank" rel="noopener">https://chaxiaoniu.oschina.io/2017/07/10/Markdown-Grammar/</a></p>
</blockquote>
<ul>
<li>hexo new “用GitHub+Hexo搭建个人网站”</li>
<li>文章格式如下<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: 用GitHub+Hexo搭建个人网站 #文章页面上的显示名称，可以任意修改</span><br><span class="line">date: date  #文章生成时间，一般不改，当然也可以任意修改</span><br><span class="line">tags: [Hexo, Ocean] #文章标签，可空。也可以按照你的习惯写分类名字，注意后面有空格，多个标签可以用[]包含，以&#96;,&#96;隔开</span><br><span class="line">categories: [技术] #分类</span><br><span class="line">---</span><br><span class="line">这里是你博客列表显示的摘要文字</span><br><span class="line">&lt;!--more--&gt;</span><br><span class="line">以下是博客的正文，以上面的格式为分隔线</span><br></pre></td></tr></table></figure></li>
<li>如果不希望显示时有目录，需要添加<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">toc: false</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="添加公式"><a href="#添加公式" class="headerlink" title="添加公式"></a>添加公式</h3><blockquote>
<p>参考：<br><a href="https://blog.csdn.net/Aoman_Hao/article/details/81381507" target="_blank" rel="noopener">https://blog.csdn.net/Aoman_Hao/article/details/81381507</a></p>
</blockquote>
<ul>
<li>npm uninstall hexo-renderer-marked —save</li>
<li>npm install hexo-renderer-kramed —save</li>
<li>修改node_modules/hexo-renderer-kramed/lib/renderer.js<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">function formatText(text) &#123;</span><br><span class="line">  &#x2F;&#x2F; Fit kramed&#39;s rule: $$ + \1 + $$</span><br><span class="line">  &#x2F;&#x2F; return text.replace(&#x2F;&#96;\$(.*?)\$&#96;&#x2F;g, &#39;$$$$$1$$$$&#39;);</span><br><span class="line">  return text;&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>npm uninstall hexo-math —save</p>
</li>
<li><p>npm install hexo-renderer-mathjax —save</p>
</li>
<li>修改node_modules/hexo-renderer-mathjax/mathjax.html，注释掉最后一行script并改为<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;script src&#x3D;&quot;https:&#x2F;&#x2F;cdnjs.cloudflare.com&#x2F;ajax&#x2F;libs&#x2F;mathjax&#x2F;2.7.1&#x2F;MathJax.js?config&#x3D;TeX-MML-AM_CHTML&quot;&gt;&lt;&#x2F;script&gt;</span><br></pre></td></tr></table></figure></li>
<li>修改node_modules/kramed/lib/rules/inline.js<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">escape: &#x2F;^\\([&#96;*\[\]()# +\-.!_&gt;])&#x2F;,</span><br><span class="line">em: &#x2F;^\*((?:\*\*|[\s\S])+?)\*(?!\*)&#x2F;,</span><br></pre></td></tr></table></figure></li>
<li>修改themes/ocean/_config.yml增加<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mathjax: true</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="添加文章封面"><a href="#添加文章封面" class="headerlink" title="添加文章封面"></a>添加文章封面</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: Post name</span><br><span class="line">photos: [</span><br><span class="line">        [&quot;img_url&quot;],</span><br><span class="line">        [&quot;img_url&quot;]</span><br><span class="line">        ]</span><br><span class="line">---</span><br></pre></td></tr></table></figure>
<h3 id="添加视频"><a href="#添加视频" class="headerlink" title="添加视频"></a>添加视频</h3><blockquote>
<p>参考：<br><a href="https://blog.csdn.net/u010953692/article/details/79075884" target="_blank" rel="noopener">https://blog.csdn.net/u010953692/article/details/79075884</a><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;iframe height&#x3D;498 width&#x3D;510 src&#x3D;&quot;http:&#x2F;&#x2F;q503tsu73.bkt.clouddn.com&#x2F;IMG_0018.mp4?e&#x3D;1580557032&amp;token&#x3D;05Ii263bPN3Z-CT3JPRaRfWi5sXIj8pwX6V1bN2j:-rUb7zOxk-WfRrhdJtNdOOGfy58&#x3D;&amp;attname&#x3D;&quot; frameborder&#x3D;0 allowfullscreen&gt;&lt;&#x2F;iframe&gt;</span><br></pre></td></tr></table></figure></p>
</blockquote>
<h3 id="文章置顶"><a href="#文章置顶" class="headerlink" title="文章置顶"></a>文章置顶</h3><ul>
<li>npm uninstall hexo-generator-index —save</li>
<li>npm install hexo-generator-index-pin-top —save</li>
<li>在需要置顶的文章上加入<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line"> title: 新增文章置顶</span><br><span class="line"> top: ture</span><br><span class="line"> ---</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="同时部署在Github和Coding上"><a href="#同时部署在Github和Coding上" class="headerlink" title="同时部署在Github和Coding上"></a>同时部署在Github和Coding上</h1><blockquote>
<p>参考：<br><a href="https://tomatoro.cn/archives/3de92cb5.html" target="_blank" rel="noopener">https://tomatoro.cn/archives/3de92cb5.html</a></p>
</blockquote>
<ul>
<li><a href="https://coding.net/" target="_blank" rel="noopener">https://coding.net/</a>上创建devops项目</li>
<li>修改blog/_config.yml中的deploy<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: </span><br><span class="line">    github: git@github.com:majing2019&#x2F;majing2019.github.io.git</span><br><span class="line">    coding: git@e.coding.net:majsunflower&#x2F;myblog.git</span><br><span class="line">  branch: master</span><br><span class="line">  message: my blog</span><br></pre></td></tr></table></figure></li>
<li>将id_rsa.pub的公钥复制到个人账户下，ssh -T git@git.coding.net验证是否成功</li>
<li>hexo deploy -g部署到coding上</li>
<li>配置静态页面即可访问：<a href="http://02ss3u.coding-pages.com/" target="_blank" rel="noopener">https://02ss3u.coding-pages.com/</a></li>
<li>在自定义域名里增加：<a href="http://majsunflower.cn/">majsunflower.cn</a></li>
<li>阿里云<a href="https://homenew.console.aliyun.com/" target="_blank" rel="noopener">https://homenew.console.aliyun.com/</a>中修改域名相关配置，区分境内和境外的访问</li>
</ul>
<h1 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h1><ul>
<li><p>部署：hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</p>
</li>
<li><p>本地测试：hexo server</p>
</li>
</ul>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Ocean</tag>
      </tags>
  </entry>
</search>

<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Rasa框架解读</title>
    <url>/2020/12/22/Rasa%E6%A1%86%E6%9E%B6%E8%A7%A3%E8%AF%BB/</url>
    <content><![CDATA[<p>Rasa是一个很活跃的开源对话框架，笔者在做语音助手时调研了很多对话框架，个人比较喜欢Rasa，今天就来讲讲它。</p>
<a id="more"></a>
<h1 id="Rasa的组成"><a href="#Rasa的组成" class="headerlink" title="Rasa的组成"></a>Rasa的组成</h1><h2 id="Rasa-NLU"><a href="#Rasa-NLU" class="headerlink" title="Rasa-NLU"></a>Rasa-NLU</h2><p>主要实现自然语言理解（即NLU）功能，本质上就是识别句子的意图和实体。如“买一张去北京的票”，我们可以定义一个意图是“购票”，实体是“北京”和“一张”。</p>
<p>意图识别本质是短文本分类任务（当然在学术界可能称为Intent Detection来和Text Classification分开）。 单纯短文本分类任务的SOTA基本上就是BERT了。</p>
<p>抽取本质是信息抽取任务。 抽取的SOTA现在一般还是BiLSTM-CRF的各种变型，或BERT之类。现在学术界的主要研究方向是多种工作结合，例如同一模型同时做意图识别和信息抽取，互相配合增加总体准确率。</p>
<p>Rasa的NLU，主要是当前的社区版，主要还是使用了各种开源技术，并没有追求学术上的SOTA。 它使用的工具包括Spacy、sklearn-crfsuite</p>
<h2 id="Rasa-Core"><a href="#Rasa-Core" class="headerlink" title="Rasa Core"></a>Rasa Core</h2><p>Rasa的核心部分，NLU有各种实现，开源的也有snips nlu等，但是core却独一无二。</p>
<p>Rasa Core主要完成了基于故事的对话管理，包括解析故事并生成对话系统中的对话管理模型（Dialog Management），输出系统决策（System Action/System Policy）。</p>
<p>学术上一般认为这部分会包含两个模型：</p>
<ul>
<li>对话状态跟踪（Dialog State Tracking / Belief Tracking）</li>
<li>对话策略（Dialog Policy / Policy Optimization）</li>
</ul>
<p>对于1，其实Rasa实现很简单，具体在它的论文 Few-Shot Generalization Across Dialogue Tasks, Vlasov et at., 2018 中说的比较具体。就是简单的基于策略的槽状态替换。</p>
<p>对于2，Rasa使用基于LSTM的Learn to Rank方法，大体上是将当前轮用户意图、上一轮系统行为、当前槽值状态向量化，然后与所有系统行为做相似度学习，以此决定当前轮次的一个或多个系统行为。</p>
<p><img src="https://uploader.shimo.im/f/m5MliMSDjPuV7q1e.png!thumbnail?fileGuid=yhpTXq3xXjJcVQR6" alt="图片"></p>
<h2 id="Rasa-X"><a href="#Rasa-X" class="headerlink" title="Rasa-X"></a>Rasa-X</h2><p>Rasa的可视化编辑工具，更方便NLU、NLG数据的管理，故事的编写。Rasa X可能暂时还不能让所有非开发人员也能快速方便的使用。不过它本质上可以方便开发人员快速开发，快速训练模型验证。</p>
<h1 id="Rasa的PipeLine"><a href="#Rasa的PipeLine" class="headerlink" title="Rasa的PipeLine"></a>Rasa的PipeLine</h1><p><img src="https://uploader.shimo.im/f/Gml5qyuefePTthMN.png!thumbnail?fileGuid=yhpTXq3xXjJcVQR6" alt="图片"></p>
<ul>
<li>用户输入文字，送入解释器，即Rasa NLU</li>
<li>NLU给出结果，如图</li>
</ul>
<p><img src="https://uploader.shimo.im/f/CfVtDjI9fJwiEqeK.png!thumbnail?fileGuid=yhpTXq3xXjJcVQR6" alt="图片"></p>
<ul>
<li>从Tracker到Policy，Tracker用于跟踪对话状态，Tracker输出的是Embedding<ul>
<li>用户意图的Embedding</li>
<li>系统动作（上一步）的Embedding</li>
<li>实体（槽值/Slot）的Embedding</li>
</ul>
</li>
<li>Policy给出系统行为</li>
<li>Tracker记录系统行为，下一次会提供给Policy使用</li>
<li>返回消息给用户</li>
</ul>
<h1 id="Rasa-NLU-1"><a href="#Rasa-NLU-1" class="headerlink" title="Rasa-NLU"></a>Rasa-NLU</h1><h2 id="基本组成"><a href="#基本组成" class="headerlink" title="基本组成"></a>基本组成</h2><h3 id="component"><a href="#component" class="headerlink" title="component"></a>component</h3><p>在我们做任何自然语言处理的任务时，不止是用单纯模型去做一些分类或者标注任务，在此之前，有相当一部分工作是对文本做一些预处理工作，包括但不限于：分词（尤其是中文文本），词性标注，特征提取（传统ML或者统计型方法），词库构建等等。在rasa中，这些不同的预处理工作以及后续的意图分类和实体识别都是通过单独的组件来完成，因此component在NLU中承担着完成NLU不同阶段任务的责任。component类型大致有以下几种：tokenizer、featurizer、extractor、classifier，当然还有emulators，这个主要用于进行对话仿真测试，我目前还没使用过，就不多描述这个组件了。</p>
<h3 id="pipeline"><a href="#pipeline" class="headerlink" title="pipeline"></a>pipeline</h3><p>有了组件之后，如何将组件按部就班，井然有序地拼装起来，并正常工作呢？因此就有了pipeline这个概念，其实在机器学习领域，pipeline这个概念已经存在很长时间了，它在很多框架中都有，比如大名鼎鼎的sklearn。使用pipeline的好处在于可以合理有序管理不同任务阶段的不同组件工具，当组件数量较多时，pipeline的好处就非常明显了。而在rasa中，pipeline的使用更为便捷，是通过yml配置文件实现。即开发者只需要定义好自己的组件，然后将组件配置在配置文件中就可以，即插即用。下图是一个简单的pipeline配置实例：</p>
<p><img src="https://uploader.shimo.im/f/D4AVlWvATEPclpc7.png!thumbnail?fileGuid=yhpTXq3xXjJcVQR6" alt="图片"></p>
<h3 id="message"><a href="#message" class="headerlink" title="message"></a>message</h3><p>在rasa中，用户发送到chatbot的所有对话内容，都需要被封装在一个对象中，这个对象就是Message.而在整个rasa工作流中，存在两个不同的message封装对象，一个是UserMessage，另一个是Message。其中UserMessage是最上层的封装对象，即直接接收用户从某个平台接口传送过来的消息。而Message则是当用户消息流到NLU模块时，将用户消息进行封装。关于UserMessage的内容在后面代码详解时会涉及到，这里先解释一下Message对象。看一下它的类部分定义，其实很简单，就是将用户的对话文本，以及时间进行封装，由于这个Message是贯穿整个NLU工作流的统一数据对象，因此还承载着记忆各个组件临时生成的中间结果（比如分词和词性标注的结果）以及最终得到的意图和实体信息。其中data存放的是意图和实体信息，在后续组件处理时，还会再Message中增加一些变量存储中间结果，即set成员方法的职责。</p>
<p>在Rasa-Core中UserMessage定义如下：</p>
<p><img src="https://uploader.shimo.im/f/nJYXcKewulgGkY3T.png!thumbnail?fileGuid=yhpTXq3xXjJcVQR6" alt="图片"></p>
<p>可以看到作为贯穿整个rasa_core处理流程的用户消息对象，它的成员结构还是比较清晰的，包括了用户发送的文本，定义的OutputChannel类型，用户的id，parse_data(主要存放用户自己定义的实体键值对，开发调试用)，inputChannel类型，以及message的id。</p>
<h2 id="技术细节"><a href="#技术细节" class="headerlink" title="技术细节"></a>技术细节</h2><h3 id="Embedding-方法"><a href="#Embedding-方法" class="headerlink" title="Embedding 方法"></a>Embedding 方法</h3><p>用户意图和系统行为会通过bag-of-word的方法分词，然后向量化，很有趣的结果。在官方论文没有仔细探讨为什么，笔者猜测是为了增加不同的意图、行为之间的语义关联。论文原文：“A bag-of-words representations for the user and the system labels are then created using token counts inside each label.”  例如:action_search_restaurant = {action, search, restaurant} 实体/槽值（Slot）的向量化就非常简单了，只是走了是否存在的binary向量</p>
<h3 id="Learn-to-Rank方法"><a href="#Learn-to-Rank方法" class="headerlink" title="Learn to Rank方法"></a>Learn to Rank方法</h3><p>很多对话系统的系统决策都采用的是分类（Classification）方法，也就是每次总是在多个系统行为中选择唯一一个。</p>
<p>而Rasa选择了排序方法，即判断当前对话状态和系统行为的相似度，这有两个可能的好处：</p>
<ul>
<li>可以更容易实现多个系统行为的同时输出。能让一个对话状态输出多个系统行为是Rasa的特色。至于为什么如此，可能有工程上的一些考虑，例如这样更方便，例如两个系统行为，一个是机器人说“请等待”，一个是真的去查询数据。</li>
<li>更方便扩展系统行为。如果是分类模型，增加一个分类，那必须重新训练整个分类器。如果是Ranking模型，如果只是增加或减少分类，可以考虑只训练新增的系统行为相关的和不相关的部分数据集，可能增加总体的训练速度。更方便快速实验、迭代。</li>
</ul>
<h2 id="给Rasa-NLU添加组件"><a href="#给Rasa-NLU添加组件" class="headerlink" title="给Rasa-NLU添加组件"></a>给Rasa-NLU添加组件</h2><p>以CRFEntityExtractor为例，讲解一下Component的主要核心要素。</p>
<p><img src="https://uploader.shimo.im/f/6kzsP0JMqFIHIDe2.png!thumbnail?fileGuid=yhpTXq3xXjJcVQR6" alt="图片"></p>
<p>首先看到，该类继承了一个EntityExtractor，这是一个二级组件抽象类（我自己定义的说法），这个二层抽象类继承自Component这个一级抽象类。因为不同组件承担的任务不同，有些组件任务比较单一，可以直接继承Component比如tokenizer,classifier，而有些组件的任务比较复杂，则需要制定这一类型的二级接口，方便扩展，如featurizer,extractor。</p>
<p>其次，每个component需要定义一个类变量provides和requires,分别表示这个组件所提供的中间成果和依赖的上游任务。对于CRFEntityExtractor来说，它提供了实体的抽取，同时为了进行实体抽取，需要先对文本进行分词，因此需要上游任务先完成tokenizer任务，提供tokens的中间成果。</p>
<p><img src="https://uploader.shimo.im/f/HdsywcZpNPzSe9ly.png!thumbnail?fileGuid=yhpTXq3xXjJcVQR6" alt="图片"></p>
<p>既然是使用条件随机场来进行实体抽取，那么就需要进行模型训练。因此需要定义train方法，来训练模型。关注train方法的两个参数training_data和config。其中，config就是之前提到的配置pipeline的配置文件的读取对象。training_data是TrainingData类型的对象。你可以将其类比于pytorch中的data_loader功能，它的主要作用是对训练数据进行封装，拆分训练集验证集，做数据校验等工作。说到这里，提一下rasa支持的原始训练数据的存放格式，主要支持markdown，wit，luis等文件格式，当然也可以提供json格式的数据。rasa如何读取这些格式的训练数据则是在training_data中定义。</p>
<p><img src="https://uploader.shimo.im/f/51JQbndhDVvb3Zd3.png!thumbnail?fileGuid=yhpTXq3xXjJcVQR6" alt="图片"></p>
<p>当模型训练完成后，需要保存和加载模型，对生产环境上的实时业务流进行处理，因此需要定义persist和load方法加载模型。</p>
<p><img src="https://uploader.shimo.im/f/RXNVPf8kSYa5csT0.png!thumbnail?fileGuid=yhpTXq3xXjJcVQR6" alt="图片"></p>
<p>process方法，这个可以说是组件里面最重要的一个方法。当前面一通操作之后，只得到了模型，如何调用这个模型并处理文本，就是process方法的工作了。最后在message中增加一个dict，名为entities，用来存放提取的实体信息，包括实体的类型，实体的在文本中的start和end的位置信息等。</p>
<h1 id="Rasa-Core-1"><a href="#Rasa-Core-1" class="headerlink" title="Rasa-Core"></a>Rasa-Core</h1><p>Rasa-Core的基本流程图如下：</p>
<p><img src="https://uploader.shimo.im/f/r5s6gqbcJHTpKryp.png!thumbnail?fileGuid=yhpTXq3xXjJcVQR6" alt="图片"></p>
<h2 id="基本组成-1"><a href="#基本组成-1" class="headerlink" title="基本组成"></a>基本组成</h2><h3 id="actions"><a href="#actions" class="headerlink" title="actions"></a>actions</h3><p>该包下面主要存放的是action具体的实现类。关于action的具体定义和描述在后面会有详细讲解，简单说就是chatbot执行的一些动作。Event对象是rasa中定义的chatbot能执行的最小粒度的动作。而Action则是比event更高层次的对象，会根据用户发送过来的消息，执行一些操作，这些操作可以是自定义的一些逻辑，也可以是系统预置的events。rasa中，action可以分为三大类。</p>
<h4 id="utterance-actions"><a href="#utterance-actions" class="headerlink" title="utterance actions"></a>utterance actions</h4><p>直接发送文本给用户，action文本模板是在domain.yml中进行定义。</p>
<h4 id="custom-actions"><a href="#custom-actions" class="headerlink" title="custom actions"></a>custom actions</h4><p>自定义action，由开发者自定义功能的action。个人认为这个是功能最强大的action，因为开发自由度很大，支持使用任何开发语言进行开发。最后只需要将其打包成一个restful服务接口暴露出来即可。因此这种action是可以和对话主系统分离部署的。下面给出自定义action server与bot agent和用户的交互流程图：</p>
<p><img src="https://uploader.shimo.im/f/Pfft9UET4KizYMJN.png!thumbnail?fileGuid=yhpTXq3xXjJcVQR6" alt="图片"></p>
<p>Rasa action支持node.js, .NET, java等开发语言，当然也支持Python。但是对于Python来说，需要安装rasa-sdk工具包。这个工具包里预置了一些有用的action模板，例如form action。</p>
<p>当然，form action以及其他预置的action模板只能实现最简单的场景，如果要实现复杂的场景，需要根据不同场景，自定义action，可以选择继承这些模板，在上面进行功能的添加和完善。</p>
<h4 id="default-action"><a href="#default-action" class="headerlink" title="default action"></a>default action</h4><p>Rasa系统内置的粒度较小的action。与rasa_sdk中的action不同，这个是直接在rasa_core/actions下面的。相对于上面的form action来说，这里的action功能更单一，与events比较像，但是还是略有不同，下面举个实例ActionRestart：</p>
<p><img src="https://uploader.shimo.im/f/D4HXcEwS8uITGsco.png!thumbnail?fileGuid=yhpTXq3xXjJcVQR6" alt="图片"></p>
<p>可以看到它使用了一个Restarted()的event，这个event的功能是重启整个对话流程，重置对话状态。除此之外，该action需要先执行读取话术模板组装bot message，并将其发送给用户后，才去重启整个会话。</p>
<h3 id="channels"><a href="#channels" class="headerlink" title="channels"></a>channels</h3><p>该包下面主要存放的是rasa与前端平台进行对接的接口。因为rasa本身只提供对话系统的功能服务，具体还需要与用户在前端界面进行交互，这个包里定义了不同的接口和不同平台进行对接。例如，console.py，定义了最简单的直接在shell命令行中进行对话交互的接口。</p>
<p>在流程图中的OutputChannel封装了chatbot需要返回给用户的信息，需要注意，chatbot返回的消息不一定是纯文本，还可能是html，json，文件附件等等，因此需要OutputChannel这个统一接口进行封装处理，因此chatbot可以支持让用户进行点选功能（当然，前提是前端界面支持点选的适配），关于如何实现用户点选功能后续会单独开一个功能小讲。</p>
<p>在流程图中的InputChannel主要负责将用户输入连同用户的身份信息封装成UserMessage对象，方便后面的Processor处理。对应的，如果在上一轮对话中，OutputChannel是点选或者其他非单纯文本输出，那么本轮对话中的InputChannel也需要接受用户点选或者其他非单纯文本的输入，封装成最终的UserMessage。</p>
<h3 id="events"><a href="#events" class="headerlink" title="events"></a>events</h3><p>这个是rasa中定义的chatbot能执行的最小粒度的动作。与action有一些关系，我们可以通过action调用不同的events来实现不同的操作。events的实例有“SlotSet”(槽位填充)，”Restarted”（重启对话，将所有状态重置）等等。</p>
<h3 id="nlg"><a href="#nlg" class="headerlink" title="nlg"></a>nlg</h3><p>rasa的response生成模块，即生产chatbot返回给用户的消息。目前，rasa支持通过模板生成话术，也支持通过machine learning的方式做NLG。nlg模块中定义了方法读取domain.yml中的预定义的话术模板，然后生成具体的消息。</p>
<h3 id="policies"><a href="#policies" class="headerlink" title="policies"></a>policies</h3><p>此模块是rasa_core最上层的对话管理控制模块。该包中，定义了不同类型的对话管理策略，rasa将依据这些策略，执行不同actions，完成多轮对话任务。这些策略包括人工规则策略如form_policy、memoization等，也包括通过机器学习、深度学习进行训练得到策略模型，如sklearn_policy、keras_policy等。</p>
<p>对话管理策略是多轮对话系统的核心功能，相当于对话系统的大脑，它负责根据当前用户的反馈，告诉Processor当前轮对话中需要采取的后续action，以及如何更新对话状态信息等。rasa支持人工规则的策略，也支持机器学习、深度学习得到的数据驱动策略。</p>
<p>以Form_policy为例，这个策略是一种表单策略，对应的rasa预置了一种类型的action，叫form的action。这种action会将所有槽位作为表单的属性column，每一轮对话，都会去主动询问用户，引导用户将这些表单的属性填充，直到所有属性填充完成。而form_policy的核心就是检索当前是否配置了form类型的action，如果是，则将下一步的action置为form。有关action的描述将在后面详细给出。可以看出这是一个典型的人工规则策略。</p>
<p>在一次对话任务中，可以使用多个policy的组合来帮助bot完成既定的任务。比如策略A是一个使用深度学习训练得到的一个策略模型，但是一般使用data-driven得到的模型不会达到100%的准确率，总会有bad case的情况，此时如果只是用该策略，那么会话极有可能会陷入到bad case中，因此需要一个兜底的策略在策略A的bad case发生时，让对话能够平稳进行下去。rasa就预置了这样一个策略，叫fall_back，将fall_back与策略A进行组合，就能得到一个更加鲁棒的一个对话策略。</p>
<p>在实际项目生产中，如果在项目初期，领域数据比较少的情况下，通常会选择form policy或者其他规则型策略。当产品上线，在积累到一定的数据后，可以使用一些data-driven的模型来做策略。</p>
<h3 id="schemas-domain"><a href="#schemas-domain" class="headerlink" title="schemas/domain"></a>schemas/domain</h3><p>这里主要放置rasa_core的配置文件domain.yml，这个配置文件主要配置槽位定义，实体定义，话术模板，使用的actions的名称定义以及其他系统配置。开发者在开发自己的对话系统时，需要自定义这个配置文件来覆盖源码中预定义的配置。</p>
<p>Domain对象的数据来自于前述章节提到的配置文件domain.yml。该对象定义不同的方法，从配置文件domain.yml读取槽位模板，话术模板，定义的action名称，自定义的policy名称等信息，并封装到domain对象中。domain对象可以在action执行时为其提供槽位信息以及话术模板等字段。</p>
<p>设计domain的好处在哪儿呢？个人认为主要是方便管理对话系统需要使用的模板信息。这里的模板信息包含定义的槽位，意图、实体、话术模板、自定义action、自定义policy。如果需要添加或者修改这些信息，只需要修改domain.yml里面的信息就可以了，不需要去修改任何代码，让配置和代码解耦。</p>
<h3 id="agent"><a href="#agent" class="headerlink" title="agent"></a>agent</h3><p>这是rasa_core专门设计的一个接口，可以将其视作bot主体，主要作用是封装和调用rasa中最重要的一些功能方法，包括上述提到的几个包里的功能模块。</p>
<h3 id="training"><a href="#training" class="headerlink" title="training"></a>training</h3><p>这里主要存放的是如何将准备的数据转化为对话系统可训练的转化方法以及可视化方法。</p>
<h3 id="interpreter"><a href="#interpreter" class="headerlink" title="interpreter"></a>interpreter</h3><p>这个方法是rasa_core与rasa_nlu的一个纽带，rasa管理模块通过定义interpreter类方法，调用rasa_nlu中的parser方法来对用户的发送到bot的消息文本进行实体抽取、意图识别等操作。</p>
<h3 id="processor"><a href="#processor" class="headerlink" title="processor"></a>processor</h3><p>定义了MessageProcess类，供agent调用，功能是有序得调用不同对话功能组件，例如调用interpreter解析用户文本、调用本轮对话的action完成一些操作、根据policy得到下一步的action、记录对话状态等。</p>
<p>Processor是对话系统的核心处理模块。它通过execute_action完成bot处理对话的流程。这里需要注意一点，在processor执行action之前，agent将会调用processor的log_message方法，使用nlu_interpreter来对用户发送的文本做实体识别和意图识别，然后将信息保存在tracker中。execute_action方法核心内容如下：</p>
<p><img src="https://uploader.shimo.im/f/NtaZ2VxnkfLIjD0v.png!thumbnail?fileGuid=yhpTXq3xXjJcVQR6" alt="图片"></p>
<h3 id="trackers"><a href="#trackers" class="headerlink" title="trackers"></a>trackers</h3><p>这个也是rasa中比较重要的一个对象，它的作用是rasa对话系统中的状态记录器，每一轮对话中，对话的状态信息都会进行更新并保存在这个对象中。例如当前已填充的槽位、用户最后一次发送的文本、当前用户的意图等等。</p>
<h4 id="DialogueStateTracker"><a href="#DialogueStateTracker" class="headerlink" title="DialogueStateTracker"></a>DialogueStateTracker</h4><p>从名字上就可以看到这个对象的功能：在多轮对话过程中全程记录对话状态信息。这个对象在开发自己的对话系统时，作用可是非常大的。很多对话状态信息，都可以从它这里得到。当然， 我们并不能直接去读写其定义的成员变量信息，需要通过其成员方法来操作成员变量，例如current_sate()，其核心内容如下：</p>
<p><img src="https://uploader.shimo.im/f/1MLXml1bdOxbxLOH.png!thumbnail?fileGuid=yhpTXq3xXjJcVQR6" alt="图片"></p>
<p>注意该方法的返回对象是一个字典，其包含了丰富的对话信息，例如用户的id、当前所有的槽位键值对（包括已填充和未被填充的）、用户最近一次发送的消息等等。</p>
<h1 id="其他参考"><a href="#其他参考" class="headerlink" title="其他参考"></a>其他参考</h1><ul>
<li><a href="https://wechaty.js.org/" target="_blank" rel="noopener">wechaty</a></li>
<li><a href="https://github.com/paschmann/rasa-ui" target="_blank" rel="noopener">rasa-ui</a></li>
<li><a href="https://juejin.cn/post/6844903921060839431" target="_blank" rel="noopener">Rasa 聊天机器人框架开发使用</a></li>
</ul>
]]></content>
      <tags>
        <tag>对话系统</tag>
      </tags>
  </entry>
  <entry>
    <title>CRF学习——基础学习</title>
    <url>/2020/05/17/CRF%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>CRF是NLP中很常用且经典的模型，今天就复习一下CRF模型。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://arxiv.org/abs/1011.4088" target="_blank" rel="noopener">https://arxiv.org/abs/1011.4088</a><br><a href="http://cseweb.ucsd.edu/~elkan/250Bwinter2012/loglinearCRFs.pdf" target="_blank" rel="noopener">http://cseweb.ucsd.edu/~elkan/250Bwinter2012/loglinearCRFs.pdf</a></p>
</blockquote>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><h2 id="生成模型-VS-判别模型"><a href="#生成模型-VS-判别模型" class="headerlink" title="生成模型 VS 判别模型"></a>生成模型 VS 判别模型</h2><p>下面这张图展示了生成模型和判别模型的联系和区别：这两种模型都是用极大似然估计来估计概率，只是两个估计的概率公式不同，生成模型是$p(x, y)$，判别模型是$p(y|x)$。为什么生成模型能生成数据呢，因为$p(x, y) = p(y) * p(x|y)$，我们可以学习到$p(x|y)$，那么当我们采样y时，就可以根据它生成x了。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/1.png" alt="图片"></p>
<p>生成模型可以用于生成，也可以用于判别。但判别模型只能用于判别。生成模型需要学习到更多有关数据的细节。比如有很多猫和狗的图片，对于生成模型，它就会学到很多猫和狗的长什么样的细节（因为学不到的话它也生成不出来）；但对于判别模型，主要学习到猫和狗的区别，如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/2.png" alt="图片"></p>
<p>生成模型的目标函数是P(x, y)，而判别函数是P(y|x)。转化$P(x,y)=P(y)P(x|y)$，也就是生成模型学习到的一个是$P(x|y)$，比如对于y=猫来说，能学习到一个x的分布（即猫的特征分布，有时这个分布用高斯分布去模拟），如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/3.png" alt="图片"></p>
<p>那怎么生成呢？事实上我们可以从$P(x|y=猫)$这个分布中去采样一个x（比如对于图片来说，x就是像素矩阵），而这个x就是一个猫的图片了。下面再以预测男、女身高为例，做一个生成模型，过程如下图（当需要生成时，直接从正太分布x~$N(\mu, \sigma^2$)中采样即可）：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/4.png" alt="图片"></p>
<p>那么在判别问题上，哪个模型效果更好呢？从生成视角看，生成式建模的是$P(x,y)=P(y)<em>P(x|y)$，从判别视角看，生成式建模的是$P(x,y)=P(x)</em>P(y|x)$。而判别式建模的就是$P(y|x)$。也就是两类模型都去做了判别$P(y|x)$，而生成模型同时学习了$P(x)$（可以理解成一个prior）。从直观上感觉，只做一件事情的判别模型会更好。因为生成模型要同时兼顾$P(x)$和$P(y|x)$，它要找到这两者之间的一个平衡点。因此，我们可以得出以下结论：</p>
<ul>
<li>当数据量大时，使用判别模型会优于生成模型</li>
<li>当数据量小时，生成模型有可能比判别模型要好（因为生成模型有prior，即p(x)，有时为了防止过拟合，会在模型中加入正则项，其实这个正则项就是一个prior）</li>
</ul>
<p>【注】：上面提到的p(x, y)、p(x|y)其实是所有概率的乘积，即<img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/5.png" alt="图片"></p>
<h2 id="Directed-Model-VS-Undirected-Model"><a href="#Directed-Model-VS-Undirected-Model" class="headerlink" title="Directed Model VS Undirected Model"></a>Directed Model VS Undirected Model</h2><p>有明确依赖关系的叫有向图。比如做金融风控，如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/6.png" alt="图片"></p>
<p>没有依赖关系的叫无向图。比如在做Image Segmentation时，需要对每个点进行分类。如果把每个点看成时独立的话，缺少了像素点之间的联系。因此可以把它看成一个graphic model。因为点之间很难有方向的关系，因此使用无向图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/7.png" alt="图片"></p>
<h2 id="Joint-Probability"><a href="#Joint-Probability" class="headerlink" title="Joint Probability"></a>Joint Probability</h2><p>使用联合概率来建模图模型（后续可以用这个联合概率进行分类）。有向图的联合概率很好写：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/8.png" alt="图片"></p>
<p>在生成模型HMM中，其概率公式如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/9.png" alt="图片"></p>
<p>无向图有两种方法建模：</p>
<ul>
<li>Maximal Clique方法：先分成团，每个团用score function计算一个契合度。而这个契合度和团里的元素的联合概率是正相关的，因此这个score function也可以用联合概率表示，如下图：</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/10.png" alt="图片"></p>
<ul>
<li>Pair-wise</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/11.png" alt="图片"></p>
<p>【注】：</p>
<ul>
<li>因为score function得出的是一个相对值，不同feature、不同算法得出的结果不一样。但最后我们是想得到P(1,2,3,4,5)这个联合概率的绝对值的（后续做判别时会用到），因此我们除以了一个归一化项z。这个z要考虑所有可能的情况，在实际情况中我们可以使用维特比来简化计算量。</li>
<li>z是一个global normalization，使用这种全局的归一化有一个好处，就是图中的结点是有一个偏好的，但是z是一个全局的量，可以修正结点的偏好，使得模型考虑得更加全面。</li>
<li>在有向图中使用的是local normalization，比如P(4|2,5)只需要关注P(2,5)，因此有向图是从局部角度考虑的</li>
<li>z的计算量比较大，如果变量是离散型的，则可以使用维特比算法；如果变量是连续型的，则只能使用一些近似算法（partition funciton，比如MCMC）</li>
</ul>
<p>接下来有一个问题，这个score function究竟是怎么计算出来的呢？</p>
<p>首先，我们假设score function计算的是变量之间的compatability。所以，变量之间关系越紧密，则score function结果越大。例如，假设a、b、c是三位学生，他们在一起做项目。则score function可以表示三位学生合作的结果。他们关系越好，合作越紧密，则得分越高。紧接着，我们要考虑这三位学生的一些特征。我们要选择一些特征，能刻画出三位学生是否能合作好。比如三个学生是否来自同一个班级(f1)，以及他们之前是否合作过(f2)，以及他们之间的能力是否互补(f3)等等。我们可以把这些特征做一个线性组合来得到score（linear model）。当然也可以使用非线性的方式构造score function，这个时候就是log-linear crf。下图概括了这个过程：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/12.png" alt="图片"></p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/13.png" alt="图片"></p>
<p>特征工程一般分两种，一种是人工特征，另一种是使用深度学习特征（比如BiLSTM-CRF，卷积核等）。</p>
<h2 id="RoadMap"><a href="#RoadMap" class="headerlink" title="RoadMap"></a>RoadMap</h2><p>我们通过对特征函数进行不同假设，可以得到不同模型：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/14.png" alt="图片"></p>
<p>由上图可以看到，假设特征函数是线性函数时，可以产生Log-Linear Model（顾名思义，就是对score funciton先取linear，再取log）。Log-Linear Model的具体模型常见的有逻辑回归和Linear-Chain CRF。那么从另一个角度，有向图模型HMM对于NLP问题有一些缺陷，我们把它从生成模型改造成一个判别模型MEMM，但MEMM有Label Bias的问题，因此我们又把MEMM改成一个无向图模型，就得到了Linear Chain CRF。对于Linear Chain CRF，我们关心的是它的参数估计和Inference算法。因为它也属于Log-Linear Model，所以我们要先学习的是Log-Linear Model的参数估计和Inference。</p>
<p>当然，如果我们假设特征函数是非线性的，会得到Non-linear CRF。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/15.png" alt="图片"></p>
<p>上面这张图是更简明一些的roadmap。对于生成模型，最简单的是朴素贝叶斯，当数据是一条链时，就是HMM，当数据是一个图的时候，就是贝叶斯网络。对于判别模型，最简单的是逻辑回归，当是一条链是就是CRF，当是一个图时就是更general的CRF。</p>
<h1 id="Log-Linear-Model"><a href="#Log-Linear-Model" class="headerlink" title="Log-Linear Model"></a>Log-Linear Model</h1><p>首先我们来看这样一个无向图模型和它的联合概率（用all cliques方法定义联合概率）：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/16.png" alt="图片"></p>
<p>那么接下来的问题就是，如何去定义这个score function呢？我们用一个指数函数计算score：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/17.png" alt="图片"></p>
<p>上图式子计算了一个clique的分数，其中$F_j(y_c)$是特征函数。比如$y_1$-$y_2$-$y_3$这个无向图中，我们有2个特征，分别就是$F_1(y_1, y_2, y_3)$、$F_2(y_1, y_2, y_3)$。也就是说，每一个clique里可以抽取出J个特征（每个特征都是一个数）。$w_j*F_j(y_c)$就是一个线性函数，整个模型的参数$\theta={w}$。</p>
<p>那么F特征是什么呢？F主要评估了变量之间的契合度。那为什么这个模型叫Log-Linear呢？看下图就知道了：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/18.png" alt="图片"></p>
<p>下面用文本的一个例子来说明计算这个联合概率的过程：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/19.png" alt="图片"></p>
<h2 id="Multinomial-Logistic-Regression"><a href="#Multinomial-Logistic-Regression" class="headerlink" title="Multinomial Logistic Regression"></a>Multinomial Logistic Regression</h2><p>我们可以将逻辑回归看成是x、y的一个无向图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/20.png" alt="图片"></p>
<p>那么逻辑回归的条件概率是什么呢？（这里我们把每个标签作为一个clique）</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/21.png" alt="图片"></p>
<p>其中特征函数f_j定义如下，可以看出来我们并没有做特征工程，而是把输入x_j直接看成是特征：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/22.png" alt="图片"></p>
<p>那么一共有多少特征呢？如果标签y有3个(即y={1,2,3})，x的维度为d，则特征总个数为3d，即J=3d。比如，我们分别令y = 1, 2, 3，则$f_j$的具体值如下：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/23.png" alt="图片"></p>
<p>接着，我们把$f_j$的值带入到$P(y|x;\theta)$中：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/24.png" alt="图片"></p>
<p>可以看出来，参数w的维度是3d。我们把上面式子在形式上进行一些简化：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/25.png" alt="图片"></p>
<p>可以看出来这就是一个多元逻辑回归模型（可以用在多分类问题上）。</p>
<h1 id="HMM存在的一些问题"><a href="#HMM存在的一些问题" class="headerlink" title="HMM存在的一些问题"></a>HMM存在的一些问题</h1><h3 id="MEMM"><a href="#MEMM" class="headerlink" title="MEMM"></a>MEMM</h3><p>假设我们用HMM做词性标注，构建如下模型，会发现有什么问题？</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/26.png" alt="图片"></p>
<p>很显然，对于词性标注而言，词性$z_1$不仅仅只依赖于当前词$x_3$，还可能跟其他词有关。那么怎么改进这个模型呢？为了让$z_1$跟所有词相关，我们把它改造成下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/27.png" alt="图片"></p>
<p>但上图这个模型又有什么问题呢？我们先写一下上图的联合概率：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/28.png" alt="图片"></p>
<p>可以发现上式中第二项，也就是发射概率，是很难表示的。如果说联合概率很难表示，能不能把它改造成一个判别模型呢，用条件概率去表示呢？下图就是改造后的判别模型：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/29.png" alt="图片"></p>
<p>我们可以看出来上图中$P(z_i|z_{i-1}, x)$是很好表示的（比如用一个线性函数+softmax），那么这个模型就是MEMM模型。下面这张图是MEMM的另一种表示：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/30.png" alt="图片"></p>
<p>但是MEMM有什么问题呢？</p>
<h3 id="Label-Bias-Problem"><a href="#Label-Bias-Problem" class="headerlink" title="Label Bias Problem"></a>Label Bias Problem</h3><p>首先，我们来看一看什么叫Label Bias。我们先来看下图，哪一条路径的概率最大呢？</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/31.png" alt="图片"></p>
<p>很明显，分支比较多的状态的概率概率较小（相对而言不对它们不是很公平），分支较少的路径概率更大。举个例子，做词性分析时，动词的后面可能的词性较少，名词后面的词性就比较多。Label bias的根本原因是local normalization。那么怎么解决这个问题呢？</p>
<p>我们可以把概率换成是score来表示，有了分数，我们就可以用global normalization了。这也就是无向图和有向图的主要区别，如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/32.png" alt="图片"></p>
<p>我们可以将(z1, z2)、(z2, z3)、(z3, z4)看成是clique，计算(z1, z2, x)等的feature function (f1, f2, f3, …f10)，再进行加权，就有了score。</p>
<h3 id="从Log-linear-Model到Log-linear-CRF"><a href="#从Log-linear-Model到Log-linear-CRF" class="headerlink" title="从Log-linear Model到Log-linear CRF"></a>从Log-linear Model到Log-linear CRF</h3><p>我们直接从Log-linear的公式进行推导，其实就是把特征函数$f_j(x,y)$表示出来。在无向图中我们把这个特征写成是$F_j(x,y)$（这里的x、y可以看成是一个时间的序列，而不是一个单独的变量）。由于y是一个状态集合，因而(x,y)的特征函数由(y1, y2, x)、(y2, y3, x)、(y3, y4, x)…组成，因此$F_j(x,y)=sum(f_j(y_i, y_{i-1}, x))$。如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/33.png" alt="图片"></p>
<p>$f_j$可以通过人工，或者神经网络的方式得到，接下来就是求解$w_j$。我们的目标是最大化似然函数：<img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/34.png" alt="图片">，因此选择好一个优化方法，就可以求解$w_j$了（比如将loss作为分子对w求偏导）。</p>
<p>【注】：</p>
<ul>
<li>上式中T是团的个数，在实践序列中，就是timestep-1</li>
<li>上式中exp有两个sum，其中第一个sum是对每个clique的结果求和（因为我们要把所有clique的结果乘起来），第二个sum是对每个clique内部所有特征的加和。？好像写反了</li>
<li>$f_j(y_i, y_{i-1}, x)$中的$f_j$在神经网络中常使用softmax</li>
<li>$f_j(y_i, y_{i-1}, x)$中的的x可以理解为x1, x2,…，即特征是根据$y_i, y_{i-1}$和所有的x计算出来的，即$f_j(y_i, y_{i-1}, x_1, x_2, …)$</li>
</ul>
<h1 id="Log-linear-CRF"><a href="#Log-linear-CRF" class="headerlink" title="Log-linear CRF"></a>Log-linear CRF</h1><p>我们的目标是求解：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/35.png" alt="图片"></p>
<p>看上去跟逻辑回归差不多，只不过这里的x、y是时间序列。</p>
<h2 id="Inference-Probelm"><a href="#Inference-Probelm" class="headerlink" title="Inference Probelm"></a>Inference Probelm</h2><p>首先我们来看一下，在x、w已知的情况下，我们怎么求解使得P(y|x;w)最大的y。这看上去和逻辑回归没有区别哈，逻辑回归里y={0,1}，这里的y是一个序列。</p>
<p>我们先把式子按照定义展开：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/36.png" alt="图片"></p>
<p>看到上面的形式，我们应首先想到维特比算法，一想到维特比算法，就要想到路径，路径的sum值是最大的，如下图（n是时间长度，m是状态个数）：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/37.png" alt="图片"></p>
<p>我们定义的子问题是：$u(k, v)$表示从$t=1$到$t=k$，且t时刻的状态为v的最好的路径的分数。我们最终想求的是$max(u(n, 1)$、$u(n, 2)$… $u(n, m))$为最终答案，如下图：<img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/38.png" alt="图片"></p>
<p>那么我们怎么把$u(k, v)$表示成一个子问题呢？从t=1到t=k一定会经过$u(k-1, 1)$或$u(k-1, 2)$…或$u(k-1, m)$，如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/39.png" alt="图片"></p>
<p>我们用式子表达出来是：<img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/40.png" alt="图片"></p>
<p>看上去，我们把这个二维的矩阵里的score值逐步填充进去就可以了。当想求解最佳状态的路径时，只需要设置好pointer，反向就能从矩阵中找到最佳路径了。整个算法的复杂度是O(m<em>n)</em>O(m)。</p>
<p>【注】：如果score函数是$g_i(y_{i-2}, y_{i-1}, y_{i})$，那么算法复杂度就可能变为$O(m^2<em>n)</em>O(m)$</p>
<h2 id="Parameter-Estimation"><a href="#Parameter-Estimation" class="headerlink" title="Parameter Estimation"></a>Parameter Estimation</h2><h3 id="Log-Linear的w求解"><a href="#Log-Linear的w求解" class="headerlink" title="Log-Linear的w求解"></a>Log-Linear的w求解</h3><p>接下来我们来看，在x、y已知的情况下，我们怎么求解w？求导数就好了。我们先来看一下Log-Linear的w是怎么求解的：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/41.png" alt="图片"></p>
<p>上式中最后一项可以看成是$F_j(x, y’)$的期望：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/42.png" alt="图片"></p>
<p>可以看到第一项$F_j(x, y)$是x、y的特征工程，后一项是关于y’的期望。</p>
<h3 id="Log-Linear-CRF的w求解"><a href="#Log-Linear-CRF的w求解" class="headerlink" title="Log-Linear CRF的w求解"></a>Log-Linear CRF的w求解</h3><ul>
<li>求解partition function z<ul>
<li>Forward算法：在CRF中，partition function z(x, w)中的x变成了一个序列，我们来看一下这个z怎么计算。我们依然可以使用动态规划算法求解$sum(g_i(y_{i-1}, y_i))$。具体如下图所示：<img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/43.png" alt="图片"></li>
</ul>
</li>
</ul>
<p>上图中$\alpha(k+1, v)$表示$y_{k+1}=v$的情况下，所有状态的可能性。为了求解$\alpha(k+1, v)$，我们把$y_k$作为一个中介，而$y_k$有1~u种可能。举个栗子，假如有4个状态，3个输出，那么我们最后想求$\alpha(4, 1)+\alpha(4,2)+\alpha(4,3)$的和作为z(x, w)​，且$\alpha(4,1)=\alpha(3,1) + \alpha(3,2) + \alpha(3,3)$，如下图：<img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/44.png" alt="图片"></p>
<ul>
<li><p>Backward算法：跟Forward相似，只不过是$y_k=u$时考虑${y_k{k+1}, y_n}$的所有可能性，这时候$y_{k+1}$成为了中介。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/45.png" alt="图片"><img src="https://uploader.shimo.im/f/wJoM9tAvdSPjIp06.png!thumbnail" alt="图片"></p>
</li>
</ul>
<ul>
<li><p>求解$P(y_k=u|x;w)$</p>
<ul>
<li>$z(x, w)$是考虑所有的可能性，因此可以认为$z(x, w)=p(y_k=1|x;w) + p(y_k=2|x, w) + … + p(y_k=m|x;w)$</li>
<li>根据前向算法和后向算法，可以算出$z(x, w)$</li>
<li>$p(y_k=u|x;w)$表示$y_k=u$的情况下，前向和后向的所有可能情况，如下图：</li>
</ul>
</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/46.png" alt="图片"></p>
<p>为了求出w，我们需要对$p(y|x;w)$求导，</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/47.png" alt="图片"></p>
]]></content>
      <tags>
        <tag>CRF</tag>
      </tags>
  </entry>
  <entry>
    <title>词对词翻译的那些事儿</title>
    <url>/2020/05/17/%E8%AF%8D%E5%AF%B9%E8%AF%8D%E7%BF%BB%E8%AF%91%E7%9A%84%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF/</url>
    <content><![CDATA[<p>当我们有大量平行语料，通过什么方法能快速构建出一个词典呢？今天讲的就是基于这个需求所调研的一些方法总结。基本方法有3种：基于统计、基于词对齐、基于词向量。由于基于词对齐比较好理解，就是使用IBM Model进行词对齐后，将translation table拿出来就可以作为一个词表，因此本文不对此进行介绍。本文重点说明另外两种方法：基于统计和基于词向量方法。</p>
<a id="more"></a>
<h1 id="基于统计的方法"><a href="#基于统计的方法" class="headerlink" title="基于统计的方法"></a>基于统计的方法</h1><blockquote>
<p>参考论文：word2word: A Collection of Bilingual Lexicons for 3,564 Language Pairs<br><a href="https://zhuanlan.zhihu.com/p/45730674" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/45730674</a><br><a href="https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf" target="_blank" rel="noopener">https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf</a><br><a href="https://cloud.tencent.com/developer/article/1436217" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1436217</a></p>
</blockquote>
<p>这篇论文的思想很简单，就是从translation pair共现的角度在平行语料库中挖掘出符合条件的pair。因此，算法首先找到跟每一个source word共现过的target word，其次使用下面三种方法对target word与source word的相关度进行重新排序，最终取前n名作为source word的释义：</p>
<ul>
<li>共现次数：$p(y | x)=\frac{p(x, y)}{p(x)} \approx \frac{c(x, y)}{c(x)} \propto c(x, y)$</li>
<li><p>互信息：$\begin{aligned}<br>\operatorname{PMl}(x, y) &amp;=\log \frac{p(x, y)}{p(x) p(y)} \\<br>&amp; \approx \log \frac{c(x, y)}{c(x) c(y)} \propto \log c(x, y)-\log c(y)<br>\end{aligned}$</p>
</li>
<li><p>Controlled Predictive Effects（CPE）：$\begin{aligned}<br>\operatorname{CPE}(y | x) &amp;=p(y | x)-\sum_{x^{\prime} \in \mathcal{X}} p\left(y | x^{\prime}\right) p\left(x^{\prime} | x\right) \\<br>&amp;=\sum_{x^{\prime} \in \mathcal{X}} \operatorname{CPE}_{y | x}\left(x^{\prime}\right) p\left(x^{\prime} | x\right)<br>\end{aligned}$</p>
</li>
</ul>
<h2 id="互信息"><a href="#互信息" class="headerlink" title="互信息"></a>互信息</h2><p>PMI是NLP中的一个重要指标，在本文中它可以用来阻止一些stop word有较高的得分。互信息是信息论里一种有用的信息度量，它可以看成是一个随机变量中包含的关于另一个随机变量的信息量，或者说是一个随机变量由于已知另一个随机变量而减少的不肯定性。</p>
<p>设两个随机变量$(X,Y)$的联合分布为$p(x, y)$，边缘分布分别为$p(x),p(y)$    ，互信息$I(X;Y)$是联合分布$p(x,y)$与边缘分布$p(x)p(y)$的相对熵，即$I(X ; Y)=\sum_{x \in X} \sum_{y \in Y} p(x, y) \log \frac{p(x, y)}{p(x) p(y)}$</p>
<p>上面的$I(X;Y)$其实是可以推导出来的。根据熵的连锁规则，有$H(X, Y)=H(X)+H(Y | X)=H(Y)+H(X | Y)$。因此，$H(X)-H(X | Y)=H(Y)-H(Y | X)$</p>
<p>这个差叫做X和Y的互信息，记作I(X;Y)。按照熵的定义展开可以得到：$\begin{aligned}<br>I(X ; Y) &amp;=H(X)-H(X | Y) \\<br>&amp;=H(X)+H(Y)-H(X, Y) \\<br>&amp;=\sum_{X} p(x) \log \frac{1}{p(x)}+\sum_{y} p(y) \log \frac{1}{p(y)}-\sum_{x, y} p(x, y) \log \frac{1}{p(x, y)} \\<br>&amp;=\sum_{x, y} p(x, y) \log \frac{p(x, y)}{p(x) p(y)}<br>\end{aligned}$</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/word2word/1.png" alt="图片"></p>
<p>直观上，互信息度量 X 和 Y 共享的信息：它度量知道这两个变量其中一个，对另一个不确定度减少的程度。例如，如果 X 和 Y 相互独立，则知道 X 不对 Y 提供任何信息，反之亦然，所以它们的互信息为零。在另一个极端，如果 X 是 Y 的一个确定性函数，且 Y 也是 X 的一个确定性函数，那么传递的所有信息被 X 和 Y 共享：知道 X 决定 Y 的值，反之亦然。因此，在此情形互信息与 Y（或 X）单独包含的不确定度相同，称作 Y（或 X）的熵。而且，这个互信息与 X 的熵和 Y 的熵相同。</p>
<p>互信息是 X 和 Y 联合分布相对于假定 X 和 Y 独立情况下的联合分布之间的内在依赖性。于是互信息以下面方式度量依赖性：I(X; Y) = 0 当且仅当 X 和 Y 为独立随机变量。从一个方向很容易看出：当 X 和 Y 独立时，$p(x,y) = p(x) p(y)$，因此：$\log \left(\frac{p(x, y)}{p(x) p(y)}\right)=\log 1=0$</p>
<p>可见，互信息是非负的（$I(X;Y) ≥ 0$），且是对称的（即 $I(X;Y) = I(Y;X)$）。</p>
<p>其实从PMI角度，可以将skip-gram word2vec模型看成是对一个shifted PMI matrix的矩阵分解的结果，同时PMI和TF-IDF的原理也十分接近。下面我们介绍一篇论文，来从PMI角度理解word2vec，这篇论文为Neural Word Embedding as Implicit Matrix Factorization，其原文在<a href="https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf" target="_blank" rel="noopener">https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf</a>中。</p>
<p>这篇文章认为，word2vec本质上是对一个word-context相关度的PMI矩阵进行分解。我们可以把word-context看成一个row为word，column为context的矩阵。首先我们定义一些变量：</p>
<ul>
<li>word: $w \in V_{W}$ </li>
<li>context: $c \in V_{C}$<ul>
<li>$w_i$的context为其窗口长度为L的附近的词：$w_{i-L}, \ldots, w_{i-1}, w_{i+1}, \ldots, w_{i+L}$</li>
</ul>
</li>
<li>word vocabulary: $V_w$</li>
<li>context vocabulary: $V_c$</li>
<li>word和context的pair集合：$D$</li>
<li>(w, c)在D中出现的次数：$c(w,c)$</li>
<li>w在D中出现的次数：$c(w)=\sum_{c^{\prime} \in V_{C}} c\left(w, c^{\prime}\right)$</li>
<li>c在D中出现的次数：$c(c)=\sum_{w^{\prime} \in V_{W}} c\left(w^{\prime}, c\right)$</li>
<li>每个词w可以表示成向量：$\vec{w} \in \mathbb{R}^{d}$，有时用$\left|V_{W}\right| \times d$的矩阵W表示，其中$W_i$表示第$i$个word的表示</li>
<li>每个context c可以表示成向量：$\vec{c} \in \mathbb{R}^{d}$，有时用$\left|V_{C}\right| \times d$ 的矩阵C表示，这里的d表示embedding的维度，其中$C_i$表示第$i$个context的表示</li>
<li>(w, c)属于D的概率：$P(D=1 | w, c)$</li>
</ul>
<p>其次，我们要建立目标函数。假设有一个(w, c)对，我们如何知道它是否属于D呢？我们使用sigmoid建模这个概率：$P(D=1 | w, c)=\sigma(\vec{w} \cdot \vec{c})=\frac{1}{1+e^{-\vec{w} \cdot \vec{c}}}$，其中的$\vec{w}$和$\vec{c}$就是我们要学习的参数。</p>
<p>所谓的负采样，就是对于在$D$中的$(w,c)$要最大化$P(D=1|w,c)$，而对于不再$D$中的$(w,c)$要最大化$P(D=0|w,c)$。因此目标函数可以写成：$\log \sigma(\vec{w} \cdot \vec{c})+k \cdot \mathbb{E}_{c_{N} \sim P_{D}}\left[\log \sigma\left(-\vec{w} \cdot \vec{c}_{N}\right)\right]$，其中$k$是负采样的个数，$c_N$是使用均匀分布$P_{D}(c)=\frac{c(c)}{|D|}$随机采样的context。</p>
<p>通常我们会使用SGD求解，全局的目标函数定义为：$\ell=\sum_{w \in V_{W}} \sum_{c \in V_{C}} c(w, c)\left(\log \sigma(\vec{w} \cdot \vec{c})+k \cdot \mathbb{E}_{c_{N} \sim P_{D}}\left[\log \sigma\left(-\vec{w} \cdot \vec{c}_{N}\right)\right]\right)$。这样的建模会使得出现在相同context的word有相近的embedding。</p>
<p>那么这个问题是如何跟矩阵分解联系上的呢？在论文中有详细的推导过程，这里我们直接写出结论。当满足下述条件时，函数$l$达到最值：$w \cdot c=\log \left(\frac{c(w, c) \cdot|D|}{c(w) \cdot c(c)}\right)-\log (k)$。其中$D$是训练文本，$c$是词$w$的上下文，$c(w, c)$ 是$(w,c)$ 在$D$中出现的次数， $c(w)$ 和$c(c)$ 以此类推。$\log \left(\frac{c(w, c) \cdot|D|}{c(w) \cdot c(c)}\right)$ 就是$(w,c)$ 的pointwise mutual information（简称PMI），即$M_{i j}^{S G N S}=w_{i} \cdot c_{j}=P M I\left(w_{i}, c_{j}\right)-\log (k)$</p>
<p>由此，我们找到了矩阵$M$ ，而这个$M$就是我们刚才说的shifted PMI。其他一些词向量表示，也可以由相似的推导过程得出。比如对于noise-contrastive estimation，计算的是(shifted) log-conditional-probability matrix：$M_{i j}^{\mathrm{NCE}}=\vec{w}_{i} \cdot \vec{c}_{j}=\log \left(\frac{c(w, c)}{c(c)}\right)-\log k=\log P(w | c)-\log k$</p>
<p>【题外话】：早期的word2vec也有基于词-词共现矩阵的方法，使用LSA将从corpus中统计的word-word共现矩阵进行分解。共现矩阵里的值有时用PMI替代：$P M I\left(w_{i}, w_{j}\right)=\log \frac{p\left(w_{i}, w_{j}\right)}{p\left(w_{i}\right) p\left(w_{j}\right)}=\log \frac{c\left(w_{i}, w_{j}\right) \cdot|\mathcal{C}|}{c\left(w_{i}\right) \cdot c\left(w_{j}\right)}$。使用SVD对矩阵分解得到：$\mathbf{P}=\mathbf{U} \Psi \mathbf{V}^{\top}$，那么当我们想得到K维的，就可以计算：$\mathbf{X}=\mathbf{U}_{k} \mathbf{\Psi}_{k}$。</p>
<p>另外，<strong>关于句子、词语、句子-词语之间相似性的计算，推荐参考这篇文章</strong>：<a href="https://cloud.tencent.com/developer/article/1436217" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1436217</a></p>
<h2 id="CPE"><a href="#CPE" class="headerlink" title="CPE"></a>CPE</h2><p>如下图所示，对目标端词pomme，我们如何区分出the、apple、juice哪个对于pomme更重要呢？这时就有了CPE排序。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/word2word/2.png" alt="图片"></p>
<p>通过引入修正量$p(y|x’)$、$p(x|x’)$来建模CPE：$\begin{aligned}<br>\operatorname{CPE}(y | x) &amp;=p(y | x)-\sum_{x^{\prime} \in \mathcal{X}} p\left(y | x^{\prime}\right) p\left(x^{\prime} | x\right) \\<br>&amp;=\sum_{x^{\prime} \in \mathcal{X}} \operatorname{CPE}_{y | x}\left(x^{\prime}\right) p\left(x^{\prime} | x\right)<br>\end{aligned}$</p>
<p>其中的x’表示confounder words，对应于上图x=apple，则x’=the 或 juice。上式中CPE项可进一步写成$\mathrm{CPE}_{y | x}\left(x^{\prime}\right)=p\left(y | x, x^{\prime}\right)-p\left(y | x^{\prime}\right)$。CPE可以表达出对于x-&gt;y这个词翻译来说，当我们看到一个comfounder word x’时，会对x-&gt;y的概率产生多大的影响。如果这个值为0，当我们观察到x’时，发现x和y不相关了。</p>
<p>对于一个词x，我们需要计算所有其CPE_{y|x}(x’)的边缘概率，这在实际中计算量非常大。因此在实际计算中，我们只对top 5000的x进行修正。</p>
<h2 id="源码解读"><a href="#源码解读" class="headerlink" title="源码解读"></a>源码解读</h2><h3 id="get-trans-co"><a href="#get-trans-co" class="headerlink" title="get_trans_co"></a>get_trans_co</h3><p>get_trans_co用于获取x-&gt;y共现最多的pair，传入参数x2ys已计算好x-&gt;y的共现次数。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def get_trans_co(x2ys, n_trans):</span><br><span class="line">    x2ys_co &#x3D; dict()</span><br><span class="line">    for x, ys in x2ys.items():</span><br><span class="line">        ys &#x3D; [y for y, cnt in sorted(</span><br><span class="line">            ys.items(),</span><br><span class="line">            key&#x3D;operator.itemgetter(1),</span><br><span class="line">            reverse&#x3D;True)[:n_trans]]</span><br><span class="line">        x2ys_co[x] &#x3D; ys</span><br><span class="line">    return x2ys_co</span><br></pre></td></tr></table></figure>
<h3 id="get-trans-pmi"><a href="#get-trans-pmi" class="headerlink" title="get_trans_pmi"></a>get_trans_pmi</h3><p>get_trans_pmi用于对x的所有translation y进行pmi排序。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def get_trans_pmi(x2ys, x2cnt, y2cnt, Nxy, Nx, Ny, width, n_trans):</span><br><span class="line">    x2ys_pmi &#x3D; dict()</span><br><span class="line">    pmi_diff &#x3D; -np.log2(Nxy) + np.log2(Nx) + np.log2(Ny)</span><br><span class="line">    for x, ys in tqdm(x2ys.items()):</span><br><span class="line">        l_scores &#x3D; []</span><br><span class="line">        for y, cnt in sorted(ys.items(), key&#x3D;operator.itemgetter(1),</span><br><span class="line">                             reverse&#x3D;True)[:width]:</span><br><span class="line">            pmi &#x3D; np.log2(cnt) - np.log2(x2cnt[x]) - np.log2(y2cnt[y])</span><br><span class="line">            pmi +&#x3D; pmi_diff</span><br><span class="line">            l_scores.append((y, pmi))</span><br><span class="line">        trans &#x3D; sorted(</span><br><span class="line">            l_scores,</span><br><span class="line">            key&#x3D;operator.itemgetter(1, 0),</span><br><span class="line">            reverse&#x3D;True)[:n_trans]</span><br><span class="line">        trans &#x3D; [each[0] for each in trans]</span><br><span class="line">        x2ys_pmi[x] &#x3D; trans</span><br><span class="line"></span><br><span class="line">    return x2ys_pmi</span><br></pre></td></tr></table></figure>
<h3 id="rerank"><a href="#rerank" class="headerlink" title="rerank"></a>rerank</h3><p>rerank用于对x的所有translation y进行cpe排序。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def rerank(x2ys, x2cnt, x2xs, width, n_trans):</span><br><span class="line">    x2ys_cpe &#x3D; dict()</span><br><span class="line">    for x, ys in tqdm(x2ys.items()):</span><br><span class="line">        cntx &#x3D; x2cnt[x]</span><br><span class="line">        y_scores &#x3D; []</span><br><span class="line">        for y, cnty in sorted(</span><br><span class="line">                ys.items(),</span><br><span class="line">                key&#x3D;operator.itemgetter(1),</span><br><span class="line">                reverse&#x3D;True)[:width]:</span><br><span class="line">            ts &#x3D; cnty &#x2F; float(cntx)  # translation score: initial value</span><br><span class="line">            if x in x2xs:</span><br><span class="line">                for x2, cntx2 in x2xs[x].items():  # Collocates</span><br><span class="line">                    p_x_x2 &#x3D; cntx2 &#x2F; float(cntx)</span><br><span class="line">                    p_x2_y2 &#x3D; 0</span><br><span class="line">                    if x2 in x2ys:</span><br><span class="line">                        p_x2_y2 &#x3D; x2ys[x2].get(y, 0) &#x2F; float(x2cnt[x2])</span><br><span class="line">                    ts -&#x3D; (p_x_x2 * p_x2_y2)</span><br><span class="line">            y_scores.append((y, ts))</span><br><span class="line">        _ys_ &#x3D; sorted(</span><br><span class="line">            y_scores,</span><br><span class="line">            key&#x3D;lambda y_score: y_score[1],</span><br><span class="line">            reverse&#x3D;True)[:n_trans]</span><br><span class="line">        _ys_ &#x3D; [each[0] for each in _ys_]</span><br><span class="line">        </span><br><span class="line">        x2ys_cpe[x] &#x3D; _ys_</span><br><span class="line">        return x2ys_cpe</span><br></pre></td></tr></table></figure>
<h3 id="rerank-mp"><a href="#rerank-mp" class="headerlink" title="rerank_mp"></a>rerank_mp</h3><p>rerank_mp使用多进程方式进行CPE重排序。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def rerank_mp(x2ys, x2cnt, x2xs, width, n_trans, num_workers):</span><br><span class="line">    from multiprocessing import Pool</span><br><span class="line">    shared_inputs &#x3D; x2ys, x2cnt, x2xs, width, n_trans</span><br><span class="line">    print(f&quot;Entering multiprocessing with &#123;num_workers&#125; workers...&quot;</span><br><span class="line">          f&quot; (#words&#x3D;&#123;len(x2ys)&#125;)&quot;)</span><br><span class="line">    with Pool(num_workers) as p:</span><br><span class="line">        x2ys_cpe &#x3D; dict(p.starmap(</span><br><span class="line">            _rerank_mp,</span><br><span class="line">            zip(x2ys.items(), it.repeat(shared_inputs)),</span><br><span class="line">        ))</span><br><span class="line">    return x2ys_cpe</span><br></pre></td></tr></table></figure>
<h1 id="基于词向量的方法"><a href="#基于词向量的方法" class="headerlink" title="基于词向量的方法"></a>基于词向量的方法</h1><blockquote>
<p>参考论文：<br>（1）A Survey Of Cross-lingual Word Embedding Models<br><a href="https://arxiv.org/abs/1706.04902" target="_blank" rel="noopener">https://arxiv.org/abs/1706.04902</a><br><a href="http://ir.hit.edu.cn/~xiachongfeng/slides/x-lingual-v1.0.pdf" target="_blank" rel="noopener">http://ir.hit.edu.cn/~xiachongfeng/slides/x-lingual-v1.0.pdf</a><br><a href="https://zhuanlan.zhihu.com/p/69366459" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/69366459</a><br>（2）Word Translation Without Parallel Data<br><a href="https://arxiv.org/abs/1710.04087" target="_blank" rel="noopener">https://arxiv.org/abs/1710.04087</a><br>（3）Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond<br><a href="https://arxiv.org/abs/1812.10464" target="_blank" rel="noopener">https://arxiv.org/abs/1812.10464</a><br><a href="https://github.com/yannvgn/laserembeddings" target="_blank" rel="noopener">https://github.com/yannvgn/laserembeddings</a><br>（4）Loss in Translation: Learning Bilingual Word Mapping with a Retrieval Criterion<br><a href="https://arxiv.org/pdf/1804.07745.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1804.07745.pdf</a><br><a href="https://fasttext.cc/docs/en/aligned-vectors.html" target="_blank" rel="noopener">https://fasttext.cc/docs/en/aligned-vectors.html</a></p>
</blockquote>
<h2 id="跨语言词向量综述"><a href="#跨语言词向量综述" class="headerlink" title="跨语言词向量综述"></a>跨语言词向量综述</h2><p>A Survey Of Cross-lingual Word Embedding Models这篇论文中对跨语言的词向量的相关研究进行了总结，并且发现很多模型其实本质上是一个模型，只是使用了不同的目标函数和数据。为了方便理解，我们先将后续用到的符号进行说明：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/word2word/3.png" alt="图片"></p>
<p>我们使用$\operatorname{sen} t_{1}^{s}, \ldots, \operatorname{sen} t_{n}^{s}$表示源语言的句子序列$\mathbf{y}_{1}^{s}, \ldots, \mathbf{y}_{n}^{s}$，$\operatorname{sen} t_{1}^{t}, \ldots, \operatorname{sen} t_{n}^{t}$表示与之对齐的目标端语言的句子序列$\mathbf{y}_{1}^{t}, \ldots, \mathbf{y}_{n}^{t}$。同样的，用$d o c_{1}^{s}, \ldots, d o c_{n}^{s}$表示源语言的文档$\mathbf{z}_{1}^{s}, \ldots, \mathbf{z}_{n}^{s}$。用$d o c_{1}^{t}, \ldots, d o c_{n}^{t}$表示目标语言的文档$\mathbf{z}_{1}^{t}, \ldots, \mathbf{z}_{n}^{t}$。</p>
<p>基本上所有模型的目标函数都可以写成：$J=\mathcal{L}^{1}+\ldots+\mathcal{L}^{\ell}+\Omega$，其中$\mathcal{L}^{\ell}$表示第l个语种的monolingual loss，$\Omega$是一个正则项。</p>
<h3 id="词向量综述"><a href="#词向量综述" class="headerlink" title="词向量综述"></a>词向量综述</h3><p>在正式介绍跨语言词向量模型前，我们简单介绍一下词向量的发展史。从刚才介绍的用LSA矩阵分解的方法开始，后续产生了基于Max-margin loss来最大化correct word sequence和incorrect word sequence的hinge loss：$\begin{aligned}<br>&amp;\mathcal{L}_{\mathrm{MML}}=\sum_{k=C+1}^{|\mathcal{C}|-C} \sum_{w^{\prime} \in V} \max \left(0,1-f\left(\left[\mathbf{x}_{w_{k-C}}, \ldots, \mathbf{x}_{w_{i}}, \ldots, \mathbf{x}_{w_{k+C}}\right]\right)\right.\\<br>&amp;\left.+f\left(\left[\mathbf{x}_{w_{k-C}}, \ldots, \mathbf{x}_{w^{\prime}}, \ldots, \mathbf{x}_{w_{k+C}}\right]\right)\right)<br>\end{aligned}$</p>
<p>接着又有了比较出名的Skip-gram with negative sampling方法：$\mathcal{L}_{\mathrm{SGNS}}=-\frac{1}{|\mathcal{C}|-C} \sum_{k=C+1}^{|\mathcal{C}|-C} \sum_{-C \leq j \leq C, j \neq 0} \log P\left(w_{k+j} | w_{k}\right)$，其中P使用softmax计算：$P\left(w_{k+j} | w_{k}\right)=\frac{\exp \left(\tilde{\mathbf{x}}_{w_{k+j}}^{\top} \mathbf{x}_{w_{k}}\right)}{\sum_{i=1}^{|V|} \exp \left(\tilde{\mathbf{x}}_{w_{i}} \top_{\mathbf{x}_{w_{k}}}\right)}$，为了减少计算量使用了negative-sample的方法：$P\left(w_{k+j} | w_{k}\right)=\log \sigma\left(\tilde{\mathbf{x}}_{w_{k+j}}^{\top} \mathbf{x}_{w_{k}}\right)+\sum_{i=1}^{N} \mathbb{E}_{w_{i} \sim P_{n}} \log \sigma\left(-\tilde{\mathbf{x}}_{w_{i}}^{\top} \mathbf{x}_{w_{k}}\right)$。正如在本篇上文中介绍，这种负采样方法，通过数学推导可以发现，本质上和矩阵分解的作用是一样的。</p>
<p>后来又有了Continuous bag-of-words（CBOW）模型：$\mathcal{L}_{\mathrm{CBOW}}=-\frac{1}{|\mathcal{C}|-C} \sum_{k=C+1}^{|\mathcal{C}|-C} \log P\left(w_{k} | w_{k-C}, \ldots, w_{k-1}, w_{k+1}, \ldots, w_{k+C}\right)$，其中$P\left(w_{k} | w_{k-C}, \ldots, w_{k+C}\right)=\frac{\exp \left(\tilde{\mathbf{x}}_{w_{k}}^{\top} \overline{\mathbf{x}}_{w_{k}}\right)}{\sum_{i=1}^{|V|} \exp \left(\tilde{\mathbf{x}}_{w_{i}} \top \overline{\mathbf{x}}_{w_{k}}\right)}$，$\overline{\mathbf{x}}_{w_{k}}$是$w_{k-C}, \dots, w_{k+C}$词向量的平均值。</p>
<p>再后来就有了我们常用的Glove词向量模型：$\mathcal{L}_{\mathrm{GloVe}}=\sum_{i, j=1}^{|V|} f\left(\mathbf{C}_{i j}\right)\left(\mathbf{x}_{w_{i}}^{\top} \tilde{\mathbf{x}}_{w_{j}}+b_{i}+\tilde{b}_{j}-\log \mathbf{C}_{i j}\right)^{2}$，其中$C_{ij}$计算$w_i$和$w_j$在给定窗口大小下共现的次数。如果我们令$b_{i}=\log c\left(w_{i}\right)$、$\tilde{b}_{j}=\log c\left(w_{j}\right)$，则会发现Glove模型实际上也是对一个PMI矩阵（进行$log|C|$ shift后）进行分解。</p>
<h3 id="Typology"><a href="#Typology" class="headerlink" title="Typology"></a>Typology</h3><p>训练跨语言词向量的方法，跟使用什么类型的语料有关，如下图，你可以使用平行词典、可比词典、平行语料或者可比语料等等：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/word2word/4.png" alt="图片"></p>
<p>下面这张图展示了整个跨语言向量的路线图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/word2word/5.png" alt="图片"><img src="https://uploader.shimo.im/f/DJJgrUv5aQXvcj9A.png!thumbnail" alt="图片"></p>
<h3 id="Word-Level-Alignment-Models"><a href="#Word-Level-Alignment-Models" class="headerlink" title="Word-Level Alignment Models"></a>Word-Level Alignment Models</h3><p>首先我们来看基于词对齐的方法。基于词对齐可以使用两种语料：平行语料、可比语料。基于平行语料的方法大致分3种：</p>
<ul>
<li>Mapping-based：通过学习平行语料或者平行词典的映射关系，现训练一个源语言的embedding，再用学习到的映射矩阵映射到目标语言词向量空间中</li>
<li>Pseudo-multi-lingual corpora-based：用跨语言的伪语料来捕捉不同语言单词间的相互作用，这个伪语料是人工构造的。例如根据翻译，可以定义英语 house 和法语 maison 是等价 的，根据词性标注，可以定义英语 car 和法语 maison 都是名词是等价的。因此这里的对齐方式不一 定是翻译，可以根据具体的任务来定义，然后利用这种对齐关系来构造双语伪语料。首先将源语言 和目标语言数据混合打乱。对于统一语料库中一句话的每一个词语，如果存在于对齐关系中，以一 定概率来替换为另一种语言的词语。通过该方法可以构建得到真实的双语语料库。例如根据翻译关 系，原始句子 build the house 经过构建可以得到 build the maison，就是将 house 替换为了 maison。 利用构建好的全部语料来使用 CBOW 算法学习词向量，由于替换以后的词语有相似的上下文，因 此会得到相似的表示。对于那些没有对齐关系的词语，例如“我吃苹果”和“I eat apple”，吃和 eat 没有对齐关系，但如果我和 I、苹果和 apple 有对齐关系，根据构造出来的语料“I 吃 apple”也可 以完成吃和 eat 的隐式对齐。这种方法对齐词语有相似表示。</li>
<li>Joint-method：利用平行语料来最小化monolingual losses + cross-lingual regularization term</li>
</ul>
<p>基于可比语料的方法大致分2种：</p>
<ul>
<li>Language grounding models：把图片作为anchor，通过图片特征来获得language similarity的特征</li>
<li>Comparable feature models：利用POS信息建立两个language的桥梁</li>
</ul>
<p>基于Mapping的方法有以下4个要素：</p>
<ul>
<li>mapping method：负责将monolingual embedding spaces转换到cross-lingual embedding space。Mapping method有以下几种方法：<ul>
<li>Regression methods<ul>
<li>学习source到target的转移矩阵，并最大化source embedding和target embedding的相似度：$\Omega_{\mathrm{MSE}}=\sum_{i=1}^{n}\left|\mathbf{W} \mathbf{x}_{i}^{s}-\mathbf{x}_{i}^{t}\right|^{2}$，其中$x^s_i$是source embedding，经过W转换后，希望最小化它和其真正的翻译$x^t_i$的embedding之间的距离。论文认为这个目标函数也可以写成这种形式：$J=\underbrace{\mathcal{L}_{\mathrm{SGNS}}\left(\mathbf{X}^{s}\right)+\mathcal{L}_{\mathrm{SGNS}}\left(\mathbf{X}^{t}\right)}_{1}+\underbrace{\Omega_{\mathrm{MSE}}\left(\underline{\mathbf{X}}^{s}, \underline{\mathbf{X}}^{t}, \mathbf{W}\right)}_{2}$。Regression method的想法来源于一个观察，就是source word之间的相关性，和他们的所对应的target word之间的相关性相似，如下图：</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/word2word/6.png" alt="图片"></p>
<ul>
<li>Orthogonal methods<ul>
<li>同Regression method，但要求W是正交的，即$\mathbf{W}^{\top} \mathbf{W}=\mathbf{I}$</li>
</ul>
</li>
<li>Canonical methods<ul>
<li>将source和target共同映射到另一个空间，并最大化两个embedding的相似度。我们定义映射后的两个单词的相关性为：$\rho\left(\mathbf{W}^{s \rightarrow} \mathbf{x}_{i}^{s}, \mathbf{W}^{t \rightarrow} \mathbf{x}_{i}^{t}\right)=\frac{\operatorname{cov}\left(\mathbf{W}^{s \rightarrow} \mathbf{x}_{i}^{s}, \mathbf{W}^{t \rightarrow} \mathbf{x}_{i}^{t}\right)}{\sqrt{\operatorname{var}\left(\mathbf{W}^{s \rightarrow \mathbf{x}_{i}^{s}}\right) \operatorname{var}\left(\mathbf{W}^{t \rightarrow \mathbf{x}_{i}^{t}}\right)}}$，则canonical method的目标是最大化所有的相关性：$\Omega_{\mathrm{CCA}}=-\sum_{i=1}^{n} \rho\left(\mathbf{W}^{s \rightarrow} \mathbf{x}_{i}^{s}, \mathbf{W}^{t \rightarrow} \mathbf{x}_{i}^{t}\right)$。论文认为这个目标函数等价于：$J=\underbrace{\mathcal{L}_{\mathrm{LSA}}\left(\mathbf{X}^{s}\right)+\mathcal{L}_{\mathrm{LSA}}\left(\mathbf{X}^{t}\right)}_{1}+\underbrace{\Omega_{\mathrm{CCA}}\left(\underline{\mathbf{X}}^{s}, \underline{\mathbf{X}}^{t}, \mathbf{W}^{s \rightarrow}, \mathbf{W}^{t \rightarrow}\right)}_{2}$</li>
</ul>
</li>
<li>Margin methods<ul>
<li>该方法将Loss函数改成了margin-based rank loss来减轻hubness的问题：$\Omega_{\mathrm{MML}}=\sum_{i=1}^{n} \sum_{j \neq i}^{k} \max \left\{0, \gamma-\cos \left(\mathbf{W} \mathbf{x}_{i}^{s}, \mathbf{x}_{i}^{t}\right)+\cos \left(\mathbf{W} \mathbf{x}_{i}^{s}, \mathbf{x}_{j}^{t}\right)\right\}$，并认为此时目标函数就是：$J=\underbrace{\mathcal{L}_{\mathrm{CBOW}}\left(\mathbf{X}^{s}\right)+\mathcal{L}_{\mathrm{CBOW}}\left(\mathbf{X}^{t}\right)}_{1}+\underbrace{\Omega_{\mathrm{MML}-\mathrm{I}}\left(\underline{\mathbf{X}}^{s}, \mathbf{X}^{t}, \mathbf{W}\right)}_{2}$</li>
<li>seed lexicon：用于学习embedding的种子字典</li>
<li>refinement：用于修正学习到的mapping</li>
<li>retrieval：用于搜索最近邻</li>
</ul>
</li>
</ul>
<h3 id="Sentence-Level-Alignment-Methods"><a href="#Sentence-Level-Alignment-Methods" class="headerlink" title="Sentence-Level Alignment Methods"></a>Sentence-Level Alignment Methods</h3><p>基于平行语料的方法也可以分为4种：</p>
<ul>
<li>Word-alignment based matrix factorization approaches<ul>
<li>基于Word-alignment的方法可以先用FastAlign找到词对齐关系。如果一个source word在target空间中只有一个翻译，那么这个target翻译的embedding应该是确定的一个，但如果它在target空间中有多个翻译，那么应该认为target的embedding应该是这些翻译的一个加权平均。这就是word-alignment方法的基本思路，其目标函数为：$\Omega_{s \rightarrow t}=\left|\mathbf{X}^{t}-\mathbf{A}^{s \rightarrow t} \mathbf{X}^{s}\right|^{2}$，也可以写成：$J=\underbrace{\mathcal{L}_{\mathrm{MML}}\left(\mathbf{X}^{t}\right)}_{1}+\underbrace{\Omega_{s \rightarrow t}\left(\underline{\mathbf{X}}^{t}, \underline{\mathbf{A}}^{s \rightarrow t}, \mathbf{X}^{s}\right)}_{2}$。之所以叫factorization approach是因为有的方法将$A^{s-&gt;t}$看成是共现矩阵并用Glove的目标函数进行分解</li>
</ul>
</li>
<li>Compositional sentence models<ul>
<li>这种方法将平行句子表示的距离最小化，即$E_{\text {dist}}\left(\operatorname{sen} t^{s}, \operatorname{sen} t^{t}\right)=\left|\mathbf{y}^{s}-\mathbf{y}^{t}\right|^{2}$，其中句子的表示使用单词表示的和，使用hinge loss作为目标函数：$\mathcal{L}=\sum_{\left(s e n t^{s}, s e n t^{t}\right) \in \mathcal{C}} \sum_{i=1}^{k} \max \left(0,1+E_{\text {dist}}\left(\operatorname{sent}^{s}, \operatorname{sent}^{t}\right)-E_{\text {dist}}\left(\operatorname{sent}^{s}, s_{i}^{t}\right)\right)$，或者$J=\mathcal{L}\left(\mathbf{X}^{s}, \mathbf{X}^{t}\right)+\Omega\left(\mathbf{X}^{s}\right)+\Omega\left(\mathbf{X}^{t}\right)$</li>
</ul>
</li>
<li>Bilingual autoencoder models<ul>
<li>从这里开始就开始无监督的工作了，Barone开始使用对抗自动编码器将源语言词嵌入转换到目标语言中，然后训练自动编码器以重建源嵌入，同时训练鉴别器以将投射的源嵌入与实际目标嵌入区分开，如下图：</li>
</ul>
</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/word2word/7.png" alt="图片"></p>
<ul>
<li>Bilingual skip-gram models</li>
</ul>
<p>论文中还讲解了Document-Level Alignment Models，训练以及评测。内容太多，也没看，在此不继续写这篇综述了。感兴趣的读者可以再去看看论文。</p>
<p>词向量训练还有BERT等方法，[Devlin et al., 2018] 提出了 Multilingual BERT，与单语 BERT 结构一样，使用共享的 Wordpiece 表示，使用了 104 中语言进行训练。训练时，无输入语言标记，也没有强制对齐的语料有相 同的表示。[Pires et al., 2019] 分析了 Multilingual BERT 的多语言表征能力，得出了几点结论： Multilingual BERT 的多语言表征能力不仅仅依赖于共享的词表，对于没有重叠（overlap）词汇语 言的 zero-shot 任务，也可以完成的很好；语言越相似，效果越好；对于语言顺序（主谓宾或者形 容词名词）不同的语言，效果不是很好；Multilingual BERT 的表示同时包含了多种语言共有的表 示，同时也包含了语言特定的表示，这一结论，[Wu and Dredze, 2019] 在语言分类任务中也指出， Multilingual BERT 由于需要完成语言模型任务，所以需要保持一定的语言特定的表示来在词表中 选择特定语言词语。</p>
<p>[Lample and Conneau, 2019] 提出了基于多种语言预训练的模型 XLMs，首先从单语语料库中 采样一些句子，对于资源稀少的语言可以增加数量，对于资源丰富的语言可以减少数量，将所有语 言使用统一 BPE 进行表示。使用三种语言模型目标来完成学习。前两个是基于单语语料库的，最 后一个是基于双语对齐数据的。第一种是 Causal Language Modeling (CLM)，根据之前的词语预 测下一个词语。第二个是 Masked Language Modeling (MLM)，和 BERT 类似，但是使用一个词 语流，而非句子对。第三种是 Translation Language Modeling (TLM)，可以随机 mask 掉其中一些 两种语言中的一些词语，然后进行预测。其模型如下：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/word2word/8.png" alt="图片"></p>
<h2 id="一些好用的工具"><a href="#一些好用的工具" class="headerlink" title="一些好用的工具"></a>一些好用的工具</h2><p>这里推荐两个，一个是fast-text的align-vector，可以在<a href="https://fasttext.cc/docs/en/aligned-vectors.html" target="_blank" rel="noopener">https://fasttext.cc/docs/en/aligned-vectors.html</a>下载，如果想训练可以参考<a href="https://github.com/facebookresearch/fastText/tree/master/alignment" target="_blank" rel="noopener">https://github.com/facebookresearch/fastText/tree/master/alignment</a>。</p>
<p>另一个是laser，其主项目在<a href="https://github.com/facebookresearch/LASER" target="_blank" rel="noopener">https://github.com/facebookresearch/LASER</a>，如果想直接使用multi-lingual的sentence embedding，可以参考<a href="https://github.com/yannvgn/laserembeddings" target="_blank" rel="noopener">https://github.com/yannvgn/laserembeddings</a>。顺便说下，这两个都是facebook的工作。</p>
]]></content>
      <tags>
        <tag>机器翻译</tag>
      </tags>
  </entry>
  <entry>
    <title>Lucene搭建搜索引擎初探</title>
    <url>/2020/05/10/Lucene%E6%90%AD%E5%BB%BA%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E5%88%9D%E6%8E%A2/</url>
    <content><![CDATA[<p>最近要做例句搜索的优化，因此重新看一看lucene，边学习边搭demo。由于平时使用惯了python，所以这一次使用pylucene做demo。本文着重于lucene的介绍，一些内容主要参考了niyanchun的博客，并增加了几个pylucene的示例代码。</p>
<a id="more"></a>
<h1 id="配置Pylucene环境"><a href="#配置Pylucene环境" class="headerlink" title="配置Pylucene环境"></a>配置Pylucene环境</h1><h3 id="安装pylucene"><a href="#安装pylucene" class="headerlink" title="安装pylucene"></a><strong>安装pylucene</strong></h3><ul>
<li>wget <a href="https://mirror.bit.edu.cn/apache/lucene/pylucene/pylucene-8.1.1-src.tar.gz" target="_blank" rel="noopener">https://mirror.bit.edu.cn/apache/lucene/pylucene/pylucene-8.1.1-src.tar.gz</a></li>
<li>tar zxvf pylucene-8.1.1-src.tar.gz  </li>
</ul>
<h3 id="安装JCC"><a href="#安装JCC" class="headerlink" title="安装JCC"></a><strong>安装JCC</strong></h3><ul>
<li>cd pylucene-8.1.1/jcc</li>
<li>setup.py中修改jdk位置</li>
<li>如果是MACOS<ul>
<li>export CC=/usr/bin/clang</li>
<li>export CXX=/usr/bin/clang++</li>
</ul>
</li>
<li>python setup.py build</li>
<li>python setup.py install</li>
</ul>
<h3 id="安装ANT"><a href="#安装ANT" class="headerlink" title="安装ANT"></a><strong>安装ANT</strong></h3><ul>
<li>wget <a href="https://mirror.bit.edu.cn/apache//ant/binaries/apache-ant-1.9.14-bin.tar.gz" target="_blank" rel="noopener">https://mirror.bit.edu.cn/apache//ant/binaries/apache-ant-1.9.14-bin.tar.gz</a></li>
<li>tar zxvf apache-ant-1.9.14-bin.tar.gz</li>
<li>export ANT_HOME=…/apache-ant-1.9.14</li>
<li>export PATH=$PATH:$ANT_HOME/bin</li>
<li>export ANT_OPTS=”-Xms1300m -Xmx2048m -XX:PermSize=128M -XX:MaxNewSize=256m -XX:MaxPermSize=256m”</li>
</ul>
<h3 id="安装lucene"><a href="#安装lucene" class="headerlink" title="安装lucene"></a><strong>安装lucene</strong></h3><ul>
<li>cd pylucene-8.1.1</li>
<li>vi Makefile<ul>
<li>PREFIX_PYTHON=…./conda-env</li>
<li>PYTHON=$(PREFIX_PYTHON)/bin/python</li>
<li>ANT=…/apache-ant-1.9.14/bin/ant</li>
<li>JCC=$(PYTHON) -m jcc.<strong>main</strong></li>
<li>NUM_FILES=8</li>
</ul>
</li>
<li>make</li>
<li>make install</li>
</ul>
<h1 id="术语总结"><a href="#术语总结" class="headerlink" title="术语总结"></a>术语总结</h1><p>索引整体的逻辑结构图，如下所示：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/1.png" alt="图片"></p>
<h2 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h2><p>对于初学全文检索的人来说，索引这个词非常具有迷惑性，主要原因是它有两个词性：</p>
<ul>
<li>动词：做动词时，一般英文写为“<em>indexing</em>”，比如“<em>索引一个文件</em>”翻译为“<em>indexing a file</em>”，它指的是我们将原始数据经过一系列的处理，最终形成可以高效全文检索（对于Lucene，就是生成倒排索引）的过程。这个过程就称之为<strong>索引（indexing）</strong>。</li>
<li>名词：做名词时，写为“<em>index</em>”。经过indexing最终形成的结果（一般以文件形式存在）称之为<strong>索引（index）</strong>。</li>
</ul>
<p>所以，见到索引这个词，你一定要分清楚是动词还是名词。后面为了清楚，凡是作为动词的时候我使用indexing，作为名词的时候使用index。Index是Lucene中的顶级逻辑结构，它是一个逻辑概念，如果对应到具体的实物，就是一个目录，目录里面所有的文件组成一个index。注意，这个目录里面不会再嵌套目录，只会包含多个文件。具体index的构成细节后面会专门写一篇文章来介绍。对应到代码里面，就是org.apache.lucene.store.Directory这个抽象类。最后要说明一点的是，Lucene中的Index和ElasticSearch里面的Index不是一个概念，ElasticSearch里面的shard对应的才是Lucene的Index。</p>
<h2 id="文档（Document）和字段（Field）"><a href="#文档（Document）和字段（Field）" class="headerlink" title="文档（Document）和字段（Field）"></a>文档（Document）和字段（Field）</h2><p>一个Index里面会包含若干个文档，文档就像MySQL里面的一行（record）或者HBase里面的一列。文档是Lucene里面索引和搜索的原子单位，就像我们在MySQL里面写数据的时候，肯定是以行为单位的；读的时候也是以行为单位的。当然我们可以指定只读/写行里面某些字段，但仍是以行为单位的，Lucene也是一样，以文档为最小单位。代码里面是这样说明的：”<em>Documents are the unit of indexing and search</em>“.每个文档都会有一个唯一的文档ID。</p>
<p>文档是一个灵活的概念，不同的业务场景对应的具体含义不同。对于搜索引擎来说，一个文档可能就代表爬虫爬到的一个网页，很多个网页（文档）组成了一个索引。而对于提供检索功能的邮件客户端来说，一个文档可能就代表一封邮件，很多封邮件（文档）组成了一个索引。再比如假设我们要构建一个带全文检索功能的商品管理系统，那一件商品就是一个文档，很多个商品组成了一个索引。对于日志处理，一般是一行日志代表一个文档。</p>
<p>文档里面包含若干个字段，真正的数据是存储在字段里面的。一个字段包含三个要素：<strong><em>名称、类型、值</em></strong>。我们要索引数据，必须将数据以文本形式存储到字段里之后才可以。Lucene的字段由一个key-value组成，就像map一样。value支持多种类型，如果value是一个map类型，那就是嵌套字段了。</p>
<p>最后需要注意的是，不同于传统的关系型数据库，Lucene不要求一个index里面的所有文档的字段要一样，如果你喜欢，每一条文档的结构都可以不一样（当然实际中不建议这样操作），而且不需要事先定义，这个特性一般称之为“<strong><em>flexible schema</em></strong>”。传统的关系型数据库要求一个表里面的所有字段的结构必须一致，而且必事先定义好，一般称之为“<strong><em>strict schema</em></strong>”或者”<strong><em>fixed schema</em></strong>“。比如，有一个名为“<em>mixture</em>”的索引包含3条Document，如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &#123; &quot;name&quot;: &quot;Ni Yanchun&quot;, &quot;gender&quot;: &quot;male&quot;, &quot;age&quot;: 28  &#125;,</span><br><span class="line">    &#123; &quot;name&quot;: &quot;Donald John Trump&quot;, &quot;gender&quot;: &quot;male&quot;, &quot;birthday&quot;: &quot;1946.06.14&quot;&#125;,</span><br><span class="line">    &#123; &quot;isbn&quot;: &quot;978-1-933988-17-7&quot;, &quot;price&quot;: 60, &quot;publish&quot;: &quot;2010&quot;, &quot;topic&quot;: [&quot;lucene&quot;, &quot;search&quot;]&#125;</span><br></pre></td></tr></table></figure>
<p>}</p>
<p>可以看到，3条Document的字段并不完全一样，这在Lucene中是合法的。</p>
<h2 id="Token和Term"><a href="#Token和Term" class="headerlink" title="Token和Term"></a>Token和Term</h2><p>Token存储在字段中的文本数据经过分词器分词后（准确的说是经过Tokenizer处理之后）产生的一系列词或者词组。比如假设有个”content”字段的存储的值为”My name is Ni Yanchun”，这个字段经过Lucene的标准分词器分词后的结果是：”my”, “name”, “is”, “ni”, “yanchun”。这里的每个词就是一个token，当然实际上除了词自身外，token还会包含一些其它属性。后面的文章中会介绍这些属性。</p>
<p>一个token加上它原来所属的字段的名称构成了<strong>Term</strong>。比如”content”和”my”组成一个term，”content”和”name”组成另外一个term。我们检索的时候搜的就是Term，而不是Token或者Document（但搜到term之后，会找到包含这个term的Document，然后返回整个Document，而不是返回单个Term）。</p>
<h2 id="Index-Segment"><a href="#Index-Segment" class="headerlink" title="Index Segment"></a>Index Segment</h2><p>在上面的图中，Document分别被一些绿框括了起来，这个称为Segment。Indexing的时候，并不是将所有数据写到一起，而是再分了一层，这层就是segment。Indexing的时候，会先将Document缓存，然后定期flush到文件。每次flush就会生成一个Segment。所以一个Index包含若干个Segment，每个Segment包含一部分Document。为了减少文件描述符的使用，这些小的Segment会定期的合并为（merge）大的Segment，数据量不大的时候，合并之后一个index可能只有一个Segment。搜索的时候，会搜索各个Segment，然后合并搜索结果。</p>
<h1 id="Analyzer"><a href="#Analyzer" class="headerlink" title="Analyzer"></a>Analyzer</h1><blockquote>
<p>参考：<br><a href="https://gist.github.com/Sennahoi/740753384999add46fc1" target="_blank" rel="noopener">https://gist.github.com/Sennahoi/740753384999add46fc1</a><br><a href="https://niyanchun.com/lucene-learning-4.html" target="_blank" rel="noopener">https://niyanchun.com/lucene-learning-4.html</a></p>
</blockquote>
<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>Analyzer像一个数据加工厂，输入是原始的文本数据，输出是经过各种工序加工的term，然后这些terms以倒排索引的方式存储起来，形成最终用于搜索的Index。所以Analyzer也是我们控制数据能以哪些方式检索的重要点。</p>
<h3 id="内置的Analyzer对比"><a href="#内置的Analyzer对比" class="headerlink" title="内置的Analyzer对比"></a>内置的Analyzer对比</h3><p>Lucene已经帮我们内置了许多Analyzer，我们先来挑几个常见的对比一下他们的分析效果吧：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">package com.niyanchun;</span><br><span class="line"></span><br><span class="line">import org.apache.lucene.analysis.Analyzer;</span><br><span class="line">import org.apache.lucene.analysis.TokenStream;</span><br><span class="line">import org.apache.lucene.analysis.core.KeywordAnalyzer;</span><br><span class="line">import org.apache.lucene.analysis.core.SimpleAnalyzer;</span><br><span class="line">import org.apache.lucene.analysis.core.WhitespaceAnalyzer;</span><br><span class="line">import org.apache.lucene.analysis.en.EnglishAnalyzer;</span><br><span class="line">import org.apache.lucene.analysis.standard.StandardAnalyzer;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line">import java.util.concurrent.atomic.AtomicInteger;</span><br><span class="line">public class AnalyzerCompare &#123;</span><br><span class="line"></span><br><span class="line">    private static final Analyzer[] ANALYZERS &#x3D; new Analyzer[]&#123;</span><br><span class="line">            new WhitespaceAnalyzer(), &#x2F;&#x2F; 仅根据空白字符（whitespace）进行分词。</span><br><span class="line">            new KeywordAnalyzer(), &#x2F;&#x2F; 不做任何分词，把整个原始输入作为一个token。所以可以看到输出只有1个token，就是原始句子。</span><br><span class="line">            new SimpleAnalyzer(), &#x2F;&#x2F; 根据非字母（non-letters）分词，并且将token全部转换为小写。所以该分词的输出的terms都是由小写字母组成的。</span><br><span class="line">            new StandardAnalyzer(EnglishAnalyzer.getDefaultStopSet()) &#x2F;&#x2F; 基于JFlex进行语法分词，然后删除停用词，并且将token全部转换为小写。标准分词器会处理停用词，但默认其停用词库为空，这里我们使用英文的停用词&#125;;</span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        String content &#x3D; &quot;My name is Ni Yanchun, I&#39;m 28 years old. You can contact me with the email niyanchun@outlook.com&quot;;</span><br><span class="line">        System.out.println(&quot;原始数据:\n&quot; + content + &quot;\n\n分析结果：&quot;);</span><br><span class="line">        for (Analyzer analyzer : ANALYZERS) &#123;</span><br><span class="line">            showTerms(analyzer, content);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private static void showTerms(Analyzer analyzer, String content) throws IOException &#123;</span><br><span class="line"></span><br><span class="line">        try (TokenStream tokenStream &#x3D; analyzer.tokenStream(&quot;content&quot;, content)) &#123;</span><br><span class="line">            StringBuilder sb &#x3D; new StringBuilder();</span><br><span class="line">            AtomicInteger tokenNum &#x3D; new AtomicInteger();</span><br><span class="line">            tokenStream.reset();</span><br><span class="line">            while (tokenStream.incrementToken()) &#123;</span><br><span class="line">                tokenStream.reflectWith(((attClass, key, value) -&gt; &#123;</span><br><span class="line">                    if (&quot;term&quot;.equals(key)) &#123;</span><br><span class="line">                        tokenNum.getAndIncrement();</span><br><span class="line">                        sb.append(&quot;\&quot;&quot;).append(value).append(&quot;\&quot;, &quot;);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;));</span><br><span class="line">            &#125;</span><br><span class="line">            tokenStream.end();</span><br><span class="line"></span><br><span class="line">            System.out.println(analyzer.getClass().getSimpleName() + &quot;:\n&quot; + tokenNum + &quot; tokens: [&quot; + sb.toString().substring(0, sb.toString().length() - 2) + &quot;]&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这段代码的功能是使用常见的四种分词器（WhitespaceAnalyzer，KeywordAnalyzer，SimpleAnalyzer，StandardAnalyzer）对“My name is Ni Yanchun, I’m 28 years old. You can contact me with the email niyanchun@outlook.com”这句话进行analyze，输出最终的terms。其中需要注意的是，标准分词器会去掉停用词（stop word），但其内置的停用词库为空，所以我们传了一个英文默认的停用词库。运行代码之后的输出如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">原始数据:</span><br><span class="line">My name is Ni Yanchun, I&#39;m 28 years old. You can contact me with the email niyanchun@outlook.com</span><br><span class="line"></span><br><span class="line">分析结果：</span><br><span class="line">WhitespaceAnalyzer:</span><br><span class="line">17 tokens: [&quot;My&quot;, &quot;name&quot;, &quot;is&quot;, &quot;Ni&quot;, &quot;Yanchun,&quot;, &quot;I&#39;m&quot;, &quot;28&quot;, &quot;years&quot;, &quot;old.&quot;, &quot;You&quot;, &quot;can&quot;, &quot;contact&quot;, &quot;me&quot;, &quot;with&quot;, &quot;the&quot;, &quot;email&quot;, &quot;niyanchun@outlook.com&quot;]</span><br><span class="line">KeywordAnalyzer:</span><br><span class="line">1 tokens: [&quot;My name is Ni Yanchun, I&#39;m 28 years old. You can contact me with the email niyanchun@outlook.com&quot;]</span><br><span class="line">SimpleAnalyzer:</span><br><span class="line">19 tokens: [&quot;my&quot;, &quot;name&quot;, &quot;is&quot;, &quot;ni&quot;, &quot;yanchun&quot;, &quot;i&quot;, &quot;m&quot;, &quot;years&quot;, &quot;old&quot;, &quot;you&quot;, &quot;can&quot;, &quot;contact&quot;, &quot;me&quot;, &quot;with&quot;, &quot;the&quot;, &quot;email&quot;, &quot;niyanchun&quot;, &quot;outlook&quot;, &quot;com&quot;]</span><br><span class="line">StandardAnalyzer:</span><br><span class="line">15 tokens: [&quot;my&quot;, &quot;name&quot;, &quot;ni&quot;, &quot;yanchun&quot;, &quot;i&#39;m&quot;, &quot;28&quot;, &quot;years&quot;, &quot;old&quot;, &quot;you&quot;, &quot;can&quot;, &quot;contact&quot;, &quot;me&quot;, &quot;email&quot;, &quot;niyanchun&quot;, &quot;outlook.com&quot;]</span><br></pre></td></tr></table></figure>
<h3 id="Analyzer原理"><a href="#Analyzer原理" class="headerlink" title="Analyzer原理"></a>Analyzer原理</h3><p>前面我们说了Analyzer就像一个加工厂，包含很多道工序。这些工序在Lucene里面分为两大类：Tokenizer和TokenFilter。Tokenizer永远是Analyzer的第一道工序，有且只有一个。它的作用是读取输入的原始文本，然后根据工序的内部定义，将其转化为一个个token输出。TokenFilter只能接在Tokenizer之后，因为它的输入只能是token。然后它将输入的token进行加工，输出加工之后的token。一个Analyzer中，TokenFilter可以没有，也可以有多个。也就是说一个Analyzer内部的流水线是这样的：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/2.png" alt="图片"></p>
<p>比如StandardAnalyzer的流水线是这样的：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/3.png" alt="图片"></p>
<p>所以，Analyzer的原理还是比较简单的，Tokenizer读入文本转化为token，然后后续的TokenFilter将token按需加工，输出需要的token。我们可以自由组合已有的Tokenizer和TokenFilter来满足自己的需求，也可以实现自己的Tokenizer和TokenFilter。</p>
<h2 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h2><h3 id="Analyzer和TokenStream"><a href="#Analyzer和TokenStream" class="headerlink" title="Analyzer和TokenStream"></a>Analyzer和TokenStream</h3><p>Analyzer对应的实现类是org.apache.lucene.analysis.Analyzer，这是一个抽象类。它的主要作用是构建一个org.apache.lucene.analysis.TokenStream对象，该对象用于分析文本。代码中的类描述是这样的：</p>
<blockquote>
<p>An Analyzer builds TokenStreams, which analyze text. It thus represents a policy for extracting index terms from text.</p>
</blockquote>
<p>因为它是一个抽象类，所以实际使用的时候需要继承它，实现具体的类。比如第一部分我们使用的4个内置Analyzer都是直接或间接继承的该类。继承的子类需要实现createComponents方法，之前说的一系列工序就是加在这个方法里的，可以认为一道工序就是整个流水线中的一个Component。Analyzer抽象类还实现了一个tokenStream方法，并且是final的。该方法会将一系列工序转化为TokenStream对象输出。比如SimpleAnalyzer的实现如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public final class SimpleAnalyzer extends Analyzer &#123;</span><br><span class="line">  public SimpleAnalyzer() &#123;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  @Override</span><br><span class="line">  protected TokenStreamComponents createComponents(final String fieldName) &#123;</span><br><span class="line">    Tokenizer tokenizer &#x3D; new LetterTokenizer();</span><br><span class="line">    return new TokenStreamComponents(tokenizer, new LowerCaseFilter(tokenizer));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  @Override</span><br><span class="line">  protected TokenStream normalize(String fieldName, TokenStream in) &#123;</span><br><span class="line">    return new LowerCaseFilter(in);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>TokenStream的作用就是流式的产生token。这些token可能来自于indexing时文档里面的字段数据，也可能来自于检索时的检索语句。其实就是之前说的indexing和查询的时候都会调用Analyzer。</p>
<h3 id="Tokenizer和TokenFilter"><a href="#Tokenizer和TokenFilter" class="headerlink" title="Tokenizer和TokenFilter"></a>Tokenizer和TokenFilter</h3><p>TokenStream有两个非常重要的抽象子类：org.apache.lucene.analysis.Tokenizer和org.apache.lucene.analysis.TokenFilter。这两个类的实质其实都是一样的，都是对Token进行处理。不同之处就是前面介绍的，Tokenizer是第一道工序，所以它的输入是原始文本，输出是token；而TokenFilter是后面的工序，它的输入是token，输出也是token。实质都是对token的处理，所以实现它两个的子类都需要实现incrementToken方法，也就是在这个方法里面实现处理token的具体逻辑。incrementToken方法是在TokenStream类中定义的。比如前面提到的StandardTokenizer就是实现Tokenizer的一个具体子类；LowerCaseFilter和StopFilter就是实现TokenFilter的具体子类。</p>
<p>最后要说一下，Analyzer的流程越长，处理逻辑越复杂，性能就越差，实际使用中需要注意权衡。Analyzer的原理及代码就分析到这里，因为篇幅，一些源码没有在文章中全部列出，如果你有兴趣，建议去看下常见的Analyzer的实现的源码，一定会有收获。</p>
<h2 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h2><p>那么在python中我们怎么使用呢？下面举个小例子，使用pylucene其实和java的lucene没有太大差别，因为实际都是调用的java的类库。这里我没自定义过Analyzer，都是将输入进行处理后，变成用空格分好的结果，再送入到WhitespaceAnalyzer。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from org.apache.lucene.analysis.core import WhitespaceAnalyzer</span><br><span class="line">from org.apache.lucene.index import IndexWriter, IndexWriterConfig</span><br><span class="line">from org.apache.lucene.store import SimpleFSDirectory</span><br><span class="line">from org.apache.lucene.document import Document, Field</span><br><span class="line">from org.apache.lucene.document import TextField</span><br><span class="line">from org.apache.lucene.search import BooleanQuery</span><br><span class="line">from org.apache.lucene.queryparser.classic import QueryParser</span><br><span class="line"># 声明</span><br><span class="line">lucene_analyzer &#x3D; WhitespaceAnalyzer()</span><br><span class="line"># 在建索引中使用analyzer</span><br><span class="line">sentence &#x3D; &#39;i am so confused .&#39;</span><br><span class="line">config &#x3D; IndexWriterConfig(self.lucene_analyzer)</span><br><span class="line">index_writer &#x3D; IndexWriter(SimpleFSDirectory(INDEXIDR), config)</span><br><span class="line">document &#x3D; Document()</span><br><span class="line">document.add(Field(&#39;sentence&#39;, sentence, TextField.TYPE_STORED))</span><br><span class="line">index_writer.addDocument(document)</span><br><span class="line"></span><br><span class="line"># 在构建query时使用analyzer</span><br><span class="line">query &#x3D; &#39;confuse&#39;</span><br><span class="line">boolean_query &#x3D; BooleanQuery.Builder()</span><br><span class="line">simple_query &#x3D; QueryParser(&quot;sentence&quot;, lucene_analyzer).parse(query)</span><br></pre></td></tr></table></figure>
<h1 id="倒排索引、Token与词向量"><a href="#倒排索引、Token与词向量" class="headerlink" title="倒排索引、Token与词向量"></a>倒排索引、Token与词向量</h1><h2 id="倒排索引（Inverted-Index）和正向索引（Forward-Index）"><a href="#倒排索引（Inverted-Index）和正向索引（Forward-Index）" class="headerlink" title="倒排索引（Inverted Index）和正向索引（Forward Index）"></a>倒排索引（Inverted Index）和正向索引（Forward Index）</h2><p>我们用一个例子来看什么是倒排索引，什么是正向索引。假设有两个文档（前面的数字为文档ID）：</p>
<ul>
<li>a good student.</li>
<li>a gifted student.</li>
</ul>
<p>这两个文档经过Analyzer之后（这里我们不去停顿词），分别得到以下两个索引表：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/4.png" alt="图片"></p>
<p>这两个表都是key-value形式的Map结构，该数据结构的最大特点就是可以根据key快速访问value。我们分别分析以下这两个表。</p>
<p>表1中，Map的key是一个个词，也就是上文中Analyzer的输出。value是包含该词的文档的ID。这种映射的好处就是如果我们知道了词，就可以很快的查出有哪些文档包含该词。大家想一下我们平时的检索是不是就是这种场景：我们知道一些关键字，然后想查有哪些网页包含该关键词。表1这种<strong>词到文档的映射</strong>结构就称之为<strong>倒排索引</strong>。</p>
<p>表2中，Map的key是文档id，而value是该文档中包含的所有词。这种结构的映射的好处是只要我们知道了文档（ID），就能知道这个文档里面包含了哪些词。这种<strong>文档到词的映射</strong>结构称之为<strong>正向索引</strong>。</p>
<p>倒排索引是文档检索系统最常用的数据结构，Lucene用的就是这种数据结构。那对于检索有了倒排索引是不是就够用了呢？我们来看一个搜索结果：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/5.png" alt="图片"></p>
<p>这里我搜索了我年少时的偶像S.H.E，一个台湾女团，Google返回了一些包含该关键字的网页，同时它将网页中该关键字用红色字体标了出来。几乎所有的搜索引擎都有该功能。大家想一下，使用上述的倒排索引结构能否做到这一点？</p>
<p>答案是做不到的。倒排索引的结构只能让我们快速判断一个文档（上述例子中一个网页就是一个文档）是否包含该关键字，但无法知道关键字出现在文档中的哪个位置。那搜索引擎是如何知道的呢？其实使用的是另外一个结构——词向量，词向量和倒排索引的信息都是在Analyze阶段计算出来的。在介绍词向量之前，我们先来看一下Analyze的输出结果——Token。</p>
<h2 id="Token"><a href="#Token" class="headerlink" title="Token"></a>Token</h2><p>Token除了包含词以外，还存在一些其它属性，下面就让我们来看看完整的token长什么样？</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import org.apache.lucene.analysis.Analyzer;</span><br><span class="line">import org.apache.lucene.analysis.TokenStream;</span><br><span class="line">import org.apache.lucene.analysis.core.WhitespaceAnalyzer;</span><br><span class="line">import org.apache.lucene.analysis.standard.StandardAnalyzer;</span><br><span class="line">import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;</span><br><span class="line"></span><br><span class="line">public class AnalysisDebug &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        Analyzer analyzer &#x3D; new StandardAnalyzer();</span><br><span class="line">        String sentence &#x3D; &quot;a good student, a gifted student.&quot;;</span><br><span class="line">        try (TokenStream tokenStream &#x3D; analyzer.tokenStream(&quot;sentence&quot;, sentence)) &#123;</span><br><span class="line">            tokenStream.reset();</span><br><span class="line"></span><br><span class="line">            while (tokenStream.incrementToken()) &#123;</span><br><span class="line">                System.out.println(&quot;token: &quot; + tokenStream.reflectAsString(false));</span><br><span class="line">            &#125;</span><br><span class="line">            tokenStream.end();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们借助TokenStream对象输出经过StandardAnalyzer处理的数据，程序运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">token: term&#x3D;a,bytes&#x3D;[61],startOffset&#x3D;0,endOffset&#x3D;1,positionIncrement&#x3D;1,positionLength&#x3D;1,type&#x3D;&lt;ALPHANUM&gt;,termFrequency&#x3D;1</span><br><span class="line">token: term&#x3D;good,bytes&#x3D;[67 6f 6f 64],startOffset&#x3D;2,endOffset&#x3D;6,positionIncrement&#x3D;1,positionLength&#x3D;1,type&#x3D;&lt;ALPHANUM&gt;,termFrequency&#x3D;1</span><br><span class="line">token: term&#x3D;student,bytes&#x3D;[73 74 75 64 65 6e 74],startOffset&#x3D;7,endOffset&#x3D;14,positionIncrement&#x3D;1,positionLength&#x3D;1,type&#x3D;&lt;ALPHANUM&gt;,termFrequency&#x3D;1</span><br><span class="line">token: term&#x3D;a,bytes&#x3D;[61],startOffset&#x3D;16,endOffset&#x3D;17,positionIncrement&#x3D;1,positionLength&#x3D;1,type&#x3D;&lt;ALPHANUM&gt;,termFrequency&#x3D;1</span><br><span class="line">token: term&#x3D;gifted,bytes&#x3D;[67 69 66 74 65 64],startOffset&#x3D;18,endOffset&#x3D;24,positionIncrement&#x3D;1,positionLength&#x3D;1,type&#x3D;&lt;ALPHANUM&gt;,termFrequency&#x3D;1</span><br><span class="line">token: term&#x3D;student,bytes&#x3D;[73 74 75 64 65 6e 74],startOffset&#x3D;25,endOffset&#x3D;32,positionIncrement&#x3D;1,positionLength&#x3D;1,type&#x3D;&lt;ALPHANUM&gt;,termFrequency&#x3D;1</span><br></pre></td></tr></table></figure>
<p>这个输出结果是非常值得探究的。可以看到sentence字段的文本数据”a good student, a gifted student”经过StandardAnalyzer分析之后输出了6个token，每个token由一些属性组成，这些属性对应的定义类在org.apache.lucene.analysis.tokenattributes包下面，有兴趣的可以查阅。这里我们简单介绍一下这些属性：</p>
<ul>
<li><strong>term</strong>：解析出来的词。<strong>注意这里的term不同于我们之前介绍的Term，它仅指提取出来的词</strong>。</li>
<li><strong>bytes</strong>：词的字节数组形式。</li>
<li><strong>startOffset, endOffset</strong>：词开始和结束的位置，从0开始计数。大家可以数一下。</li>
<li><strong>positionIncrement</strong>：当前词和上个词的距离，默认为1，表示词是连续的。如果有些token被丢掉了，这个值就会大于1了。可以将上述代码中注释掉的那行放开，同时将原来不带停用词的analyzer注释掉，这样解析出的停用词token就会被移除，你就会发现有些token的该字段的值会变成2。该字段主要用于支持”phrase search”, “span search”以及”highlight”，这些搜索都需要知道关键字在文档中的position，以后介绍搜索的时候再介绍。另外这个字段还有一个非常重要的用途就是支持同义词查询。我们将该某个token的positionIncrement置为0，就表示该token和上个token没有距离，搜索的时候，不论搜这两个token任何一个，都会返回它们两对应的文档。假设第一个token是土豆，下一个token是马铃薯，马铃薯对应的token的positionIncrement为0，那我们搜马铃薯时，也会给出土豆相关的信息，反之亦然。</li>
<li><strong>positionLength</strong>：该字段跨了多少个位置。代码注释中说极少有Analyzer会产生该字段，基本都是使用默认值1.</li>
<li><strong>type</strong>：字段类型。需要注意的是这个类型是由每个Analyzer的Tokenizer定义的，不同的Analyer定义的类型可能不同。比如StandardAnalyzer使用的StandardTokenizer定义了这几种类型：<ALPHANUM>、<NUM>、<SOUTHEAST_ASIAN>、<IDEOGRAPHIC>、<HIRAGANA>、<KATAKANA>、<HANGUL>、<EMOJI>。</li>
<li><strong>termFrequency</strong>：词频。注意这里的词频不是token在句子中出现的频率，而是让用户自定义的，比如我们想让某个token在评分的时候更重要一些，那我们就可以将其词频设置大一些。如果不设置，默认都会初始化为1。比如上面输出结果中有两个”a”字段，词频都为初始值1，这个在后续的流程会合并，合并之后，词频会变为2。</li>
</ul>
<p>除了以上属性外，还有一个可能存在的属性就是payload，我们可以在这个字段里面存储一些信息。以上就是一个完整的Token。</p>
<h2 id="词向量（Term-Vector）"><a href="#词向量（Term-Vector）" class="headerlink" title="词向量（Term Vector）"></a>词向量（Term Vector）</h2><p>Analyzer分析出来的Token并不会直接写入Index，还需要做一些转化：</p>
<ul>
<li>取token中的词，以及包含该词的字段信息、文档信息（doc id），形成词到字段信息、文档信息的映射，也就是我们前面介绍的倒排索引。</li>
<li>取token中的词，以及包含该词的positionIncrement、startOffset、endOffset、termFrequency信息，组成从token到后面四个信息的映射，这就是<strong>词向量</strong>。</li>
</ul>
<p>所以，倒排索引和词向量都是从term到某个value的映射，只是value的值不一样。这里需要注意，倒排索引是所有文档范围内的，而词向量是某个文档范围的。简言之就是一个index对应一个倒排索引，而一个document就有一个词向量。<strong>有了倒排索引，我们就知道搜索关键字包含在index的哪些document的字段中。有了词向量，我们就知道关键字在匹配到的document的具体位置。</strong>下面让我们从代码角度来验证一下上面的理论。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import org.apache.lucene.analysis.Analyzer;</span><br><span class="line">import org.apache.lucene.analysis.standard.StandardAnalyzer;</span><br><span class="line">import org.apache.lucene.document.Document;</span><br><span class="line">import org.apache.lucene.document.Field;</span><br><span class="line">import org.apache.lucene.document.FieldType;</span><br><span class="line">import org.apache.lucene.index.*;</span><br><span class="line">import org.apache.lucene.search.DocIdSetIterator;</span><br><span class="line">import org.apache.lucene.store.Directory;</span><br><span class="line">import org.apache.lucene.store.FSDirectory;</span><br><span class="line"></span><br><span class="line">import java.nio.file.Paths;</span><br><span class="line"></span><br><span class="line">public class TermVectorShow &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        &#x2F;&#x2F; 构建索引</span><br><span class="line">        final String indexPath &#x3D; &quot;indices&#x2F;tv-show&quot;;</span><br><span class="line">        Directory indexDir &#x3D; FSDirectory.open(Paths.get(indexPath));</span><br><span class="line"></span><br><span class="line">        Analyzer analyzer &#x3D; new StandardAnalyzer();</span><br><span class="line">        IndexWriterConfig iwc &#x3D; new IndexWriterConfig(analyzer);</span><br><span class="line">        iwc.setOpenMode(IndexWriterConfig.OpenMode.CREATE);</span><br><span class="line">        IndexWriter writer &#x3D; new IndexWriter(indexDir, iwc);</span><br><span class="line"></span><br><span class="line">        String sentence &#x3D; &quot;a good student, a gifted student&quot;;</span><br><span class="line">        &#x2F;&#x2F; 默认不会保存词向量，这里我们通过一些设置来保存词向量的相关信息</span><br><span class="line">        FieldType fieldType &#x3D; new FieldType();</span><br><span class="line">        fieldType.setStored(true);</span><br><span class="line">        fieldType.setStoreTermVectors(true);</span><br><span class="line">        fieldType.setStoreTermVectorOffsets(true);</span><br><span class="line">        fieldType.setStoreTermVectorPositions(true);</span><br><span class="line">        fieldType.setIndexOptions(</span><br><span class="line">             IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);</span><br><span class="line">        Field field &#x3D; new Field(&quot;content&quot;, sentence, fieldType);</span><br><span class="line">        Document document &#x3D; new Document();</span><br><span class="line">        document.add(field);</span><br><span class="line">        writer.addDocument(document);</span><br><span class="line">        writer.close();</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 从索引读取Term Vector信息</span><br><span class="line">        IndexReader indexReader &#x3D; DirectoryReader.open(indexDir);</span><br><span class="line">        Terms termVector &#x3D; indexReader.getTermVector(0, &quot;content&quot;);</span><br><span class="line">        TermsEnum termIter &#x3D; termVector.iterator();</span><br><span class="line">        while (termIter.next() !&#x3D; null) &#123;</span><br><span class="line">            PostingsEnum postingsEnum &#x3D; termIter.postings(null, PostingsEnum.ALL);</span><br><span class="line">            while (postingsEnum.nextDoc() !&#x3D; DocIdSetIterator.NO_MORE_DOCS) &#123;</span><br><span class="line">                int freq &#x3D; postingsEnum.freq();</span><br><span class="line">                System.out.printf(&quot;term: %s, freq: %d,&quot;, termIter.term().utf8ToString(), freq);</span><br><span class="line">                while (freq &gt; 0) &#123;</span><br><span class="line">                    System.out.printf(&quot; nextPosition: %d,&quot;, postingsEnum.nextPosition());</span><br><span class="line">                    System.out.printf(&quot; startOffset: %d, endOffset: %d&quot;,</span><br><span class="line">                            postingsEnum.startOffset(), postingsEnum.endOffset());</span><br><span class="line">                    freq--;</span><br><span class="line">                &#125;</span><br><span class="line">                System.out.println();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这段代码实现的功能是先indexing 1条document，形成index，然后我们读取index，从中获取那条document content字段的词向量。需要注意，indexing时默认是不存储词向量相关信息的，我们需要通过FieldType做显式的设置，否则你读取出来的Term Vector会是null。我们看一下程序的输出结果：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">term: a, freq: 2, nextPosition: 0, startOffset: 0, endOffset: 1 nextPosition: 3, startOffset: 16, endOffset: 17</span><br><span class="line">term: gifted, freq: 1, nextPosition: 4, startOffset: 18, endOffset: 24</span><br><span class="line">term: good, freq: 1, nextPosition: 1, startOffset: 2, endOffset: 6</span><br><span class="line">term: student, freq: 2, nextPosition: 2, startOffset: 7, endOffset: 14 nextPosition: 5, startOffset: 25, endOffset: 32</span><br></pre></td></tr></table></figure>
<p>这里我们indexing的数据和上一节token部分的数据是一样的，而且都使用的是StandardAnalyzer，所以我们可以对比着看上一节输出的token和这里输出的term vector数据。可以看到，之前重复的token（a和student）到这里已经被合并了，并且词频也相应的变成了2。然后我们看一下position信息和offset信息也是OK的。而像token中的positionLength、type等信息都丢弃了。<br>词向量的信息量比较大，所以默认并不记录，我们想要保存时需要针对每个字段做显式的设置，Lucene 8.2.0中包含如下一些选项（见org.apache.lucene.index.IndexOptions枚举类）：</p>
<ul>
<li>NONE：不索引</li>
<li>DOCS：只索引字段，不保存词频等位置信息</li>
<li>DOCS_AND_FREQS：索引字段并保存词频信息，但不保存位置信息</li>
<li>DOCS_AND_FREQS_AND_POSITIONS：索引字段并保存词频及位置信息</li>
<li>DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS：索引字段并保存词频、位置、偏移量等信息</li>
</ul>
<p>phrase search和span search需要position信息支持，所以一般全文搜索引擎默认会采用DOCS_AND_FREQS_AND_POSITIONS策略，这样基本就能覆盖常用的搜索需求了。而需要高亮等功能的时候，才需要记录offset信息。</p>
<h1 id="字段及其属性"><a href="#字段及其属性" class="headerlink" title="字段及其属性"></a>字段及其属性</h1><p>在创建Field的时候，第一个参数是字段名，第二个是字段值，第三个就是字段属性了。字段的属性决定了字段如何Analyze，以及Analyze之后存储哪些信息，进而决定了以后我们可以使用哪些方式进行检索。</p>
<h2 id="Field类"><a href="#Field类" class="headerlink" title="Field类"></a>Field类</h2><p>Field对应的类是org.apache.lucene.document.Field，该类实现了org.apache.lucene.document.IndexableField接口，代表用于indexing的一个字段。Field类比较底层一些，所以Lucene实现了许多Field子类，用于不同的场景，比如下图是IDEA分析出来的Field的子类：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/6.png" alt="图片"></p>
<p>如果有某个子类能满足我们的场景，那推荐使用子类。在介绍常用子类之前，需要了解一下字段的三大类属性：</p>
<ul>
<li>是否indexing（只有indexing的数据才能被搜索）</li>
<li>是否存储（即是否保存字段的原始值）</li>
<li>是否保存term vector</li>
</ul>
<p>这些属性就是由之前文章中介绍的org.apache.lucene.index.IndexOptions枚举类定义的：</p>
<ul>
<li>NONE：不索引</li>
<li>DOCS：只索引字段，不保存词频等位置信息</li>
<li>DOCS_AND_FREQS：索引字段并保存词频信息，但不保存位置信息</li>
<li>DOCS_AND_FREQS_AND_POSITIONS：索引字段并保存词频及位置信息</li>
<li>DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS：索引字段并保存词频、位置、偏移量等信息</li>
</ul>
<p>Field的各个子类就是实现了对不同类型字段的存储，同时选择了不同的字段属性，这里列举几个常用的：</p>
<ul>
<li>TextField：存储字符串类型的数据。indexing+analyze；不存储原始数据；不保存term vector。适用于需要全文检索的数据，比如邮件内容，网页内容等。</li>
<li>StringField：存储字符串类型的数据。indexing但不analyze，即整个字符串就是一个token，之前介绍的KeywordAnalyzer就属于这种；不存储原始数据；不保存term vector。适用于文章标题、人名、ID等只需精确匹配的字符串。</li>
<li>IntPoint, LongPoint, FloatPoint, DoublePoint：用于存储各种不同类型的数值型数据。indexing；不存储原始数据；不保存term vector。适用于数值型数据的存储。</li>
</ul>
<p>所以，对于Field及其子类我们需要注意以下两点：</p>
<ul>
<li>几乎所有的Field子类（除StoredField）都默认不存储原始数据，如果需要存储原始数据，就要额外增加一个StoredField类型的字段，专门用于存储原始数据。注意该类也是Field的一个子类。当然也有一些子类的构造函数中提供了参数来控制是否存储原始数据，比如StringField，创建实例时可以通过传递Field.Store.YES参数来存储原始数据。</li>
<li>Field子类的使用场景和对应的属性都已经设置好了，如果子类不能满足我们的需求，就需要对字段属性进行自定义，但子类的属性一般是不允许更改的，需要直接使用Field类，再配合FieldType类进行自定义化。</li>
</ul>
<h2 id="FieldType类"><a href="#FieldType类" class="headerlink" title="FieldType类"></a>FieldType类</h2><p>org.apache.lucene.document.FieldType类实现了org.apache.lucene.index.IndexableFieldType接口，用于描述字段的属性，如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">FieldType fieldType &#x3D; new FieldType();</span><br><span class="line">fieldType.setStored(true);</span><br><span class="line">fieldType.setStoreTermVectors(true);</span><br><span class="line">fieldType.setStoreTermVectorOffsets(true);</span><br><span class="line">fieldType.setStoreTermVectorPositions(true);</span><br><span class="line">fieldType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);</span><br><span class="line">Field field &#x3D; new Field(&quot;content&quot;, sentence, fieldType);</span><br></pre></td></tr></table></figure>
<p>该类定义了一些成员变量，这些成员变量就是字段的一些属性，这里列一下代码中的成员变量：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private boolean stored;</span><br><span class="line">private boolean tokenized &#x3D; true;</span><br><span class="line">private boolean storeTermVectors;</span><br><span class="line">private boolean storeTermVectorOffsets;</span><br><span class="line">private boolean storeTermVectorPositions;</span><br><span class="line">private boolean storeTermVectorPayloads;</span><br><span class="line">private boolean omitNorms;</span><br><span class="line">private IndexOptions indexOptions &#x3D; IndexOptions.NONE;</span><br><span class="line">private boolean frozen;</span><br><span class="line">private DocValuesType docValuesType &#x3D; DocValuesType.NONE;</span><br><span class="line">private int dataDimensionCount;</span><br><span class="line">private int indexDimensionCount;</span><br><span class="line">private int dimensionNumBytes;</span><br><span class="line">private Map&lt;String, String&gt; attributes;</span><br></pre></td></tr></table></figure>
<p>大部分属性含义已经比较清楚了，这里再简单介绍一下其含义：</p>
<ul>
<li><em>stored</em>：是否存储字段，默认为false。</li>
<li><em>tokenized</em>：是否analyze，默认为true。</li>
<li><em>storeTermVectors</em>：是否存储TermVector（如果是true，也不存储offset、position、payload信息），默认为false。</li>
<li><em>storeTermVectorOffsets</em>：是否存储offset信息，默认为false。</li>
<li><em>storeTermVectorPositions</em>：是否存储position信息，默认为false。</li>
<li><em>storeTermVectorPayloads</em>：是否存储payload信息，默认为false。</li>
<li><em>omitNorms</em>：是否忽略norm信息，默认为false。那什么是norm信息呢？Norm的全称是“Normalization”，理解起来非常简单，按照TF-IDF的计算方式，包含同一个搜索词的多个文本，文本越短其权重（或者叫相关性）越高。比如有两个文本都包含搜索词，一个文本只有100个词，另外文本一个有1000个词，那按照TF-IDF的算法，第一个文本跟搜索词的相关度比第二个文本高。这个信息就是norm信息。如果我们将其忽略掉，那在计算相关性的时候，会认为长文本和短文本的权重得分是一样的。</li>
<li><em>indexOptions</em>：即org.apache.lucene.index.IndexOptions，已经介绍过了。默认值为DOCS_AND_FREQS_AND_POSITIONS。</li>
<li><em>frozen</em>：该值设置为true之后，字段的各个属性就不允许再更改了，比如Field的TextField、StringField等子类都将该值设置为true了，因为他们已经将字段的各个属性定制好了。</li>
<li><em>dataDimensionCount</em>、<em>indexDimensionCount</em>、<em>dimensionNumBytes</em>：这几个和数值型的字段类型有关系，Lucene 6.0开始，对于数值型都改用Point来组织。dataDimensionCount和indexDimensionCount都可以理解为是Point的维度，类似于数组的维度。dimensionNumBytes则是Point中每个值所使用的字节数，比如IntPoint和FloatPoint是4个字节，LongPoint和DoublePoint则是8个字节。</li>
<li><em>attributes</em>：可以选择性的以key-value的形式给字段增加一些元数据信息，但注意这个key-value的map不是线程安全的。</li>
<li><em>docValuesType</em>：指定字段的值指定以何种类型索引DocValue。那什么是DocValue？DocValue就是从文档到Term的一个正向索引，需要这个东西是因为排序、聚集等操作需要根据文档快速访问文档内的字段。Lucene 4.0之前都是在查询的时候将所有文档的相关字段信息加载到内存缓存，一方面用的时候才加载，所以慢，另一方面对于内存压力很大。4.0中引入了DocValue的概念，在indexing阶段除了创建倒排索引，也可以选择性的创建一个正向索引，这个正向索引就是DocValue，主要用于排序、聚集等操作。其好处是存储在磁盘上，减小了内存压力，而且因为是事先计算好的，所以使用时速度也很快。弊端就是磁盘使用量变大（需要耗费 document个数*每个document的字段数 个字节），同时indexing的速度慢了。</li>
</ul>
<p>对于我们使用（包括ES、Solr等基于Lucene的软件）而言，只需要知道我们检索一个字段的时候可以控制保存哪些信息，以及这些信息在什么场景下使用，能带来什么好处，又会产生什么弊端即可。举个例子：比如我们在设计字段的时候，如果一个字段不会用来排序，也不会做聚集，那就没有必要生成DocValue，既能节省磁盘空间，又能提高写入速度。另外对于norm信息，如果你的场景只关注是否包含，那无需保存norm信息，但如果也关注相似度评分，并且文本长短是一个需要考虑的因素，那就应该保存norm信息。</p>
<h2 id="基本使用-1"><a href="#基本使用-1" class="headerlink" title="基本使用"></a>基本使用</h2><blockquote>
<p>参考：<br><a href="https://blog.51cto.com/8744704/2086852" target="_blank" rel="noopener">https://blog.51cto.com/8744704/2086852</a><br><a href="https://www.cnblogs.com/leeSmall/p/9011405.html" target="_blank" rel="noopener">https://www.cnblogs.com/leeSmall/p/9011405.html</a><br><a href="https://www.amazingkoala.com.cn/Lucene/DocValues/2019/0412/48.html" target="_blank" rel="noopener">https://www.amazingkoala.com.cn/Lucene/DocValues/2019/0412/48.html</a><br><a href="https://www.cnblogs.com/cnjavahome/p/9192467.html" target="_blank" rel="noopener">https://www.cnblogs.com/cnjavahome/p/9192467.html</a></p>
</blockquote>
<p>在例句搜索中主要使用了以下几种FieldType:</p>
<ul>
<li>TextField</li>
<li>IntPoint<ul>
<li>把整型存入索引中，必须同时加入NumericDocValuesField和StoredField，是看IntPoint源码注释中写的，不知道为什么一定要这样写</li>
</ul>
</li>
<li>FloatPoint<ul>
<li>把浮点数存入索引中，必须同时加入FloatDocValuesField和StoredField</li>
</ul>
</li>
<li>数组类型<ul>
<li>复杂类型存储</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">confidence &#x3D; int(data_json[&#39;confidence&#39;])</span><br><span class="line">document.add(IntPoint(&quot;confidence&quot;, confidence))</span><br><span class="line">document.add(NumericDocValuesField(&quot;confidence&quot;, confidence))</span><br><span class="line">document.add(StoredField(&quot;confidence&quot;, confidence))</span><br><span class="line"></span><br><span class="line">score &#x3D; float(data_json[&#39;score&#39;])</span><br><span class="line">document.add(FloatPoint(&quot;score&quot;, score))</span><br><span class="line">document.add(FloatDocValuesField(&quot;score&quot;, score))</span><br><span class="line">document.add(StoredField(&quot;score&quot;, score))</span><br><span class="line"></span><br><span class="line">document.add(SortedSetDocValuesField(&quot;keyword&quot;, BytesRef(keyword_text)))</span><br><span class="line">document.add(StoredField(&quot;keyword&quot;, keyword_text))</span><br><span class="line">document.add(Field(&quot;keyword&quot;, keyword_text, TextField.TYPE_STORED))</span><br></pre></td></tr></table></figure>
<h1 id="索引存储文件介绍"><a href="#索引存储文件介绍" class="headerlink" title="索引存储文件介绍"></a>索引存储文件介绍</h1><h3 id="索引文件格式"><a href="#索引文件格式" class="headerlink" title="索引文件格式"></a>索引文件格式</h3><p>不论是Solr还是ES，底层index的存储都是完全使用Lucene原生的方式，没有做改变，所以本文会以ES为例来介绍。需要注意的是Lucene的index在ES中称为shard，本文中提到的index都指的是Lucene的index，即ES中的shard。先来看一个某个index的数据目录：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/7.png" alt="图片"></p>
<p>可以看到一个索引包含了很多文件，似乎很复杂。但仔细观察之后会发现乱中似乎又有些规律：很多文件前缀一样，只是后缀不同，比如有很多_3c开头的文件。回想一下之前文章的介绍，index由若干个segment组成，而<strong>一个index目录下前缀相同表示这些文件都属于同一个segment</strong>。</p>
<p>那各种各样的后缀又代表什么含义呢？Lucene存储segment时有两种方式：</p>
<ul>
<li><strong>multifile格式</strong>。该模式下会产生很多文件，不同的文件存储不同的信息，其弊端是读取index时需要打开很多文件，可能造成文件描述符超出系统限制。</li>
<li><strong>compound格式</strong>。一般简写为CFS(Compound File System)，该模式下会将很多小文件合并成一个大文件，以减少文件描述符的使用。</li>
</ul>
<p>我们先来介绍multifile格式下的各个文件：</p>
<ul>
<li>write.lock：每个index目录都会有一个该文件，用于防止多个IndexWriter同时写一个文件。</li>
<li>segments_N：该文件记录index所有segment的相关信息，比如该索引包含了哪些segment。IndexWriter每次commit都会生成一个（N的值会递增），新文件生成后旧文件就会删除。所以也说该文件用于保存commit point信息。</li>
</ul>
<p>上面这两个文件是针对当前index的，所以每个index目录下都只会有1个（segments_N可能因为旧的没有及时删除临时存在两个）。下面介绍的文件都是针对segment的，每个segment就会有1个。</p>
<ul>
<li>.si：<em>Segment Info</em>的缩写，用于记录segment的一些元数据信息。</li>
<li>.fnm：<em>Fields</em>，用于记录fields设置类信息，比如字段的index option信息，是否存储了norm信息、DocValue等。</li>
<li>.fdt：<em>Field Data</em>，存储字段信息。当通过StoredField或者Field.Store.YES指定存储原始field数据时，这些数据就会存储在该文件中。</li>
<li>.fdx：<em>Field Index</em>，.fdt文件的索引/指针。通过该文件可以快速从.fdt文件中读取field数据。</li>
<li>.doc：<em>Frequencies</em>，存储了一个documents列表，以及它们的term frequency信息。</li>
<li>.pos：<em>Positions</em>，和.doc类似，但保存的是position信息。</li>
<li>.pay：Payloads<em>，和</em>.doc类似，但保存的是payloads和offset信息。</li>
<li>.tim：<em>Term Dictionary</em>，存储所有文档analyze出来的term信息。同时还包含term对应的document number以及若干指向.doc, .pos, .pay的指针，从而可以快速获取term的term vector信息。。</li>
<li>.tip：<em>Term Index</em>，该文件保存了Term Dictionary的索引信息，使得可以对Term Dictionary进行随机访问。</li>
<li>.nvd, .nvm：<em>Norms</em>，这两个都是用来存储Norms信息的，前者用于存储norms的数据，后者用于存储norms的元数据。</li>
<li>.dvd, .dvm：<em>Per-Document Values</em>，这两个都是用来存储DocValues信息的，前者用于数据，后者用于存储元数据。</li>
<li>.tvd：<em>Term Vector Data</em>，用于存储term vector数据。</li>
<li>.tvx：<em>Term Vector Index</em>，用于存储Term Vector Data的索引数据。</li>
<li>.liv：<em>Live Documents</em>，用于记录segment中哪些documents没有被删除。一般不存在该文件，表示segment内的所有document都是live的。如果有documents被删除，就会产生该文件。以前是使用一个.del后缀的文件来记录被删除的documents，现在改为使用该文件了。</li>
<li>.dim,.dii：<em>Point values</em>，这两个文件用于记录indexing的Point信息，前者保存数据，后者保存索引/指针，用于快速访问前者。</li>
</ul>
<p>上面介绍了很多文件类型，实际中不一定都有，如果indexing阶段不保存字段的term vector信息，那存储term vector的相关文件可能就不存在。如果一个index的segment非常多，那将会有非常非常多的文件，检索时，这些文件都是要打开的，很可能会造成文件描述符不够用，所以Lucene引入了前面介绍的CFS格式，它把上述每个segment的众多文件做了一个合并压缩（.liv和.si没有被合并，依旧单独写文件），最终形成了两个新文件：.cfs和.cfe，前者用于保存数据，后者保存了前者的一个Entry Table，用于快速访问。所以，如果使用CFS的话，最终对于每个segment，最多就只存在.cfs, .cfe, .si, .liv4个文件了。Lucene从1.4版本开始，默认使用CFS来保存segment数据，但开发者仍然可以选择使用multifile格式。一般来说，对于小的segment使用CFS，对于大的segment，使用multifile格式。比如Lucene的org.apache.lucene.index.MergePolicy构造函数中就提供merge时在哪些条件下使用CFS：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">  &#x2F;**</span><br><span class="line">   * Default ratio for compound file system usage. Set to &lt;tt&gt;1.0&lt;&#x2F;tt&gt;, always use </span><br><span class="line">   * compound file system.</span><br><span class="line">   *&#x2F;</span><br><span class="line">  protected static final double DEFAULT_NO_CFS_RATIO &#x3D; 1.0;</span><br><span class="line"></span><br><span class="line">  &#x2F;**</span><br><span class="line">   * Default max segment size in order to use compound file system. Set to &#123;@link Long#MAX_VALUE&#125;.</span><br><span class="line">   *&#x2F;</span><br><span class="line">  protected static final long DEFAULT_MAX_CFS_SEGMENT_SIZE &#x3D; Long.MAX_VALUE;</span><br><span class="line"></span><br><span class="line">  &#x2F;** If the size of the merge segment exceeds this ratio of</span><br><span class="line">   *  the total index size then it will remain in</span><br><span class="line">   *  non-compound format *&#x2F;</span><br><span class="line">  protected double noCFSRatio &#x3D; DEFAULT_NO_CFS_RATIO;</span><br><span class="line">  </span><br><span class="line">  &#x2F;** If the size of the merged segment exceeds</span><br><span class="line">   *  this value then it will not use compound file format. *&#x2F;</span><br><span class="line">  protected long maxCFSSegmentSize &#x3D; DEFAULT_MAX_CFS_SEGMENT_SIZE;</span><br><span class="line"></span><br><span class="line">  &#x2F;**</span><br><span class="line">   * Creates a new merge policy instance.</span><br><span class="line">   *&#x2F;</span><br><span class="line">  public MergePolicy() &#123;</span><br><span class="line">    this(DEFAULT_NO_CFS_RATIO, DEFAULT_MAX_CFS_SEGMENT_SIZE);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  &#x2F;**</span><br><span class="line">   * Creates a new merge policy instance with default settings for noCFSRatio</span><br><span class="line">   * and maxCFSSegmentSize. This ctor should be used by subclasses using different</span><br><span class="line">   * defaults than the &#123;@link MergePolicy&#125;</span><br><span class="line">   *&#x2F;</span><br><span class="line">  protected MergePolicy(double defaultNoCFSRatio, long defaultMaxCFSSegmentSize) &#123;</span><br><span class="line">    this.noCFSRatio &#x3D; defaultNoCFSRatio;</span><br><span class="line">    this.maxCFSSegmentSize &#x3D; defaultMaxCFSSegmentSize;</span><br></pre></td></tr></table></figure>
<h3 id=""><a href="#" class="headerlink" title="}"></a>}</h3><p>栗子</p>
<p>首先在ES中创建一个索引：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PUT nyc-test</span><br><span class="line">&#123;</span><br><span class="line">  &quot;settings&quot;: &#123;</span><br><span class="line">    &quot;number_of_shards&quot;: 1,</span><br><span class="line">    &quot;number_of_replicas&quot;: 0,</span><br><span class="line">    &quot;refresh_interval&quot;: -1</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里设置1个shard，0个副本，并且将refresh_interval设置为-1，表示不自动刷新。创建完之后就可以在es的数据目录找到该索引，es的后台索引的目录结构为：&lt;数据目录&gt;/nodes/0/indices/&lt;索引UUID&gt;/<shard>/index，这里的shard就是Lucene的index。我们看下刚创建的index的目录：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-&gt; % ll</span><br><span class="line">总用量 4.0K</span><br><span class="line">-rw-rw-r-- 1 allan allan 230 10月 11 21:45 segments_2</span><br><span class="line">-rw-rw-r-- 1 allan allan   0 10月 11 21:45 write.lock</span><br></pre></td></tr></table></figure>
<p>可以看到，现在还没有写入任何数据，所以只有index级别的segments_N和write.lock文件，没有segment级别的文件。写入1条数据并查看索引目录的变化：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PUT nyc-test&#x2F;doc&#x2F;1</span><br><span class="line">&#123;</span><br><span class="line">  &quot;name&quot;: &quot;Jack&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 查看索引目录</span><br><span class="line">-&gt; % ll</span><br><span class="line">总用量 4.0K</span><br><span class="line">-rw-rw-r-- 1 allan allan   0 10月 11 22:20 _0.fdt</span><br><span class="line">-rw-rw-r-- 1 allan allan   0 10月 11 22:20 _0.fdx</span><br><span class="line">-rw-rw-r-- 1 allan allan 230 10月 11 22:19 segments_2</span><br><span class="line">-rw-rw-r-- 1 allan allan   0 10月 11 22:19 write.lock</span><br></pre></td></tr></table></figure>
<p>可以看到出现了1个segment的数据，因为ES把数据缓存在内存里面，所以文件大小为0。然后再写入1条数据，并查看目录变化：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PUT nyc-test&#x2F;doc&#x2F;2</span><br><span class="line">&#123;</span><br><span class="line">  &quot;name&quot;: &quot;Allan&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 查看目录</span><br><span class="line">-&gt; % ll</span><br><span class="line">总用量 4.0K</span><br><span class="line">-rw-rw-r-- 1 allan allan   0 10月 11 22:20 _0.fdt</span><br><span class="line">-rw-rw-r-- 1 allan allan   0 10月 11 22:20 _0.fdx</span><br><span class="line">-rw-rw-r-- 1 allan allan 230 10月 11 22:19 segments_2</span><br><span class="line">-rw-rw-r-- 1 allan allan   0 10月 11 22:19 write.lock</span><br></pre></td></tr></table></figure>
<p>因为ES缓存机制的原因，目录没有变化。显式的refresh一下，让内存中的数据落地：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">POST nyc-test&#x2F;_refresh</span><br><span class="line"></span><br><span class="line">-&gt; % ll</span><br><span class="line">总用量 16K</span><br><span class="line">-rw-rw-r-- 1 allan allan  405 10月 11 22:22 _0.cfe</span><br><span class="line">-rw-rw-r-- 1 allan allan 2.5K 10月 11 22:22 _0.cfs</span><br><span class="line">-rw-rw-r-- 1 allan allan  393 10月 11 22:22 _0.si</span><br><span class="line">-rw-rw-r-- 1 allan allan  230 10月 11 22:19 segments_2</span><br><span class="line">-rw-rw-r-- 1 allan allan    0 10月 11 22:19 write.lock</span><br></pre></td></tr></table></figure>
<p>ES的refresh操作会将内存中的数据写入到一个新的segment中，所以refresh之后写入的两条数据形成了一个segment，并且使用CFS格式存储了。然后再插入1条数据，接着update这条数据：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 触发Lucene commit</span><br><span class="line">POST nyc-test&#x2F;_flush?wait_if_ongoing</span><br><span class="line"></span><br><span class="line"># 查看目录</span><br><span class="line">-&gt; % ll</span><br><span class="line">总用量 32K</span><br><span class="line">-rw-rw-r-- 1 allan allan  405 10月 11 22:22 _0.cfe</span><br><span class="line">-rw-rw-r-- 1 allan allan 2.5K 10月 11 22:22 _0.cfs</span><br><span class="line">-rw-rw-r-- 1 allan allan  393 10月 11 22:22 _0.si</span><br><span class="line">-rw-rw-r-- 1 allan allan   67 10月 11 22:24 _1_1.liv</span><br><span class="line">-rw-rw-r-- 1 allan allan  405 10月 11 22:24 _1.cfe</span><br><span class="line">-rw-rw-r-- 1 allan allan 2.5K 10月 11 22:24 _1.cfs</span><br><span class="line">-rw-rw-r-- 1 allan allan  393 10月 11 22:24 _1.si</span><br><span class="line">-rw-rw-r-- 1 allan allan  361 10月 11 22:25 segments_3</span><br><span class="line">-rw-rw-r-- 1 allan allan    0 10月 11 22:19 write.lock</span><br><span class="line"></span><br><span class="line"># 查看segment信息</span><br><span class="line">GET _cat&#x2F;segments&#x2F;nyc-test?v</span><br><span class="line"></span><br><span class="line">index    shard prirep ip        segment generation docs.count docs.deleted  size size.memory committed searchable version compound</span><br><span class="line">nyc-test 0     p      10.8.4.42 _0               0          2            0 3.2kb        1184 true      true       7.4.0   true</span><br><span class="line">nyc-test 0     p      10.8.4.42 _1               1          1            2 3.2kb        1184 true      true       7.4.0   true</span><br></pre></td></tr></table></figure>
<p>触发Lucene commit之后，可以看到segments_2变成了segments_3。然后调用_cat接口查看索引的segment信息也能看到目前有2个segment，而且都已经commit过了，并且compound是true，表示是CFS格式存储的。当然Lucene的segment是可以合并的。我们通过ES的forcemerge接口进行合并，并且将所有segment合并成1个segment，forcemerge的时候会自动调用flush，即会触发Lucene commit：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">POST nyc-test&#x2F;_forcemerge?max_num_segments&#x3D;1</span><br><span class="line"></span><br><span class="line">-&gt; % ll</span><br><span class="line">总用量 60K</span><br><span class="line">-rw-rw-r-- 1 allan allan  69 10月 11 22:27 _2.dii</span><br><span class="line">-rw-rw-r-- 1 allan allan 123 10月 11 22:27 _2.dim</span><br><span class="line">-rw-rw-r-- 1 allan allan 142 10月 11 22:27 _2.fdt</span><br><span class="line">-rw-rw-r-- 1 allan allan  83 10月 11 22:27 _2.fdx</span><br><span class="line">-rw-rw-r-- 1 allan allan 945 10月 11 22:27 _2.fnm</span><br><span class="line">-rw-rw-r-- 1 allan allan 110 10月 11 22:27 _2_Lucene50_0.doc</span><br><span class="line">-rw-rw-r-- 1 allan allan  80 10月 11 22:27 _2_Lucene50_0.pos</span><br><span class="line">-rw-rw-r-- 1 allan allan 287 10月 11 22:27 _2_Lucene50_0.tim</span><br><span class="line">-rw-rw-r-- 1 allan allan 145 10月 11 22:27 _2_Lucene50_0.tip</span><br><span class="line">-rw-rw-r-- 1 allan allan 100 10月 11 22:27 _2_Lucene70_0.dvd</span><br><span class="line">-rw-rw-r-- 1 allan allan 469 10月 11 22:27 _2_Lucene70_0.dvm</span><br><span class="line">-rw-rw-r-- 1 allan allan  59 10月 11 22:27 _2.nvd</span><br><span class="line">-rw-rw-r-- 1 allan allan 100 10月 11 22:27 _2.nvm</span><br><span class="line">-rw-rw-r-- 1 allan allan 572 10月 11 22:27 _2.si</span><br><span class="line">-rw-rw-r-- 1 allan allan 296 10月 11 22:27 segments_4</span><br><span class="line">-rw-rw-r-- 1 allan allan   0 10月 11 22:19 write.lock</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">GET _cat&#x2F;segments&#x2F;nyc-test?v</span><br><span class="line"></span><br><span class="line">index    shard prirep ip        segment generation docs.count docs.deleted  size size.memory committed searchable version compound</span><br><span class="line">nyc-test 0     p      10.8.4.42 _2               2          3            0 3.2kb        1224 true      true       7.4.0   false</span><br></pre></td></tr></table></figure>
<p>可以看到，force merge之后只有一个segment了，并且使用了multifile格式存储，而不是compound。当然这并非Lucene的机制，而是ES自己的设计。<br>最后用图总结一下：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/8.png" alt="图片"></p>
<p>想了解更详细的，可以阅读：<a href="https://lucene.apache.org/core/8_2_0/core/org/apache/lucene/codecs/lucene80/package-summary.html#package.description" target="_blank" rel="noopener">https://lucene.apache.org/core/8_2_0/core/org/apache/lucene/codecs/lucene80/package-summary.html#package.description</a></p>
<h1 id="Query"><a href="#Query" class="headerlink" title="Query"></a>Query</h1><blockquote>
<p>参考：<br><a href="https://niyanchun.com/lucene-learning-8.html" target="_blank" rel="noopener">https://niyanchun.com/lucene-learning-8.html</a><br><a href="https://niyanchun.com/lucene-learning-9.html" target="_blank" rel="noopener">https://niyanchun.com/lucene-learning-9.html</a><br><a href="https://pythonhosted.org/lupyne/examples.html" target="_blank" rel="noopener">https://pythonhosted.org/lupyne/examples.html</a></p>
</blockquote>
<p>在Lucene中，Term是查询的基本单元(unit)，所有查询类的父类是org.apache.lucene.search.Query，本文会介绍下图中这些主要的Query子类：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/9.png" alt="图片"></p>
<p>DisjunctionMaxQuery主要用于控制评分机制，SpanQuery代表一类查询，有很多的实现。这两类查询不是非常常用。</p>
<h2 id="TermQuery"><a href="#TermQuery" class="headerlink" title="TermQuery"></a>TermQuery</h2><p>TermQuery是最基础最常用的的一个查询了，对应的类是org.apache.lucene.search.TermQuery。其功能很简单，就是查询哪些文档中包含指定的term。看下面代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line"> * Query Demo.</span><br><span class="line"> *</span><br><span class="line"> * @author NiYanchun</span><br><span class="line"> **&#x2F;</span><br><span class="line">public class QueryDemo &#123;</span><br><span class="line"></span><br><span class="line">    &#x2F;**</span><br><span class="line">     * 搜索的字段</span><br><span class="line">     *&#x2F;</span><br><span class="line">    private static final String SEARCH_FIELD &#x3D; &quot;contents&quot;;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        &#x2F;&#x2F; 索引保存目录</span><br><span class="line">        final String indexPath &#x3D; &quot;indices&#x2F;poems-index&quot;;</span><br><span class="line">        &#x2F;&#x2F; 读取索引</span><br><span class="line">        IndexReader indexReader &#x3D; DirectoryReader.open(FSDirectory.open(Paths.get(indexPath)));</span><br><span class="line">        IndexSearcher searcher &#x3D; new IndexSearcher(indexReader);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; TermQuery</span><br><span class="line">        termQueryDemo(searcher);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private static void termQueryDemo(IndexSearcher searcher) throws IOException &#123;</span><br><span class="line">        System.out.println(&quot;TermQuery, search for &#39;death&#39;:&quot;);</span><br><span class="line">        TermQuery termQuery &#x3D; new TermQuery(new Term(SEARCH_FIELD, &quot;death&quot;));</span><br><span class="line"></span><br><span class="line">        resultPrint(searcher, termQuery);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private static void resultPrint(IndexSearcher searcher, Query query) throws IOException &#123;</span><br><span class="line">        TopDocs topDocs &#x3D; searcher.search(query, 10);</span><br><span class="line">        if (topDocs.totalHits.value &#x3D;&#x3D; 0) &#123;</span><br><span class="line">            System.out.println(&quot;not found!\n&quot;);</span><br><span class="line">            return;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        ScoreDoc[] hits &#x3D; topDocs.scoreDocs;</span><br><span class="line"></span><br><span class="line">        System.out.println(topDocs.totalHits.value + &quot; result(s) matched: &quot;);</span><br><span class="line">        for (ScoreDoc hit : hits) &#123;</span><br><span class="line">            Document doc &#x3D; searcher.doc(hit.doc);</span><br><span class="line">            System.out.println(&quot;doc&#x3D;&quot; + hit.doc + &quot; score&#x3D;&quot; + hit.score + &quot; file: &quot; + doc.get(&quot;path&quot;));</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面代码先读取索引文件，然后执行了一个term查询，查询所有包含death关键词的文档。为了方便打印，我们封装了一个resultPrint函数用于打印查询结果。<em>On Death</em>一诗包含了<em>death</em>关键字，所以程序执行结果为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">TermQuery, search for &#39;death&#39;:</span><br><span class="line">1 result(s) matched: </span><br><span class="line">doc&#x3D;3 score&#x3D;0.6199532 file: data&#x2F;poems&#x2F;OnDeath.txt</span><br></pre></td></tr></table></figure>
<h2 id="BooleanQuery"><a href="#BooleanQuery" class="headerlink" title="BooleanQuery"></a>BooleanQuery</h2><p>BooleanQuery用于将若干个查询按照与或的逻辑关系组织起来，支持嵌套。目前支持4个逻辑关系：</p>
<ul>
<li><strong><em>SHOULD</em></strong>：逻辑<strong>或</strong>的关系，文档满足任意一个查询即视为匹配。</li>
<li><strong><em>MUST</em></strong>：逻辑<strong>与</strong>的关系，文档必须满足所有查询才视为匹配。</li>
<li><strong><em>FILTER</em></strong>：逻辑<strong>与</strong>的关系，与must的区别是不计算score，所以性能会比must好。如果只关注是否匹配，而不关注匹配程度（即得分），应该优先使用filter。</li>
<li><strong><em>MUST NOT</em></strong>：逻辑与的关系，且取反。文档不满足所有查询的条件才视为匹配。</li>
</ul>
<p>使用方式也比较简单，以下的代码使用BooleanQuery查询contents字段包含<em>love</em>但不包含<em>seek</em>的词：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private static void booleanQueryDemo(IndexSearcher searcher) throws IOException &#123;</span><br><span class="line">    System.out.println(&quot;BooleanQuery, must contain &#39;love&#39; but absolutely not &#39;seek&#39;: &quot;);</span><br><span class="line">    BooleanQuery.Builder builder &#x3D; new BooleanQuery.Builder();</span><br><span class="line">    builder.add(new TermQuery(new Term(SEARCH_FIELD, &quot;love&quot;)), BooleanClause.Occur.MUST);</span><br><span class="line">    builder.add(new TermQuery(new Term(SEARCH_FIELD, &quot;seek&quot;)), BooleanClause.Occur.MUST_NOT);</span><br><span class="line">    BooleanQuery booleanQuery &#x3D; builder.build();</span><br><span class="line"></span><br><span class="line">    resultPrint(searcher, booleanQuery);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><em>Love’s Secret</em>和<em>Freedom and Love</em>两首诗中均包含了<em>love</em>一词，但前者还包含了<em>seek</em>一词，所以最终的搜索结果为<em>Freedom and Love</em>。</p>
<h2 id="PhraseQuery"><a href="#PhraseQuery" class="headerlink" title="PhraseQuery"></a>PhraseQuery</h2><p>PhraseQuery用于搜索term序列，比如搜索“hello world”这个由两个term组成的一个序列。对于Phrase类的查询需要掌握两个点：</p>
<ul>
<li>Phrase查询需要term的position信息，所以如果indexing阶段没有保存position信息，就无法使用phrase类的查询。</li>
<li>理解slop的概念：Slop就是两个term或者两个term序列的edit distance。后面的FuzzyQuery也用到了该概念，这里简单介绍一下。</li>
</ul>
<p>PhraseQuery使用的是Levenshtein distance，且默认的slop值是0，也就是只检索完全匹配的term序列。看下面这个例子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private static void phraseQueryDemo(IndexSearcher searcher) throws IOException &#123;</span><br><span class="line">    System.out.println(&quot;\nPhraseQuery, search &#39;love that&#39;&quot;);</span><br><span class="line"></span><br><span class="line">    PhraseQuery.Builder builder &#x3D; new PhraseQuery.Builder();</span><br><span class="line">    builder.add(new Term(SEARCH_FIELD, &quot;love&quot;));</span><br><span class="line">    builder.add(new Term(SEARCH_FIELD, &quot;that&quot;));</span><br><span class="line">    PhraseQuery phraseQueryWithSlop &#x3D; builder.build();</span><br><span class="line"></span><br><span class="line">    resultPrint(searcher, phraseQueryWithSlop);</span><br><span class="line">&#125;</span><br><span class="line">&#x2F;&#x2F; 运行结果</span><br><span class="line">PhraseQuery, search &#39;love that&#39;</span><br><span class="line">1 result(s) matched: </span><br><span class="line">doc&#x3D;2 score&#x3D;0.7089927 file: data&#x2F;poems&#x2F;Love&#39;sSecret.txt</span><br></pre></td></tr></table></figure>
<p><em>Love‘s Secret</em>里面有这么一句：”<em>Love that never told shall be</em>“，是能够匹配”<em>love that</em>“的。我们也可以修改slop的值，使得与搜索序列的edit distance小于等于slop的文档都可以被检索到，同时距离越小的文档评分越高。看下面例子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private static void phraseQueryWithSlopDemo(IndexSearcher searcher) throws IOException &#123;</span><br><span class="line">    System.out.println(&quot;PhraseQuery with slop: &#39;love &lt;slop&gt; never&quot;);</span><br><span class="line">    PhraseQuery phraseQueryWithSlop &#x3D; new PhraseQuery(1, SEARCH_FIELD, &quot;love&quot;, &quot;never&quot;);</span><br><span class="line"></span><br><span class="line">    resultPrint(searcher, phraseQueryWithSlop);</span><br><span class="line">&#125;</span><br><span class="line">&#x2F;&#x2F; 运行结果</span><br><span class="line">PhraseQuery with slop: &#39;love &lt;slop&gt; never</span><br><span class="line">1 result(s) matched: </span><br><span class="line">doc&#x3D;2 score&#x3D;0.43595996 file: data&#x2F;poems&#x2F;Love&#39;sSecret.txt</span><br></pre></td></tr></table></figure>
<h2 id="MultiPhraseQuery"><a href="#MultiPhraseQuery" class="headerlink" title="MultiPhraseQuery"></a>MultiPhraseQuery</h2><p>不论是官方文档或是网上的资料，对于MultiPhraseQuery讲解的都比较少。但其实它的功能很简单，举个例子就明白了：我们提供两个由term组成的数组：[“love”, “hate”], [“him”, “her”]，然后把这两个数组传给MultiPhraseQuery，它就会去检索 “love him”, “love her”, “hate him”, “hate her”的组合，每一个组合其实就是一个上面介绍的PhraseQuery。当然MultiPhraseQuery也可以接受更高维的组合。</p>
<p>由上面的例子可以看到PhraseQuery其实是MultiPhraseQuery的一种特殊形式而已，如果给MultiPhraseQuery传递的每个数组里面只有一个term，那就退化成PhraseQuery了。在MultiPhraseQuery中，一个数组内的元素匹配时是 <strong>或(OR)</strong> 的关系，也就是这些term共享同一个position。 还记得之前的文章中我们说过在同一个position放多个term，可以实现同义词的搜索。的确MultiPhraseQuery实际中主要用于同义词的查询。比如查询一个“我爱土豆”，那可以构造这样两个数组传递给MultiPhraseQuery查询：[“喜欢”，“爱”], [“土豆”，”马铃薯”，”洋芋”]，这样查出来的结果就会更全面一些。最后来个例子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private static void multiPhraseQueryDemo(IndexSearcher searcher) throws IOException &#123;</span><br><span class="line">    System.out.println(&quot;MultiPhraseQuery:&quot;);</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; On Death 一诗中有这样一句: I know not what into my ear</span><br><span class="line">    &#x2F;&#x2F; Fog 一诗中有这样一句: It sits looking over harbor and city</span><br><span class="line">    &#x2F;&#x2F; 以下的查询可以匹配 &quot;know harbor, know not, over harbor, over not&quot; 4种情况</span><br><span class="line">    MultiPhraseQuery.Builder builder &#x3D; new MultiPhraseQuery.Builder();</span><br><span class="line">    Term[] termArray1 &#x3D; new Term[2];</span><br><span class="line">    termArray1[0] &#x3D; new Term(SEARCH_FIELD, &quot;know&quot;);</span><br><span class="line">    termArray1[1] &#x3D; new Term(SEARCH_FIELD, &quot;over&quot;);</span><br><span class="line">    Term[] termArray2 &#x3D; new Term[2];</span><br><span class="line">    termArray2[0] &#x3D; new Term(SEARCH_FIELD, &quot;harbor&quot;);</span><br><span class="line">    termArray2[1] &#x3D; new Term(SEARCH_FIELD, &quot;not&quot;);</span><br><span class="line">    builder.add(termArray1);</span><br><span class="line">    builder.add(termArray2);</span><br><span class="line">    MultiPhraseQuery multiPhraseQuery &#x3D; builder.build();</span><br><span class="line"></span><br><span class="line">    resultPrint(searcher, multiPhraseQuery);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 程序输出</span><br><span class="line">MultiPhraseQuery:</span><br><span class="line">2 result(s) matched: </span><br><span class="line">doc&#x3D;0 score&#x3D;2.7032354 file: data&#x2F;poems&#x2F;Fog.txt</span><br><span class="line">doc&#x3D;3 score&#x3D;2.4798129 file: data&#x2F;poems&#x2F;OnDeath.txt</span><br></pre></td></tr></table></figure>
<h2 id="PrefixQuery、WildcardQuery、RegexpQuery"><a href="#PrefixQuery、WildcardQuery、RegexpQuery" class="headerlink" title="PrefixQuery、WildcardQuery、RegexpQuery"></a>PrefixQuery、WildcardQuery、RegexpQuery</h2><p>这三个查询提供模糊模糊查询的功能：</p>
<ul>
<li>PrefixQuery只支持指定前缀模糊查询，用户指定一个前缀，查询时会匹配所有该前缀开头的term。</li>
<li>WildcardQuery比PrefixQuery更进一步，支持 <strong>*</strong>（匹配0个或多个字符）和 <strong>?</strong>（匹配一个字符） 两个通配符。从效果上看，PrefixQuery是WildcardQuery的一种特殊情况，但其底层不是基于WildcardQuery，而是另外一种单独的实现。</li>
<li>RegexpQuery是比WildcardQuery更宽泛的查询，它支持正则表达式。支持的正则语法范围见org.apache.lucene.util.automaton.RegExp类。</li>
</ul>
<p>需要注意，WildcardQuery和RegexpQuery的性能会差一些，因为它们需要遍历很多文档。特别是极力不推荐以模糊匹配开头。当然这里的差是相对其它查询来说的，我粗略测试过，2台16C+32G的ES，比较简短的文档，千万级以下的查询也能毫秒级返回。最后看几个使用的例子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private static void prefixQueryDemo(IndexSearcher searcher) throws IOException &#123;</span><br><span class="line">    System.out.println(&quot;PrefixQuery, search terms begin with &#39;co&#39;&quot;);</span><br><span class="line">    PrefixQuery prefixQuery &#x3D; new PrefixQuery(new Term(SEARCH_FIELD, &quot;co&quot;));</span><br><span class="line"></span><br><span class="line">    resultPrint(searcher, prefixQuery);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private static void wildcardQueryDemo(IndexSearcher searcher) throws IOException &#123;</span><br><span class="line">    System.out.println(&quot;WildcardQuery, search terms &#39;har*&#39;&quot;);</span><br><span class="line">    WildcardQuery wildcardQuery &#x3D; new WildcardQuery(new Term(SEARCH_FIELD, &quot;har*&quot;));</span><br><span class="line"></span><br><span class="line">    resultPrint(searcher, wildcardQuery);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private static void regexpQueryDemo(IndexSearcher searcher) throws IOException &#123;</span><br><span class="line">    System.out.println(&quot;RegexpQuery, search regexp &#39;l[ao]*&#39;&quot;);</span><br><span class="line">    RegexpQuery regexpQuery &#x3D; new RegexpQuery(new Term(SEARCH_FIELD, &quot;l[ai].*&quot;));</span><br><span class="line"></span><br><span class="line">    resultPrint(searcher, regexpQuery);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 程序输出</span><br><span class="line">PrefixQuery, search terms begin with &#39;co&#39;</span><br><span class="line">2 result(s) matched: </span><br><span class="line">doc&#x3D;0 score&#x3D;1.0 file: data&#x2F;poems&#x2F;Fog.txt</span><br><span class="line">doc&#x3D;2 score&#x3D;1.0 file: data&#x2F;poems&#x2F;Love&#39;sSecret.txt</span><br><span class="line"></span><br><span class="line">WildcardQuery, search terms &#39;har*&#39;</span><br><span class="line">1 result(s) matched: </span><br><span class="line">doc&#x3D;0 score&#x3D;1.0 file: data&#x2F;poems&#x2F;Fog.txt</span><br><span class="line"></span><br><span class="line">RegexpQuery, search regexp &#39;l[ao]*&#39;</span><br><span class="line">2 result(s) matched: </span><br><span class="line">doc&#x3D;0 score&#x3D;1.0 file: data&#x2F;poems&#x2F;Fog.txt</span><br><span class="line">doc&#x3D;3 score&#x3D;1.0 file: data&#x2F;poems&#x2F;OnDeath.txt</span><br></pre></td></tr></table></figure>
<h2 id="FuzzyQuery"><a href="#FuzzyQuery" class="headerlink" title="FuzzyQuery"></a>FuzzyQuery</h2><p>FuzzyQuery和PhraseQuery一样，都是基于上面介绍的edit distance做匹配的，差异是在PhraseQuery中搜索词的是一个term序列，此时edit distance中定义的一个symbol就是一个词；而FuzzyQuery的搜索词就是一个term，所以它对应的edit distance中的symbol就是一个字符了。另外使用时还有几个注意点：</p>
<ul>
<li>PhraseQuery采用Levenshtein distance计算edit distance，即相邻symbol交换是2个slop，而FuzzyQuery默认使用Damerau–Levenshtein distance，所以相邻symbol交换是1个slop，但支持用户使用Levenshtein distance。</li>
<li>FuzzyQuery限制最大允许的edit distance为2（LevenshteinAutomata.MAXIMUM_SUPPORTED_DISTANCE值限定），因为对于更大的edit distance会匹配出特别多的词，但FuzzyQuery的定位是解决诸如美式英语和英式英语在拼写上的细微差异。</li>
<li>FuzzyQuery匹配的时候还有个要求就是搜索的term和待匹配的term的edit distance必须小于它们二者长度的最小值。比如搜索词为”abcd”，设定允许的maxEdits（允许的最大edit distance）为2，那么按照edit distance的计算方式”ab”这个词是匹配的，因为它们的距离是2，不大于设定的maxEdits。但是，由于 2 &lt; min( len(“abcd”), len(“ab”) ) = 2不成立，所以算不匹配。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private static void fuzzyQueryDemo(IndexSearcher searcher) throws IOException &#123;</span><br><span class="line">    System.out.println(&quot;FuzzyQuery, search &#39;remembre&#39;&quot;);</span><br><span class="line">    &#x2F;&#x2F; 这里把remember拼成了remembre</span><br><span class="line">    FuzzyQuery fuzzyQuery &#x3D; new FuzzyQuery(new Term(SEARCH_FIELD, &quot;remembre&quot;), 1);</span><br><span class="line"></span><br><span class="line">    resultPrint(searcher, fuzzyQuery);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 程序输出</span><br><span class="line">FuzzyQuery, search &#39;remembre&#39;</span><br><span class="line">1 result(s) matched: </span><br><span class="line">doc&#x3D;1 score&#x3D;0.4473783 file: data&#x2F;poems&#x2F;FreedomAndLove.txt</span><br></pre></td></tr></table></figure>
<h2 id="PointRangeQuery"><a href="#PointRangeQuery" class="headerlink" title="PointRangeQuery"></a>PointRangeQuery</h2><p>前面介绍Field的时候，我们介绍过几种常用的数值型Field：IntPoint、LongPoint、FloatPoint、DoublePoint。PointRangeQuery就是给数值型数据提供范围查询的一个Query，功能和原理都很简单，我们直接看一个完整的例子吧：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line"> * Point Query Demo.</span><br><span class="line"> *</span><br><span class="line"> * @author NiYanchun</span><br><span class="line"> **&#x2F;</span><br><span class="line">public class PointQueryDemo &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        &#x2F;&#x2F; 索引保存目录</span><br><span class="line">        final String indexPath &#x3D; &quot;indices&#x2F;point-index&quot;;</span><br><span class="line">        Directory indexDir &#x3D; FSDirectory.open(Paths.get(indexPath));</span><br><span class="line">        IndexWriterConfig iwc &#x3D; new IndexWriterConfig(new StandardAnalyzer());</span><br><span class="line">        iwc.setOpenMode(IndexWriterConfig.OpenMode.CREATE);</span><br><span class="line">        IndexWriter writer &#x3D; new IndexWriter(indexDir, iwc);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 向索引中插入10条document，每个document包含一个field字段，字段值是0~10之间的数字</span><br><span class="line">        for (int i &#x3D; 0; i &lt; 10; i++) &#123;</span><br><span class="line">            Document doc &#x3D; new Document();</span><br><span class="line">            Field pointField &#x3D; new IntPoint(&quot;field&quot;, i);</span><br><span class="line">            doc.add(pointField);</span><br><span class="line">            writer.addDocument(doc);</span><br><span class="line">        &#125;</span><br><span class="line">        writer.close();</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 查询</span><br><span class="line">        IndexReader indexReader &#x3D; DirectoryReader.open(FSDirectory.open(Paths.get(indexPath)));</span><br><span class="line">        IndexSearcher searcher &#x3D; new IndexSearcher(indexReader);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 查询field字段值在[5, 8]范围内的文档</span><br><span class="line">        Query query &#x3D; IntPoint.newRangeQuery(&quot;field&quot;, 5, 8);</span><br><span class="line">        TopDocs topDocs &#x3D; searcher.search(query, 10);</span><br><span class="line"></span><br><span class="line">        if (topDocs.totalHits.value &#x3D;&#x3D; 0) &#123;</span><br><span class="line">            System.out.println(&quot;not found!&quot;);</span><br><span class="line">            return;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        ScoreDoc[] hits &#x3D; topDocs.scoreDocs;</span><br><span class="line"></span><br><span class="line">        System.out.println(topDocs.totalHits.value + &quot; result(s) matched: &quot;);</span><br><span class="line">        for (ScoreDoc hit : hits) &#123;</span><br><span class="line">            System.out.println(&quot;doc&#x3D;&quot; + hit.doc + &quot; score&#x3D;&quot; + hit.score);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 程序输出</span><br><span class="line">4 result(s) matched: </span><br><span class="line">doc&#x3D;5 score&#x3D;1.0</span><br><span class="line">doc&#x3D;6 score&#x3D;1.0</span><br><span class="line">doc&#x3D;7 score&#x3D;1.0</span><br><span class="line">doc&#x3D;8 score&#x3D;1.0</span><br></pre></td></tr></table></figure>
<h2 id="TermRangeQuery"><a href="#TermRangeQuery" class="headerlink" title="TermRangeQuery"></a>TermRangeQuery</h2><p>TermRangeQuery和PointRangeQuery功能类似，不过它比较的是字符串，而非数值。比较基于org.apache.lucene.util.BytesRef.compareTo(BytesRef other)方法。直接看例子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private static void termRangeQueryDemo(IndexSearcher searcher) throws IOException &#123;</span><br><span class="line">    System.out.println(&quot;TermRangeQuery, search term between &#39;loa&#39; and &#39;lov&#39;&quot;);</span><br><span class="line">    &#x2F;&#x2F; 后面的true和false分别表示 loa &lt;&#x3D; 待匹配的term &lt; lov</span><br><span class="line">    TermRangeQuery termRangeQuery &#x3D; new TermRangeQuery(SEARCH_FIELD, new BytesRef(&quot;loa&quot;), new BytesRef(&quot;lov&quot;), true, false);</span><br><span class="line"></span><br><span class="line">    resultPrint(searcher, termRangeQuery);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 程序输出</span><br><span class="line">TermRangeQuery, search term between &#39;loa&#39; and &#39;lov&#39;</span><br><span class="line">1 result(s) matched: </span><br><span class="line">doc&#x3D;0 score&#x3D;1.0 file: data&#x2F;poems&#x2F;Fog.txt    &#x2F;&#x2F; Fog中的term &#39;looking&#39; 符合搜索条件</span><br></pre></td></tr></table></figure>
<h2 id="ConstantScoreQuery"><a href="#ConstantScoreQuery" class="headerlink" title="ConstantScoreQuery"></a>ConstantScoreQuery</h2><p>ConstantScoreQuery很简单，它的功能是将其它查询包装起来，并将它们查询结果中的评分改为一个常量值（默认为1.0）。上面FuzzyQuery一节里面最后举得例子中返回的查询结果score=0.4473783，现在我们用ConstantScoreQuery包装一下看下效果：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private static void constantScoreQueryDemo(IndexSearcher searcher) throws IOException &#123;</span><br><span class="line">    System.out.println(&quot;ConstantScoreQuery:&quot;);</span><br><span class="line">    ConstantScoreQuery constantScoreQuery &#x3D; new ConstantScoreQuery(</span><br><span class="line">            new FuzzyQuery(new Term(SEARCH_FIELD, &quot;remembre&quot;), 1));</span><br><span class="line"></span><br><span class="line">    resultPrint(searcher, constantScoreQuery);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 运行结果</span><br><span class="line">ConstantScoreQuery:</span><br><span class="line">1 result(s) matched: </span><br><span class="line">doc&#x3D;1 score&#x3D;1.0 file: data&#x2F;poems&#x2F;FreedomAndLove.txt</span><br></pre></td></tr></table></figure>
<p>另外有个知识点需要注意：ConstantScoreQuery嵌套Filter和BooleanQuery嵌套Filter的查询结果不考虑评分的话是一样的，但前面在BooleanQuery中介绍过Filter，其功能与MUST相同，但不计算评分；而ConstantScoreQuery就是用来设置一个评分的。所以两者的查询结果是一样的，但ConstantScoreQuery嵌套Filter返回结果是附带评分的，而BooleanQuery嵌套Filter的返回结果是没有评分的（score字段的值为0）。</p>
<h2 id="MatchAllDocsQuery"><a href="#MatchAllDocsQuery" class="headerlink" title="MatchAllDocsQuery"></a>MatchAllDocsQuery</h2><p>这个查询很简单，就是匹配所有文档，用于没有特定查询条件，只想预览部分数据的场景。直接看例子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private static void matchAllDocsQueryDemo(IndexSearcher searcher) throws IOException &#123;</span><br><span class="line">    System.out.println(&quot;MatchAllDocsQueryDemo:&quot;);</span><br><span class="line">    MatchAllDocsQuery matchAllDocsQuery &#x3D; new MatchAllDocsQuery();</span><br><span class="line"></span><br><span class="line">    resultPrint(searcher, matchAllDocsQuery);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 程序输出</span><br><span class="line">MatchAllDocsQueryDemo:</span><br><span class="line">4 result(s) matched: </span><br><span class="line">doc&#x3D;0 score&#x3D;1.0 file: data&#x2F;poems&#x2F;Fog.txt</span><br><span class="line">doc&#x3D;1 score&#x3D;1.0 file: data&#x2F;poems&#x2F;FreedomAndLove.txt</span><br><span class="line">doc&#x3D;2 score&#x3D;1.0 file: data&#x2F;poems&#x2F;Love&#39;sSecret.txt</span><br><span class="line">doc&#x3D;3 score&#x3D;1.0 file: data&#x2F;poems&#x2F;OnDeath.txt</span><br></pre></td></tr></table></figure>
<p>想看更多资料，可参考：<a href="https://lucene.apache.org/core/8_2_0/core/org/apache/lucene/search/package-summary.html" target="_blank" rel="noopener">https://lucene.apache.org/core/8_2_0/core/org/apache/lucene/search/package-summary.html</a></p>
<h1 id="QueryParser"><a href="#QueryParser" class="headerlink" title="QueryParser"></a>QueryParser</h1><p>QueryParser定义了一些查询语法，通过这些语法几乎可以实现前文介绍的所有Query API提供的功能，但它的存在并不是为了替换那些API，而是用在一些交互式场景中。比如下面代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class SearchFilesMinimal &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        &#x2F;&#x2F; 索引保存目录</span><br><span class="line">        final String indexPath &#x3D; &quot;&#x2F;Users&#x2F;allan&#x2F;Git&#x2F;allan&#x2F;github&#x2F;CodeSnippet&#x2F;Java&#x2F;lucene-learning&#x2F;indices&#x2F;poems-index&quot;;</span><br><span class="line">        &#x2F;&#x2F; 搜索的字段</span><br><span class="line">        final String searchField &#x3D; &quot;contents&quot;;</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 从索引目录读取索引信息</span><br><span class="line">        IndexReader indexReader &#x3D; DirectoryReader.open(FSDirectory.open(Paths.get(indexPath)));</span><br><span class="line">        &#x2F;&#x2F; 创建索引查询对象</span><br><span class="line">        IndexSearcher searcher &#x3D; new IndexSearcher(indexReader);</span><br><span class="line">        &#x2F;&#x2F; 使用标准分词器</span><br><span class="line">        Analyzer analyzer &#x3D; new StandardAnalyzer();</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 从终端获取查询语句</span><br><span class="line">        BufferedReader in &#x3D; new BufferedReader(new InputStreamReader(System.in));</span><br><span class="line">        &#x2F;&#x2F; 创建查询语句解析对象</span><br><span class="line">        QueryParser queryParser &#x3D; new QueryParser(searchField, analyzer);</span><br><span class="line">        while (true) &#123;</span><br><span class="line">            System.out.println(&quot;Enter query: &quot;);</span><br><span class="line"></span><br><span class="line">            String input &#x3D; in.readLine();</span><br><span class="line">            if (input &#x3D;&#x3D; null) &#123;</span><br><span class="line">                break;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            input &#x3D; input.trim();</span><br><span class="line">            if (input.length() &#x3D;&#x3D; 0) &#123;</span><br><span class="line">                break;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F; 解析用户输入的查询语句：build query</span><br><span class="line">            Query query &#x3D; queryParser.parse(input);</span><br><span class="line">            System.out.println(&quot;searching for: &quot; + query.toString(searchField));</span><br><span class="line">            &#x2F;&#x2F; 查询</span><br><span class="line">            TopDocs results &#x3D; searcher.search(query, 10);</span><br><span class="line">            &#x2F;&#x2F; 省略后面查询结果打印的代码</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在这段代码中，先读取了已经创建好的索引文件，然后创建了一个QueryParser实例(queryParser)。接着不断读取用户输入(input)，并传给QueryParser的parse方法，该方法通过用户的输入构建一个Query对象用于查询。<br>QueryParser的构造函数为QueryParser(String f, Analyzer a)，第1个参数指定一个默认的查询字段，如果后面输入的<em>input</em>里面没有指定查询字段，则默认查询该该字段，比如输入hello表示在默认字段中查询”<em>hello</em>“，而content: hello则表示在<em>content</em>字段中查询”<em>hello</em>“。第2个参数指定一个分析器，一般该分析器应该选择和索引阶段同样的Analyzer。</p>
<p><strong>另外有两个点需要特别注意：</strong></p>
<ul>
<li><strong>QueryParser默认使用TermQuery进行多个Term的OR关系查询</strong>（后文布尔查询那里会再介绍）。比如输入hello world，表示先将hello world分词（一般会分为hello和world两个词），然后使用TermQuery查询。如果需要全词匹配（即使用PhraseQuery），则需要将搜索词用<strong>双引号</strong>引起来，比如”hello world”。</li>
<li><strong>指定搜索字段时，该字段仅对紧随其后的第一个词或第一个用双引号引起来的串有效</strong>。比如title:hello world这个输入，<em>title</em>仅对<em>hello</em>有效，即搜索时只会在<em>title</em>字段中搜索<em>hello</em>，然后在默认搜索字段中搜索<em>world</em>。如果想要在一个字段中搜索多个词或多个用双引号引起来的词组时，将这些词用小括号括起来即可，比如title:(hello world)。</li>
</ul>
<h3 id="Wildcard搜索"><a href="#Wildcard搜索" class="headerlink" title="Wildcard搜索"></a>Wildcard搜索</h3><p>通配符搜索和WildcardQuery API一样，仅支持?和<em>两个通配符，前者用于匹配1个字符，后者匹配0到多个字符。输入title:te?t，则可以匹配到</em>title<em>中的”</em>test<em>“、”</em>text*”等词。</p>
<p><strong>注意：使用QueryParser中的wildcard搜索时，不允许以?和*开头，否则会抛异常，但直接使用WildcardQuery API时，允许以通配符开头，只是因为性能原因，不推荐使用。</strong>这样设计的原因我猜是因为QueryParser的输入是面向用户的，用户对于通配符开头造成的后果并不清楚，所以直接禁掉；而WildcardQuery是给开发者使用的，开发者在开发阶段很清楚如果允许这样做造成的后果是否可以接受，如果不能接受，也是可以通过接口禁掉开头就是用通配符的情况。</p>
<h3 id="Regexp搜索"><a href="#Regexp搜索" class="headerlink" title="Regexp搜索"></a>Regexp搜索</h3><p>正则搜索和RegexpQuery一样，不同之处在于QueryParser中输入的正则表达式需要使用<strong>两个斜线</strong>(“/“)包围起来，比如匹配”moat”或”boat”的正则为/[mb]oat/。</p>
<h3 id="Fuzzy搜索"><a href="#Fuzzy搜索" class="headerlink" title="Fuzzy搜索"></a>Fuzzy搜索</h3><p>在QueryParser中，通过在搜索词后面加<strong>波浪字符</strong>来实现FuzzyQuery，比如love~，默认edit distance是2，可以在波浪符后面加具体的整数值来修改默认值，合法的值为0、1、2.</p>
<h3 id="Phrase-slop搜索"><a href="#Phrase-slop搜索" class="headerlink" title="Phrase slop搜索"></a>Phrase slop搜索</h3><p>PhraseQuery中可以指定slop（默认值为0，精确匹配）来实现相似性搜索，QueryParser中同样可以，使用方法与Fuzzy类似——<strong>将搜索字符串用双引号引起来，然后在末尾加上波浪符</strong>，比如”jakarta apache”~10。这里对数edit distance没有限制，合法值为非负数，默认值为0.</p>
<h3 id="Range搜索"><a href="#Range搜索" class="headerlink" title="Range搜索"></a>Range搜索</h3><p>QueryParser的范围搜索同时支持TermRangeQuery和数值型的范围搜索，排序使用的是<strong>字典序</strong>。<strong>开区间使用大括号，闭区间使用方括号</strong>。比如搜索修改日期介于2019年9月份和10月份的文档：mod_date:[20190901 TO 20191031]，再比如搜索标题字段中包含<em>hate</em>到<em>love</em>的词（但不包含这两个词）的文档：title:{hate TO love}.</p>
<h3 id="提升权重-boost"><a href="#提升权重-boost" class="headerlink" title="提升权重(boost)"></a>提升权重(boost)</h3><p>查询时可以通过给搜索的关键字或双引号引起来的搜索串后面添加脱字符(^)及一个正数来提升其计算相关性时的权重（默认为1），比如love^5 China或”love China”^0.3。</p>
<h3 id="Boolean操作符"><a href="#Boolean操作符" class="headerlink" title="Boolean操作符"></a>Boolean操作符</h3><p>QueryParser中提供了5种布尔操作符：AND、+、OR、NOT、-，所有的<strong>操作符必须大写</strong>。</p>
<ul>
<li><strong>OR是默认的操作符</strong>，表示满足任意一个term即可。比如搜索love China，<em>love</em>和<em>China</em>之间就是OR的关系，检索时文档匹配任意一个词即视为匹配。OR也可以使用可用||代替。</li>
<li>AND表示必须满足<strong>所有term</strong>才可以，可以使用&amp;&amp;代替。</li>
<li>+用在term之前，表示该term必须存在。比如+love China表示匹配文档中必须包含<em>love</em>，<em>China</em>则可包含也可不含。</li>
<li>-用在term之前，表示该term必须不存在。比如-“hate China” “love China”表示匹配文档中包含”<em>love China</em>“，但不包含”<em>hate China</em>“的词。</li>
</ul>
<h3 id="分组"><a href="#分组" class="headerlink" title="分组"></a>分组</h3><p>前面已经介绍过，可以使用<strong>小括号</strong>进行分组，通过分组可以表达一些复杂的逻辑。举两个例子：</p>
<ul>
<li>(jakarta OR apache) AND website表示匹配文档中必须包含<em>webiste</em>，同时需要至少包含<em>jakarta</em>或<em>apache</em>二者之一。</li>
<li>title:(+return +”pink panther”)表示匹配文档中的title字段中必须同时存在<em>return</em>和<em>“pink panther”</em>串。</li>
</ul>
<h3 id="特殊字符"><a href="#特殊字符" class="headerlink" title="特殊字符"></a>特殊字符</h3><p>从前面的介绍可知，有很多符号在QueryParser中具有特殊含义，目前所有的特殊符号包括：+- &amp;&amp; || ! ( ) { } [ ] ^ “ ~ <em> ? :  /。如果搜索关键字中存在这些特殊符号，则需要使用反斜线()转义。比如搜索(1+1)</em>2则必须写为(1+1)*2。</p>
<p>相比于Lucene的其它搜索API，QueryParser提供了一种方式，让普通用户可以不需要写代码，只是掌握一些语法就可以进行复杂的搜索，在一些交互式检索场景中，还是非常方便的。</p>
<h3 id="基本使用-2"><a href="#基本使用-2" class="headerlink" title="基本使用"></a>基本使用</h3><p>下面展示一个使用pylucene构建一个基于term和关键词的Query，这里把keyword命中进行了加权，使用分数和长度进行排序，并将结果写进result中的过程：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">boolean_query &#x3D; BooleanQuery.Builder()</span><br><span class="line">simple_query &#x3D; QueryParser(</span><br><span class="line">    &quot;en_tokenized&quot;,</span><br><span class="line">    self.lucene_analyzer).parse(query)</span><br><span class="line">keyword_query &#x3D; QueryParser(</span><br><span class="line">    &quot;keyword&quot;,</span><br><span class="line">    self.lucene_analyzer).parse(query)</span><br><span class="line">boost_keyword_query &#x3D; BoostQuery(keyword_query, 2.0)</span><br><span class="line">boolean_query.add(simple_query, BooleanClause.Occur.SHOULD)</span><br><span class="line">boolean_query.add(boost_keyword_query, BooleanClause.Occur.SHOULD)</span><br><span class="line"># searcher</span><br><span class="line">lucene_searcher &#x3D; IndexSearcher(</span><br><span class="line">    DirectoryReader.open(self.indir))</span><br><span class="line">sorter &#x3D; Sort([</span><br><span class="line">    SortField.FIELD_SCORE,</span><br><span class="line">    SortField(&#39;origin_score&#39;, SortField.Type.FLOAT, True),</span><br><span class="line">    SortField(&#39;en_sent_lenth&#39;, SortField.Type.INT, True)])</span><br><span class="line"></span><br><span class="line"># rerank</span><br><span class="line">collector &#x3D; TopFieldCollector.create(sorter, self.maxrecal, self.maxrecal)</span><br><span class="line">lucene_searcher.search(boolean_query.build(), collector)</span><br><span class="line">scoreDocs &#x3D; collector.topDocs().scoreDocs</span><br><span class="line">result &#x3D; []</span><br><span class="line">for hit in scoreDocs:</span><br><span class="line">    doc &#x3D; lucene_searcher.doc(hit.doc)</span><br><span class="line">    result.append(...)</span><br></pre></td></tr></table></figure>
<h1 id="相似度评分机制"><a href="#相似度评分机制" class="headerlink" title="相似度评分机制"></a>相似度评分机制</h1><h2 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h2><h3 id="Bad-of-Words模型"><a href="#Bad-of-Words模型" class="headerlink" title="Bad-of-Words模型"></a>Bad-of-Words模型</h3><p>先介绍一下NLP和IR领域里面非常简单且使用极其广泛的bag-fo-words model，即词袋模型。假设有这么一句话：<em>“John likes to watch movies. Mary likes movies too.”</em>。那这句话用JSON格式的词袋模型表示的话就是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">BoW &#x3D; &#123;&quot;John&quot;:1,&quot;likes&quot;:2,&quot;to&quot;:1,&quot;watch&quot;:1,&quot;movies&quot;:2,&quot;Mary&quot;:1,&quot;too&quot;:1&#125;;</span><br></pre></td></tr></table></figure>
<p>可以看到，词袋模型关注的是词的出现次数，而没有记录词的位置信息。所以不同的语句甚至相反含义的语句其词袋可能是一样的，比如<em>“Mary is quicker than John”</em>和<em>“John is quicker than Mary”</em>这两句话，其词袋是一样的，但含义是完全相反的。所以凡是完全基于词袋模型的一些算法一般也存在这样该问题。</p>
<h3 id="Term-frequency"><a href="#Term-frequency" class="headerlink" title="Term frequency"></a>Term frequency</h3><p>词频就是一个词（term）在一个文档中（document）出现的次数（frequency），记为tf_{t,d}。这是一种最简单的定义方式，实际使用中还有一些变种：</p>
<ul>
<li>布尔词频：如果词在文档中出现，则tf_{t,d}=1，否则为0。</li>
<li>根据文档长短做调整的词频：tf_{t,d}/lenth，其中length为文档中的总词数。</li>
<li>对数词频：log(1+tf_{t,d})，加1是防止对0求对数（0没有对数）。 一般选取常用对数或者自然对数。</li>
</ul>
<p>词频的优点是简单，但缺点也很显然：</p>
<ol>
<li>词频中没有包含词的位置信息，所以从词频的角度来看，<em>“Mary is quicker than John”</em>和<em>“John is quicker than Mary”</em>两条文档是完全一致的，但显然它们的含义是完全相反的。</li>
<li>词频没有考虑不同词的重要性一般是不一样的，比如停用词的词频都很高，但它们并不重要。</li>
</ol>
<h3 id="Inverse-document-frequency"><a href="#Inverse-document-frequency" class="headerlink" title="Inverse document frequency"></a>Inverse document frequency</h3><p>一个词的逆文档频率用于衡量该词提供了多少信息，计算方式定义如下：$i d f_{t}=\log \frac{N}{d f_{t}}=-\log \frac{d f_{t}}{N}$</p>
<p>其中，t代表term，D代表文档，N代表语料库中文档总数，df_t代表语料库中包含t的文档的数据，即<strong>文档频率</strong>（document frequency）。如果语料库中不包含t，那df_t就等于0，为了避免除零操作，可以采用后面的公式，将df_t作为分子，也有的变种给df_t加了1。</p>
<p>对于固定的语料库，N是固定的，一个词的df_t越大，其idf(t,D)</p>
<p> 就越小。所以那些很稀少的词的idf值会很高，而像停用词这种出现频率很高的词idf值很低。</p>
<h3 id="TF-IDF-Model"><a href="#TF-IDF-Model" class="headerlink" title="TF-IDF Model"></a>TF-IDF Model</h3><p>TF-IDF就是将TF和IDF结合起来，其实就是简单的相乘：$t f i d f(t, d)=t f_{t, d} \cdot i d f_{t}$。从公式可以分析出来，一个词t在某个文档d中的tf-idf值：</p>
<ul>
<li>当该词在<strong>少数文档</strong>中<strong>出现很多次</strong>的时候，其值接近最大值；（<em>场景1</em>）</li>
<li>当该词在文档中出现次数少或者在很多文档中都出现时，其值较小；（<em>场景2</em>）</li>
<li>当该词几乎在所有文档中都出现时，其值接近最小值。（<em>场景3</em>）</li>
</ul>
<p>下面用一个例子来实战一下，还是以文中的4首英文短诗中的前3首为例。假设这3首诗组成了我们的语料库，每首诗就是一个文档（doc1：<em>Fog</em>、doc2：<em>Freedom And Love</em>、doc3：<em>Love’s Secret</em>），诗里面的每个单词就是一个个词（我们把标题也包含在里面）。然后我们选取<em>“the”、 “freedom”、”love”</em>三个词来分别计算它们在每个文档的TF-IDF，计算中使用自然对数形式。</p>
<ul>
<li>“<em>the</em>“在doc1中出现了1次，在doc2中出现了2次，在doc3中出现了1次，整个语料库有3个文档，包含”the”的文档也是3个。所以：</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/14.png" alt="图片"></p>
<ul>
<li>“<em>freedom</em>“在doc1中出现了0次，在doc2中出现了1次，在doc3中出现了0次，语料库中包含”freedom”的文档只有1个。所以：</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/15.png" alt="图片"></p>
<ul>
<li>“<em>love</em>“在doc1中现了0次，在doc2中出现了3次，在doc3中出现了5次，整个语料库有3个文档，包含”love”的文档有2个。所以：</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/16.png" alt="图片"></p>
<p>我们简单分析一下结果：”<em>the</em>“在所有文档中都出现了，所以其tf-idf值最低，为0，验证了上面公式分析中的场景3；”<em>freedom</em>“只有在第2个文档中出现了，所以其它两个的tf-idf值为0，表示不包含该词；”<em>love</em>“在第2、3个文档中都出现了，但在第3个文档中出现的频率更高，所以其tf-idf值最高。所以tf-idf算法的结果还是能很好的表示实际结果的。</p>
<h2 id="Vector-Space-Model"><a href="#Vector-Space-Model" class="headerlink" title="Vector Space Model"></a>Vector Space Model</h2><p>通过TF-IDF算法，我们可以计算出每个词在语料库中的权重，而通过VSM（Vector Space Model），则可以计算两个文档的相似度。</p>
<p>假设有两个文档：</p>
<ul>
<li>文档1：”Jack Ma regrets setting up Alibaba.”</li>
<li>文档2：”Richard Liu does not know he has a beautiful wife.”</li>
</ul>
<p>这是原始的文档，然后通过词袋模型转化后为：</p>
<ul>
<li>BoW1 = {“jack”:1, “ma”:1, “regret”:1, “set”:1, “up”:1, “alibaba”:1}</li>
<li>BoW2 = {“richard”:1, “liu”:1, “does”:1, “not”:1, “know”:1, “he”:1, “has”:1, “a”: 1, “beautiful”:1, “wife”:1}</li>
</ul>
<p>接着，分别用TF-IDF算法计算每个文档词袋中每个词的tf-idf值（值是随便写的，仅供原理说明）：</p>
<ul>
<li>tf-idf_doc1 = { 0.41, 0.12, 0.76, 0.83, 0.21, 0.47 }</li>
<li>tf-idf_doc2 = { 0.12, 0.25, 0.67, 0.98, 0.43, 0.76, 0.89, 0.51, 0.19, 0.37 }</li>
</ul>
<p>如果将上面的tf-idf_doc1和tf-idf_doc2看成是2个向量，那我们就通过上面的方式将原始的文档转换成了向量，这个向量就是VSM中的Vector。在VSM中，一个Vector就代表一个文档，记为V(q)，Vector中的每个值就是原来文档中term的权重（这个权重一般使用tf-idf计算，也可以通过其他方式计算）。这样语料库中的很多文档就会产生很多的向量，这些向量一起构成了一个向量空间，也就是Vector Space。</p>
<p>假设有一个查询语句为”Jack Alibaba”，我们可以用同样的方式将其转化一个向量，假设这个向量叫查询向量V(q)。<strong>这样在语料库中检索和 q相近文档的问题就转换成求语料库中每个向量V(d)与</strong>V(q)<strong>的相似度问题了</strong>。而衡量两个向量相似度最常用的方法就是余弦相似度，用公式表示就是：<script type="math/tex">\operatorname{cosineSimilarity(q,d)}=\frac{V(q) \cdot V(d)}{|V(q)||V(d)|}=v(q) \cdot v(d)</script>，这个就是Vector Space Model。</p>
<h2 id="TfidfSimilarity"><a href="#TfidfSimilarity" class="headerlink" title="TfidfSimilarity"></a>TfidfSimilarity</h2><blockquote>
<p>参考：<br><a href="https://en.wikipedia.org/wiki/Boolean_model_of_information_retrieval" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Boolean_model_of_information_retrieval</a></p>
</blockquote>
<p>Lucene使用<a href="http://en.wikipedia.org/wiki/Standard_Boolean_model" target="_blank" rel="noopener">Boolean model (BM) of Information Retrieval</a>模型来计算一个文档是否和搜索词匹配，对于匹配的文档使用基于VSM的评分算法来计算得分。具体的实现类是org.apache.lucene.search.similarities.TFIDFSimilarity，但做了一些修正。本文不讨论BM算法，只介绍评分算法。TFIDFSimilarity采用的评分公式如下：<script type="math/tex">\operatorname{Score}(q, d)=\sum_{t \in q}\left(t f_{t, d} \cdot i d f_{t}^{2} \cdot t . \text { get } \operatorname{Boost}() \cdot \text { norm }(t, d)\right)</script>，我们从外到内剖析一下这个公式：</p>
<ul>
<li>最外层的累加。搜索语句一般是由多个词组成的，比如”Jack Alibaba”就是有”Jack”和”Alibaba”两个词组成。计算搜索语句和每个匹配文档的得分的时候就是计算搜索语句中每个词和匹配文档的得分，然后累加起来就是搜索语句和该匹配文档的得分。这就是最外层的累加。</li>
<li>t.getBoost()：之前的系列文章中介绍过，在查询或者索引阶段我们可以人为设定某些term的权重，t.getBoost()获取的就是这个阶段设置的权重。所以查询或索引阶段设置的权重也就是在这个时候起作用的。</li>
<li>norm(t, d)：之前的系列文章中也介绍过，查询的时候一个文档的长短也是会影响词的重要性，匹配次数一样的情况下，越长的文档评分越低。这个也好理解，比如我们搜”Alibaba”，有两个文档里面都出现了一次该词，但其中一个文档总共包含100万个词，而另外一个只包含10个词，很显然，极大多数情况下，后者与搜索词的相关度是比前者高的。实际计算的时候使用的公式如下：<script type="math/tex">\operatorname{norm}(t, d)=\frac{1}{\sqrt{\operatorname{length}}}</script>，length是文档d的长度。</li>
<li>计算<script type="math/tex">t f_{t, d} \cdot i d f_{t}^{2}</script>：Lucene假设一个词在搜索语句中的词频为1（即使出现多次也不影响，就是重复计算多次而已），所以可以把这个公式拆开写：<script type="math/tex">t f_{t, d} \cdot i d f_{t}^{2}=t f_{t, d} \cdot i d f_{t} \cdot 1 \cdot i d f_{t}=\left(t f_{t, d} \cdot i d f_{t}\right) \cdot\left(t f_{t, q} \cdot i d f_{t}\right)</script>，这里的<script type="math/tex">\left(t f_{t, d} \cdot i d f_{t}\right) \cdot\left(t f_{t, q} \cdot i d f_{t}\right)</script>就对应上面的<script type="math/tex">-v(d) \cdot v(q)</script>!</li>
<li>在Lucene中，采用的TF计算公式为：<script type="math/tex">t f_{t, d}=\sqrt{\text {frequency}}</script>，IDF计算公式为：<script type="math/tex">i d f_{t}=1+\log \frac{N+1}{d f_{t}+1}</script></li>
</ul>
<p>其实TFIDFSimilarity是一个抽象类，真正实现上述相似度计算的是org.apache.lucene.search.similarities.ClassicSimilarity类，上面列举的公式在其对应的方法中也可以找到。除了基于TFIDF这种方式外，Lucene还支持另外一种相似度算法BM25，并且从6.0.0版本开始，BM25已经替代ClassicSimilarity，作为默认的评分算法。</p>
<h2 id="BM25Similarity"><a href="#BM25Similarity" class="headerlink" title="BM25Similarity"></a>BM25Similarity</h2><p>BM25全称“Best Match 25”，其中“25”是指现在BM25中的计算公式是第25次迭代优化。该算法是几位大牛在1994年TREC-3（Third <strong>T</strong>ext <strong>RE</strong>trieval <strong>C</strong>onference）会议上提出的，它将文本相似度问题转化为概率模型，可以看做是TF-IDF的改良版，我们看下它是如何进行改良的。</p>
<h3 id="对IDF的改良"><a href="#对IDF的改良" class="headerlink" title="对IDF的改良"></a>对IDF的改良</h3><p>BM25中的IDF公式为：<script type="math/tex">i d f_{t}^{B M 25}=\log \left(1+\frac{N-d f_{t}+0.5}{d f_{t}+0.5}\right)</script>。原版BM25的log中是没有加1的，Lucene为了防止产生负值，做了一点小优化。虽然对公式进行了更改，但其实和原来的公式没有实质性的差异，下面是新旧函数曲线对比：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/10.png" alt="图片"></p>
<h3 id="对TF的改良1"><a href="#对TF的改良1" class="headerlink" title="对TF的改良1"></a>对TF的改良1</h3><p>BM25中TF的公式为：<script type="math/tex">t f_{t, d}^{B M 25}=((k+1) * t f) /(k+t f)</script>，其中tf是传统的词频值。先来看下改良前后的函数曲线对比吧（下图中k=1.2）：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/11.png" alt="图片"></p>
<p>可以看到，传统的tf计算公式中，词频越高，tf值就越大，没有上限。但BM中的tf，随着词频的增长，tf值会无限逼近(k+1)，相当于是有上限的。这就是二者的区别。一般 k</p>
<p>k取 1.2，Lucene中也使用1.2作为k的默认值。</p>
<h3 id="对TF的改良2"><a href="#对TF的改良2" class="headerlink" title="对TF的改良2"></a>对TF的改良2</h3><p>在传统的计算公式中，还有一个norm。BM25将这个因素加到了TF的计算公式中，结合了norm因素的BM25中的TF计算公式为：<script type="math/tex">t f_{t, d}^{B M 25}=((k+1) * t f) /(k *(1.0-b+b * L)+t f)</script>，和之前相比，就是给分母上面的k加了一个乘数(1.0 - b + b * L)，其中的L的计算公式为：$L=|d|/avgDl$，其中，|d|是当前文档的长度，avgDl是语料库中所有文档的平均长度。b是一个常数，用来控制L对最总评分影响的大小，一般取0~1之间的数（取0则代表完全忽略L）。Lucene中b的默认值为 0.75.</p>
<p>通过这些细节上的改良，BM25在很多实际场景中的表现都优于传统的TF-IDF，所以从Lucene 6.0.0版本开始，上位成为默认的相似度评分算法。</p>
<h1 id="代码实践"><a href="#代码实践" class="headerlink" title="代码实践"></a>代码实践</h1><h2 id="Java"><a href="#Java" class="headerlink" title="Java"></a>Java</h2><p>原始数据为4首英文短诗，每个诗对应一个文件，文件名为诗名。这里列出内容，方便后面讨论。</p>
<ul>
<li>Fog（迷雾）：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">The fog comes</span><br><span class="line">on little cat feet.</span><br><span class="line">It sits looking over harbor and city</span><br><span class="line">on silent haunches</span><br><span class="line">and then, moves on.</span><br></pre></td></tr></table></figure>
<ul>
<li>Freedom And Love（自由与爱情）：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">How delicious is the winning</span><br><span class="line">Of a kiss at loves beginning,</span><br><span class="line">When two mutual hearts are sighing</span><br><span class="line">For the knot there&#39;s no untying.</span><br><span class="line">Yet remember, &#39;mist your wooing,</span><br><span class="line">Love is bliss, but love has ruining;</span><br><span class="line">Other smiles may make you fickle,</span><br><span class="line">Tears for charm may tickle.</span><br></pre></td></tr></table></figure>
<ul>
<li>Love’s Secret（爱情的秘密）：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Never seek to tell thy love,</span><br><span class="line">Love that never told shall be;</span><br><span class="line">For the gentle wind does move</span><br><span class="line">Silently, invisibly.</span><br><span class="line">I told my love, I told my love,</span><br><span class="line">I told her all my heart,</span><br><span class="line">Trembling, cold, in ghastly fears.</span><br><span class="line">Ah! she did depart!</span><br><span class="line">Soon after she was gone from me,</span><br><span class="line">A traveller came by,</span><br><span class="line">Silently, invisibly:</span><br><span class="line">He took her with a sigh.</span><br></pre></td></tr></table></figure>
<ul>
<li>On Death（死亡）：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Death stands above me, whispering low</span><br><span class="line">I know not what into my ear:</span><br><span class="line">Of his strange language all I know</span><br><span class="line">Is, there is not a word of fear.</span><br></pre></td></tr></table></figure>
<p>因为原始数据已经是文本格式了，所以我们构建索引的流程如下：<br><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/12.png" alt="图片"></p>
<p>其中的<strong>分析</strong>就是我们之前说的分词。然后先看代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 省略包等信息，完整文件见源文件</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * Minimal Index Files code.</span><br><span class="line"> **&#x2F;</span><br><span class="line">public class IndexFilesMinimal &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        &#x2F;&#x2F; 原数据存放路径</span><br><span class="line">        final String docsPath &#x3D; &quot;&#x2F;Users&#x2F;allan&#x2F;Git&#x2F;allan&#x2F;github&#x2F;CodeSnippet&#x2F;Java&#x2F;lucene-learning&#x2F;data&#x2F;poems&quot;;</span><br><span class="line">        &#x2F;&#x2F; 索引保存目录</span><br><span class="line">        final String indexPath &#x3D; &quot;&#x2F;Users&#x2F;allan&#x2F;Git&#x2F;allan&#x2F;github&#x2F;CodeSnippet&#x2F;Java&#x2F;lucene-learning&#x2F;indices&#x2F;poems-index&quot;;</span><br><span class="line"></span><br><span class="line">        final Path docDir &#x3D; Paths.get(docsPath);</span><br><span class="line">        Directory indexDir &#x3D; FSDirectory.open(Paths.get(indexPath));</span><br><span class="line">        &#x2F;&#x2F; 使用标准分析器</span><br><span class="line">        Analyzer analyzer &#x3D; new StandardAnalyzer();</span><br><span class="line"></span><br><span class="line">        IndexWriterConfig iwc &#x3D; new IndexWriterConfig(analyzer);</span><br><span class="line">        &#x2F;&#x2F; 每次都重新创建索引</span><br><span class="line">        iwc.setOpenMode(IndexWriterConfig.OpenMode.CREATE);</span><br><span class="line">        &#x2F;&#x2F; 创建IndexWriter用于写索引</span><br><span class="line">        IndexWriter writer &#x3D; new IndexWriter(indexDir, iwc);</span><br><span class="line"></span><br><span class="line">        System.out.println(&quot;index start...&quot;);</span><br><span class="line">        &#x2F;&#x2F; 遍历数据目录，对目录下的每个文件进行索引</span><br><span class="line">        Files.walkFileTree(docDir, new SimpleFileVisitor&lt;Path&gt;() &#123;</span><br><span class="line">            @Override</span><br><span class="line">            public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException &#123;</span><br><span class="line">                indexDoc(writer, file);</span><br><span class="line">                return FileVisitResult.CONTINUE;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        writer.close();</span><br><span class="line"></span><br><span class="line">        System.out.println(&quot;index ends.&quot;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private static void indexDoc(IndexWriter writer, Path file) throws IOException &#123;</span><br><span class="line">        try (InputStream stream &#x3D; Files.newInputStream(file)) &#123;</span><br><span class="line">            System.out.println(&quot;indexing file &quot; + file);</span><br><span class="line">            &#x2F;&#x2F; 创建文档对象</span><br><span class="line">            Document doc &#x3D; new Document();</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F; 将文件绝对路径加入到文档中</span><br><span class="line">            Field pathField &#x3D; new StringField(&quot;path&quot;, file.toString(), Field.Store.YES);</span><br><span class="line">            doc.add(pathField);</span><br><span class="line">            &#x2F;&#x2F; 将文件内容加到文档中</span><br><span class="line">            Field contentsField &#x3D; new TextField(&quot;contents&quot;, new BufferedReader(new InputStreamReader(stream)));</span><br><span class="line">            doc.add(contentsField);</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F; 将文档写入索引中</span><br><span class="line">            writer.addDocument(doc);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这段代码的功能是遍历4首诗对应的文件，对其进行分词、索引，最终形成索引文件，供以后检索。里面有几个API比较关键，这里稍作一下介绍：</p>
<ul>
<li><em>FSDirectory</em>：该类实现了索引文件存储到文件系统的功能。我们无需关注底层文件系统的类型，该类会帮我们处理好。当然还有其它几个类型的Directory，以后再介绍。</li>
<li><em>StandardAnalyzer</em>：Lucene内置的标准分词器，其分词的方法是去掉停用词（stop word），全部转化为小写，根据空白字符分成一个个词/词组。Lucene还支持好几种其它分词器，我们也可以实现自己的分词器，以后再介绍。</li>
<li><em>IndexWriter</em>：该类是索引（<em>此处为动词</em>）文件的核心类，负责索引的创建和维护。</li>
</ul>
<p>我们可以这样理解Lucene里面的组织形式：索引（Index）是最顶级的概念，可以理解为MySQL里面的表；索引里面包含很多个Document，一个Document可以理解为MySQL中的一行记录；一个Document里面可以包含很多个Field，每一个Field都是一个类似Map的结构，由字段名和字段内容组成，内容可再嵌套。在MySQL中，表结构是确定的，每一行记录的格式都是一样的，但Lucene没有这个要求，每个Document里面的字段可以完全不一样，即所谓的”<em>flexible schema</em>“。</p>
<p>在上述代码运行完之后，我们就生成了一个名叫<strong>poems-index</strong>的索引，该索引里面包含4个Document，每个Document对应一首短诗。每个Document由<em>path</em>和<em>contents</em>两个字段组成，<em>path</em>里面存储的是诗歌文件的绝对路径，<em>contents</em>里面存储的是诗歌的内容。最终生成的索引目录包含如下一些文件：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/13png.png" alt="图片"></p>
<p>这样后台索引构建的工作就算完成了，接下来我们来看一下如何利用索引进行高效的搜索：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 省略包等信息，完整文件见源文件</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * Minimal Search Files code</span><br><span class="line"> **&#x2F;</span><br><span class="line">public class SearchFilesMinimal &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        &#x2F;&#x2F; 索引保存目录</span><br><span class="line">        final String indexPath &#x3D; &quot;&#x2F;Users&#x2F;allan&#x2F;Git&#x2F;allan&#x2F;github&#x2F;CodeSnippet&#x2F;Java&#x2F;lucene-learning&#x2F;indices&#x2F;poems-index&quot;;</span><br><span class="line">        &#x2F;&#x2F; 搜索的字段</span><br><span class="line">        final String searchField &#x3D; &quot;contents&quot;;</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 从索引目录读取索引信息</span><br><span class="line">        IndexReader indexReader &#x3D; DirectoryReader.open(FSDirectory.open(Paths.get(indexPath)));</span><br><span class="line">        &#x2F;&#x2F; 创建索引查询对象</span><br><span class="line">        IndexSearcher searcher &#x3D; new IndexSearcher(indexReader);</span><br><span class="line">        &#x2F;&#x2F; 使用标准分词器</span><br><span class="line">        Analyzer analyzer &#x3D; new StandardAnalyzer();</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 从终端获取查询语句</span><br><span class="line">        BufferedReader in &#x3D; new BufferedReader(new InputStreamReader(System.in));</span><br><span class="line">        &#x2F;&#x2F; 创建查询语句解析对象</span><br><span class="line">        QueryParser queryParser &#x3D; new QueryParser(searchField, analyzer);</span><br><span class="line">        while (true) &#123;</span><br><span class="line">            System.out.println(&quot;Enter query: &quot;);</span><br><span class="line"></span><br><span class="line">            String input &#x3D; in.readLine();</span><br><span class="line">            if (input &#x3D;&#x3D; null) &#123;</span><br><span class="line">                break;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            input &#x3D; input.trim();</span><br><span class="line">            if (input.length() &#x3D;&#x3D; 0) &#123;</span><br><span class="line">                break;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F; 解析用户输入的查询语句：build query</span><br><span class="line">            Query query &#x3D; queryParser.parse(input);</span><br><span class="line">            System.out.println(&quot;searching for: &quot; + query.toString(searchField));</span><br><span class="line">            &#x2F;&#x2F; 查询</span><br><span class="line">            TopDocs results &#x3D; searcher.search(query, 10);</span><br><span class="line">            ScoreDoc[] hits &#x3D; results.scoreDocs;</span><br><span class="line">            if (results.totalHits.value &#x3D;&#x3D; 0) &#123;</span><br><span class="line">                System.out.println(&quot;no result matched!&quot;);</span><br><span class="line">                continue;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F; 输出匹配到的结果</span><br><span class="line">            System.out.println(results.totalHits.value + &quot; results matched: &quot;);</span><br><span class="line">            for (ScoreDoc hit : hits) &#123;</span><br><span class="line">                Document doc &#x3D; searcher.doc(hit.doc);</span><br><span class="line">                System.out.println(&quot;doc&#x3D;&quot; + hit.doc + &quot; score&#x3D;&quot; + hit.score + &quot; file: &quot; + doc.get(&quot;path&quot;));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这段代码的核心流程是先从上一步创建的索引目录加载构建好的索引，然后获取用户输入并解析为查询语句（build query），接着运行查询（run query），如果有匹配到的，就输出匹配的结果。这里对查询比较重要的API做下简单说明：</p>
<ul>
<li><em>IndexReader</em>：打开一个索引；</li>
<li><em>IndexSearcher</em>：搜索<em>IndexReader</em>打开的索引，返回<em>TopDocs</em>对象；</li>
<li><em>QueryParser</em>：该类的parse方法解析用户输入的查询语句，返回一个<em>Query</em>对象；</li>
</ul>
<p>下面我们来运行一下程序：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Enter query: </span><br><span class="line">love</span><br><span class="line">searching for: love</span><br><span class="line">2 results matched: </span><br><span class="line">doc&#x3D;0 score&#x3D;0.48849338 file: &#x2F;Users&#x2F;allan&#x2F;Git&#x2F;allan&#x2F;github&#x2F;CodeSnippet&#x2F;Java&#x2F;lucene-learning&#x2F;data&#x2F;poems&#x2F;Love&#39;sSecret.txt</span><br><span class="line">doc&#x3D;1 score&#x3D;0.41322997 file: &#x2F;Users&#x2F;allan&#x2F;Git&#x2F;allan&#x2F;github&#x2F;CodeSnippet&#x2F;Java&#x2F;lucene-learning&#x2F;data&#x2F;poems&#x2F;FreedomAndLove.txt</span><br></pre></td></tr></table></figure>
<p>我们输入关键字”<em>love</em>“，搜索出来两个Document，分别对应Love’s Secret和Freedom And Love。<em>doc=</em>后面的数字是Document的ID，唯一标识一个Document。<em>score</em>后面的数字是搜索结果与我们搜索的关键字的相关度。然后我们再输入”<em>LOVE</em>“（注意字母都大写了）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Enter query: </span><br><span class="line">Love</span><br><span class="line">searching for: love</span><br><span class="line">2 results matched: </span><br><span class="line">doc&#x3D;0 score&#x3D;0.48849338 file: &#x2F;Users&#x2F;allan&#x2F;Git&#x2F;allan&#x2F;github&#x2F;CodeSnippet&#x2F;Java&#x2F;lucene-learning&#x2F;data&#x2F;poems&#x2F;Love&#39;sSecret.txt</span><br><span class="line">doc&#x3D;1 score&#x3D;0.41322997 file: &#x2F;Users&#x2F;allan&#x2F;Git&#x2F;allan&#x2F;github&#x2F;CodeSnippet&#x2F;Java&#x2F;lucene-learning&#x2F;data&#x2F;poems&#x2F;FreedomAndLove.txt</span><br></pre></td></tr></table></figure>
<p>可以看到搜索结果与之前是一样的，这是因为我们搜索时使用了和构建索引时相同的分词器<em>StandardAnalyzer</em>，该分词器会将所有词转化为小写。然后我们再尝试一下其它搜索：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Enter query: </span><br><span class="line">fog</span><br><span class="line">searching for: fog</span><br><span class="line">1 results matched: </span><br><span class="line">doc&#x3D;2 score&#x3D;0.67580885 file: &#x2F;Users&#x2F;allan&#x2F;Git&#x2F;allan&#x2F;github&#x2F;CodeSnippet&#x2F;Java&#x2F;lucene-learning&#x2F;data&#x2F;poems&#x2F;Fog.txt</span><br><span class="line"></span><br><span class="line">Enter query: </span><br><span class="line">above</span><br><span class="line">searching for: above</span><br><span class="line">1 results matched: </span><br><span class="line">doc&#x3D;3 score&#x3D;0.6199532 file: &#x2F;Users&#x2F;allan&#x2F;Git&#x2F;allan&#x2F;github&#x2F;CodeSnippet&#x2F;Java&#x2F;lucene-learning&#x2F;data&#x2F;poems&#x2F;OnDeath.txt</span><br><span class="line"></span><br><span class="line">Enter query: </span><br><span class="line">death</span><br><span class="line">searching for: death</span><br><span class="line">1 results matched: </span><br><span class="line">doc&#x3D;3 score&#x3D;0.6199532 file: &#x2F;Users&#x2F;allan&#x2F;Git&#x2F;allan&#x2F;github&#x2F;CodeSnippet&#x2F;Java&#x2F;lucene-learning&#x2F;data&#x2F;poems&#x2F;OnDeath.txt</span><br><span class="line"></span><br><span class="line">Enter query: </span><br><span class="line">abc</span><br><span class="line">searching for: abc</span><br><span class="line">no result matched!</span><br></pre></td></tr></table></figure>
<p>都工作正常，最后一个关键字”<em>abc</em>“没有搜到，因为原文中也没有这个词。我们再来看一个复杂点的查询：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Enter query: </span><br><span class="line">+love -seek</span><br><span class="line">searching for: +love -seek</span><br><span class="line">1 results matched: </span><br><span class="line">doc&#x3D;1 score&#x3D;0.41322997 file: &#x2F;Users&#x2F;allan&#x2F;Git&#x2F;allan&#x2F;github&#x2F;CodeSnippet&#x2F;Java&#x2F;lucene-learning&#x2F;data&#x2F;poems&#x2F;FreedomAndLove.txt</span><br></pre></td></tr></table></figure>
<p>这里我们输入的关键字为”<em>+love -seek</em>“，这是一个高级一点的查询，含义是“包含love但不包含seek”，于是就只搜出来Freedom And Love一首诗了。</p>
<h2 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import lucene</span><br><span class="line">from java.nio.file import Paths</span><br><span class="line"># from org.apache.lucene.analysis.cjk import CJKAnalyzer</span><br><span class="line">from org.apache.lucene.document import Document, Field, FieldType, StoredField</span><br><span class="line">from org.apache.lucene.document import TextField, FloatPoint, IntPoint</span><br><span class="line">from org.apache.lucene.document import NumericDocValuesField</span><br><span class="line">from org.apache.lucene.document import FloatDocValuesField</span><br><span class="line">from org.apache.lucene.document import SortedSetDocValuesField</span><br><span class="line">from org.apache.lucene.index import FieldInfo, IndexWriter, IndexWriterConfig</span><br><span class="line">from org.apache.lucene.store import SimpleFSDirectory</span><br><span class="line">from org.apache.lucene.util import Version</span><br><span class="line">from org.apache.lucene.search import IndexSearcher, Sort, SortField</span><br><span class="line">from org.apache.lucene.search import BooleanQuery</span><br><span class="line">from org.apache.lucene.search import BooleanClause</span><br><span class="line">from org.apache.lucene.search import TopFieldCollector, BoostQuery</span><br><span class="line">from org.apache.lucene.queryparser.classic import QueryParser</span><br><span class="line">from org.apache.lucene.index import DirectoryReader</span><br><span class="line">from org.apache.lucene.analysis.core import WhitespaceAnalyzer</span><br><span class="line">from org.apache.lucene.util import BytesRef</span><br><span class="line">from strsimpy.levenshtein import Levenshtein</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class LuceneECSearch(object):</span><br><span class="line">    def __init__(self, indir, seg_model_path, mode&#x3D;&#39;search&#39;,</span><br><span class="line">                 maxrecall&#x3D;10000, maxdoc&#x3D;30):</span><br><span class="line">        lucene.initVM()</span><br><span class="line">        self.indir &#x3D; indir</span><br><span class="line">        self.lucene_analyzer &#x3D; WhitespaceAnalyzer()</span><br><span class="line">        if mode &#x3D;&#x3D; &#39;search&#39;:</span><br><span class="line">            self.indir &#x3D; SimpleFSDirectory(Paths.get(indir))</span><br><span class="line">        self.maxdoc &#x3D; maxdoc</span><br><span class="line">        self.maxrecal &#x3D; maxrecall</span><br><span class="line">        self.levenshtein &#x3D; Levenshtein()</span><br><span class="line"></span><br><span class="line">    def search(self, query):</span><br><span class="line">        # 构造Query</span><br><span class="line">        query &#x3D; self.processor.process(query, &#39;en&#39;)</span><br><span class="line">        boolean_query &#x3D; BooleanQuery.Builder()</span><br><span class="line">        simple_query &#x3D; QueryParser(</span><br><span class="line">            &quot;en_tokenized&quot;,</span><br><span class="line">            self.lucene_analyzer).parse(query)</span><br><span class="line">        keyword_query &#x3D; QueryParser(</span><br><span class="line">            &quot;keyword&quot;,</span><br><span class="line">            self.lucene_analyzer).parse(query)</span><br><span class="line">        boost_keyword_query &#x3D; BoostQuery(keyword_query, 2.0)</span><br><span class="line">        boolean_query.add(simple_query, BooleanClause.Occur.SHOULD)</span><br><span class="line">        boolean_query.add(boost_keyword_query, BooleanClause.Occur.SHOULD)</span><br><span class="line"></span><br><span class="line">        # searcher</span><br><span class="line">        lucene_searcher &#x3D; IndexSearcher(</span><br><span class="line">            DirectoryReader.open(self.indir))</span><br><span class="line">        sorter &#x3D; Sort([</span><br><span class="line">            SortField.FIELD_SCORE,</span><br><span class="line">            SortField(&#39;origin_score&#39;, SortField.Type.FLOAT, True),</span><br><span class="line">            SortField(&#39;en_sent_lenth&#39;, SortField.Type.INT, True)])</span><br><span class="line"></span><br><span class="line">        # rerank</span><br><span class="line">        collector &#x3D; TopFieldCollector.create(</span><br><span class="line">            sorter, self.maxrecal, self.maxrecal)</span><br><span class="line">        lucene_searcher.search(boolean_query.build(), collector)</span><br><span class="line">        scoreDocs &#x3D; collector.topDocs().scoreDocs</span><br><span class="line">        result &#x3D; []</span><br><span class="line">        for hit in scoreDocs:</span><br><span class="line">            doc &#x3D; lucene_searcher.doc(hit.doc)</span><br><span class="line">            keyword_info &#x3D; json.loads(doc.get(&#39;keyword_info&#39;))</span><br><span class="line">            keyword_score &#x3D; 0.0</span><br><span class="line">            for word, score in keyword_info.items():</span><br><span class="line">                word_splits &#x3D; word.split(&#39; &#39;)</span><br><span class="line">                for split in word_splits:</span><br><span class="line">                    if split &#x3D;&#x3D; query:</span><br><span class="line">                        keyword_score +&#x3D; score</span><br><span class="line">            json_answer &#x3D; &#123;</span><br><span class="line">                &#39;en_tokenized&#39;: doc.get(&quot;en_tokenized&quot;),</span><br><span class="line">                &#39;en_sent&#39;: doc.get(&quot;en_sent&quot;),</span><br><span class="line">                &#39;en_sent_lenth&#39;: doc.get(&quot;en_sent_lenth&quot;),</span><br><span class="line">                &#39;cn_tokenized&#39;: doc.get(&quot;cn_tokenized&quot;),</span><br><span class="line">                &#39;cn_sent&#39;: doc.get(&quot;cn_sent&quot;),</span><br><span class="line">                &#39;confidence&#39;: int(doc.get(&quot;confidence&quot;)),</span><br><span class="line">                &#39;origin_score&#39;: float(doc.get(&quot;origin_score&quot;)),</span><br><span class="line">                &#39;new_score&#39;: float(doc.get(&quot;new_score&quot;))&#125;</span><br><span class="line">            result.append(json_answer)</span><br><span class="line">        result &#x3D; self.rerank(query, result)</span><br><span class="line">        return result</span><br><span class="line"></span><br><span class="line">    def rerank(self, query, candidates):</span><br><span class="line">        candidates.sort(key&#x3D;lambda x: x[&quot;origin_score&quot;], reverse&#x3D;True)</span><br><span class="line">        # candidates &#x3D; candidates[:self.maxdoc * 2]</span><br><span class="line">        candidates.sort(key&#x3D;lambda x: x[&quot;new_score&quot;], reverse&#x3D;True)</span><br><span class="line">        # 先把有释义的拿出来</span><br><span class="line">        return candidates</span><br><span class="line"></span><br><span class="line">    def build(self, docdir, modeldir):</span><br><span class="line">        if os.path.exists(self.indir):</span><br><span class="line">            shutil.rmtree(self.indir)</span><br><span class="line">        lucene.initVM()</span><br><span class="line">        INDEXIDR &#x3D; Paths.get(self.indir)</span><br><span class="line">        indexdir &#x3D; SimpleFSDirectory(INDEXIDR)</span><br><span class="line"></span><br><span class="line">        config &#x3D; IndexWriterConfig(self.lucene_analyzer)</span><br><span class="line">        index_writer &#x3D; IndexWriter(indexdir, config)</span><br><span class="line"></span><br><span class="line">        cnt &#x3D; 0</span><br><span class="line">        with open(docdir, &#39;r&#39;, encoding&#x3D;&#39;utf-8&#39;) as f:</span><br><span class="line">            for line in f.readlines():</span><br><span class="line">                line &#x3D; line.strip()</span><br><span class="line">                try:</span><br><span class="line">                    data_json &#x3D; json.loads(line)</span><br><span class="line">                except Exception:</span><br><span class="line">                    print(&#39;Json load error!&#39;)</span><br><span class="line">                    continue</span><br><span class="line">                try:</span><br><span class="line">                    document &#x3D; Document()</span><br><span class="line"></span><br><span class="line">                    en_tokenized &#x3D; data_json[&#39;en_tokenized&#39;]</span><br><span class="line">                    # TODO 去掉停用词</span><br><span class="line">                    document.add(Field(&quot;en_tokenized&quot;, en_tokenized,</span><br><span class="line">                                       TextField.TYPE_STORED))</span><br><span class="line"></span><br><span class="line">                    en_sent &#x3D; data_json[&#39;en_sent&#39;]</span><br><span class="line">                    document.add(Field(&quot;en_sent&quot;, en_sent,</span><br><span class="line">                                       TextField.TYPE_STORED))</span><br><span class="line"></span><br><span class="line">                    en_sent_lenth &#x3D; len(en_tokenized)</span><br><span class="line">                    document.add(IntPoint(&quot;en_sent_lenth&quot;, en_sent_lenth))</span><br><span class="line">                    document.add(NumericDocValuesField(</span><br><span class="line">                        &quot;en_sent_lenth&quot;, en_sent_lenth))</span><br><span class="line">                    document.add(StoredField(&quot;en_sent_lenth&quot;, en_sent_lenth))</span><br><span class="line"></span><br><span class="line">                    cn_tokenized &#x3D; data_json[&#39;cn_tokenized&#39;]</span><br><span class="line">                    document.add(Field(&quot;cn_tokenized&quot;, cn_tokenized,</span><br><span class="line">                                       TextField.TYPE_STORED))</span><br><span class="line"></span><br><span class="line">                    cn_sent &#x3D; data_json[&#39;cn_sent&#39;]</span><br><span class="line">                    document.add(Field(&quot;cn_sent&quot;, cn_sent,</span><br><span class="line">                                       TextField.TYPE_STORED))</span><br><span class="line"></span><br><span class="line">                    confidence &#x3D; int(data_json[&#39;confidence&#39;])</span><br><span class="line">                    document.add(IntPoint(&quot;confidence&quot;, confidence))</span><br><span class="line">                    document.add(NumericDocValuesField(</span><br><span class="line">                        &quot;confidence&quot;, confidence))</span><br><span class="line">                    document.add(StoredField(&quot;confidence&quot;, confidence))</span><br><span class="line"></span><br><span class="line">                    origin_score &#x3D; float(data_json[&#39;origin_score&#39;])</span><br><span class="line">                    document.add(FloatPoint(&quot;origin_score&quot;, origin_score))</span><br><span class="line">                    document.add(FloatDocValuesField(&quot;origin_score&quot;, origin_score))</span><br><span class="line">                    document.add(StoredField(&quot;origin_score&quot;, origin_score))</span><br><span class="line"></span><br><span class="line">                    keyword_info &#x3D; &#123;&#125;</span><br><span class="line">                    for keyword in data_json[&#39;keyword&#39;]:</span><br><span class="line">                        keyword_text &#x3D; keyword[&#39;text&#39;]</span><br><span class="line">                        keyword_score &#x3D; keyword[&#39;score&#39;]</span><br><span class="line">                        document.add(</span><br><span class="line">                            SortedSetDocValuesField(</span><br><span class="line">                                &quot;keyword&quot;, BytesRef(keyword_text)))</span><br><span class="line">                        document.add(</span><br><span class="line">                            StoredField(&quot;keyword&quot;, keyword_text))</span><br><span class="line">                        document.add(</span><br><span class="line">                            Field(&quot;keyword&quot;, keyword_text,</span><br><span class="line">                                  TextField.TYPE_STORED))</span><br><span class="line">                        keyword_info[keyword_text] &#x3D; keyword_score</span><br><span class="line">                    keyword_info_str &#x3D; json.dumps(</span><br><span class="line">                        keyword_info, ensure_ascii&#x3D;False)</span><br><span class="line">                    document.add(</span><br><span class="line">                        Field(</span><br><span class="line">                            &quot;keyword_info&quot;,</span><br><span class="line">                            keyword_info_str,</span><br><span class="line">                            TextField.TYPE_STORED))</span><br><span class="line">                    </span><br><span class="line">                    new_score &#x3D; ... # 经过模型计算出来的        </span><br><span class="line">0                    new_score &#x3D; </span><br><span class="line">                    document.add(FloatPoint(&quot;new_score&quot;, new_score))</span><br><span class="line">                    document.add(</span><br><span class="line">                        FloatDocValuesField(&quot;new_score&quot;, new_score))</span><br><span class="line">                    document.add(</span><br><span class="line">                        StoredField(&quot;new_score&quot;, new_score))</span><br><span class="line">                    index_writer.addDocument(document)</span><br><span class="line">                except Exception:</span><br><span class="line">                    print(&#39;Index write error!&#39;)</span><br><span class="line">                    continue</span><br><span class="line">                cnt +&#x3D; 1</span><br><span class="line">                if cnt % 1000 &#x3D;&#x3D; 0:</span><br><span class="line">                    print(&#39;Writing &#39;, cnt)</span><br><span class="line"></span><br><span class="line">        index_writer.commit()</span><br></pre></td></tr></table></figure>
<pre><code>    index_writer.close()
</code></pre>]]></content>
      <tags>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title>EM学习——基础学习</title>
    <url>/2020/04/19/EM%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>接下来几天将复习Graphical Model的一系列模型。今天先复习一下EM算法。</p>
<a id="more"></a>
<h1 id="Graphical-Model和Latent-Variable"><a href="#Graphical-Model和Latent-Variable" class="headerlink" title="Graphical Model和Latent Variable"></a>Graphical Model和Latent Variable</h1><p>首先，我们了解一下graphical model的技术思路：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/em/1.png" alt="图片"></p>
<p>从上图中可以看到，Graphical Model可以分成两类，一类是有向图，典型代表是HMM；一类是无向图，典型代表是CRF。当我们能找出状态之间的依赖关系时，可以使用HMM，因为HMM可以利用conditional independence条件独立性质；当我们刻画不出依赖关系时，可以使用CRF。其中，从HMM可以推导出MEMM模型，从MEMM可以推导出CRF模型。我们通常使用的CRF是Linear CRF，其中的Linear是从Log Linear Model中得来的。Log Linear Model可推出的另外一个模型是logistic regressioin，区别是逻辑回归针对static data，CRF针对sequencial data。整个Graphical Model的参数估计使用的是EM算法，推理使用Viterbi算法。</p>
<p>在有隐含变量（latent variable）的模型中，我们经常使用EM算法。那什么是latent variable呢？一个Latent variable model可以用z~x表示，意思是用看不见的z去生成看得见的x。以下图为例，假设任务是生成人脸，那么隐含变量就包含性别、眼睛颜色、头发颜色等信息：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/em/2.png" alt="图片"></p>
<p>EM的核心是计算参数theta，如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/em/3.png" alt="图片"></p>
<p>在计算时分为两种情况：</p>
<ul>
<li>Complete case：(z, x)都是可观测的，如逻辑回归（已知x预测$\theta​$），一般使用MLE</li>
<li>Imcomplete case：x是可观测的，z是不可观测的，使用EM style算法</li>
</ul>
<h1 id="MLE-for-Complete-and-Imcomplete-Case"><a href="#MLE-for-Complete-and-Imcomplete-Case" class="headerlink" title="MLE for Complete and Imcomplete Case"></a>MLE for Complete and Imcomplete Case</h1><p>下面我们来看一下MLE的求解思路。MLE的核心思想是求解$argmax_{\theta}(D|\theta)​$，也就是求最大化D情况下的theta值。比如在逻辑回归中就是求解$argmax_{\theta}P({x_i, y_i}_{i=1}^n|\theta)​$。而在graphic model中变为$argmax_{\theta}P({x, z|\theta})​$。在Complete Case和Incomplete Case中MLE可以分别继续写成如下形式（可以看到$logp(z|\theta_z)​$是典型的MLE，我们可以通过数据统计反推出theta值）：<img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/em/4.png" alt="图片"></p>
<blockquote>
<p>【注】：在图模型中联合概率很难表示，有几个技巧可以尝试：<br>（1）使用conditional independence对形如$P(a,b,c,d,e|\theta)$进行转换：例如n-gram语言模型P(I hate ad|y=垃圾)，在朴素贝叶斯中就会近似为P(I|y=垃圾)P(hate|y=垃圾)P(ad|y=垃圾)<br>（2）使用D-seperation或Markov Blanket对形如$P(a|b,c,d,e,f)$进行转换：比如发现d、e、f对a没有影响，转换为P(a|b,c)，进而再转化为函数形式f(\theta)，再使用优化方法找出解或者近似解<br>（3）在Incomplete Case中，由于z也是未知的，所以把z边缘化了<br>（4）上面图中$p(z|\theta)$和$p(x|z, \theta)$也可以认为是两个函数$f(\theta)和g(\theta)$，也可以使用优化算法进行求解。对于Incomplete Case，$f(\theta)和g(\theta)$很难求解，所以使用EM-style算法求解</p>
</blockquote>
<p>在Complete Case中基本是通过统计方式计算的，以下图为例（假设观测到的是单词，隐变量是词性，建立一个HMM模型，我们可以统计词性之间的Transition Probabilty和词性到单词的Emition Probability）：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/em/5.png" alt="图片"></p>
<h1 id="EM-Derivation"><a href="#EM-Derivation" class="headerlink" title="EM Derivation"></a>EM Derivation</h1><p>EM中一共有3类变量：</p>
<ul>
<li>$\theta​$: model parameter</li>
<li>$z​$: latent variable</li>
<li>$x$: obversation</li>
</ul>
<p>目标：用MLE最大化$L(\theta)=logP(x|\theta)​$，也就是求解$argmax_{\theta}L(\theta)=argmax_{\theta}logP(x|\theta)​$。</p>
<p>假设：使用iterative算法，已经求解出$\theta_1$,$\theta_2$, …, $\theta_n$，则如何计算$\theta_{n+1}$?</p>
<p>已知：$\theta_n$是对应于$L(\theta_n)$的解，那么$\theta_{n+1}$就是使得$argmax_{\theta}L(\theta) - argmax_{\theta_{n+1}}L(\theta_{n+1})$最大的值（可以理解为使得目标函数提升最大的值），且$argmax_{\theta}L(\theta) - argmax_{\theta_{n+1}}L(\theta_{n+1}) = logP(x|\theta) - logP(x|\theta_n)$，上面思考过程来源于下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/em/6.png" alt="图片"></p>
<p>从上面图中看到，当我们已经优化到log sum的形式时，就很难继续求解了。所以我们想到用一个不等式去近似它。这个不等式就叫做Jensen’s Inequality：<img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/em/7.png" alt="图片">（【注】: Jensen’s Inequality可以从凸函数的性质得出来）</p>
<p>经过Jensen’s neuqlity简化，上图中式子可推导成：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/em/8.png" alt="图片"></p>
<p>继续推导可以得到下图（第6行是由于是常量所以消去了）：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/em/9.png" alt="图片"></p>
<p>我们可以把最后一项写成Expectation的形式，怎么理解呢？$P(z|x, \theta_n)$是所有可能的z的一个概率分布，我们把每一个可能的z带入到$logP(x, z|\theta)$中，我们就可以计算$logP(x, z|\theta)$的 一个平均值，也就是期望值。由于已知$x$、$\theta_n$，我们可以求出z的期望值。那么对于$logP(x,z|\theta)$来说，由于$x$、$z$、$\theta_n$都是已知的，想求解最大似然，就变成了一个complete case，我们可以通过MLE求解使其最大的$\theta$。</p>
<blockquote>
<p>【注】：你可能想问，已知$x$, $\theta_n$，是怎么求出$P(z|x, \theta_n)$的呢？在具体问题中，我们通常会把$P(z|x, \theta_n)$写成一个函数的形式，自然就可以求出来了</p>
</blockquote>
<p>EM算法其实就是两个步骤的一个循环，已知$x$, $\theta$求$z$的期望，再用这个$z$求解$\theta$，再去迭代得求解$z$，如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/em/10.png" alt="图片"></p>
<p>EM算法一定是收敛的，它本质上是coordinate descent，即按照坐标轴去优化。</p>
<h1 id="K-means"><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h1><p>K-means整体上就是选中心点—&gt;根据与中心点的举例聚类—&gt;重新计算中心点，这样一个迭代的过程，如下图所示：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/em/11.png" alt="图片"></p>
<h2 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h2><p>Kmeans的代价函数如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/em/12.png" alt="图片"></p>
<p>可以看到有两个参数：</p>
<ul>
<li>$\mu_k$表示中心点，可以看成是模型参数</li>
<li>$\gamma_{ik}$表示对$\mu_k$的选择，可以看成是隐变量</li>
</ul>
<h2 id="K-means和EM的关系"><a href="#K-means和EM的关系" class="headerlink" title="K-means和EM的关系"></a>K-means和EM的关系</h2><p>K-means的迭代过程就是先选择$\mu_k$，再计算$\gamma_{ik}$，再根据聚成的簇重新选择$\mu_k$。但K-means是一个EM-style的算法，而不是一个严格意义的EM。因为在EM中，要计算的是隐变量的期望值，即对于$\gamma_{ik}$表示$x_i$属于第$k$个cluster的概率，则$\gamma_{i0}=0.8$, $\gamma_{i1}=0.2$….（有点像GMM哈）。但在K-means中，我们直接令$\gamma_{i0}=1$了。因此Kmeans也叫Hard-Clustering，GMM也叫Soft-Clustering。如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/em/13.png" alt="图片"></p>
]]></content>
      <tags>
        <tag>EM</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：ALBert</title>
    <url>/2020/04/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AALBert/</url>
    <content><![CDATA[<p>Albert在quora question pair上得分最高，我确始终没看过论文，今天就来补一补。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://paperswithcode.com/paper/albert-a-lite-bert-for-self-supervised" target="_blank" rel="noopener">https://paperswithcode.com/paper/albert-a-lite-bert-for-self-supervised</a><br>语篇分析：<a href="http://www.shuang0420.com/2017/09/20/NLP%20%E7%AC%94%E8%AE%B0%20-%20Discourse%20Analysis/" target="_blank" rel="noopener">http://www.shuang0420.com/2017/09/20/NLP%20%E7%AC%94%E8%AE%B0%20-%20Discourse%20Analysis/</a><br><a href="https://kexue.fm/archives/7187" target="_blank" rel="noopener">https://kexue.fm/archives/7187</a></p>
</blockquote>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>ALBERT主要是从减少模型参数的角度对BERT进行优化，主要有如下3个优化点：</p>
<ul>
<li>Factorized embedding parameterization：</li>
<li>Cross-layer parameter sharing</li>
<li>Inter-sentence coherence loss</li>
</ul>
<h1 id="Factorized-embedding-parameterization"><a href="#Factorized-embedding-parameterization" class="headerlink" title="Factorized embedding parameterization"></a>Factorized embedding parameterization</h1><p>在BERT、XLNet、RoBERTa等模型中，由于模型结构的限制，WordePiece embedding的大小$E$总是与隐层大小$H$相同，即$E \equiv H$。从建模的角度考虑，词嵌入学习的是单词与上下文无关的表示，而隐层则是学习与上下文相关的表示。显然后者更加复杂，需要更多的参数，也就是说模型应当增大隐层大小$H$ ，或者说满足$H \gg E$。但实际上词汇表的大小通$V$常非常大，如果$E=H$的话，增加隐层大小$H$后将会使embedding matrix的维度$V \times E$非常巨大。</p>
<p>因此本文想要打破$E$与$H$之间的绑定关系，从而减小模型的参数量，同时提升模型表现。具体做法是将embedding matrix分解为两个大小分别为$V \times E$和$E \times H$矩阵，也就是说先将单词投影到一个低维的embedding空间$E$，再将其投影到高维的隐藏空间$H$ 。这使得embedding matrix的维度从$O(V \times H)$减小到$O(V \times E+E \times H)$。当$H \gg E$时，参数量减少非常明显。在实现时，随机初始化$V \times E$和$E \times H$的矩阵，计算某个单词的表示需用一个单词的one-hot向量乘以$V \times E$维的矩阵（也就是lookup），再用得到的结果乘$E \times H$维的矩阵即可。两个矩阵的参数通过模型学习。</p>
<h1 id="Cross-layer-parameter-sharing"><a href="#Cross-layer-parameter-sharing" class="headerlink" title="Cross-layer parameter sharing"></a>Cross-layer parameter sharing</h1><p>本文提出的另一个减少参数量的方法就是层之间的参数共享，即多个层使用相同的参数。参数共享有三种方式：只共享feed-forward network的参数、只共享attention的参数、共享全部参数。ALBERT默认是共享全部参数的，在后续实验结果中我们可以看到几种方式的模型表现。</p>
<p>如下图所示，实验表明加入参数共享之后，每一层的输入embedding和输出embedding的L2距离和余弦相似度都比BERT稳定了很多。这证明参数共享能够使模型参数更加稳定。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ALBERT/1.png" alt="图片"></p>
<h1 id="Inter-sentence-coherence-loss"><a href="#Inter-sentence-coherence-loss" class="headerlink" title="Inter-sentence coherence loss"></a>Inter-sentence coherence loss</h1><p>除了减少模型参数外，本外还对BERT的预训练任务Next-sentence prediction (NSP)进行了改进。在BERT中，NSP任务的正例是文章中连续的两个句子，而负例则是从两篇文档中各选一个句子构造而成。在先前的研究中，已经证明NSP是并不是一个合适的预训练任务。本文推测其原因是模型在判断两个句子的关系时不仅考虑了两个句子之间的连贯性（coherence），还会考虑到两个句子的话题（topic）。而两篇文档的话题通常不同，模型会更多的通过话题去分析两个句子的关系，而不是句子间的连贯性，这使得NSP任务变成了一个相对简单的任务。</p>
<p>因此本文提出了Sentence-order prediction (SOP)来取代NSP。具体来说，其正例与NSP相同，但负例是通过选择一篇文档中的两个连续的句子并将它们的顺序交换构造的。这样两个句子就会有相同的话题，模型学习到的就更多是句子间的连贯性。</p>
<h1 id="Model-setup"><a href="#Model-setup" class="headerlink" title="Model setup"></a>Model setup</h1><p>本文为ALBERT选择了四种参数设置：base, large, xlarge和xxlarge。如下图所示</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ALBERT/2.png" alt="图片"></p>
<h1 id="Experimental-Results"><a href="#Experimental-Results" class="headerlink" title="Experimental Results"></a>Experimental Results</h1><h2 id="BERT-vs-ALBERT"><a href="#BERT-vs-ALBERT" class="headerlink" title="BERT vs. ALBERT"></a>BERT vs. ALBERT</h2><p>从下图的实验结果可见，ALBERT的训练速度明显比BERT快，ALBERT-xxlarge的表现更是全方面超过了BERT。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ALBERT/3png.png" alt="图片"></p>
<h2 id="Factorized-Embedding-Parameterization"><a href="#Factorized-Embedding-Parameterization" class="headerlink" title="Factorized Embedding Parameterization"></a>Factorized Embedding Parameterization</h2><p>从下图实验结果可见，对于不共享参数的情况，$E$几乎是与大越好；而共享参数之后，$E$太大反而会使模型表现变差，$E=128$模型表现最好，因此ALBERT的默认参数设置中$E=128$。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ALBERT/4.png" alt="图片"></p>
<p>考虑到ALBERT-base的$H=768$ ，那么$E=768$时，模型应该可以看作没有减少embedding参数量的情况。而不共享参数的实验结果表明此时模型表现更好，那么似乎说明了Factorized embedding在一定程度上降低了模型的表现。</p>
<h2 id="Cross-Layer-Parameter-Sharing"><a href="#Cross-Layer-Parameter-Sharing" class="headerlink" title="Cross-Layer Parameter Sharing"></a>Cross-Layer Parameter Sharing</h2><p>如下图所示，可以看出参数共享几乎也是对模型结果有负面影响的。但是考虑到其能够大幅削减参数量，并且对结果影响不是特别大，因此权衡之下选择了参数共享。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ALBERT/5.png" alt="图片"></p>
<h2 id="SOP"><a href="#SOP" class="headerlink" title="SOP"></a>SOP</h2><p>如下图实验结果所示，如果不使用NSP或SOP作为预训练任务的话，模型在NSP和SOP两个任务上表现都很差；如果使用NSP作为预训练任务的话，模型确实能很好的解决NSP问题，但是在SOP问题上表现却很差，几乎相当于随机猜，因此说明NSP任务确实很难学到句子间的连贯性；而如果用SOP作为预训练任务，则模型也可以较好的解决NSP问题，同时模型在下游任务上表现也更好。说明SOP确实是更好的预训练任务。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ALBERT/6.png" alt="图片"></p>
<h2 id="Effect-Of-Network-Depth-And-Width"><a href="#Effect-Of-Network-Depth-And-Width" class="headerlink" title="Effect Of Network Depth And Width"></a>Effect Of Network Depth And Width</h2><p>从下面两个对比试验结果我们可以看出，增加模型的层数或隐层大小确实能够在一定程度上提升模型的表现。但当大小增加到一定量时，反而会使模型表现变差。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ALBERT/7.png" alt="图片"></p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ALBERT/8.png" alt="图片"></p>
<h2 id="Additional-Training-Data-And-Dropout-Effects"><a href="#Additional-Training-Data-And-Dropout-Effects" class="headerlink" title="Additional Training Data And Dropout Effects"></a>Additional Training Data And Dropout Effects</h2><p>ALBERT训练时还加入了XLNet和RoBERTa训练时用的额外数据，实验表明加入额外数据（W additional data）确实会提升模型表现。此外，作者还观察到模型似乎一直没有过拟合数据，因此去除了Dropout，从对比试验可以看出，去除Dropout（W/O Dropout）后模型表现确实更好。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ALBERT/9.png" alt="图片"></p>
<h2 id="Current-State-Of-The-Art-On-NLU-Tasks"><a href="#Current-State-Of-The-Art-On-NLU-Tasks" class="headerlink" title="Current State-Of-The-Art On NLU Tasks"></a>Current State-Of-The-Art On NLU Tasks</h2><p>最后是ALBERT在各个NLU任务上的表现，几乎都达到了state-of-the-art的表现。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ALBERT/10.png" alt="图片"></p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ALBERT/11.png" alt="图片"></p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><ul>
<li>本文提出了Factorized embedding和层之间参数共享两种削减参数量的方式，在大家都想着把模型做大的时候给大家指出了另一条可行的路，意义重大。但本文提出的两种方法实际上都带来了模型效果的下降，也就是说本文似乎也还没有找到BERT中真正的冗余参数，减少模型参数量这方面还需要更多的研究。</li>
<li>本文提出了SOP，很好地替换了NSP作为预训练任务，给模型表现带来了明显提升。</li>
<li>本文的削减参数使模型表现下降，结果更好主要是靠SOP、更大的$H$ 、更多的数据、去除dropout。那么如果不削减参数的话再使用SOP、加更多的数据、去除dropout呢？</li>
<li>本文的削减参数量参数量带来了模型训练速度的提升，但是ALBERT-xxlarge比BERT-xlarge参数量少了约1000M，而训练速度并没有太大的提升（只有1.2倍）。原因应该是更少的参数量的确能带来速度上的提升，但是本文提出的Factorized embedding引入了额外的矩阵运算，并且同时ALBERT-xxlarge大幅增加了$H$，实际上增加了模型的计算量。</li>
<li>本文还有两个小细节可以学习，一个是在模型不会过拟合的情况下不使用dropout，文中有提到batch_norm+dropout可能带来负面效果，dropout存在训练和推断的不一致问题，也就是“严格来讲训练模型和预测模型并不是同一个模型”，模型变大变深时，这种不一致性可能会进一步放大，所以dropout对于超大模型可能并不是一种有效的防止过拟合的方法；另一个则是warm-start，即在训练深层网络（例如12层）时，可以先训练浅层网络（例如6层），再在其基础上做fine-tune，这样可以加快深层模型的收敛。</li>
</ul>
<h1 id="Extend"><a href="#Extend" class="headerlink" title="Extend"></a>Extend</h1><p>在<a href="https://kexue.fm/archives/7187" target="_blank" rel="noopener">https://kexue.fm/archives/7187</a>中提出了一个能提升albert在下游任务中表现的方法：    在下游任务中，放弃albert的权重共享的约束，也就是把albert当bert用。</p>
]]></content>
      <tags>
        <tag>论文阅读</tag>
        <tag>Bert</tag>
      </tags>
  </entry>
  <entry>
    <title>句子对齐开源代码解读</title>
    <url>/2020/04/15/%E5%8F%A5%E5%AD%90%E5%AF%B9%E9%BD%90%E5%BC%80%E6%BA%90%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/</url>
    <content><![CDATA[<p>最近需要根据句子对齐，给中英句对进行打分，因此看了一下相关的开源项目。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://blog.csdn.net/ykf173/article/details/86747592" target="_blank" rel="noopener">https://blog.csdn.net/ykf173/article/details/86747592</a><br><a href="http://www.cips-cl.org/static/anthology/CCL-2015/CCL-15-019.pdf" target="_blank" rel="noopener">http://www.cips-cl.org/static/anthology/CCL-2015/CCL-15-019.pdf</a></p>
</blockquote>
<h1 id="Gale和Church的句对齐算法"><a href="#Gale和Church的句对齐算法" class="headerlink" title="Gale和Church的句对齐算法"></a>Gale和Church的句对齐算法</h1><blockquote>
<p>参考：<br><a href="https://zhuanlan.zhihu.com/p/59071889" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/59071889</a><br><a href="https://github.com/NLPpupil/gale_and_church_align" target="_blank" rel="noopener">https://github.com/NLPpupil/gale_and_church_align</a><br><a href="https://www.aclweb.org/anthology/J93-1004.pdf" target="_blank" rel="noopener">https://www.aclweb.org/anthology/J93-1004.pdf</a></p>
</blockquote>
<p>Gale和Church在1993年提出了一个基于长度进行句对齐的算法，并在附录里公开了C源代码。这篇论文相当经典，以至于之后的关于句对齐的论文大多数要引用它。论文的题目是 A Program for Aligning Sentences in Bilingual Corpora。这个方法适合欧美语系，思想就是根据句子的长度来比较的。比较出名的hunalign工具是基于galechurch想法写的，并做了改进。Hunalign可以用于十几种语句的对齐，但是很遗憾，中文不太使用，但是也不是完全不适用，只是效果不太好。LF就是根据它做了一些小的改进，对其效果还可以。</p>
<p>对齐分两步。第一步是段落对其，第二步是在段落内部进行句对齐。段落对齐重要，不过简单，问题在于段落内部的句对齐。所以本文只解析已知段落对齐，怎样在段落内进行句对齐。首先定义几个概念，所有论文中出现的符号都对应定义里的符号。</p>
<ul>
<li><strong>句子</strong> 一个短的字符串。</li>
<li><strong>段落</strong> 语文里的自然段。分为源语言L1的段落和目标语言L2的段落，或称原文段落和译文段落。段落由一个序列的连续句子组成。</li>
<li><strong>片段</strong> 一个序列的连续的句子，是段落的子集。对应论文中的portion of text。</li>
<li><strong>片段对</strong> 原文片段和译文片段组成的对。</li>
<li>$l_1, l_2$分别对应片段对中原文部分和译文部分的字符总数。</li>
<li>$c, s^2$ 假设源语言中的一个字符在目标语言中对应的字符数是一个随机变量，且该随机变量服从正态分布 N(c, s^2) 。（如何估计可参考：<a href="https://www.zhihu.com/question/39080163" target="_blank" rel="noopener">https://www.zhihu.com/question/39080163</a>）</li>
<li>$\delta$ 论文中定义为 $\left(l_{2}-l_{1} c\right) / \sqrt{l_{1} s^{2}}$。每一个片段对都有自己的一个$\delta$$。</li>
<li><strong>对齐模式</strong> 或称<strong>匹配模式</strong>，描述一个句块对由几个原文句子和几个译文句子组成。比如1-2表示一个原文句子翻译成两个译文句子的对齐模式。</li>
<li><strong>match</strong> 对齐模式的概率分布。</li>
<li><strong>距离</strong>（distance measure） 衡量片段对两个片段之间的距离。距离度量是对$-\log (\operatorname{Prob}(\operatorname{match} | \delta))$的估计。当一个片段对确定后，我们就知道它的mathc和$\delta$。距离越大，此片段对对齐的概率越小。</li>
<li><strong>片段对序列</strong> 一个序列的片段对，这些片段对的原文部分的集合是原文段落的一个划分，译文部分的集合是译文段落的一个划分。</li>
<li><strong>距离和</strong> 距离和是片段对序列中所有片段对的</li>
<li><strong>对齐序列</strong> 距离和最小的片段对序列。</li>
</ul>
<p>对齐算法的输入是某一对相互对齐的段落，输出是对齐序列。接下来就变成了动态规划问题，类似最小编辑距离。片段对序列那么多，哪个是对齐序列呢？如果用穷举法，计算量太大，显然不现实。换个角度想，假设我们已经知道了对齐序列，用$D(i, j)$表示该对齐序列的距离和，其中$i$是原文段落最后一个句子的index，$j$是译文段落最后一个句子的index。对齐序列的距离和可以表示成最后一个片段对的距离加上去掉最后一个片段对的剩下的片段对序列的距离和（可以认为对齐序列的子序列也是对齐序列）。最后一个片段对有六种对齐模式，所以要对每种模式分情况讨论，选择结果最小的那个。动态规划的递归式就是这么来的。</p>
<p>$D(i, j)=\min \left\{\begin{array}{ccc}D(i, j-1) &amp; + &amp; d\left(0, t_{j} ; 0,0\right) \ D(i-1, j) &amp; + &amp; d\left(s_{i}, 0 ; 0,0\right) \ D(i-1, j-1) &amp; + &amp; d\left(s_{i}, t_{j} ; 0,0\right) \ D(i-1, j-2) &amp; + &amp; d\left(s_{i}, t_{j} ; 0, t_{j-1}\right) \ D(i-2, j-1) &amp; + &amp; d\left(s_{i}, t_{j} ; s_{i-1}, 0\right) \ D(i-2, j-2) &amp; + &amp; \left.d\left(s_{i}, t_{j};s_{i-1}, t_{j-1}\right)\right\}\end{array}\right.$</p>
<p>递归式的基础情况D(0,0)=0,通过递归式，我们可以求出对齐序列的距离和，在求距离和的过程中，我们顺便记录了对齐轨迹，也就是顺便求出了对齐序列。这就是算法的主干思想。下面就是它的主要代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import math</span><br><span class="line">import scipy.stats</span><br><span class="line"># 先使用统计的方法计算出对齐模式的概率分布</span><br><span class="line">match &#x3D; &#123;(1, 2): 0.023114355231143552,  </span><br><span class="line">         (1, 3): 0.0012165450121654502, </span><br><span class="line">         (2, 2): 0.006082725060827251, </span><br><span class="line">         (3, 1): 0.0006082725060827251, </span><br><span class="line">         (1, 1): 0.9422141119221411, </span><br><span class="line">         (2, 1): 0.0267639902676399&#125;</span><br><span class="line"># 源语言的一个字符对应于目标语言的字符数(正太分布)的均值</span><br><span class="line">c &#x3D; 1.467</span><br><span class="line"># 源语言的一个字符对应于目标语言的字符数(正太分布)的方差</span><br><span class="line">s2 &#x3D; 6.315</span><br><span class="line"></span><br><span class="line">def prob_delta(delta):</span><br><span class="line">    return scipy.stats.norm(0,1).cdf(delta) </span><br><span class="line"></span><br><span class="line">def length(sentence):</span><br><span class="line">    punt_list &#x3D; &#39;,.!?:;~，。！？：；～”“《》&#39;</span><br><span class="line">    sentence &#x3D; sentence</span><br><span class="line">    return sum(1 for char in sentence if char not in punt_list)</span><br><span class="line"></span><br><span class="line">def distance(partition1,partition2,match_prob):</span><br><span class="line">    l1 &#x3D; sum(map(length,partition1))</span><br><span class="line">    l2 &#x3D; sum(map(length,partition2))</span><br><span class="line">    try:</span><br><span class="line">        delta &#x3D; (l2-l1*c)&#x2F;math.sqrt(l1*s2)</span><br><span class="line">    except ZeroDivisionError:</span><br><span class="line">        return float(&#39;inf&#39;)</span><br><span class="line">    prob_delta_given_match &#x3D; 2*(1 - prob_delta(abs(delta)))    </span><br><span class="line">    try:</span><br><span class="line">        return - math.log(prob_delta_given_match) - math.log(match_prob)</span><br><span class="line">    except ValueError:</span><br><span class="line">        return float(&#39;inf&#39;)</span><br><span class="line"></span><br><span class="line">def align(para1,para2):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    输入两个句子序列，生成句对</span><br><span class="line">    句对是倒序的，从段落结尾开始向开头对齐</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    align_trace &#x3D; &#123;&#125; </span><br><span class="line">    for i in range(len(para1) + 1):</span><br><span class="line">        for j in range(len(para2) + 1):     </span><br><span class="line">            if i &#x3D;&#x3D; j &#x3D;&#x3D; 0:</span><br><span class="line">                align_trace[0, 0] &#x3D; (0, 0, 0) </span><br><span class="line">            else:</span><br><span class="line">                align_trace[i,j] &#x3D; (float(&#39;inf&#39;),0,0)</span><br><span class="line">                for (di, dj), match_prob in match.items():</span><br><span class="line">                    if i-di&gt;&#x3D;0 and j-dj&gt;&#x3D;0:</span><br><span class="line">                        align_trace[i,j] &#x3D; min(align_trace[i,j],(align_trace[i-di, j-dj][0] + distance(para1[i-di:i],para2[j-dj:j],match_prob), di, dj))</span><br><span class="line">                </span><br><span class="line">    i, j &#x3D; len(para1), len(para2)</span><br><span class="line">    while True:</span><br><span class="line">        (c, di, dj) &#x3D; align_trace[i, j]</span><br><span class="line">        if di &#x3D;&#x3D; dj &#x3D;&#x3D; 0:</span><br><span class="line">            break</span><br><span class="line">        yield &#39;&#39;.join(para1[i-di:i]), &#39;&#39;.join(para2[j-dj:j])</span><br><span class="line">        i -&#x3D; di</span><br><span class="line">        j -&#x3D; dj</span><br></pre></td></tr></table></figure>
<h1 id="Champollion"><a href="#Champollion" class="headerlink" title="Champollion"></a>Champollion</h1><blockquote>
<p>参考：<br><a href="https://www.aclweb.org/anthology/L06-1465/" target="_blank" rel="noopener">https://www.aclweb.org/anthology/L06-1465/</a><br><a href="https://www.aclweb.org/anthology/C10-2081.pdf" target="_blank" rel="noopener">https://www.aclweb.org/anthology/C10-2081.pdf</a><br><a href="https://github.com/LowResourceLanguages/champollion" target="_blank" rel="noopener">https://github.com/LowResourceLanguages/champollion</a></p>
</blockquote>
<p>Champollion是基于长度和词典的对齐算法，是中国人写的，对于中-英对齐比较好。相比于Gale&amp;Church这种对长度比较敏感（适合于英-法）的算法，Champollion更多关注到了内容。Champollion相比于其他算法的优点如下：</p>
<ul>
<li>Champollion假设输入有很大噪声（以往假设源语言与目标语言的match模式主要为1:1，但在中英语料中，句子对齐噪声非常大），使得删除和插入的次数变得很重要</li>
<li>与其他以词典为基础的算法不同，每个词根据对齐的重要性不同赋予了不同的权重。文中举例如下：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a. Marketplace bombing kills 23 in Iraq</span><br><span class="line">b. 伊拉克 集市 爆炸 造成 23 人 死亡</span><br></pre></td></tr></table></figure>
<p>在这个例子中，(23, 23)这个pair相比于(Iraq, 伊拉克)更重要。因为(Iraq, 伊拉克)相比于(23, 23)更经常出现。<br>Champollion因此赋予更少出现的translation pair更大的权重。</p>
<ul>
<li>Champollion对于每个segment pair（每个segment有1～多句话）进行打分，对于非1～1对齐进行了惩罚</li>
</ul>
<h2 id="句子相似度计算"><a href="#句子相似度计算" class="headerlink" title="句子相似度计算"></a>句子相似度计算</h2><p>论文将计算相似度问题转化为检索系统中计算query跟document相似度的问题。计算stf=segment term frequency（某个term在一个segment中出现的次数），定义$\text {idtf}=\frac{T}{ \text {occurences}_{-} \text {in}_{-} \text {the}_{-} \text {document}}$，其中T是document中的总term数。stf衡量term在segment中重要性，idtf衡量term在document中重要性。 Stf-idtf衡量了一个translate-pair对两个segment对齐的重要性。</p>
<p>Champollion将两个segment看成对顺序不敏感的词袋：$\begin{array}{l}E=\left\{e_{1}, e_{2}, \ldots, e_{m-1}, e_{m}\right\} \ C=\left\{c_{1}, c_{2}, \ldots, c_{n-1}, c_{n}\right\}\end{array}$。</p>
<p>定义k个在两个segment中出现的translate-pair：$P=\left\{\left(e_{1}^{\prime}, c_{1}^{\prime}\right),\left(e_{2}^{\prime}, c_{2}^{\prime}\right) \ldots\left(e_{k}^{\prime}, c_{k}^{\prime}\right)\right\}$，则E和C的相似度定义为：</p>
<p>$\begin{array}{l}\operatorname{sim}(E, C)=\sum_{i=1}^{k} \lg \left(\operatorname{stf}\left(e_{i}^{\prime}, c_{i}^{\prime}\right)^{<em>} i d t f\left(e_{i}^{\prime}\right)\right. \ </em> \text { alignment }_{-} \text {penalty } \ \text { +length_penalty }(E, C)\end{array}$</p>
<p>其中alignment_penalty是一个[0, 1]的值，对于1-1的对齐其值为1。length_penalty是一个函数，对于长度不匹配的翻译要进行一下惩罚。</p>
<h2 id="动态规划算法"><a href="#动态规划算法" class="headerlink" title="动态规划算法"></a>动态规划算法</h2><p>Champollion允许1-0, 0-1, 1-1, 2-1, 1-2, 1-3, 3-1, 1-4 和 4-1对齐，其动态规划转移方程为：</p>
<p>$S(i, j)=\max \left\{\begin{array}{c}S(i-1, j)+\operatorname{sim}(i, \phi) \ S(i, j-1)+\operatorname{sim}(\phi, j) \ S(i-1, j-1)+\operatorname{sim}(i, j) \ S(i-1, j-2)+\operatorname{sim}(i, j-1) \ S(i-2, j-1)+\operatorname{sim}(i-1, j) \ S(i-2, j-2)+\operatorname{sim}(i-1, j-1) \ S(i-1, j-3)+\operatorname{sim}(i, j-2) \ S(i-3, j-1)+\operatorname{sim}(i-2, j) \ S(i-1, j-4)+\operatorname{sim}(i, j-3) \ S(i-4, j-1)+\operatorname{sim}(i-3, j)\end{array}\right.$</p>
<h1 id="YALIGN"><a href="#YALIGN" class="headerlink" title="YALIGN"></a>YALIGN</h1><blockquote>
<p>参考：<br><a href="https://github.com/machinalis/yalign" target="_blank" rel="noopener">https://github.com/machinalis/yalign</a><br><a href="https://mailman.uib.no/public/corpora/2013-September/018912.html" target="_blank" rel="noopener">https://mailman.uib.no/public/corpora/2013-September/018912.html</a></p>
</blockquote>
<p>Yalign工具使用了一下，但效果不太好，它主要提供了两个功能：</p>
<ul>
<li>一个句子相似度度量：给定两个句子，它就会对这两个句子相互翻译的可能性产生一个粗略的估计(0到1之间的一个数字)。</li>
<li>一个序列对齐工具：这样给定两个文档(一个句子列表)，它产生一个对齐，最大化单个(每个句子对)相似度的总和。所以Yalign的主要算法实际上是一个标准序列对齐算法的包装。</li>
</ul>
<p>在序列对齐上，Yalign使用Needleman-Wunch算法的一个变体来查找两个给定文档中的句子之间的最佳对齐。它带来的优点是，使该算法具有多项式时间最坏情况的复杂性，并产生最优对齐。反之其缺点是不能处理相互交叉的对齐或从两个句子到一个句子的对齐。关于Needleman-Wunch算法可参考<a href="https://zhuanlan.zhihu.com/p/26212767" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26212767</a>。</p>
<p>对齐之后，只有翻译概率高的句子才会被包含在最终的对齐中。也就是说，有些结果会被过滤，以提供高质量的校准。使用一个阈值以便在句子相似度度量足够差时排除该对。</p>
<p>对于句子相似度度量，Yalign使用统计分类器的似然输出，并将其调整为0-1范围。分类器被训练来确定一对句子是否互相翻译。Yalign使用支持向量机作为分类器，对齐的质量不仅取决于输入，还取决于经过训练的分类器的质量。</p>
<p>下面是它的一些重要函数或类的说明：</p>
<ul>
<li>Sentence: 训练数据的基本类型，继承list</li>
<li>input_conversation.py：将文本/tmx/html格式的训练数据转化为Sentence</li>
<li>YalignModel: 主类，配合basic_model函数进行模型训练、导入、预测</li>
<li>SentencePairScore：定义句对的特征并进行打分</li>
<li>SequenceAligner：序列对齐类</li>
<li>SVMClassifier：训练一个支持向量机</li>
<li>WordPairScore：词及词翻译的概率，可以使用fast-align获取s</li>
</ul>
<h1 id="Bleualign"><a href="#Bleualign" class="headerlink" title="Bleualign"></a>Bleualign</h1><blockquote>
<p>参考：<br><a href="https://www.aclweb.org/anthology/W11-4624.pdf" target="_blank" rel="noopener">https://www.aclweb.org/anthology/W11-4624.pdf</a><br><a href="https://github.com/rsennrich/Bleualign" target="_blank" rel="noopener">https://github.com/rsennrich/Bleualign</a></p>
</blockquote>
<p>Bleualign借助机器翻译的结果进行对齐。使用机器翻译的目的是用目标语表示原文的大概意思，然后和译文进行比较，其算法的主要流程如下：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%8F%A5%E5%AD%90%E5%AF%B9%E9%BD%90/1.png" alt="图片"></p>
<p>Bleualign没使用过，不知道实际效果怎么样。</p>
<h1 id="Vecalign"><a href="#Vecalign" class="headerlink" title="Vecalign"></a>Vecalign</h1><blockquote>
<p>参考：<br><a href="https://github.com/thompsonb/vecalign" target="_blank" rel="noopener">https://github.com/thompsonb/vecalign</a><br>Vecalign: Improved Sentence Alignment in Linear Time and Space（<a href="https://www.aclweb.org/anthology/D19-1136.pdf）" target="_blank" rel="noopener">https://www.aclweb.org/anthology/D19-1136.pdf）</a></p>
</blockquote>
<p>vecalign计算句子相似度的方法跟之前不同，它使用了facebook开源的laser embedding进行句子相似度计算，计算公式如下：$c(x, y)=\frac{(1-\cos (x, y))_{\text {nSents }}(x) \text { nSents }(y)}{\sum_{s=1}^{S} 1-\cos \left(x, y_{s}\right)+\sum_{s=1}^{S} 1-\cos \left(x_{s}, y\right)}$，且对于非1-1对齐采取了一定的惩罚。下面是它的核心代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def vecalign(vecs0,</span><br><span class="line">             vecs1,</span><br><span class="line">             final_alignment_types,</span><br><span class="line">             del_percentile_frac,</span><br><span class="line">             width_over2,</span><br><span class="line">             max_size_full_dp,</span><br><span class="line">             costs_sample_size,</span><br><span class="line">             num_samps_for_norm,</span><br><span class="line">             norms0&#x3D;None,</span><br><span class="line">             norms1&#x3D;None):</span><br><span class="line">    if width_over2 &lt; 3:</span><br><span class="line">        logger.warning(</span><br><span class="line">            &#39;width_over2 was set to %d, which does not make sense. &#39;</span><br><span class="line">            &#39;increasing to 3.&#39;, width_over2)</span><br><span class="line">        width_over2 &#x3D; 3</span><br><span class="line"></span><br><span class="line">    # make sure input embeddings are norm&#x3D;&#x3D;1</span><br><span class="line">    make_norm1(vecs0)</span><br><span class="line">    make_norm1(vecs1)</span><br><span class="line"></span><br><span class="line">    # save off runtime stats for summary</span><br><span class="line">    runtimes &#x3D; OrderedDict()</span><br><span class="line"></span><br><span class="line">    # Determine stack depth</span><br><span class="line">    s0, s1 &#x3D; vecs0.shape[1], vecs1.shape[1]</span><br><span class="line">    max_depth &#x3D; 0</span><br><span class="line">    while s0 * s1 &gt; max_size_full_dp ** 2:</span><br><span class="line">        max_depth +&#x3D; 1</span><br><span class="line">        s0 &#x3D; s0 &#x2F;&#x2F; 2</span><br><span class="line">        s1 &#x3D; s1 &#x2F;&#x2F; 2</span><br><span class="line"></span><br><span class="line">    # init recursion stack</span><br><span class="line">    # depth is 0-based (full size is 0, 1 is half, 2 is quarter, etc)</span><br><span class="line">    stack &#x3D; &#123;0: &#123;&#39;v0&#39;: vecs0, &#39;v1&#39;: vecs1&#125;&#125;</span><br><span class="line"></span><br><span class="line">    # downsample sentence vectors</span><br><span class="line">    t0 &#x3D; time()</span><br><span class="line">    for depth in range(1, max_depth + 1):</span><br><span class="line">        stack[depth] &#x3D; &#123;&#39;v0&#39;: downsample_vectors(stack[depth - 1][&#39;v0&#39;]),</span><br><span class="line">                        &#39;v1&#39;: downsample_vectors(stack[depth - 1][&#39;v1&#39;])&#125;</span><br><span class="line">    runtimes[&#39;Downsample embeddings&#39;] &#x3D; time() - t0</span><br><span class="line"></span><br><span class="line">    # compute norms for all depths, add sizes, add alignment types</span><br><span class="line">    t0 &#x3D; time()</span><br><span class="line">    for depth in stack:</span><br><span class="line">        stack[depth][&#39;size0&#39;] &#x3D; stack[depth][&#39;v0&#39;].shape[1]</span><br><span class="line">        stack[depth][&#39;size1&#39;] &#x3D; stack[depth][&#39;v1&#39;].shape[1]</span><br><span class="line">        if depth &#x3D;&#x3D; 0:</span><br><span class="line">            stack[depth][&#39;alignment_types&#39;] &#x3D; final_alignment_types</span><br><span class="line">        else:</span><br><span class="line">            stack[depth][&#39;alignment_types&#39;] &#x3D; [(1, 1)]</span><br><span class="line"></span><br><span class="line">        if depth &#x3D;&#x3D; 0 and norms0 is not None:</span><br><span class="line">            if norms0.shape !&#x3D; vecs0.shape[:2]:</span><br><span class="line">                print(&#39;norms0.shape:&#39;, norms0.shape)</span><br><span class="line">                print(&#39;vecs0.shape[:2]:&#39;, vecs0.shape[:2])</span><br><span class="line">                raise Exception(&#39;norms0 wrong shape&#39;)</span><br><span class="line">            stack[depth][&#39;n0&#39;] &#x3D; norms0</span><br><span class="line">        else:</span><br><span class="line">            stack[depth][&#39;n0&#39;] &#x3D; compute_norms(</span><br><span class="line">                stack[depth][&#39;v0&#39;], stack[depth][&#39;v1&#39;], num_samps_for_norm)</span><br><span class="line"></span><br><span class="line">        if depth &#x3D;&#x3D; 0 and norms1 is not None:</span><br><span class="line">            if norms1.shape !&#x3D; vecs1.shape[:2]:</span><br><span class="line">                print(&#39;norms1.shape:&#39;, norms1.shape)</span><br><span class="line">                print(&#39;vecs1.shape[:2]:&#39;, vecs1.shape[:2])</span><br><span class="line">                raise Exception(&#39;norms1 wrong shape&#39;)</span><br><span class="line">            stack[depth][&#39;n1&#39;] &#x3D; norms1</span><br><span class="line">        else:</span><br><span class="line">            stack[depth][&#39;n1&#39;] &#x3D; compute_norms(</span><br><span class="line">                stack[depth][&#39;v1&#39;], stack[depth][&#39;v0&#39;], num_samps_for_norm)</span><br><span class="line"></span><br><span class="line">    runtimes[&#39;Normalize embeddings&#39;] &#x3D; time() - t0</span><br><span class="line"></span><br><span class="line">    # Compute deletion penalty for all depths</span><br><span class="line">    t0 &#x3D; time()</span><br><span class="line">    for depth in stack:</span><br><span class="line">        stack[depth][&#39;del_knob&#39;] &#x3D; make_del_knob(</span><br><span class="line">            e_laser&#x3D;stack[depth][&#39;v0&#39;][0, :, :],</span><br><span class="line">            f_laser&#x3D;stack[depth][&#39;v1&#39;][0, :, :],</span><br><span class="line">            e_laser_norms&#x3D;stack[depth][&#39;n0&#39;][0, :],</span><br><span class="line">            f_laser_norms&#x3D;stack[depth][&#39;n1&#39;][0, :],</span><br><span class="line">            sample_size&#x3D;costs_sample_size)</span><br><span class="line">        stack[depth][&#39;del_penalty&#39;] &#x3D; \</span><br><span class="line">            stack[depth][&#39;del_knob&#39;].percentile_frac_to_del_penalty(</span><br><span class="line">                del_percentile_frac)</span><br><span class="line">        logger.debug(&#39;del_penalty at depth %d: %f&#39;,</span><br><span class="line">                     depth, stack[depth][&#39;del_penalty&#39;])</span><br><span class="line">    runtimes[&#39;Compute deletion penalties&#39;] &#x3D; time() - t0</span><br><span class="line">    tt &#x3D; time() - t0</span><br><span class="line">    logger.debug(</span><br><span class="line">        &#39;%d x %d full DP make features: %.6fs (%.3e per dot product)&#39;,</span><br><span class="line">        stack[max_depth][&#39;size0&#39;], stack[max_depth][&#39;size1&#39;], tt,</span><br><span class="line">        tt &#x2F; (stack[max_depth][&#39;size0&#39;] + 1e-6) &#x2F;</span><br><span class="line">        (stack[max_depth][&#39;size1&#39;] + 1e-6))</span><br><span class="line">    # full DP at maximum recursion depth</span><br><span class="line">    t0 &#x3D; time()</span><br><span class="line">    stack[max_depth][&#39;costs_1to1&#39;] &#x3D; make_dense_costs(stack[max_depth][&#39;v0&#39;],</span><br><span class="line">                                                      stack[max_depth][&#39;v1&#39;],</span><br><span class="line">                                                      stack[max_depth][&#39;n0&#39;],</span><br><span class="line">                                                      stack[max_depth][&#39;n1&#39;])</span><br><span class="line"></span><br><span class="line">    runtimes[&#39;Full DP make features&#39;] &#x3D; time() - t0</span><br><span class="line">    t0 &#x3D; time()</span><br><span class="line">    _, stack[max_depth][&#39;x_y_tb&#39;] &#x3D; dense_dp(</span><br><span class="line">        stack[max_depth][&#39;costs_1to1&#39;], stack[max_depth][&#39;del_penalty&#39;])</span><br><span class="line">    stack[max_depth][&#39;alignments&#39;] &#x3D; dense_traceback(</span><br><span class="line">        stack[max_depth][&#39;x_y_tb&#39;])</span><br><span class="line">    runtimes[&#39;Full DP&#39;] &#x3D; time() - t0</span><br><span class="line"></span><br><span class="line">    # upsample the path up to the top resolution</span><br><span class="line">    compute_costs_times &#x3D; []</span><br><span class="line">    dp_times &#x3D; []</span><br><span class="line">    upsample_depths &#x3D; [0, ] if max_depth &#x3D;&#x3D; 0 else list(</span><br><span class="line">        reversed(range(0, max_depth)))</span><br><span class="line">    for depth in upsample_depths:</span><br><span class="line">        if max_depth &gt; 0:  # upsample previoius alignment to current resolution</span><br><span class="line">            course_alignments &#x3D; upsample_alignment(</span><br><span class="line">                stack[depth + 1][&#39;alignments&#39;])</span><br><span class="line">            # features may have been truncated when downsampleing,</span><br><span class="line">            # so alignment may need extended</span><br><span class="line">            extend_alignments(</span><br><span class="line">                course_alignments, stack[depth][&#39;size0&#39;],</span><br><span class="line">                stack[depth][&#39;size1&#39;])  # in-place</span><br><span class="line">        else:</span><br><span class="line">            # We did a full size 1-1 search,</span><br><span class="line">            # so search same size with more alignment types</span><br><span class="line">            course_alignments &#x3D; stack[0][&#39;alignments&#39;]</span><br><span class="line"></span><br><span class="line">        # convert couse alignments to a searchpath</span><br><span class="line">        stack[depth][&#39;searchpath&#39;] &#x3D; alignment_to_search_path(</span><br><span class="line">            course_alignments)</span><br><span class="line"></span><br><span class="line">        # compute ccosts for sparse DP</span><br><span class="line">        t0 &#x3D; time()</span><br><span class="line">        stack[depth][&#39;a_b_costs&#39;], stack[depth][&#39;b_offset&#39;] &#x3D; \</span><br><span class="line">            make_sparse_costs(stack[depth][&#39;v0&#39;], stack[depth][&#39;v1&#39;],</span><br><span class="line">                              stack[depth][&#39;n0&#39;], stack[depth][&#39;n1&#39;],</span><br><span class="line">                              stack[depth][&#39;searchpath&#39;],</span><br><span class="line">                              stack[depth][&#39;alignment_types&#39;],</span><br><span class="line">                              width_over2)</span><br><span class="line"></span><br><span class="line">        tt &#x3D; time() - t0</span><br><span class="line">        num_dot_products &#x3D; len(stack[depth][&#39;b_offset&#39;]) * \</span><br><span class="line">            len(stack[depth][&#39;alignment_types&#39;]) * width_over2 * 2</span><br><span class="line">        logger.debug(&#39;%d x %d sparse DP (%d alignment types, %d window) &#39;</span><br><span class="line">                     &#39;make features: %.6fs (%.3e per dot product)&#39;,</span><br><span class="line">                     stack[max_depth][&#39;size0&#39;], stack[max_depth][&#39;size1&#39;],</span><br><span class="line">                     len(stack[depth][&#39;alignment_types&#39;]), width_over2 * 2,</span><br><span class="line">                     tt, tt &#x2F; (num_dot_products + 1e6))</span><br><span class="line"></span><br><span class="line">        compute_costs_times.append(time() - t0)</span><br><span class="line">        t0 &#x3D; time()</span><br><span class="line">        # perform sparse DP</span><br><span class="line">        stack[depth][&#39;a_b_csum&#39;], stack[depth][&#39;a_b_xp&#39;], \</span><br><span class="line">            stack[depth][&#39;a_b_yp&#39;], stack[depth][&#39;new_b_offset&#39;] &#x3D; \</span><br><span class="line">            sparse_dp(</span><br><span class="line">                stack[depth][&#39;a_b_costs&#39;], stack[depth][&#39;b_offset&#39;],</span><br><span class="line">                stack[depth][&#39;alignment_types&#39;], stack[depth][&#39;del_penalty&#39;],</span><br><span class="line">                stack[depth][&#39;size0&#39;], stack[depth][&#39;size1&#39;])</span><br><span class="line"></span><br><span class="line">        # performace traceback to get alignments and alignment scores</span><br><span class="line">        # for debugging, avoid overwriting stack[depth][&#39;alignments&#39;]</span><br><span class="line">        akey &#x3D; &#39;final_alignments&#39; if depth &#x3D;&#x3D; 0 else &#39;alignments&#39;</span><br><span class="line">        stack[depth][akey], stack[depth][&#39;alignment_scores&#39;] &#x3D; \</span><br><span class="line">            sparse_traceback(stack[depth][&#39;a_b_csum&#39;],</span><br><span class="line">                             stack[depth][&#39;a_b_xp&#39;],</span><br><span class="line">                             stack[depth][&#39;a_b_yp&#39;],</span><br><span class="line">                             stack[depth][&#39;new_b_offset&#39;],</span><br><span class="line">                             stack[depth][&#39;size0&#39;],</span><br><span class="line">                             stack[depth][&#39;size1&#39;])</span><br><span class="line">        dp_times.append(time() - t0)</span><br><span class="line"></span><br><span class="line">    runtimes[&#39;Upsample DP compute costs&#39;] &#x3D; sum(compute_costs_times[:-1])</span><br><span class="line">    runtimes[&#39;Upsample DP&#39;] &#x3D; sum(dp_times[:-1])</span><br><span class="line"></span><br><span class="line">    runtimes[&#39;Final DP compute costs&#39;] &#x3D; compute_costs_times[-1]</span><br><span class="line">    runtimes[&#39;Final DP&#39;] &#x3D; dp_times[-1]</span><br><span class="line">    return stack</span><br></pre></td></tr></table></figure>
<p>其他相关资源</p>
<ul>
<li><a href="https://github.com/cocoxu/Shakespeare/tree/master/bilingual-sentence-aligner" target="_blank" rel="noopener">https://github.com/cocoxu/Shakespeare/tree/master/bilingual-sentence-aligner</a></li>
<li>[<a href="https://github.com/loomchild/maligna](" target="_blank" rel="noopener">https://github.com/loomchild/maligna](</a></li>
</ul>
]]></content>
      <tags>
        <tag>句子对齐</tag>
      </tags>
  </entry>
  <entry>
    <title>HMM之——基础学习</title>
    <url>/2020/04/12/HMM%E4%B9%8B%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>最近工作中经常要用到HMM，所以专门来复习下，主要讲解的是HMM的Forward和Backward算法，以及参数估计算法。</p>
<a id="more"></a>
<h1 id="HMM总览"><a href="#HMM总览" class="headerlink" title="HMM总览"></a>HMM总览</h1><p>HMM是一个时序的模型，每时每刻都有一个观测者（observation，下图中绿色点，我们已知的）和一个隐式变量（latent variable，下图中灰色点，我们未知的）。下图为一个HMM基本模型，每一条边都是有方向的，隐式变量可以理解为一个状态，每个状态下都会有一个可以观测到的值。横线表示了状态的转移，竖线表示了从状态到观测的生产过程。因此，HMM是一个有向的生成模型（生成观测者），当然也可以把HMM做成一个判别模型。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/hmm-1/1.png" alt="图片"></p>
<h1 id="Forward-Backward算法"><a href="#Forward-Backward算法" class="headerlink" title="Forward/Backward算法"></a>Forward/Backward算法</h1><p>HMM中最参数估计使用的是Forward/Backward算法，解码使用的是Viterbi算法。首先来看一下Forward/Backward算法。</p>
<p>F/B算法的主要目的是计算给定观测值x时，其中某个给定的$z_k$的概率值是多少，即$P(z_k|x)$。Forward算法的目的是计算联合概率$P(z_k, x_{1:k})$，其中$x_1:k = (x_1, x_2, …, x_k)$。Backward算法的目的是计算条件概率$P(x_{k+1:n}|z_k)$。那么为什么要用Forward和Backward算法呢？事实上，我们估计HMM参数时，需要计算$P(z_k|x)$，而$P(z_k|x)$可以拆分成$P(z_k, x_{1:k})$和$P(x_{k+1:n}|z_k)$这两项，具体推导如下：</p>
<ul>
<li>根据贝叶斯定理：$P(z_k|x) = P(z_k, x) / P(x)$，其中$P(x)$对任何$z_k$都是等同的，因此可以认为$P(z_k|x)$是正比于$P(z_k, x)$的。在进行计算时，要注意归一化，即$P(z_k|x) = P(z_k, x) / sum(P(z=j, x))$。</li>
<li>下面我们把$P(z_k, x)$拆分成Forward和Backward的形式<ul>
<li>根据贝叶斯定理：$P(z_k, x) = P(x_{k+1:n}|z_k, x_{1:k}) * P(z_k, x_{1:k})$，其中第二项就是我们的Forward算法</li>
<li>我们看上式中第一项，发现跟Backward算法就差了一个$x_{1:k}$，由于$x_{1:k}$和$x_{k+1:n}$条件独立于$z_k$的（即$x_{1:k}$全部作用在$z_k$上，而不会对$x_{k+1:n}$产生影响）。因此$P(x_{k+1:n}|z_k, x_{1:k}) = P(x_{k+1:n}|z_k) * P(z_k, x_{1:k})$，其中$P(x_{k+1:n}|z_k)$就是Backward算法，$P(z_k, x_{1:k})$是Forward算法</li>
</ul>
</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/hmm-1/2png.png" alt="图片"></p>
<h2 id="Forward算法"><a href="#Forward算法" class="headerlink" title="Forward算法"></a>Forward算法</h2><p>Forward本质是一个动态规划算法，其目标是计算$P(z_k, x_{1:k})$。既然用动归，我们就要想清楚如何构造$P(z_k, x_{1:k})$的子问题。下面讲一下构造动归的具体思路：</p>
<ul>
<li>我们想构造出：<img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/hmm-1/3.png" alt="图片"></li>
<li>为了引入$z_{k-1}$，考虑使用边缘化的性质：<img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/hmm-1/4.png" alt="图片"><ul>
<li>关于边缘化，可参考：<a href="https://cloud.tencent.com/developer/article/1096441" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1096441</a></li>
</ul>
</li>
<li>继续推导可得出：</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/hmm-1/5.png" alt="图片"></p>
<ul>
<li>公式化简时使用了条件独立性，判断是否条件独立可以使用D-seperation方法</li>
<li>状态的初始化如下：</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/hmm-1/6.png" alt="图片"></p>
<h2 id="Backward算法"><a href="#Backward算法" class="headerlink" title="Backward算法"></a>Backward算法</h2><p>Backward算法就是Forward的相反方向，解决问题的思路也是一样的。其目标是计算$P(x_{k+1:n}|z_k)​$，跟Forward一样，我们要把它拆分成更小的子问题，推导过程如下：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/hmm-1/16.png" alt="图片"></p>
<h1 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h1><p>HMM参数估计涉及到的参数包括：</p>
<ul>
<li>A：状态到状态的转移矩阵，假设状态有m个，则$A.shape = m * m$</li>
<li>B：状态到观测值的发射矩阵，假设观测值为单词，且词表大小为V，则$B.shape = m * V$</li>
<li>pi：初始状态值，$pi.shape = m * 1$</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/hmm-1/8png.png" alt="图片"></p>
<h2 id="Complete情况"><a href="#Complete情况" class="headerlink" title="Complete情况"></a>Complete情况</h2><p>Complete case指的是每一个latent variable是知道的，如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/hmm-1/9.png" alt="图片"></p>
<h3 id="估计pi"><a href="#估计pi" class="headerlink" title="估计pi"></a>估计pi</h3><p>统计每个状态初始出现的次数，如上图中有3个状态，次数分别为[2, 1, 0]，转换成概率为[2/3, 1/3, 0]。</p>
<h3 id="估计状态转移概率A"><a href="#估计状态转移概率A" class="headerlink" title="估计状态转移概率A"></a>估计状态转移概率A</h3><p>统计状态间的转移瓷土，上图中的HMM统计如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">1</th>
<th style="text-align:center">2</th>
<th style="text-align:center">3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">2</td>
<td style="text-align:center">1</td>
<td style="text-align:center">2</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">1</td>
<td style="text-align:center">2</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">0</td>
<td style="text-align:center">2</td>
<td style="text-align:center">1</td>
</tr>
</tbody>
</table>
</div>
<p>转换成概率值：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">1</th>
<th style="text-align:center">2</th>
<th style="text-align:center">3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">2/5</td>
<td style="text-align:center">1/5</td>
<td style="text-align:center">2/5</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">1/4</td>
<td style="text-align:center">2/4</td>
<td style="text-align:center">1/4</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">0</td>
<td style="text-align:center">2/3</td>
<td style="text-align:center">1/3</td>
</tr>
</tbody>
</table>
</div>
<h3 id="估计发射概率B"><a href="#估计发射概率B" class="headerlink" title="估计发射概率B"></a>估计发射概率B</h3><p>统计每个状态下，看到观测值的数目，上图中HMM的统计如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">a</th>
<th style="text-align:center">b</th>
<th style="text-align:center">c</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">3</td>
<td style="text-align:center">2</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">3</td>
<td style="text-align:center">0</td>
<td style="text-align:center">3</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">1</td>
<td style="text-align:center">2</td>
<td style="text-align:center">1</td>
</tr>
</tbody>
</table>
</div>
<p>转换成概率值如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">a</th>
<th style="text-align:center">b</th>
<th style="text-align:center">c</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">3/5</td>
<td style="text-align:center">2/5</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">1/2</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1/2</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">1/4</td>
<td style="text-align:center">1/2</td>
<td style="text-align:center">1/4</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Incomplete情况"><a href="#Incomplete情况" class="headerlink" title="Incomplete情况"></a>Incomplete情况</h2><p>Incomplete case指的是latent variable，即z，是不知道的。那么这种情况下怎么计算z呢？如果我们知道A、B、$\pi$这三个参数的话，就可以用Forward/Backward算法计算z的期望值，即$P(z_k|x)$，如下图中红色：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/hmm-1/10.png" alt="图片"></p>
<p>上图中红色部分，每个概率值我们可以认为是一个expectation count，也就是说在上图情况下，第1个状态出现了0.8词，第2个状态出现了0.1次，第3个状态出现了0.1次。</p>
<p>那么反过来，如果已知了z的期望值，通过上面统计的方式我们可以计算出参数值。我们可以用这个参数值，再去更新z的期望值，如此循环，直到收敛。一般情况下，我们是先初始化z，在通过z计算参数。</p>
<h3 id="估计-pi"><a href="#估计-pi" class="headerlink" title="估计$\pi$"></a>估计$\pi$</h3><p>计算$P(z_1|x)$，比如对于$x_1$, $p(z_1=1|x) = 0.7$, $p(z_1=2|x) = 0.2$, $p(z_1=3|x) = 0.3$，这里的概率值我们认为是出现的次数就可以。如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/hmm-1/11.png" alt="图片"></p>
<p>接着我们统计每个状态出现的总数：[0.7 + 0.4 + 0.6, 0.2 + 0.4 + 0.3, 0.1 + 0.2 + 0.1] = [1.7, 0.9, 0.4]，转变为概率为[1.7/3, 0.9/3, 0.4/3]。</p>
<h3 id="估计状态转移概率A-1"><a href="#估计状态转移概率A-1" class="headerlink" title="估计状态转移概率A"></a>估计状态转移概率A</h3><p>A的估计跟语言模型很像。语言模型中，计算bigram概率$P(w_j|w_i)=c(w_i, w_j)/c(w_i)$，其中$c(w_i, w_j)$表示一种联合状态，即$w_i$、$w_j$共现的次数。</p>
<p>在HMM中，我们要计算的是$P(z_k=i, z_{k+1}=j|x)$，而这个值可以使用Forward/Backward计算出来：</p>
<ul>
<li>先计算$P(z_k=1, z_{k+1}=1, x)$、$P(z_k=1, z_{k+1}=2, x)$…$P(z_k=m, z_{k+1}=m, x)$，这是一个$m * m$的矩阵</li>
<li>$P(z_k=i, z_{k+1}=j|x)$是正比于$P(z_k=i, z_{k+1}=j, x)$的</li>
<li>$P(z_k=i, z_{k+1}=j, x) = P(z_k=i, z_{k+1}=j, x_{1:k}, x_{k+1}, x_{k+2:n})$</li>
<li>根据D-separation（如下图），上式可以写成：$P(z_k, x_{1:k}) <em> P(x_{k+2:n}|z_{k+1}) </em> P(z_{k+1}|z_k) * P(x_{k+1}|z_{k+1})$，由此我们把联合概率转化成了条件概率</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/hmm-1/12.png" alt="图片"></p>
<ul>
<li>上式中$P(z_k, x_{1:k})$是Forward算式，$P(x_{k+2:n}|z_{k+1}) $是Backward算式，$P(z_{k+1}|z_k)$是状态转移，$P(x_{k+1}|z_{k+1})$是发射概率</li>
</ul>
<p>举一个具体的例子（A_{12}、A_{13}同理，我们想得到的就是A_{ij}矩阵）：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/hmm-1/13png.png" alt="图片"></p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/hmm-1/14.png" alt="图片"></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">1</th>
<th style="text-align:center">2</th>
<th style="text-align:center">3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">…</td>
<td style="text-align:center">1.72/6</td>
<td style="text-align:center">(0.3+0.2+0.1+0.3+0.4+0.2+0.2+0.1+0.2+0.1+0.01+0.1)/(0.6+.5+0.6+0.7+0.6+0.5+0.4+0.3+0.6+0.3+0.5+0.1+0.3)=2.27/6</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
</tr>
</tbody>
</table>
</div>
<p>转换成概率值：</p>
<h3 id="估计发射概率B-1"><a href="#估计发射概率B-1" class="headerlink" title="估计发射概率B"></a>估计发射概率B</h3><p>计算$P(z_k|x)$，如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/hmm-1/15.png" alt="图片"></p>
<p>统计每个状态下，看到观测值的数目，上图中HMM的统计如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">a</th>
<th style="text-align:center">b</th>
<th style="text-align:center">c</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">0.6+0.4+0.1+0.3+0.3=1.7</td>
<td style="text-align:center">2.9</td>
<td style="text-align:center">1.6</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">1.7</td>
<td style="text-align:center">2.3</td>
<td style="text-align:center">0.7</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">1.6</td>
<td style="text-align:center">1.8</td>
<td style="text-align:center">0.7</td>
</tr>
</tbody>
</table>
</div>
<p>转换成概率值如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">a</th>
<th style="text-align:center">b</th>
<th style="text-align:center">c</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">1.7/6.2</td>
<td style="text-align:center">2.9/6.2</td>
<td style="text-align:center">1.6/6.2</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">1.7/4.7</td>
<td style="text-align:center">2.3/4.7</td>
<td style="text-align:center">0.7/4.7</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">1.6/4.1</td>
<td style="text-align:center">1.8/4.1</td>
<td style="text-align:center">0.7/4.1</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <tags>
        <tag>HMM</tag>
      </tags>
  </entry>
  <entry>
    <title>令人头大之IBM Model</title>
    <url>/2020/03/29/%E4%BB%A4%E4%BA%BA%E5%A4%B4%E5%A4%A7%E4%B9%8BIBM-Model/</url>
    <content><![CDATA[<p>最近一段时间工作中，急需补充giza、fast align算法的背后原理，因此集中补一补这些令人头大的算法。本来打算看完IBM-Model1~Model5和HMM，但后来卡到了Model-3上，准备在后续的博客中继续更新。因此本篇将重点介绍Model-1~Model2。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://wenku.baidu.com/view/4cda374769eae009581becd2.html" target="_blank" rel="noopener">https://wenku.baidu.com/view/4cda374769eae009581becd2.html</a><br><a href="https://www.cl.uni-heidelberg.de/courses/ss15/smt/scribe2.pdf" target="_blank" rel="noopener">https://www.cl.uni-heidelberg.de/courses/ss15/smt/scribe2.pdf</a></p>
</blockquote>
<h1 id="词对齐算法之IBM-Model"><a href="#词对齐算法之IBM-Model" class="headerlink" title="词对齐算法之IBM Model"></a>词对齐算法之IBM Model</h1><p>假设任意一个英语句子e和一个法语句子f，定义f翻译成e的概率为Pr(e|f)，其归一化条件为$\sum_{e} \operatorname{Pr}(e | f)=1$，于是将f翻译成e的问题就变成求解$\hat{e}=\operatorname{argmax} \operatorname{Pr}(e | f)$。我们可以把它理解成一个噪声信道模型，假设我们看到的源语言文本F是由一段目标语言文本E经过某种奇怪的编码得到的，那么翻译的目标就是要将F还原成E，这也就是就是一个解码的过程，如下图：<img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ibmmodel/1.png" alt="图片"></p>
<p>表达成公式如下：$\mathrm{E}=\arg \max _{\mathrm{E}} P(\mathrm{E}) P(\mathrm{F} | \mathrm{E})$。P(E)为语言模型，它反映“E像一个句子”的程度，即流利度；P(F|E)为翻译模型，它反映“F像E”的程度，即忠实度；联合使用两个模型效果好于单独使用翻译模型，因为后者容易导致一些不好的译文。因此，统计机器翻译要解决的是3个问题：</p>
<ul>
<li>语言模型<em>P</em>(E)的建模和参数估计</li>
<li>翻译模型<em>P</em>(F|E)的建模和参数估计</li>
<li>解码（搜索）算法</li>
</ul>
<p>语言模型给出任何一个句子的出现概率$\operatorname{Pr}\left(E=e_{1} e_{2} \ldots e_{n}\right)$，N元语法模型是最简单也是最常见的语言模型，其他语言模型包括：隐马尔科夫模型（HMM）（加入词性标记信息）、概率上下文无关语法（PCFG）（加入短语结构信息）、概率链语法（Probabilistic Link Grammar）（加入链语法的结构信息）。N元语言模型公式如下：</p>
<p>$\begin{aligned} P(w) &amp;=\prod_{i=1}^{n} P\left(w_{i} | w_{1} w_{2} \ldots w_{i-1}\right) \ &amp; \approx \prod_{i=1}^{n} P\left(w_{i} | w_{i-N+1} w_{i-N+2} \ldots w_{i-1}\right) \end{aligned}$</p>
<p>用一张概率转移图可形象表示，如下为一个2元语言模型：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ibmmodel/2.png" alt="图片"></p>
<p>翻译模型<em>P</em>(F|E)反映的是一个源语言句子E翻译成一个目标语言句子F的概率，由于源语言句子和目标语言句子几乎不可能在语料库中出现过，因此这个概率无法直接从语料库统计得到，必须分解成词语翻译的概率和句子结构（或者顺序）翻译的概率。因此翻译模型的计算引入了隐含变量词对齐：$P(\mathrm{F} | \mathrm{E})=\sum_{A} P(\mathrm{F}, \mathrm{A} | \mathrm{E})$</p>
<p>翻译概率<em>P</em>(F|E)的计算转化为对齐概率<em>P</em>(F,A|E)的估计。IBM Model安成了对P(F,A|E)的估计。那么IBM Model从1~3分别有什么不同呢？IBM Model 1仅考虑词对词的互译概率，IBM Model 2加入了词的位置变化的概率，IBM Model 3加入了一个词翻译成多个词。</p>
<p>IBM模型是份经典的研究工作，这5个模型既是当初基于词的统计机器翻译模型的基础，也是现在统计机器翻译中主流技术中的重要一步。作为一个生成模型，IBM模型有着自身”严密”的模型演绎。总的来说，Model 1和2是在一个展开公式下的建模，而Model 3、4和5则是在另一个展开公式下的建模(fertility based model)。IBM Model属于single word based model，它只允许一对一和一对多的对齐，不存在多对一的对齐，这跟phrase based SMT模型不同。当然，从模型的复杂程度上讲，这5个模型之间的关系是1<2<3<4<5，从模型的计算顺序来讲，是1->2-&gt;3-&gt;4-&gt;5。</p>
<h2 id="IBM-Model-1"><a href="#IBM-Model-1" class="headerlink" title="IBM Model-1"></a>IBM Model-1</h2><blockquote>
<p>参考：<br><a href="https://www.nltk.org/_modules/nltk/translate/ibm1.html" target="_blank" rel="noopener">https://www.nltk.org/_modules/nltk/translate/ibm1.html</a><br><a href="http://mt-class.org/jhu/slides/lecture-ibm-model1.pdf" target="_blank" rel="noopener">http://mt-class.org/jhu/slides/lecture-ibm-model1.pdf</a><br><a href="https://zhuanlan.zhihu.com/p/72160554" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/72160554</a></p>
</blockquote>
<p>IBM模型1&amp;2的推导过程：</p>
<ul>
<li>猜测目标语言句子长度</li>
<li>从左至右，对于每个目标语言单词<ul>
<li>首先猜测该单词由哪一个源语言单词翻译而来</li>
<li>再猜测该单词应该翻译成什么目标语言词</li>
</ul>
</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ibmmodel/3.png" alt="图片"></p>
<p>IBM Model-1进行了如下假设：</p>
<ul>
<li>假设翻译的目标语言句子为： $\mathrm{F}=f_{1}^{m}=f_{1} f_{2} \cdots f_{m}$</li>
<li>假设翻译的源语言句子为：$\mathrm{E}=e_{1}^{l}=e_{1} e_{2} \cdots e_{l}$</li>
<li>假设词语对齐表示为：$\mathrm{A}=a_{1}^{m}=a_{1} a_{2} \cdots a_{m}, \forall i \in\{1, \cdots, m\}, a_{i} \in\{0, \cdots, l\}$</li>
<li>那么词语对齐的概率可以表示为：$\operatorname{Pr}(\mathrm{F}, \mathrm{A} | \mathrm{E})=\operatorname{Pr}(m | \mathrm{E}) \prod_{j=1}^{m} \operatorname{Pr}\left(a_{j} | a_{1}^{j-1}, f_{1}^{j-1}, m, \mathrm{E}\right) \operatorname{Pr}\left(f_{j} | a_{1}^{j}, f_{1}^{j-1}, m, \mathrm{E}\right)$</li>
<li>在Model-1中，假设所有翻译长度都是等概率的</li>
<li>假设词语对齐只与源语言长度有关，与其他因素无关：$\operatorname{Pr}\left(a_{j} | a_{1}^{j-1}, f_{1}^{j-1}, m, \mathrm{E}\right)=\frac{1}{l+1}$</li>
<li>假设目标词语的选择只与其对应的源语言词语有关，与其他因素无关：$\operatorname{Pr}\left(f_{j} | a_{1}^{j}, f_{1}^{j-1}, m, \mathrm{E}\right)=t\left(f_{j} | e_{a_{j}}\right)$</li>
<li>那么对齐概率可以表示为：$\operatorname{Pr}(\mathrm{F}, \mathrm{A} | \mathrm{E})=\frac{\varepsilon}{(l+1)^{m}} \prod_{j=1}^{m} t\left(f_{j} | e_{a_{j}}\right)$</li>
<li>对所有可能的对齐求和，那么翻译概率就可以表示为：$\operatorname{Pr}(\mathrm{F} | \mathrm{E})=\sum_{\mathrm{A}} \operatorname{Pr}(\mathrm{F}, \mathrm{A} | \mathrm{E})=\frac{\varepsilon}{(l+1)^{m}} \sum_{a_{1}=1}^{l} \cdots \sum_{a_{m}=1}^{l} \prod_{j=1}^{m} t\left(f_{j} | e_{a_{j}}\right)$</li>
</ul>
<p>这就是IBM Model 1的翻译模型公式，也就是说，给定参数t(f|e)，我们就可以计算出句子E翻译成句子F的概率。对于其中翻译概率表t(f|e)满足归一约束条件：$\sum_{f} t(f | e)=1$</p>
<p>延伸：在IBM-Model2中增加如下假设：</p>
<ul>
<li>假设词语对齐只与源语言长度、目标语言的长度和两个词的位置有关，与其他因素无关：$\operatorname{Pr}\left(a_{j} | a_{1}^{j-1}, f_{1}^{j-1}, m, \mathrm{E}\right)=a\left(a_{j} | j, m, l\right)$，归一化条件为$\sum_{i=0}^{l} a(i | j, m, l)=1$</li>
</ul>
<h3 id="翻译概率的定义"><a href="#翻译概率的定义" class="headerlink" title="翻译概率的定义"></a>翻译概率的定义</h3><p>对于长度为$l_f$的外语句子$f={f_1,…,f_{l_f}}$，长度为$l_e$的英文句子$e={e_1,…,e_{l_e}}$，从英文到外文的词对齐$e_j-&gt;f_i$关系$a:j-&gt;i$，有翻译概率定义：</p>
<p>$p(\mathbf{e}, a | \mathbf{f})=\frac{\epsilon}{\left(l_{f}+1\right)^{l_{e}}} \prod_{j=1}^{l_{e}} t\left(e_{j} | f_{a(j)}\right)$，例如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ibmmodel/4.png" alt="图片"></p>
<h3 id="参数求解"><a href="#参数求解" class="headerlink" title="参数求解"></a>参数求解</h3><p>根据最大似然估计，我们希望得到一组概率分布，使得我们的训练语料库出现的概率最大。也就是说，给定训练语料库E和F，我们要求解一个概率分布t(f|e)，使得翻译概率Pr(F|E)最大。这是一个受约束的极值问题，约束条件即是t(f|e)的归一性条件。为了求解这个问题，我们需要引入拉格朗日乘子，构造一个辅助函数，将上述受约束的极值问题转换成一个不受约束的极值问题。</p>
<p>引入拉格朗日乘子$\Lambda_{e}$，构造辅助函数：$h(t, \lambda) \equiv \frac{\varepsilon}{(l+1)^{m}} \sum_{a_{i}=1}^{l} \cdots \sum_{a_{m}=1}^{l} \prod_{j=1}^{m} t\left(f_{j} | e_{a_{j}}\right)-\sum_{e} \lambda_{e}\left(\sum_{f} t(f | e)-1\right)$。将上述函数对t(f|e)求导得到：$\frac{\partial h(t, \lambda)}{\partial t(f | e)}=\frac{\varepsilon}{(l+1)^{m}} \sum_{a_{1}=1}^{l} \cdots \sum_{a_{n}=1}^{l} \delta\left(f, f_{j}\right) \delta\left(e, e_{a_{j}}\right) \frac{\prod_{k=1}^{m} t\left(f_{k} | e_{a_{k}}\right)}{\mathrm{t}(f | e)}-\lambda_{e}$。</p>
<p>令上式为0，我们得到：$t(f | e)=\lambda_{e}^{-1} \frac{\varepsilon}{(l+1)^{m}} \sum_{a_{1}=1}^{l} \cdots \sum_{a_{m}=1}^{l} \delta\left(f, f_{j}\right) \delta\left(e, e_{a_{j}}\right) \prod_{k=1}^{m} t\left(f_{k} | e_{a_{k}}\right)$。我们看到，这个公式的左边和右边都出现了t(f|e)，我们无法直接用这个公式从给定的语料库(F|E)中计算出t(f|e)，我们可以将这个公式看成是一个迭代公式，给定一个初值t(f|e)，利用这个公式反复迭代，最后可以收敛到一个稳定的t(f|e)值，这就是EM算法。其中$\sum_{j=1}^{m} \delta\left(f, f_{j}\right) \delta\left(e, e_{a_{j}}\right)$表示对齐A中e连接到f的次数。</p>
<p>定义在E和F的所有可能的对齐A下e和f连接数的均值为：$c(f | e ; \mathrm{F}, \mathrm{E}) \equiv \sum_{\mathrm{A}} \operatorname{Pr}(\mathrm{A} | \mathrm{F}, \mathrm{E}) \sum_{j=1}^{m} \delta\left(f, f_{j}\right) \delta\left(e, e_{a_{j}}\right)$，且$\begin{array}{c}c(f | e ; \mathrm{F}, \mathrm{E})=\sum_{\mathrm{A}} \frac{\operatorname{Pr}(\mathrm{F}, \mathrm{A} | \mathrm{E})}{\operatorname{Pr}(\mathrm{F} | \mathrm{E})} \sum_{j=1}^{m} \delta\left(f, f_{j}\right) \delta\left(e, e_{a_{j}}\right) \ =\frac{\sum_{A} \operatorname{Pr}(\mathrm{F}, \mathrm{A} | \mathrm{E}) \sum_{j=1}^{m} \delta\left(f, f_{j}\right) \delta\left(e, e_{a_{j}}\right)}{\operatorname{Pr}(\mathrm{F}[\mathrm{E})}\end{array}$。</p>
<p>将c(f|e;F,E)代入迭代公式，并将Pr(F|E)并入参数λe，我们得到新的迭代公式：$t(f | e)=\lambda_{e}^{-1} c(f | e ; \mathrm{F}, \mathrm{E})$</p>
<p>这个新的迭代公式可以理解为：</p>
<ul>
<li>一旦我们得到了一组参数t(f|e)，我们就可以计算所有的词语对齐的概率Pr(F,A|E)</li>
<li>有了每个词语对齐的概率Pr(F,A|E)，我们就可以计算新的t(f|e)的值，就是所有的出现词语链接(e,f)的词语对齐概率之和，并对e进行归一化。</li>
</ul>
<p>以上就是EM算法的中心思想。</p>
<h3 id="EM算法迭代"><a href="#EM算法迭代" class="headerlink" title="EM算法迭代"></a>EM算法迭代</h3><p>我们可以使用EM算法迭代求解出对齐概率t，下面为EM算法求解过程：</p>
<ul>
<li><p>初始化$t(e|f)$</p>
</li>
<li><p>E-step: probability of alignments</p>
<ul>
<li><p>计算目标函数$p(a | \mathbf{e}, \mathbf{f})=\frac{p(\mathbf{e}, a | \mathbf{f})}{p(\mathbf{e} | \mathbf{f})}$, (使用上面公式计算$p(e,a|f)$)</p>
<ul>
<li><p>计算$p(e|f)$</p>
<p>$\begin{aligned} p(\mathbf{e} | \mathbf{f}) &amp;=\sum_{a} p(\mathbf{e}, a | \mathbf{f}) \ &amp;=\sum_{a(1)=0}^{l_{f}} \ldots \sum_{a\left(l_{e}\right)=0}^{l_{f}} p(\mathbf{e}, a | \mathbf{f}) \ &amp;=\sum_{a(1)=0}^{l_{f}} \ldots \sum_{a\left(l_{e}\right)=0} \frac{\epsilon}{\left(l_{f}+1\right)^{l_{e}}} \prod_{j=1}^{l_{e}} t\left(e_{j} | f_{a(j)}\right) \end{aligned}$</p>
<p>​        $\begin{array}{l}{=\frac{\epsilon}{\left(l_{f}+1\right)^{l_{e}}} \sum_{a(1)=0}^{l_{f}} \ldots \sum_{a\left(l_{e}\right)=0}^{l_{f}} \prod_{j=1}^{l_{e}} t\left(e_{j} | f_{a(j)}\right)} \ {=\frac{\epsilon}{\left(l_{f}+1\right)^{l_{e}}} \prod_{j=1}^{l_{e}} \sum_{i=0}^{l_{f}} t\left(e_{j} | f_{i}\right)}\end{array}​$</p>
</li>
</ul>
<p>​        一个计算例子如下：<br><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ibmmodel/5.png" alt="图片"></p>
</li>
<li><p>计算$p(a|e,f)$</p>
<p>  ​    $\begin{aligned} p(\mathbf{a} | \mathbf{e}, \mathbf{f}) &amp;=p(\mathbf{e}, \mathbf{a} | \mathbf{f}) / p(\mathbf{e} | \mathbf{f}) \ &amp;=\frac{\frac{\epsilon}{\left(l_{f}+1\right)^{l_{e}}} \prod_{j=1}^{l_{e}} t\left(e_{j} | f_{a(j)}\right)}{\frac{\epsilon}{\left(l_{f}+1\right)^{l_{e}}} \prod_{j=1}^{l_{e}} \sum_{i=0}^{l_{f}} t\left(e_{j} | f_{i}\right)} \ &amp;=\prod_{j=1}^{l_{e}} \frac{t\left(e_{j} | f_{a(j)}\right)}{\sum_{i=0}^{l_{f}} t\left(e_{j} | f_{i}\right)} \end{aligned}$</p>
</li>
<li><p>M-step: count collection</p>
<ul>
<li>计算$c(e | f ; \mathbf{e}, \mathbf{f})=\sum_{a} p(a | \mathbf{e}, \mathbf{f}) \sum_{j=1}^{l_{e}} \delta\left(e, e_{j}\right) \delta\left(f, f_{a(j)}\right)​$，并可以简化为$c(e | f ; \mathbf{e}, \mathbf{f})=\frac{t(e | f)}{\sum_{i=0}^{l_{f}} t\left(e | f_{i}\right)} \sum_{j=1}^{l_{e}} \delta\left(e, e_{j}\right) \sum_{i=0}^{l_{f}} \delta\left(f, f_{i}\right)​$</li>
<li>估计模型$t(e | f ; \mathbf{e}, \mathbf{f})=\frac{\left.\sum_{\mathbf{e}} \mathbf{f}_{\mathbf{j}} c(e | f ; \mathbf{e}, \mathbf{f})\right)}{\left.\sum_{e} \sum_{(\mathbf{e}, \mathbf{f})} c(e | f ; \mathbf{e}, \mathbf{f})\right)}$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="计算实例"><a href="#计算实例" class="headerlink" title="计算实例"></a>计算实例</h3><ul>
<li>训练句子：<ul>
<li>sentence1:  the house ||| la maison</li>
<li>sentence2:  house ||| maison</li>
</ul>
</li>
<li>画出所有对齐的可能<ul>
<li>sentence1: <ul>
<li>a1:  the-&gt;la、house-&gt;maison</li>
<li>a2: the-&gt;maison、house-&gt;la</li>
</ul>
</li>
<li>sentence2: <ul>
<li>a3: house-&gt;aison</li>
</ul>
</li>
</ul>
</li>
<li>初始化<ul>
<li>source_side_vocabulary: {the, house}, size=2</li>
<li>t(la|the) = 1/size = 1/2</li>
<li>t(maison|the) = 1/size = 1/2</li>
<li>t(la|house) = 1/size = 1/2</li>
<li>t(maison|house) = 1/size = 1/2</li>
</ul>
</li>
<li>第一次迭代<ul>
<li>Expectation:<ul>
<li>alignment probability<ul>
<li>p(e,a1|f) = t(la|the) <em> t(maison|house) =1/2 </em> 1/2  = 1/4</li>
<li>p(e, a2|f) = t(maison|the) <em> t(la|house) = 1/2 </em> 1/2 = 1/4</li>
<li>p(e, a3|f) = t(maison|house) = 1/2</li>
</ul>
</li>
<li>normalize alignment probability<ul>
<li>p(a1|E,F) = p(e,a1|f)/sum(p(E,a|F)) = p(e,a1|f)/(p(e,a1|f)+p(e, a2|f)) = 1/4/(1/4+1/4) = 1/2</li>
<li>p(a2|E,F) = p(e,a2|f)/(p(e,a1|f)+p(e, a2|f)) = 1/4/(1/4+1/4) = 1/2</li>
<li>p(a3|E,F) = p(e,a3|f)/p(e,a3|f) = 1</li>
</ul>
</li>
</ul>
</li>
<li>Max:<ul>
<li>collect counts<ul>
<li>c(la|the) = p(a1|E,F) <em> count(la|the) = 1/2 </em> 1 = 1/2</li>
<li>c(maison|the) = p(a2|E,F) <em> count(maison|the) = 1/2 </em> 1 = 1/2</li>
<li>c(la|house) = p(a2|E,F) <em> count(la|house) = 1/2 </em> 1 = 1/2</li>
<li>c(maison|house) = p(a1|E,F) <em> count(maison|house) + p(a3|E,F) </em> count(maison|house) = 1/2 <em> 1 + 1 </em> 1 = 3/2</li>
</ul>
</li>
<li>normalize<ul>
<li>t(la|the) = c(la|the)/sum(c(*|the)) = c(la|the)/(c(la|the) + c(maison|the) ) = 1/2/(1/2 + 1/2) = 1/2</li>
<li>t(maison|the) = c(maison|the)/sum(c(*|the)) = c(maison|the)/(c(la|the) + c(maison|the) ) = 1/2/(1/2 + 1/2) = 1/2</li>
<li>t(la|house) = 1/2/(1/2 + 3/2) = 1/4</li>
<li>t(maison|house) = 3/2/(1/2 + 3/2) = 3/4</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>第二次迭代<ul>
<li>Expectation:<ul>
<li>alignment probability<ul>
<li>p(e,a1|f) = 1/2 * 3/4 = 3/8</li>
<li>p(e, a2|f) = 1/2 * 1/4 = 1/8</li>
<li>p(e, a3|f) = 3/4</li>
</ul>
</li>
<li>normalize alignment probability<ul>
<li>p(a1|E,F) = 3/8/(3/8 + 1/8) = 3/4</li>
<li>p(a2|E,F) = 1/8/(3/8 + 1/8) = 1/4</li>
<li>p(a3|E,F) = 3/4/3/4 = 1</li>
</ul>
</li>
</ul>
</li>
<li>Max:<ul>
<li>collect counts<ul>
<li>c(la|the) = 3/4 * 1 = 3/4</li>
<li>c(maison|the) = 1/4 * 1 = 1/4</li>
<li>c(la|house) = 1/4 * 1 = 1/4</li>
<li>c(maison|house) = 3/4 <em> 1 + 1 </em> 1 = 7/4</li>
</ul>
</li>
<li>normalize<ul>
<li>t(la|the) = 3/4/(3/4 + 1/4) = 3/4</li>
<li>t(maison|the) = 1/4/(3/4 + 1/4) = 1/4</li>
<li>t(la|house) = 1/4/(1/4 + 7/4) = 1/8</li>
<li>t(maison|house) = 7/4/(1/4 + 7/4) = 7/8</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="NLTK源码分析"><a href="#NLTK源码分析" class="headerlink" title="NLTK源码分析"></a>NLTK源码分析</h3><ul>
<li>整体代码</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">counts &#x3D; Counts()</span><br><span class="line">for aligned_sentence in parallel_corpus:</span><br><span class="line">    trg_sentence &#x3D; aligned_sentence.words</span><br><span class="line">    src_sentence &#x3D; [None] + aligned_sentence.mots</span><br><span class="line"></span><br><span class="line">    # E step (a): Compute normalization factors to weigh counts</span><br><span class="line">    total_count &#x3D; self.prob_all_alignments(src_sentence, trg_sentence)</span><br><span class="line"></span><br><span class="line">    # E step (b): Collect counts</span><br><span class="line">    for t in trg_sentence:</span><br><span class="line">        for s in src_sentence:</span><br><span class="line">            count &#x3D; self.translation_table[t][s]</span><br><span class="line">            normalized_count &#x3D; count &#x2F; total_count[t]</span><br><span class="line">            counts.t_given_s[t][s] +&#x3D; normalized_count</span><br><span class="line">            counts.any_t_given_s[s] +&#x3D; normalized_count</span><br><span class="line">    </span><br><span class="line">    # M step: Update probabilities with maximum likelihood estimate</span><br><span class="line">    self.maximize_lexical_translation_probabilities(counts)</span><br></pre></td></tr></table></figure>
<ul>
<li>计算$\sum_{a} p(\mathbf{e}, a | \mathbf{f})$</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def prob_all_alignments(self, src_sentence, trg_sentence):</span><br><span class="line">  alignment_prob_for_t &#x3D; defaultdict(lambda: 0.0)</span><br><span class="line">  for t in trg_sentence:</span><br><span class="line">      for s in src_sentence:</span><br><span class="line">          alignment_prob_for_t[t] +&#x3D; self.translation_table[t][s]</span><br><span class="line">  return alignment_prob_for_t</span><br></pre></td></tr></table></figure>
<ul>
<li>计算M-step</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def maximize_lexical_translation_probabilities(self, counts):</span><br><span class="line">    for t, src_words in counts.t_given_s.items():</span><br><span class="line">        for s in src_words:</span><br><span class="line">            estimate &#x3D; counts.t_given_s[t][s] &#x2F; counts.any_t_given_s[s]</span><br><span class="line">            self.translation_table[t][s] &#x3D; max(estimate, IBMModel.MIN_PROB)</span><br></pre></td></tr></table></figure>
<ul>
<li>给定句对计算alignment</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">best_alignment &#x3D; []</span><br><span class="line">for j, trg_word in enumerate(sentence_pair.words):</span><br><span class="line">    best_prob &#x3D; max(self.translation_table[trg_word][None], IBMModel.MIN_PROB)</span><br><span class="line">    best_alignment_point &#x3D; None</span><br><span class="line">    for i, src_word in enumerate(sentence_pair.mots):</span><br><span class="line">        align_prob &#x3D; self.translation_table[trg_word][src_word]</span><br><span class="line">        if align_prob &gt;&#x3D; best_prob:  # prefer newer word in case of tie</span><br><span class="line">            best_prob &#x3D; align_prob</span><br><span class="line">            best_alignment_point &#x3D; i</span><br><span class="line"></span><br><span class="line">    best_alignment.append((j, best_alignment_point))</span><br><span class="line">sentence_pair.alignment &#x3D; Alignment(best_alignment)</span><br></pre></td></tr></table></figure>
<h2 id="IBM-Model-2"><a href="#IBM-Model-2" class="headerlink" title="IBM Model-2"></a>IBM Model-2</h2><blockquote>
<p>参考：<br><a href="https://www.nltk.org/_modules/nltk/translate/ibm2.html" target="_blank" rel="noopener">https://www.nltk.org/_modules/nltk/translate/ibm2.html</a><br><a href="http://www.ai.mit.edu/courses/6.891-nlp/l11.pdf" target="_blank" rel="noopener">http://www.ai.mit.edu/courses/6.891-nlp/l11.pdf</a></p>
</blockquote>
<p>IBM模型1的一个问题是它的重新排序能力非常弱，因为$p(f,a|s)$仅使用词法转换概率$t(t|s)$来计算。因此，如果模型有2个候选$t_1$和$t_2$具有相同的词汇翻译，但是对翻译后的单词进行了不同的重新排序，那么模型对这两个翻译都给出相同的分数。IBM-Model2使用对齐概率模型$p(i|j,s,f)$表示位置i、j的对齐概率，并用它计算$\operatorname{Pr}(t, a | s)=\frac{\epsilon}{(J+1)^{I}} \prod_{j=1}^{J} \operatorname{tr}\left(t_{j} | s_{a(j)}\right) \operatorname{Pr}_{a}(a(j) | j, J, I)$，其中$\operatorname{Pr}_{a}(a(j) | j, J, I)​$模拟一个单词在源句中的位置i被重新排序到目标句中的位置j的概率。</p>
<p>问题定义为：求解概率P(f|e)，其中$e=\{e_1,…,e_l\}$, $f=\{f_1,..,f_m\}$, 对齐定义为$\{a_1, …, a_m\}$且$a_j\in \{0,..,l\}$, 一共有$(l+1)^m$个对齐。IBM Model1的对齐概率认为是等概率的，即$P(\mathbf{a} | \mathbf{e})=C \times \frac{1}{(l+1)^{m}}$，且C是常数$C=\operatorname{prob}(\operatorname{length}(\mathbf{f})=m)$. 对于IBM Model1来说它生成出一个句子的具体过程为：</p>
<ul>
<li>选择长度为f(均等概率C产生的长度)</li>
<li>使用均等概率$1/(l+1)^m$产生对齐a</li>
<li>使用概率$P(\mathbf{f} | \mathbf{a}, \mathbf{e})=\prod_{j=1}^{m} \mathbf{T}\left(f_{j} | e_{a_{j}}\right)$产生目标语言</li>
<li>最终结果$P(\mathbf{f}, \mathbf{a} | \mathbf{e})=P(\mathbf{a} | e) P(\mathbf{f} | \mathbf{a}, e)=\frac{C}{(l+1)^{m}} \prod_{j=1}^{m} \mathbf{T}\left(f_{j} | e_{a_{j}}\right)$</li>
</ul>
<h3 id="翻译概率的定义-1"><a href="#翻译概率的定义-1" class="headerlink" title="翻译概率的定义"></a>翻译概率的定义</h3><p>在IBM Model-2中，定义D(i|j,l,m) = 给定source lenth e和target lenth f，第j个target word和第i个source word对齐的概。因此定义$P\left(\mathbf{a}=\left\{a_{1}, \ldots a_{m}\right\} | \mathbf{e}, l, m\right)=\prod_{j=1}^{m} \mathbf{D}\left(a_{j} | j, l, m\right)$，则翻译概率定义为：$P(\mathbf{f}, \mathbf{a} | \mathbf{e}, l, m)=\prod_{j=1}^{m} \mathbf{D}\left(a_{j} | j, l, m\right) \mathbf{T}\left(f_{j} | e_{a_{j}}\right)$。可以看出来，IBM-Model1是Model2的一个特例，在IBM-Model1中$\mathrm{D}(i | j, l, m)=\frac{1}{l+1}$</p>
<p>因此对于IBM Model2来说它生成出一个句子的具体过程为：</p>
<ul>
<li>选择长度为f(均等概率C产生的长度)</li>
<li>使用概率$\prod_{j=1}^{m} \mathrm{D}\left(a_{j} | j, l, m\right)$产生对齐$a=\{a_1,…,a_m\}$</li>
<li>使用概率$P(\mathbf{f} | \mathbf{a}, \mathbf{e})=\prod_{j=1}^{m} \mathbf{T}\left(f_{j} | e_{a_{j}}\right)$产生目标语言</li>
<li>最终结果$P(\mathbf{f}, \mathbf{a} | \mathbf{e})=P(\mathbf{a} | \mathbf{e}) P(\mathbf{f} | \mathbf{a}, \mathbf{e})=C \prod_{j=1}^{m} \mathbf{D}\left(a_{j} | j, l, m\right) \mathbf{T}\left(f_{j} | e_{a_{j}}\right)$</li>
</ul>
<h3 id="参数求解-1"><a href="#参数求解-1" class="headerlink" title="参数求解"></a>参数求解</h3><p>假设Model-2翻译模型定义为：$\operatorname{Pr}(\mathrm{F} | \mathrm{E})=\varepsilon \prod_{j=1}^{m} \sum_{i=0}^{l} t\left(f_{j} | e_{a_{j}}\right) a\left(a_{j} | j, m, l\right)$，同样通过引入拉格朗日乘子推导可以得到：</p>
<p>$t(f | e)=\lambda_{e}^{-1} c(f | e ; \mathrm{F}, \mathrm{E})$</p>
<p>$a(i | j, m, l)=\mu_{j m l}^{-1} c(i | j, m, l ; \mathrm{F}, \mathrm{E})$</p>
<p>$c(f | e ; \mathrm{F}, \mathrm{E})=\sum_{j=1}^{m} \sum_{i=0}^{l} \frac{t(f | e) a(i | j, m, l) \delta\left(f, f_{j}\right) \delta\left(e, e_{i}\right)}{t\left(f | e_{0}\right) a(0 | j, m, l)+\cdots+t\left(f | e_{l}\right) a(l | j, m, l)}$</p>
<p>$c(i | j, m, l ; \mathrm{F}, \mathrm{E})=\frac{t\left(f_{j} | e_{i}\right) a(i | j, m, l)}{t\left(f_{j} | e_{0}\right) a(0 | j, m, l)+\cdots+t\left(f_{j} | e_{l}\right) a(l | j, m, l)}$</p>
<p>考虑到训练语料库(F|E)是由一系列句子对组成的：$\left(\mathrm{F}^{(1)}, \mathrm{E}^{(\mathrm{i})}\right),\left(\mathrm{F}^{(2)}, \mathrm{E}^{(2)}\right), \cdots,\left(\mathrm{F}^{(\mathrm{s})}, \mathrm{E}^{(\mathrm{s})}\right)$</p>
<p>因此实际计算时我们采用以下公式：</p>
<p>$t(f | e)=\lambda_{e}^{-1} \sum_{s} c\left(f | e ; \mathrm{F}^{(\mathrm{s})}, \mathrm{E}^{(\mathrm{s})}\right)$</p>
<p>$a(i | j, m, l)=\mu_{j m l}^{-1} \sum_{s} c\left(i | j, m, l ; \mathrm{F}^{(\mathrm{s})}, \mathrm{E}^{(\mathrm{s})}\right)$</p>
<p>这里$\Lambda_{e}$和$\mu_{j m}$仅仅起到归一化因子的作用。</p>
<h3 id="EM算法迭代-1"><a href="#EM算法迭代-1" class="headerlink" title="EM算法迭代"></a>EM算法迭代</h3><ul>
<li>初始化$\begin{array}{r}\mathrm{T}(f | e) \ \mathrm{D}(i | j, l, m)\end{array}$</li>
<li>计算对齐概率$a[i, j, k]=\frac{\mathrm{D}\left(a_{j}=i | j, l, m\right) \mathrm{T}\left(f_{j} | e_{i}\right)}{\sum_{i^{\prime}=0}^{l} \mathrm{D}\left(a_{j}=i^{\prime} | j, l, m\right) \mathrm{T}\left(f_{j} | e_{i^{\prime}}\right)}$</li>
<li>E步：<ul>
<li>计算e和f对齐的期望次数：$\text { tcount }(e, f)=\sum_{\{|k, j|=f} a[i, j, k]$</li>
<li>计算e和任意target word对齐的期望次数：$\text { scount }(e)=\sum_{e[k, i]=\ell}^{i, k} \sum_{j=1}^{m[k]} a[i, j, k]$</li>
<li>计算对于source lenth 为l和target lenth 为m的句对ei和fj对齐的期望次数：$\operatorname{acount}(i, j, l, m)=\sum_{, m[k]=m} a_{\vdots=m}[i, j, k]$</li>
<li>计算表示source lenth 为l和target lenth 为m的期望次数：$\operatorname{acount}(j, l, m)=|\{k: l[k]=l, m[k]=m\}|$</li>
</ul>
</li>
<li>M步：<ul>
<li>重新估算翻译概率T(f|e)：$P(f | e)=\frac{\text { tcount }(e, f)}{\text { scount }(e)}$</li>
<li>重新估算对齐概率D(i|j,l,m)：$\left.P\left(a_{j}=i | j, l, m\right)\right)=\frac{a \operatorname{count}(i, j, l, m)}{\operatorname{acount}(j, l, m)}$</li>
</ul>
</li>
</ul>
<h3 id="计算实例-1"><a href="#计算实例-1" class="headerlink" title="计算实例"></a>计算实例</h3><ul>
<li>训练句子：<ul>
<li>sentence1:  the dog ||| le chien</li>
<li>sentence2:  the cat ||| le chat</li>
<li>sentence3: the bus ||| I’ autobus</li>
</ul>
</li>
<li>初始化<ul>
<li>source_side_vocabulary: {the, dog, cat, bus}, size=4</li>
<li>随机初始化<ul>
<li>T(f|e)</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ibmmodel/6.png" alt="图片"></p>
<p>​        * D(i|j,l,m)</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ibmmodel/7.png" alt="图片"></p>
<ul>
<li>E步：<ul>
<li>计算tcount(e,f)</li>
</ul>
</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ibmmodel/8.png" alt="图片"></p>
<p>如tcount(the, le) = a(1, 1, 0) + a(1, 1, 1) = 0.5264 + 0.4665 = 0.9929</p>
<ul>
<li>计算scount(e)</li>
<li>计算account(i,j,l,m)</li>
<li>计算account(j,l,m)</li>
<li>M步：<ul>
<li>重新估算T(f|e)(下图为经过几轮迭代后的变化)</li>
</ul>
</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ibmmodel/9.png" alt="图片"></p>
<ul>
<li>重新估算D(i|j,l,m)</li>
<li>IBM paper中建议用Model-1估计T(f|e)并用来初始化Model-2</li>
</ul>
<h3 id="NLTK源码分析-1"><a href="#NLTK源码分析-1" class="headerlink" title="NLTK源码分析"></a>NLTK源码分析</h3><ul>
<li>整体代码<ul>
<li>初始化</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if probability_tables is None:</span><br><span class="line">    ibm1 &#x3D; IBMModel1(sentence_aligned_corpus, 2 * iterations)</span><br><span class="line">    self.translation_table &#x3D; ibm1.translation_table</span><br><span class="line">    # a(i | j,l,m) &#x3D; 1 &#x2F; (l+1) for all i, j, l, m</span><br><span class="line">    l_m_combinations &#x3D; set()</span><br><span class="line">    for aligned_sentence in sentence_aligned_corpus:</span><br><span class="line">        l &#x3D; len(aligned_sentence.mots)</span><br><span class="line">        m &#x3D; len(aligned_sentence.words)</span><br><span class="line">        if (l, m) not in l_m_combinations:</span><br><span class="line">            l_m_combinations.add((l, m))</span><br><span class="line">            initial_prob &#x3D; 1 &#x2F; (l + 1)</span><br><span class="line">            for i in range(0, l + 1):</span><br><span class="line">                for j in range(1, m + 1):</span><br><span class="line">                    self.alignment_table[i][j][l][m] &#x3D; initial_prob</span><br><span class="line">else:</span><br><span class="line">    self.translation_table &#x3D; probability_tables[&quot;translation_table&quot;]</span><br><span class="line">    # D(i|j,l,m)</span><br><span class="line">    self.alignment_table &#x3D; probability_tables[&quot;alignment_table&quot;]</span><br><span class="line">    for n in range(0, iterations):</span><br><span class="line">        self.train(sentence_aligned_corpus)</span><br><span class="line">    self.align_all(sentence_aligned_corpus)</span><br></pre></td></tr></table></figure>
<ul>
<li>训练</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def train(self, parallel_corpus):</span><br><span class="line">    counts &#x3D; Model2Counts()</span><br><span class="line">    for aligned_sentence in parallel_corpus:</span><br><span class="line">        src_sentence &#x3D; [None] + aligned_sentence.mots</span><br><span class="line">        trg_sentence &#x3D; [&quot;UNUSED&quot;] + aligned_sentence.words</span><br><span class="line">        l &#x3D; len(aligned_sentence.mots)</span><br><span class="line">        m &#x3D; len(aligned_sentence.words)</span><br><span class="line">        # E step (a): Compute normalization factors to weigh counts</span><br><span class="line">        alignment_prob_for_t &#x3D; defaultdict(lambda: 0.0)</span><br><span class="line">        for j in range(1, len(trg_sentence)):</span><br><span class="line">            t &#x3D; trg_sentence[j]</span><br><span class="line">            for i in range(0, len(src_sentence)):</span><br><span class="line">                alignment_prob_for_t[t] +&#x3D; self.prob_alignment_point(</span><br><span class="line">                    i, j, src_sentence, trg_sentence</span><br><span class="line">                )</span><br><span class="line">        total_count &#x3D; alignment_prob_for_t</span><br><span class="line">        # E step (b): Collect counts</span><br><span class="line">        for j in range(1, m + 1):</span><br><span class="line">            t &#x3D; trg_sentence[j]</span><br><span class="line">            for i in range(0, l + 1):</span><br><span class="line">                s &#x3D; src_sentence[i]</span><br><span class="line">                l &#x3D; len(src_sentence) - 1</span><br><span class="line">                m &#x3D; len(trg_sentence) - 1</span><br><span class="line">                s &#x3D; src_sentence[i]</span><br><span class="line">                t &#x3D; trg_sentence[j]</span><br><span class="line">                count &#x3D; self.translation_table[t][s] * self.alignment_table[i][j][l][m]</span><br><span class="line">                normalized_count &#x3D; count &#x2F; total_count[t]</span><br><span class="line">                self.t_given_s[t][s] +&#x3D; normalized_count</span><br><span class="line">                self.any_t_given_s[s] +&#x3D; normalized_count</span><br><span class="line">                self.alignment[i][j][l][m] +&#x3D; normalized_count</span><br><span class="line">                self.alignment_for_any_i[j][l][m] +&#x3D; normalized_count</span><br><span class="line">    # M step: Update probabilities with maximum likelihood estimates</span><br><span class="line">    self.maximize_lexical_translation_probabilities(counts)</span><br><span class="line">    self.maximize_alignment_probabilities(counts)</span><br></pre></td></tr></table></figure>
<ul>
<li>计算M-step</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def maximize_alignment_probabilities(self, counts):</span><br><span class="line">    MIN_PROB &#x3D; IBMModel.MIN_PROB</span><br><span class="line">    for i, j_s in counts.alignment.items():</span><br><span class="line">        for j, src_sentence_lengths in j_s.items():</span><br><span class="line">            for l, trg_sentence_lengths in src_sentence_lengths.items():</span><br><span class="line">                for m in trg_sentence_lengths:</span><br><span class="line">                    estimate &#x3D; (</span><br><span class="line">                        counts.alignment[i][j][l][m]</span><br><span class="line">                        &#x2F; counts.alignment_for_any_i[j][l][m]</span><br><span class="line">                    )</span><br><span class="line">                    self.alignment_table[i][j][l][m] &#x3D; max(estimate, MIN_PROB)</span><br></pre></td></tr></table></figure>
<ul>
<li>给定句对计算alignment：$a_{j}^{*, 2}=\operatorname{argmax}_{j}\left(\mathrm{T}\left(f_{j} | e_{a_{j}}\right) \mathrm{D}(j | i, l, m)\right)​$</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def align(self, sentence_pair):</span><br><span class="line">    best_alignment &#x3D; []</span><br><span class="line">    l &#x3D; len(sentence_pair.mots)</span><br><span class="line">    m &#x3D; len(sentence_pair.words)</span><br><span class="line">    for j, trg_word in enumerate(sentence_pair.words):</span><br><span class="line">        # Initialize trg_word to align with the NULL token</span><br><span class="line">        best_prob &#x3D; (</span><br><span class="line">            self.translation_table[trg_word][None]</span><br><span class="line">            * self.alignment_table[0][j + 1][l][m]</span><br><span class="line">        )</span><br><span class="line">        best_prob &#x3D; max(best_prob, IBMModel.MIN_PROB)</span><br><span class="line">        best_alignment_point &#x3D; None</span><br><span class="line">        for i, src_word in enumerate(sentence_pair.mots):</span><br><span class="line">            align_prob &#x3D; (</span><br><span class="line">                self.translation_table[trg_word][src_word]</span><br><span class="line">                * self.alignment_table[i + 1][j + 1][l][m]</span><br><span class="line">            )</span><br><span class="line">            if align_prob &gt;&#x3D; best_prob:</span><br><span class="line">                best_prob &#x3D; align_prob</span><br><span class="line">                best_alignment_point &#x3D; i</span><br><span class="line"></span><br><span class="line">        best_alignment.append((j, best_alignment_point))</span><br><span class="line"></span><br><span class="line">    sentence_pair.alignment &#x3D; Alignment(best_alignment)</span><br></pre></td></tr></table></figure>
<h2 id="IBM-Model-3"><a href="#IBM-Model-3" class="headerlink" title="IBM Model-3"></a>IBM Model-3</h2><blockquote>
<p>参考：<br><a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1162/handouts/cs224n-lecture3-MT.pdf" target="_blank" rel="noopener">https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1162/handouts/cs224n-lecture3-MT.pdf</a><br><a href="http://www.ai.mit.edu/courses/6.891-nlp/l11.pdf" target="_blank" rel="noopener">http://www.ai.mit.edu/courses/6.891-nlp/l11.pdf</a><br><a href="http://www.ai.mit.edu/courses/6.891-nlp/l12.pdf" target="_blank" rel="noopener">http://www.ai.mit.edu/courses/6.891-nlp/l12.pdf</a><br><a href="https://www.nltk.org/_modules/nltk/translate/ibm3" target="_blank" rel="noopener">https://www.nltk.org/_modules/nltk/translate/ibm3</a>.html<br><a href="http://www1.maths.lth.se/matematiklth/vision/publdb/reports/pdf/schoenemann-ccnll-10.pdf" target="_blank" rel="noopener">http://www1.maths.lth.se/matematiklth/vision/publdb/reports/pdf/schoenemann-ccnll-10.pdf</a></p>
</blockquote>
<p>从Model-3到Model-5，翻译模型如下图所示：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ibmmodel/10.png" alt="图片"></p>
<p>具体步骤是：</p>
<ul>
<li>首先根据源语言词语的繁殖概率，确定每个源语言词翻译成多少个目标语言词</li>
<li>根据每个源语言词语的目标语言词数，将每个源语言词复制若干次</li>
<li>将复制后得到的每个源语言词，根据翻译概率，翻译成一个目标语言词</li>
<li>根据调序概率，将翻译得到的目标语言词重新调整顺序，得到目标语言句子</li>
</ul>
<p>对于Model-3来说，其推导过程为：</p>
<ul>
<li>对于句子中每一个英语单词e，选择一个产出率φ，其概率为n(φ|e)</li>
<li>对于所有单词的产出率求和得到m-prime</li>
<li>按照下面的方式构造一个新的英语单词串：删除产出率为0的单词，复制产出率为1的单词，复制两遍产出率为2的单词，依此类推</li>
<li>在这m-prime个单词的每一个后面，决定是否插入一个空单词NULL，插入和不插入的概率分别为p1和p0</li>
<li>φ0为插入的空单词NULL的个数</li>
<li>设m为目前的总单词数：m-prime+φ0</li>
<li>根据概率表t(f|e)，将每一个单词e替换为外文单词f</li>
<li>对于不是由空单词NULL产生的每一个外语单词，根据概率表d(j|i,l,m)，赋予一个位置。这里j是法语单词在法语串中的位置，i是产生当前这个法语单词的对应英语单词在英语句子中的位置，l是英语串的长度，m是法语串的长度</li>
<li>如果任何一个目标语言位置被多重登录（含有一个以上单词），则返回失败</li>
<li>给空单词NULL产生的单词赋予一个目标语言位置。这些位置必须是空位置（没有被占用）。任何一个赋值都被认为是等概率的，概率值为1/φ0</li>
<li>最后，读出法语串，其概率为上述每一步概率的乘积</li>
</ul>
<h3 id="翻译概率的定义-2"><a href="#翻译概率的定义-2" class="headerlink" title="翻译概率的定义"></a>翻译概率的定义</h3><p>Model-3引入fertility参数来约束一个源语言单词可以产生多少个目标语言单词, 并且从Model-3开始其建模过程和Model-1、Model-2有所不同，其产生句子的总体过程如下：</p>
<ul>
<li>对每个ej产生一个fertility $\phi_{j}$只依赖于ej）</li>
<li>对每个ej生成$\phi_{j}$个target word词（只依赖于ej而不依赖于任何其他context）</li>
<li>给每个target words选择一个位置（只依赖于ej的位置和句子长度）</li>
</ul>
<p>具体过程如下：</p>
<ul>
<li>有英文句子$e=\{e_1,…,e_l\}$, 想要建模$P(f|e)$</li>
<li>使用概率$P\left(\left\{\phi_{0} \ldots \phi_{l}\right\} | \mathbf{e}\right)$选择$l+1$个fertilities$\{\phi_{0} \ldots \phi_{l}\}$<ul>
<li>$P\left(\left\{\phi_{0} \ldots \phi_{l}\right\} | \mathbf{e}\right)=P\left(\phi_{0} | \phi_{1} \ldots \phi_{l}\right) \prod_{i=1}^{l} \mathrm{F}\left(\phi_{i} | e_{i}\right)$<ul>
<li>$F(\phi|e)$表示e和$\phi$个单词对齐的概率：F(0|the)=0.1、F(1|the)=0.9、F(2|the)=0…F(0|not)=0.01、F(1|not)=0.09、F(2|not)=0.9</li>
<li>$P\left(\phi_{0} | \phi_{1} \ldots \phi_{l}\right)$表示以出现$p_1$正面的概率硬币m次，出现$\phi_0$次正面的概率：$P\left(\phi_{0} | \phi_{1} \ldots \phi_{l}\right)=\frac{m !}{\left(m-\phi_{0}\right) ! \phi_{0} !} p_{1}^{\phi_{0}}\left(1-p_{1}\right)^{m-\phi_{0}}$<ul>
<li>$m=\sum_{i=1}^{l} \phi_{i}$，可以这样了理解：假设已经有m个source word产生，则这m个词的每个词都增加一个$p_1$概率的空对齐</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>对于每一个$e_i$, 使用概率$\prod_{i=0}^{l} \prod_{k=1}^{\phi_{i}} \mathrm{R}\left(\pi_{i, k} | i, l, m\right) \mathrm{T}\left(f_{i, k} | e_{i}\right)$选择位置$\pi_{i,k}\in 1…m$和target word $f_{i,k}$<ul>
<li>$R(j|i,l,m)$表示给定source lenth l、target lenth m 和 source position i，产生target position j的概率</li>
<li>$R(j|i,l,m)$要注意与之前的$D(i|j,l,m)$区别，$D(i|j,l,m)$表示给定source lenth l、target lenth m 和 target position j，产生source position i的概率</li>
</ul>
</li>
<li>根据上面推导产生模型:</li>
</ul>
<p>$\begin{aligned} \phi=&amp;\left\{\phi_{0} \ldots \phi_{m}\right\} \ \pi=&amp;\left\{\pi_{i, k}: i=0 \ldots m, k=1 \ldots \phi_{i}\right\} \ \mathbf{f} 2=&amp;\left\{f_{i, k}: i=0 \ldots m, k=1 \ldots \phi_{i}\right\} \ &amp; P(\phi, \pi, \mathbf{f} 2 | \mathbf{e})=\ \frac{m !}{\left(m-\phi_{0}\right) ! \phi_{0} !} p_{1}^{\phi_{0}}\left(1-p_{1}\right)^{m-\phi_{0}} &amp;\left(\prod_{i=0}^{l} \prod_{k=1}^{\phi_{i}} \mathbf{R}\left(\pi_{i, k} | i, l, m\right) \mathbf{T}\left(f_{i, k} | e_{i}\right)\right) \end{aligned}$</p>
<ul>
<li>进一步优化：</li>
</ul>
<p>$\begin{array}{c}P(\mathbf{f}, \mathbf{a} | \mathbf{e})= \ \frac{m !}{\left(m-\phi_{0}\right) ! \phi_{0} !} p_{1}^{\phi_{0}}\left(1-p_{1}\right)^{m-\phi_{0}}\left(\prod_{i=1}^{l} \mathbf{F}\left(\phi_{i} | e_{i}\right) \phi_{i} !\right)\left(\prod_{i=1}^{l} \prod_{k=1}^{\phi_{i}} \mathbf{R}\left(\pi_{i, k} | i, l, m\right)\right)\left(\prod_{i=0}^{l} \prod_{k=1}^{\phi_{i}} \mathbf{T}\left(f_{i, k} | e_{i}\right)\right)\end{array}$</p>
<h3 id="EM算法迭代-2"><a href="#EM算法迭代-2" class="headerlink" title="EM算法迭代"></a>EM算法迭代</h3><ul>
<li>初始化<ul>
<li>fertility_table：$F(\phi | e)$</li>
<li>translation_table：T(f|e)用Model-2的结果初始化</li>
<li>distortion_table：R(j|i,l,m)</li>
<li>p1=0.5</li>
<li>alignment_table：a[i,j,k]用Model-2的结果初始化</li>
</ul>
</li>
<li>E步：Model-1和Model-2可以计算出expect count，Model-3计算量太大，所以使用了一个近似算法<ul>
<li>什么是expect count？以句对e = I do not understand the logic of these people，f = Je ne comprends pas la logique de ces gens -la为例，在给定当前模型参数下，$\phi_{3}$（not 的 fertility）的期望值为$\sum_{\mathbf{a} \in \mathcal{A}} P(\mathbf{a} | \mathbf{f}, \mathbf{e}) \phi_{3}(\mathbf{a})=\sum_{\mathbf{a} \in \mathcal{A}} \frac{P(\mathbf{f}, \mathbf{a} | \mathbf{e})}{\sum_{\mathbf{a}^{\prime} \in \mathcal{A}} P\left(\mathbf{f}, \mathbf{a}^{\prime} | \mathbf{e}\right)} \phi_{3}(\mathbf{a})$，其中$\phi_{3}(\mathbf{a})$表示对齐a中$\phi_{3}(\mathbf{a})$的值</li>
<li>在Model-3中使用high probability alignments $\overline{\mathcal{A}}$计算$\sum_{\mathbf{a} \in \mathcal{A}} \frac{P(\mathbf{f}, \mathbf{a} | \mathbf{e})}{\sum_{\mathbf{a}^{\prime} \in \overline{\mathcal{A}}} P\left(\mathbf{f}, \mathbf{a}^{\prime} | \mathbf{e}\right)} \phi_{3}(\mathbf{a})$减少计算复杂度</li>
</ul>
</li>
<li>M步：重新估算参数概率</li>
</ul>
<h3 id="Viterbi训练"><a href="#Viterbi训练" class="headerlink" title="Viterbi训练"></a>Viterbi训练</h3><blockquote>
<p>参考：<br>词语对齐的对数线性模型：<a href="http://nlp.ict.ac.cn/~liuyang/papers/acl05_chn.pdf" target="_blank" rel="noopener">http://nlp.ict.ac.cn/~liuyang/papers/acl05_chn.pdf</a></p>
</blockquote>
<p>Viterbi参数训练算法的总体思路：</p>
<ul>
<li>给定初始参数</li>
<li>用已有的参数求概率最大（Viterbi）的词语对齐</li>
<li>用得到的概率最大的词语对齐重新计算参数</li>
<li>回到第二步，直到收敛为止</li>
</ul>
<p>在对参数计算公式无法化简的情况下，采用Viterbi参数训练算法是一种可行的做法，这种算法通常可以迅速收敛到一个可以接受的结果。</p>
<p>由于IBM Model 1和2存在简化的迭代公式，实际上在EM算法迭代是并不用真的去计算所有的对齐，而是可以利用迭代公式直接计算下一次的参数；由于IBM Model 3、4、5的翻译模型公式无法化简，理论上应该进行EM迭代。由于实际上由于计算所有词语对齐的代价太大，通常采用Viterbi训练，每次E步骤只生成最好的一个或者若干个对齐。</p>
<p>Generalized Iterative Scaling算法(GIS)。</p>
<h3 id="计算实例-2"><a href="#计算实例-2" class="headerlink" title="计算实例"></a>计算实例</h3><p>以一个实际例子来说明翻译过程：I do not understand the logic of these people翻译成法语。</p>
<ul>
<li>pick fertilities：以概率$\begin{aligned} P\left(\phi_{1} \ldots \phi_{l} | \mathbf{e}\right) &amp;=\prod_{i=1}^{l} \mathbf{F}\left(\phi_{i} | e_{i}\right) \ &amp;=\mathrm{F}(1 | I) \mathrm{F}(0 | d o) \mathrm{F}(2 | n o t) \mathrm{F}(1 | \text { understand }) \mathrm{F}(1 | \text { the }) \ &amp; \mathrm{F}(1 | \text { logic }) \mathrm{F}(1 | \text { of }) \mathrm{F}(1 | \text { these }) \mathrm{F}(1 | \text { people }) \end{aligned}$产生</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ibmmodel/11.png" alt="图片"></p>
<ul>
<li>replace words：以概率$\begin{aligned} \prod_{i=1}^{l} \phi_{i} ! \prod_{k=1}^{\phi_{i}} \mathrm{T}\left(\mathrm{f}_{i, k} | e_{i}\right)=&amp; 1 ! \times 0 ! \times 2 ! \times 1 ! \times 1 ! \times 1 ! \times 1 ! \times 1 ! \times 1 ! \times \ &amp; \mathrm{T}(J e | I) \mathrm{T}(n e | n o t) \mathrm{T}(p a s | n o t) \times \ &amp; \mathrm{T}(\text {comprends} | \text {understand}) \mathrm{T}(l a | \text {the}) \mathrm{T}(\text {logique } | \text {logic}) \times \ &amp; \mathrm{T}(\text {de } | \text {of}) \mathrm{T}(\text {ces} | \text {these}) \mathrm{T}(\text {gens } | \text {people}) \end{aligned}​$产生目标词汇</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ibmmodel/15.png" alt="图片"></p>
<ul>
<li>reorder：以概率$\begin{aligned} \prod_{i=1}^{1} \prod_{k=1}^{\phi_{i}} \mathrm{R}\left(\pi_{i, k} | i, l, m\right)=&amp; \mathrm{R}(j=1 | i=1, l=9, m=10) \mathrm{R}(2 | 3,9,10) \mathrm{R}(3 | 4,9,10) \times \ &amp; \mathrm{R}(4 | 3,9,10) \mathrm{R}(5 | 5,9,10) \mathrm{R}(6 | 6,9,10) \times \ &amp; \mathrm{R}(7 | 7,9,10) \mathrm{R}(8 | 8,9,10) \mathrm{R}(9 | 9,9,10) \end{aligned}$重新排序</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ibmmodel/13.png" alt="图片"></p>
<ul>
<li>spurious words：以概率$\begin{aligned} P\left(\phi_{0} | \phi_{1} \ldots \phi_{1}\right) \prod_{k=1}^{60} \mathrm{T}\left(\mathrm{f}_{0, k} | N U L L\right) &amp;=\frac{n !}{\left(n-\phi_{0}\right) ! \phi_{0} !} p_{1}^{00}\left(1-p_{1}\right)^{n-\phi_{0}} \prod_{k=1}^{60} \mathrm{T}\left(\mathrm{f}_{0, k} | N U L L\right) \ &amp;=\frac{9 !}{811 !} p_{1}\left(1-p_{1}\right)^{8} \mathrm{T}(-l a | N U L L) \ &amp;=9 p_{1}\left(1-p_{1}\right)^{8} \mathrm{T}(-l a | N U L L) \end{aligned}​$产生</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ibmmodel/14.png" alt="图片">，这里$n=\sum_{i=1}^{l} \phi_{l}=m-\phi_{0}$</p>
<ul>
<li>p1是$\phi_{0}$（即空对齐）的概率，spurious words即产生T(f|NULL)</li>
</ul>
<h3 id="NLTK源码分析-2"><a href="#NLTK源码分析-2" class="headerlink" title="NLTK源码分析"></a>NLTK源码分析</h3><ul>
<li>整体代码<ul>
<li>初始化</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if probability_tables is None:</span><br><span class="line">    ibm2 &#x3D; IBMModel2(sentence_aligned_corpus, iterations)</span><br><span class="line">    self.translation_table &#x3D; ibm2.translation_table</span><br><span class="line">    self.alignment_table &#x3D; ibm2.alignment_table</span><br><span class="line">    # d(j | i,l,m) &#x3D; 1 &#x2F; m for all i, j, l, m</span><br><span class="line">    l_m_combinations &#x3D; set()</span><br><span class="line">    for aligned_sentence in sentence_aligned_corpus:</span><br><span class="line">        l &#x3D; len(aligned_sentence.mots)</span><br><span class="line">        m &#x3D; len(aligned_sentence.words)</span><br><span class="line">        if (l, m) not in l_m_combinations:</span><br><span class="line">            l_m_combinations.add((l, m))</span><br><span class="line">            initial_prob &#x3D; 1 &#x2F; m</span><br><span class="line">            for j in range(1, m + 1):</span><br><span class="line">                for i in range(0, l + 1):</span><br><span class="line">                    self.distortion_table[j][i][l][m] &#x3D; initial_prob</span><br><span class="line">     # simple initialization, taken from GIZA++</span><br><span class="line">    self.fertility_table[0] &#x3D; defaultdict(lambda: 0.2)</span><br><span class="line">    self.fertility_table[1] &#x3D; defaultdict(lambda: 0.65)</span><br><span class="line">    self.fertility_table[2] &#x3D; defaultdict(lambda: 0.1)</span><br><span class="line">    self.fertility_table[3] &#x3D; defaultdict(lambda: 0.04)</span><br><span class="line">    MAX_FERTILITY &#x3D; 10</span><br><span class="line">    initial_fert_prob &#x3D; 0.01 &#x2F; (MAX_FERTILITY - 4)</span><br><span class="line">    for phi in range(4, MAX_FERTILITY):</span><br><span class="line">        self.fertility_table[phi] &#x3D; defaultdict(lambda: initial_fert_prob)</span><br><span class="line">    self.p1 &#x3D; 0.5</span><br><span class="line">else:</span><br><span class="line">    self.translation_table &#x3D; probability_tables[&quot;translation_table&quot;]</span><br><span class="line">    self.alignment_table &#x3D; probability_tables[&quot;alignment_table&quot;]</span><br><span class="line">    self.fertility_table &#x3D; probability_tables[&quot;fertility_table&quot;]</span><br><span class="line">    self.p1 &#x3D; probability_tables[&quot;p1&quot;]</span><br><span class="line">    self.distortion_table &#x3D; probability_tables[&quot;distortion_table&quot;]</span><br></pre></td></tr></table></figure>
<ul>
<li>训练</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def train(self, parallel_corpus):</span><br><span class="line">        counts &#x3D; Model3Counts()</span><br><span class="line">        for aligned_sentence in parallel_corpus:</span><br><span class="line">            l &#x3D; len(aligned_sentence.mots)</span><br><span class="line">            m &#x3D; len(aligned_sentence.words)</span><br><span class="line">            # Sample the alignment space</span><br><span class="line">            sampled_alignments, best_alignment &#x3D; self.sample(aligned_sentence)</span><br><span class="line">            # Record the most probable alignment</span><br><span class="line">            aligned_sentence.alignment &#x3D; Alignment(best_alignment.zero_indexed_alignment())</span><br><span class="line">            # E step (a): Compute normalization factors to weigh counts, https:&#x2F;&#x2F;github.com&#x2F;nltk&#x2F;nltk&#x2F;blob&#x2F;5023d6b933ef1a5b1f25fba1d5ed11a8a43a47e4&#x2F;nltk&#x2F;translate&#x2F;ibm_model.py中有详细计算</span><br><span class="line">            total_count &#x3D; self.prob_of_alignments(sampled_alignments)</span><br><span class="line">            # E step (b): Collect counts</span><br><span class="line">            for alignment_info in sampled_alignments:</span><br><span class="line">                count &#x3D; self.prob_t_a_given_s(alignment_info)</span><br><span class="line">                normalized_count &#x3D; count &#x2F; total_count</span><br><span class="line">                for j in range(1, m + 1):</span><br><span class="line">                   counts.update_lexical_translation(normalized_count, alignment_info, j)</span><br><span class="line">                   counts.update_distortion(normalized_count, alignment_info, j, l, m)</span><br><span class="line">                counts.update_null_generation(normalized_count, alignment_info)</span><br><span class="line">                counts.update_fertility(normalized_count, alignment_info)</span><br><span class="line">        # M step:</span><br><span class="line">        # If any probability is less than MIN_PROB, clamp it to MIN_PROB</span><br><span class="line">        existing_alignment_table &#x3D; self.alignment_table</span><br><span class="line">        self.reset_probabilities()</span><br><span class="line">        self.alignment_table &#x3D; existing_alignment_table  # don&#39;t retrain</span><br><span class="line"></span><br><span class="line">        self.maximize_lexical_translation_probabilities(counts)</span><br><span class="line">        self.maximize_distortion_probabilities(counts)</span><br><span class="line">        self.maximize_fertility_probabilities(counts)</span><br><span class="line">        self.maximize_null_generation_probabilities(counts)</span><br></pre></td></tr></table></figure>
<ul>
<li>计算M-step<ul>
<li>最大化distortion概率</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def maximize_distortion_probabilities(self, counts):</span><br><span class="line">    MIN_PROB &#x3D; IBMModel.MIN_PROB</span><br><span class="line">    for j, i_s in counts.distortion.items():</span><br><span class="line">        for i, src_sentence_lengths in i_s.items():</span><br><span class="line">            for l, trg_sentence_lengths in src_sentence_lengths.items():</span><br><span class="line">                for m in trg_sentence_lengths:</span><br><span class="line">                    estimate &#x3D; (counts.distortion[j][i][l][m]&#x2F; counts.distortion_for_any_j[i][l][m])</span><br><span class="line">                    self.distortion_table[j][i][l][m] &#x3D; max(estimate, MIN_PROB)</span><br></pre></td></tr></table></figure>
<ul>
<li>计算给定source sentence，产生target sentence和alignment的概率</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def prob_t_a_given_s(self, alignment_info):</span><br><span class="line">    src_sentence &#x3D; alignment_info.src_sentence</span><br><span class="line">    trg_sentence &#x3D; alignment_info.trg_sentence</span><br><span class="line">    l &#x3D; len(src_sentence) - 1  # exclude NULL</span><br><span class="line">    m &#x3D; len(trg_sentence) - 1</span><br><span class="line">    p1 &#x3D; self.p1</span><br><span class="line">    p0 &#x3D; 1 - p1</span><br><span class="line"></span><br><span class="line">    probability &#x3D; 1.0</span><br><span class="line">    MIN_PROB &#x3D; IBMModel.MIN_PROB</span><br><span class="line"></span><br><span class="line">    # Combine NULL insertion probability</span><br><span class="line">    null_fertility &#x3D; alignment_info.fertility_of_i(0)</span><br><span class="line">    probability *&#x3D; pow(p1, null_fertility) * pow(p0, m - 2 * null_fertility)</span><br><span class="line">    if probability &lt; MIN_PROB:</span><br><span class="line">        return MIN_PROB</span><br><span class="line"></span><br><span class="line">    # Compute combination (m - null_fertility) choose null_fertility</span><br><span class="line">    for i in range(1, null_fertility + 1):</span><br><span class="line">        probability *&#x3D; (m - null_fertility - i + 1) &#x2F; i</span><br><span class="line">        if probability &lt; MIN_PROB:</span><br><span class="line">            return MIN_PROB</span><br><span class="line"></span><br><span class="line">    # Combine fertility probabilities</span><br><span class="line">    for i in range(1, l + 1):</span><br><span class="line">        fertility &#x3D; alignment_info.fertility_of_i(i)</span><br><span class="line">        probability *&#x3D; (</span><br><span class="line">            factorial(fertility) * self.fertility_table[fertility][src_sentence[i]]</span><br><span class="line">        )</span><br><span class="line">        if probability &lt; MIN_PROB:</span><br><span class="line">            return MIN_PROB</span><br><span class="line"></span><br><span class="line">    # Combine lexical and distortion probabilities</span><br><span class="line">    for j in range(1, m + 1):</span><br><span class="line">        t &#x3D; trg_sentence[j]</span><br><span class="line">        i &#x3D; alignment_info.alignment[j]</span><br><span class="line">        s &#x3D; src_sentence[i]</span><br><span class="line">        probability *&#x3D; (self.translation_table[t][s] * self.distortion_table[j][i][l][m])</span><br><span class="line">        if probability &lt; MIN_PROB:</span><br><span class="line">            return MIN_PROB</span><br><span class="line"></span><br><span class="line">    return probability</span><br></pre></td></tr></table></figure>
<h3 id="计算alignment"><a href="#计算alignment" class="headerlink" title="计算alignment"></a>计算alignment</h3><ul>
<li>用Model-2计算最可能的alignment：$\mathbf{a}^{<em>, 2}=\operatorname{argmax}_{\mathbf{a} \in \mathcal{A}} P_{2}(\mathbf{f}, \mathbf{a} | \mathbf{e})$、$a_{j}^{</em> 2}=\operatorname{argmax}_{j}\left(\mathrm{T}\left(f_{j} | e_{a_{j}}\right) \mathrm{D}(j | i, l, m)\right)$</li>
<li>计算a2的邻居（邻居指通过替换a2中某个对齐或交换某两个对齐产生的新的对齐）</li>
<li>迭代计算a3（初值设置为a2）：$\mathbf{a}^{<em>, 3}=\operatorname{argmax}_{\mathbf{a} \in \mathcal{N}\left(\mathbf{a}^{</em>, 3}\right)} \quad P_{3}(\mathbf{a}, \mathbf{f} | \mathbf{e})​$<ul>
<li>上式等价于求解概率的负对数</li>
<li>对于所有的source position $j \in\{1, \ldots, J\}$和target position $i \in\{0, \ldots, \tilde{I}\}$,引入$x_{i j} \in\{0,1\}$表示i和j是否对齐，且由于每个i只能和1个j对齐，因此存在约束$\sum_{i} x_{i j}=1 \quad, j=1, \ldots, J$，从而求解目标为：$c_{i j}^{x}=-\log \left[p\left(f_{j} | e_{i}\right) \cdot p(j | i)\right]$</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>SMT</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：《Jointly Learning to Align and Translate with Transformer》</title>
    <url>/2020/03/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E3%80%8AJointly-Learning-to-Align-and-Translate-with-Transformer%E3%80%8B/</url>
    <content><![CDATA[<p>最近在对齐方面看的比较多，这一篇是去年看到的使用多任务学习提高对齐效果的文章。今天仔细读一遍。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://arxiv.org/pdf/1909.02074.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1909.02074.pdf</a></p>
</blockquote>
<h1 id="论文简介"><a href="#论文简介" class="headerlink" title="论文简介"></a>论文简介</h1><p>神经机器翻译已经统治了翻译领域，其中的attention机制是从词对齐借鉴出来的，但是NMT中的attention和词对齐又有很大不同。Attention更倾向于attend到context word而不是source word本身，而且现在的multi-layer、multi-head机制又使得attention非常复杂。</p>
<p>由于词对齐可以用在很多地方，比如对于实体名词的翻译，或者对于low-resource语言的借助词表的翻译有很大作用。因此本文提出了一个多任务学习的方法，使用NMT的negative log likelihood（NLL）loss和alignment loss结合作为多任务学习的loss。而且和NMT的auto-regressive式的模型不同，NMT在翻译时需要借助past target context的信息，而对于词对齐来说是不够用的，因此本文在多任务中使用了不同的context信息进行生成。</p>
<h1 id="问题定义和基线模型"><a href="#问题定义和基线模型" class="headerlink" title="问题定义和基线模型"></a>问题定义和基线模型</h1><p>给定source sentence：f(1,J)=f1,….,fj,…fJ和target translation：e(1,I)=f1,….,fj,…fI，则对齐就是位置的笛卡尔序列：$\mathcal{A} \subseteq\{(j, i): j=1, \ldots, J ; i=1, \ldots, I\}$。词对齐任务就是找到这样的多对多的对应关系。Transformer模型计算过程如下：</p>
<p>$\tilde{\mathbf{q}}_{n}^{i}=\mathbf{q}^{i} W_{n}^{Q}, \tilde{K}_{n}=K W_{n}^{K}, \tilde{V}_{n}=V W_{n}^{V}​$</p>
<p>$H_{n}^{i}=\text { Attention }\left(\tilde{\mathbf{q}}_{n}^{i}, \tilde{K}_{n}, \tilde{V}_{n}\right)$</p>
<p>$\mathcal{M}\left(\mathbf{q}^{i}, K, V\right)=\operatorname{Concat}\left(H_{1}^{i}, \ldots, H_{N}^{i}\right) W^{O}$</p>
<p>$\text { Attention }\left(\tilde{\mathbf{q}}_{n}^{i}, \tilde{K}_{n}, \tilde{V}_{n}\right)=\mathbf{a}_{n}^{i} \tilde{V}_{n}$</p>
<p>$\mathbf{a}_{n}^{i}=\operatorname{softmax}\left(\frac{\tilde{\mathbf{q}}_{n}^{i} \tilde{K}_{n}^{T}}{\sqrt{d_{k}}}\right)​$</p>
<p>其中$\mathbf{a}_{n}^{i} \in \mathbb{R}^{1 \times J}$表示第i个source token和全部target token的关系，整个attention matrix为$A_{I \times J}$。基线模型就是Transformer的attention矩阵抽取出的对齐， 论文在这里介绍了两个前人工作，这里不做介绍了。</p>
<h1 id="本文方法"><a href="#本文方法" class="headerlink" title="本文方法"></a>本文方法</h1><h2 id="Averaging-Layer-wise-Attention-Scores"><a href="#Averaging-Layer-wise-Attention-Scores" class="headerlink" title="Averaging Layer-wise Attention Scores"></a>Averaging Layer-wise Attention Scores</h2><p>单一的attention矩阵是对称的，但是不同层、不同head的attention学习到的是不同的东西，因此我们把所有head的attention矩阵加起来做平均，这样能更好地观察对齐。并且我们发现倒数第二层的attention矩阵G更好得表达了对齐。</p>
<h2 id="Multi-task-Learning"><a href="#Multi-task-Learning" class="headerlink" title="Multi-task Learning"></a>Multi-task Learning</h2><p>由于标注alignment是个很费力的事，本文使用G来指导attention。Gij是一个0-1矩阵（可以使用layer-wise attention或giza++产生的alignment），Aij是某一个head的attention，通过最小化Gij和Aij的KL散度来进行优化：$\mathcal{L}_{a}(A)=-\frac{1}{I} \sum_{i=1}^{I} \sum_{j=1}^{J} G_{i, j}^{p} \log \left(A_{i, j}\right)$。整个模型的损失函数为：$\mathcal{L}=\mathcal{L}_{t}+\lambda \mathcal{L}_{a}(A)$，其中Lt是翻译的NLL loss。</p>
<h2 id="Providing-Full-Target-Context"><a href="#Providing-Full-Target-Context" class="headerlink" title="Providing Full Target Context"></a>Providing Full Target Context</h2><p>训练翻译模型时用的时auto-regressive进行解码，也就是生成每个target tokens时只依赖于source tokens和之前生成的target token，但对于对齐任务来说，需要指导全部的target tokens。本文使用的方法是对于不同的loss，使用不同的context计算（计算两次前向）：</p>
<p>$\mathcal{L}_{t}=-\frac{1}{I} \sum_{i=1}^{I} \log \left(p\left(e_{i} | f_{1}^{J}, e_{1}^{i-1}\right)\right)$</p>
<p>$\mathcal{L}_{a}^{\prime}=\mathcal{L}_{a}\left(A | f_{1}^{J}, e_{1}^{I}\right)$</p>
<h1 id="实验部分"><a href="#实验部分" class="headerlink" title="实验部分"></a>实验部分</h1><p>评价指标选择alignment error rate（AER）。Transformer选择base model参数设置如下：</p>
<ul>
<li>embed_size=512</li>
<li>6 encoder + 6 decoder</li>
<li>8 attention head</li>
<li>share input and output embedding</li>
<li>relu activation</li>
<li>sinusoidal positional embedding（参考：<a href="https://www.zhihu.com/question/307293465" target="_blank" rel="noopener">https://www.zhihu.com/question/307293465</a>）</li>
<li>validation translation loss for early stopping</li>
<li>Adam, learning rate=3e-4, beta1=0.9, beta2=0.98</li>
<li>warmup step=4000</li>
<li>learning rate scheduler = inverse square root</li>
<li>dropout=0.1</li>
<li>label smooth=0.1</li>
</ul>
<p>本文选择的Statistical Baseline设置如下：</p>
<ul>
<li>5次迭代IBM1 + HMM + IBM3 + IBM4</li>
<li>使用grow-diagonal将两个方向的对齐合并</li>
</ul>
<p>最终实验结果如下：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/temp1.png" alt="图片"></p>
<p>在实验中我们发现，模型更容易将代词和名词对齐，提示我们可以将对齐分为sure和possible两种。在基于统计的GIZA++对齐中possible对齐较少出现，这可能是因为giza是通过统计共现来实现的。可以认为，将context进行更好得建模有助于对齐。</p>
]]></content>
      <tags>
        <tag>论文阅读</tag>
        <tag>LaserTagger</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：《Encode, Tag, Realize_ High-Precision Text Editing》</title>
    <url>/2020/03/28/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E3%80%8AEncode-Tag-Realize-High-Precision-Text-Editing%E3%80%8B/</url>
    <content><![CDATA[<p>最近想看一下语法检查的东西，关注到了这一篇谷歌去年出的论文。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://arxiv.org/abs/1909.01187" target="_blank" rel="noopener">https://arxiv.org/abs/1909.01187</a><br><a href="https://zhuanlan.zhihu.com/p/82196470" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/82196470</a></p>
</blockquote>
<h1 id="论文简介"><a href="#论文简介" class="headerlink" title="论文简介"></a>论文简介</h1><p>论文提出了一种用于sequence tagging的模型LASERTAGGER，它的基本思想是将文本生成任务转化为3种token操作的组合：keep、delete和add。它使用了bert作为编码器，transformer作为解码器，并将该模型用于 sentence fusion、sentence splitting、abstractive summarization和grammar correction任务中。LASERTAGGER在训练数据量少的情况下依然能达到不错的效果，且速度提升了很多倍。</p>
<p>研究人员是怎么想到的这个方法的呢？其实在很多text generation任务中，output和input有很大重合， 在Incorporating copying mechanism in sequence-to-sequence learning这篇论文中使用了copy机制用于在解码时选择copy一个source端词汇，抑或是生成一个新的词。但是这样的模型依旧需要庞大的训练集以保证解码端的vocabulary size是足够的。</p>
<p>通过研究发现，使用一个相对较小的output tags的集合来表示文本的deletion、rephrasing和reordering，对于生成训练语料中的大部分文本是足够的。这样就会使模型可以使用比较小的vocabulary来训练，并且由于输出长度只跟输入长度有关，就可以使用更少的语料达到精确的结果。</p>
<p>模型的主要流程如下图所式：先使用encode模块构建输入的表示，tag模块产生edit tags，realize模块通过规则将tags转换成输出tokens。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lasertagger/1.png" alt="图片"></p>
<p>本文提出两种LASERTAGGER架构，一种只使用BERT，另一种使用了BERT encoder+transformer deocder。实验表明LASERTAGGER有更快的推理速度，需要更少的训练语料，相比于seq2seq更可控（因为词表小了），更不易产生hallucination问题（生成的输出不受输入文本支持）。</p>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><h2 id="Text-Simplification"><a href="#Text-Simplification" class="headerlink" title="Text Simplification"></a>Text Simplification</h2><p>这个任务很适用edit operations modeling的方法解决。Dong等人提出了一个文本编辑模型，类似于本文，主要差异是：</p>
<ul>
<li>使用了interpreter module语言模型，来实现本文Realize部分的功能</li>
<li>使用了full vocabulary来生成added tokens，而本文使用了optimized set of frequently added phrases集合</li>
</ul>
<p>虽然Dong的模型生成更多样化的输出，但它可能会在推理时间、精度和数据效率上产生负面影响。另外一个跟本文相似的论文为Gu等人提出的Levenshtein Transformer模型，通过sequence of deletion and insertion actions来生成文本。</p>
<h2 id="Single-document-summarization"><a href="#Single-document-summarization" class="headerlink" title="Single-document summarization"></a>Single-document summarization</h2><p>这个任务可以使用在token-level和sentence-level上的deletion-based方法解决。也有论文使用了seq2seq做抽象摘要，但其缺陷是产生的操作不仅仅是删除。因此，Jing和McKeown(2000)的工作使用还原、组合、句法转换、词汇释义、泛化和重新排序操作解决。也有论文使用copy机制来使模型更容易复制source端词汇。</p>
<h2 id="Grammatical-Error-Correction"><a href="#Grammatical-Error-Correction" class="headerlink" title="Grammatical Error Correction"></a>Grammatical Error Correction</h2><p>此类任务需要利用task-specific knowledge，比如无监督得对某类错误类型构造分类器。这种error detection任务也可以使用sequence label方法，或者seq2seq方法（需要大量数据）。</p>
<h1 id="序列标注任务"><a href="#序列标注任务" class="headerlink" title="序列标注任务"></a>序列标注任务</h1><p>本文使用的方法是将text edit转换为序列标注的问题，主要包含3个部分：定义tagging operation；将训练数据的plain-text target转换为tagging格式；将tag转变为输出文本。</p>
<h2 id="定义Tagging-Operations"><a href="#定义Tagging-Operations" class="headerlink" title="定义Tagging Operations"></a>定义Tagging Operations</h2><h3 id="基本定义"><a href="#基本定义" class="headerlink" title="基本定义"></a>基本定义</h3><p>一个Tag包含两个部分：base tag（KEEP/DELETE） + added phrase（表示在token前需要增加phrase，可以为空）。Added phrase事先在词表V中定义，确保通过在input中加入后可以转换成output。这种tag+phrase的组合定义为B^P，数量大约有2^V个。</p>
<p>不同任务可以有task-specific的tag，比如在sentence fusion任务中可以把SWAP标记添加到第一句话的最后部分（如下图）；还比如为了用代词替换命名实体，可以用PRONOMINALIZE标签，在realize阶段通过查找知识库中命名实体的gender信息来用she、he、they替换（这比用she^DELETE, he^DELETE, they^DELETE要好）</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lasertagger/2.png" alt="图片"></p>
<h3 id="优化Phrase-Vocabulary"><a href="#优化Phrase-Vocabulary" class="headerlink" title="优化Phrase Vocabulary"></a>优化Phrase Vocabulary</h3><p>从词表V中选择出一个子集P，使其覆盖到所有候选added phrase且P最大为l，这个问题类似于minimum k-union问题（参考<a href="https://xbuba.com/questions/12424155" target="_blank" rel="noopener">https://xbuba.com/questions/12424155</a>）。可以证明这个问题是NP-hard问题。具体实现时本文时这样做的：先使用最长公共字串将source text和target text对齐，将没对齐的n-grams加入到P中，再从P中选择最频繁出现的l个phrase作为phrase vocabulary。（本文也尝试过使用贪心策略，每次从P中选择对覆盖率增加最多的token，但发现一个问题，如’(‘和’)’这样的token通常成对出现，但增加’(‘和’)’都对覆盖率影响很小，但如果成对增加’()’则会覆盖到很多）</p>
<h2 id="Train-targets转Tags"><a href="#Train-targets转Tags" class="headerlink" title="Train targets转Tags"></a>Train targets转Tags</h2><p>具体算法如下：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lasertagger/3.png" alt="图片"></p>
<p>本文特别强调了，即便在转换时遇到了想要增加的phrase不在V中，也不会影响整个模型的质量。比如虽然’;’不在V中，也会用’,’替代。</p>
<h2 id="Tags转输出文本"><a href="#Tags转输出文本" class="headerlink" title="Tags转输出文本"></a>Tags转输出文本</h2><p>对于不同任务，可以使用不同realization。比如上面提到的关于entity mention的替换方法，可以让我们对于代词的替换更confidence。另外一个优点是specific loss patterns can be addressed by adding specialized realization rules。比如说对于模式entity mention’s，使用his^DELETE，则对于这种模式模型更会DELETE掉entity mention后面的’s。</p>
<h1 id="模型整体框架"><a href="#模型整体框架" class="headerlink" title="模型整体框架"></a>模型整体框架</h1><p>模型使用encoder+decoder架构，encoder采用了bert-base架构，用pretrained case-sensitive BERT-base model初始化。原始的bert在encoder logits上做argmax作为解码输出，忽略了解码端前后的dependency。为了更好地建模output tags之间的依赖关系，在bert encoder上面增加一层transformer decoder。在计算decoder和encoder的联系时有两种方式，一种是计算全部的attention activation，另一种是只计算当前step的encoder activation。本文发现使用后一种方法效果更好，速度也更快。整体框架图如下：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lasertagger/4.png" alt="图片"></p>
<h1 id="实验部分"><a href="#实验部分" class="headerlink" title="实验部分"></a>实验部分</h1><h2 id="Sentence-Fusion"><a href="#Sentence-Fusion" class="headerlink" title="Sentence Fusion"></a>Sentence Fusion</h2><p>实验首先分析了不同的vocabulary size对结果的影响，发现vocabulary size上升到一定值后，提升就很小了。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lasertagger/5.png" alt="图片"></p>
<p>同时也对不同的baseline进行了比较，为了公平，本文也重新训练了一个用BERT结构的seq2seq模型，来说明本文方法的效果。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lasertagger/6.png" alt="图片"></p>
<p>同时本文也发现，当训练数据量减少到450或4500时，本文方法仍能表现良好。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lasertagger/7.png" alt="图片"></p>
<h2 id="Abstractive-Summarization"><a href="#Abstractive-Summarization" class="headerlink" title="Abstractive Summarization"></a>Abstractive Summarization</h2><p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lasertagger/8.png" alt="图片"></p>
<h2 id="Grammatical-Error-Correction-1"><a href="#Grammatical-Error-Correction-1" class="headerlink" title="Grammatical Error Correction"></a>Grammatical Error Correction</h2><p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lasertagger/9.png" alt="图片"></p>
<h2 id="推理时间比较"><a href="#推理时间比较" class="headerlink" title="推理时间比较"></a>推理时间比较</h2><p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lasertagger/10.png" alt="图片"></p>
<h2 id="质量评测"><a href="#质量评测" class="headerlink" title="质量评测"></a>质量评测</h2><p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lasertagger/11.png" alt="图片"></p>
]]></content>
      <tags>
        <tag>论文阅读</tag>
        <tag>LaserTagger</tag>
      </tags>
  </entry>
  <entry>
    <title>字符串模糊匹配的方法都有哪些</title>
    <url>/2020/03/11/%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%A8%A1%E7%B3%8A%E5%8C%B9%E9%85%8D%E7%9A%84%E6%96%B9%E6%B3%95%E9%83%BD%E6%9C%89%E5%93%AA%E4%BA%9B/</url>
    <content><![CDATA[<p>工作中经常遇到文本处理上的两个问题，一个是如何在长的文本串中找到跟短文本串最像的子串；另一个是如何将两个文本串进行对齐，忽略掉其中不同的部分。准备专门写一个工具来解决这些问题，因此先调研了模糊匹配和字符串对齐的工具。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://zhuanlan.zhihu.com/p/53135935" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/53135935</a><br><a href="https://www.thinbug.com/q/17740833" target="_blank" rel="noopener">https://www.thinbug.com/q/17740833</a><br><a href="https://github.com/eseraygun/python-alignment" target="_blank" rel="noopener">https://github.com/eseraygun/python-alignment</a><br><a href="https://pypi.org/project/StringDist/" target="_blank" rel="noopener">https://pypi.org/project/StringDist/</a><br><a href="https://pypi.org/project/edlib/" target="_blank" rel="noopener">https://pypi.org/project/edlib/</a><br><a href="https://pypi.org/project/strsimpy/" target="_blank" rel="noopener">https://pypi.org/project/strsimpy/</a><br><a href="https://github.com/gfairchild/pyxDamerauLevenshtein" target="_blank" rel="noopener">https://github.com/gfairchild/pyxDamerauLevenshtein</a><br><a href="https://github.com/mbreese/swalign/" target="_blank" rel="noopener">https://github.com/mbreese/swalign/</a><br>打印表格：<a href="https://pypi.org/project/tabulate/" target="_blank" rel="noopener">https://pypi.org/project/tabulate/</a><br><a href="https://pypi.org/project/weighted-levenshtein/" target="_blank" rel="noopener">https://pypi.org/project/weighted-levenshtein/</a><br><a href="https://pypi.org/project/nwalign/" target="_blank" rel="noopener">https://pypi.org/project/nwalign/</a><br><a href="https://pypi.org/project/pyhacrf-datamade/" target="_blank" rel="noopener">https://pypi.org/project/pyhacrf-datamade/</a><br>打印表格：<a href="https://pypi.org/project/Frmt/" target="_blank" rel="noopener">https://pypi.org/project/Frmt/</a></p>
</blockquote>
<h1 id="difflib"><a href="#difflib" class="headerlink" title="difflib"></a>difflib</h1><blockquote>
<p>参考：<br><a href="https://blog.csdn.net/Lockey23/article/details/77913855" target="_blank" rel="noopener">https://blog.csdn.net/Lockey23/article/details/77913855</a><br><a href="https://blog.csdn.net/gavin_john/article/details/78951698" target="_blank" rel="noopener">https://blog.csdn.net/gavin_john/article/details/78951698</a><br><a href="https://docs.python.org/3.5/library/difflib.html" target="_blank" rel="noopener">https://docs.python.org/3.5/library/difflib.html</a></p>
</blockquote>
<p>difflib模块提供的类和方法用来进行序列的差异化比较，它能够比对文件并生成差异结果文本或者html格式的差异化比较页面。</p>
<h2 id="SequenceMatcher"><a href="#SequenceMatcher" class="headerlink" title="SequenceMatcher"></a>SequenceMatcher</h2><p>SequenceMatcher类可以用来比较两个任意类型的数据，只要是可以哈希的。它使用一个算法来计算序列的最长连续子序列，并且忽略没有意义的“无用数据”。下面代码计算了模糊匹配的相似度：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; import difflib</span><br><span class="line">&gt;&gt;&gt; difflib.SequenceMatcher(None,&quot;amazing&quot;,&quot;amaging&quot;).ratio()</span><br><span class="line">0.8571428571428571</span><br></pre></td></tr></table></figure>
<p>其基本算法比Ratcliff和Obershelp在20世纪80年代末发表的“格式塔模式匹配”(gestalt pattern matching)算法更早，也更新奇。其思想是寻找不包含“垃圾”元素的最长连续匹配子序列;这些“垃圾”元素在某种意义上是无趣的，比如空白行或空白(垃圾信息处理是Ratcliff和Obershelp算法的扩展)。然后，将相同的思想递归地应用到匹配子序列的左子序列和右子序列。这不会产生最小的编辑序列，但是会产生人们“看起来正确”的匹配。<br>在时间复杂度上，基本的Ratcliff-Obershelp算法在最坏情况下是三次时间，在期望情况下是二次时间。SequenceMatcher是最坏情况下的二次时间，它的期望情况行为以一种复杂的方式依赖于序列有多少个公共元素;最好的情况是时间是线性的。</p>
<p>SequenceMatcher支持一种自动将某些序列项视为垃圾的启发式方法。启发式计算每个单独的项目在序列中出现的次数。如果一个项目的重复项(在第一个之后)占序列的1%以上，并且序列至少有200个项目长，则该项目将被标记为“popular”，并被视为垃圾，以便进行序列匹配。在创建SequenceMatcher时，可以通过将autojunk参数设置为False来关闭这种启发式。下面代码可以得到编辑距离的所有操作：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import difflib</span><br><span class="line">s1 &#x3D; [1, 2, 3, 5, 6, 4]</span><br><span class="line">s2 &#x3D; [2, 3, 5, 4, 6, 1]</span><br><span class="line"># 忽略所有空格</span><br><span class="line"># SequenceMatcher(lambda x: x &#x3D;&#x3D; &#39; &#39;, A, B)</span><br><span class="line">matcher &#x3D; difflib.SequenceMatcher(None, s1, s2)</span><br><span class="line">for tag, i1, i2, j1, j2 in reversed(matcher.get_opcodes()):</span><br><span class="line">    if tag &#x3D;&#x3D; &#39;delete&#39;:</span><br><span class="line">        print(&#39;Remove &#123;&#125; from positions [&#123;&#125;:&#123;&#125;]&#39;.format(</span><br><span class="line">            s1[i1:i2], i1, i2))</span><br><span class="line">        print(&#39;  before &#x3D;&#39;, s1)</span><br><span class="line">        del s1[i1:i2]</span><br><span class="line">    elif tag &#x3D;&#x3D; &#39;equal&#39;:</span><br><span class="line">        print(&#39;s1[&#123;&#125;:&#123;&#125;] and s2[&#123;&#125;:&#123;&#125;] are the same&#39;.format(</span><br><span class="line">            i1, i2, j1, j2))</span><br><span class="line">    elif tag &#x3D;&#x3D; &#39;insert&#39;:</span><br><span class="line">        print(&#39;Insert &#123;&#125; from s2[&#123;&#125;:&#123;&#125;] into s1 at &#123;&#125;&#39;.format(</span><br><span class="line">            s2[j1:j2], j1, j2, i1))</span><br><span class="line">        print(&#39;  before &#x3D;&#39;, s1)</span><br><span class="line">        s1[i1:i2] &#x3D; s2[j1:j2]</span><br><span class="line">    elif tag &#x3D;&#x3D; &#39;replace&#39;:</span><br><span class="line">        print((&#39;Replace &#123;&#125; from s1[&#123;&#125;:&#123;&#125;] &#39;</span><br><span class="line">               &#39;with &#123;&#125; from s2[&#123;&#125;:&#123;&#125;]&#39;).format(</span><br><span class="line">                   s1[i1:i2], i1, i2, s2[j1:j2], j1, j2))</span><br><span class="line">        print(&#39;  before &#x3D;&#39;, s1)</span><br><span class="line">        s1[i1:i2] &#x3D; s2[j1:j2]</span><br><span class="line">    print(&#39;   after &#x3D;&#39;, s1, &#39;\n&#39;)</span><br><span class="line">print(&#39;s1 &#x3D;&#x3D; s2:&#39;, s1 &#x3D;&#x3D; s2)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Replace [4] from s1[5:6] with [1] from s2[5:6]</span><br><span class="line">  before &#x3D; [1, 2, 3, 5, 6, 4]</span><br><span class="line">   after &#x3D; [1, 2, 3, 5, 6, 1] </span><br><span class="line"></span><br><span class="line">s1[4:5] and s2[4:5] are the same</span><br><span class="line">   after &#x3D; [1, 2, 3, 5, 6, 1] </span><br><span class="line"></span><br><span class="line">Insert [4] from s2[3:4] into s1 at 4</span><br><span class="line">  before &#x3D; [1, 2, 3, 5, 6, 1]</span><br><span class="line">   after &#x3D; [1, 2, 3, 5, 4, 6, 1] </span><br><span class="line"></span><br><span class="line">s1[1:4] and s2[0:3] are the same</span><br><span class="line">   after &#x3D; [1, 2, 3, 5, 4, 6, 1] </span><br><span class="line"></span><br><span class="line">Remove [1] from positions [0:1]</span><br><span class="line">  before &#x3D; [1, 2, 3, 5, 4, 6, 1]</span><br><span class="line">   after &#x3D; [2, 3, 5, 4, 6, 1]</span><br></pre></td></tr></table></figure>
<h2 id="get-matching-blocks"><a href="#get-matching-blocks" class="headerlink" title="get_matching_blocks"></a>get_matching_blocks</h2><p>返回匹配子序列的三元组列表。每个三元组的形式是(i, j, n)，表示a[i:i+n] == b[j:j+n]。在i和j中，三元组是单调递增的。最后一个三元组是一个哑元，它的值是(len(a)， len(b)， 0)，它是唯一一个n == 0的三元组。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; s &#x3D; SequenceMatcher(None, &quot;abxcd&quot;, &quot;abcd&quot;)</span><br><span class="line">&gt;&gt;&gt; s.get_matching_blocks()</span><br><span class="line">[Match(a&#x3D;0, b&#x3D;0, size&#x3D;2), Match(a&#x3D;3, b&#x3D;2, size&#x3D;2), Match(a&#x3D;5, b&#x3D;4, size&#x3D;0)]</span><br></pre></td></tr></table></figure>
<p>如果想获得所有match的子串，可以这样写：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import difflib</span><br><span class="line">def matches(large_string, query_string, threshold):</span><br><span class="line">    words &#x3D; large_string.split()</span><br><span class="line">    for word in words:</span><br><span class="line">        s &#x3D; difflib.SequenceMatcher(None, word, query_string)</span><br><span class="line">        match &#x3D; &#39;&#39;.join(word[i:i+n] for i, j, n in s.get_matching_blocks() if n)</span><br><span class="line">        if len(match) &#x2F; float(len(query_string)) &gt;&#x3D; threshold:</span><br><span class="line">            yield match</span><br><span class="line">large_string &#x3D; &quot;thelargemanhatanproject is a great project in themanhattincity&quot;</span><br><span class="line">query_string &#x3D; &quot;manhattan&quot;</span><br></pre></td></tr></table></figure>
<p>结果：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; print(list(matches(large_string, query_string, 0.8)))</span><br><span class="line">[&#39;manhatan&#39;, &#39;manhattn&#39;]</span><br></pre></td></tr></table></figure>
<h1 id="fuzzywuzzy"><a href="#fuzzywuzzy" class="headerlink" title="fuzzywuzzy"></a>fuzzywuzzy</h1><blockquote>
<p>参考：<br><a href="https://github.com/seatgeek/fuzzywuzzy" target="_blank" rel="noopener">https://github.com/seatgeek/fuzzywuzzy</a><br><a href="https://zhuanlan.zhihu.com/p/77166627" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/77166627</a><br><a href="https://blog.csdn.net/laobai1015/article/details/80451371" target="_blank" rel="noopener">https://blog.csdn.net/laobai1015/article/details/80451371</a><br><a href="https://stackoverflow.com/questions/48671270/use-sklearn-tfidfvectorizer-with-already-tokenized-inputs" target="_blank" rel="noopener">https://stackoverflow.com/questions/48671270/use-sklearn-tfidfvectorizer-with-already-tokenized-inputs</a><br><a href="https://github.com/ing-bank/sparse_dot_topn" target="_blank" rel="noopener">https://github.com/ing-bank/sparse_dot_topn</a></p>
</blockquote>
<p>FuzzyWuzzy 是一个简单易用的模糊字符串匹配工具包。它依据 Levenshtein Distance 算法 计算两个序列之间的差异。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from fuzzywuzzy import fuzz</span><br><span class="line">from fuzzywuzzy import process</span><br><span class="line"></span><br><span class="line"># 简单匹配</span><br><span class="line">fuzz.ratio(&quot;this is a test&quot;, &quot;this is a test!&quot;)</span><br><span class="line"></span><br><span class="line"># 非完全匹配</span><br><span class="line">fuzz.partial_ratio(&quot;this is a test&quot;, &quot;this is a test!&quot;)</span><br><span class="line"></span><br><span class="line"># 忽略顺序匹配</span><br><span class="line">fuzz.ratio(&quot;fuzzy wuzzy was a bear&quot;, &quot;wuzzy fuzzy was a bear&quot;)</span><br><span class="line">fuzz.token_sort_ratio(&quot;fuzzy wuzzy was a bear&quot;, &quot;wuzzy fuzzy was a bear&quot;)</span><br><span class="line"></span><br><span class="line"># 去重子集匹配</span><br><span class="line">fuzz.token_sort_ratio(&quot;fuzzy was a bear&quot;, &quot;fuzzy fuzzy was a bear&quot;)</span><br><span class="line">fuzz.token_set_ratio(&quot;fuzzy was a bear&quot;, &quot;fuzzy fuzzy was a bear&quot;)</span><br><span class="line"></span><br><span class="line"># 返回模糊匹配的字符串和相似度</span><br><span class="line">choices &#x3D; [&quot;Atlanta Falcons&quot;, &quot;New York Jets&quot;, &quot;New York Giants&quot;, &quot;Dallas Cowboys&quot;]</span><br><span class="line">process.extract(&quot;new york jets&quot;, choices, limit&#x3D;2)</span><br><span class="line"># 返回：[(&#39;New York Jets&#39;, 100), (&#39;New York Giants&#39;, 78)]</span><br><span class="line">process.extractOne(&quot;cowboys&quot;, choices)</span><br><span class="line"># 返回：(&quot;Dallas Cowboys&quot;, 90)</span><br><span class="line"># 可以传入附加参数到 extractOne 方法来设置使用特定的匹配模式，一个典型的用法是来匹配文件路径</span><br><span class="line">process.extractOne(&quot;System of a down - Hypnotize - Heroin&quot;, songs)</span><br><span class="line"># 返回：(&#39;&#x2F;music&#x2F;library&#x2F;good&#x2F;System of a Down&#x2F;2005 - Hypnotize&#x2F;01 - Attack.mp3&#39;, 86)</span><br><span class="line">process.extractOne(&quot;System of a down - Hypnotize - Heroin&quot;, songs, scorer&#x3D;fuzz.token_sort_ratio)</span><br><span class="line"># 返回：(&quot;&#x2F;music&#x2F;library&#x2F;good&#x2F;System of a Down&#x2F;2005 - Hypnotize&#x2F;10 - She&#39;s Like Heroin.mp3&quot;, 61)</span><br></pre></td></tr></table></figure>
<p>上面方法可以用于在候选answers中找到最接近query的answer，但在面临大数据时，会遇到速度慢的问题。我们可以通过先确定一个候选answers的子集，再进行fuzzywuzzy的方式缩短运行时间。<br>首先，我们先将候选answers转换成tf-idf向量：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from sklearn.feature_extraction.text import TfidfVectorizer</span><br><span class="line">import nltk</span><br><span class="line">nltk.download(&#39;stopwords&#39;)</span><br><span class="line">from nltk.corpus import stopwords</span><br><span class="line">stop_words &#x3D; set(stopwords.words(&#39;english&#39;))</span><br><span class="line">choices &#x3D; [[&quot;candle&quot;], [&quot;Don&#39;t&quot;, &quot;trouble&quot;, &quot;trouble&quot;, &quot;until&quot;, &quot;trouble&quot;, &quot;troubles&quot;, &quot;you.&quot;], [&quot;A&quot;, &quot;bad&quot;, &quot;excuse&quot;, &quot;is&quot;, &quot;better&quot;, &quot;than&quot;, &quot;none&quot;, &quot;at&quot;, &quot;all.&quot;], [&quot;Bad&quot;, &quot;excuses&quot;, &quot;are&quot;, &quot;worse&quot;, &quot;than&quot;, &quot;none.&quot;], [&quot;A&quot;, &quot;bribe&quot;, &quot;in&quot;, &quot;hand&quot;, &quot;betrays&quot;, &quot;mischief&quot;, &quot;at&quot;, &quot;heart.&quot;], [&quot;A&quot;, &quot;candle&quot;, &quot;lights&quot;, &quot;others&quot;, &quot;and&quot;, &quot;consumes&quot;, &quot;itself.&quot;], [&quot;Don&#39;t&quot;, &quot;teach&quot;, &quot;your&quot;, &quot;grandmother&quot;, &quot;to&quot;, &quot;suck&quot;, &quot;eggs.&quot;], [&quot;A&quot;, &quot;teacher&quot;, &quot;is&quot;, &quot;just&quot;, &quot;a&quot;, &quot;candle&quot;, &quot;,&quot;, &quot;lights&quot;, &quot;others&quot;, &quot;and&quot;, &quot;consumes&quot;, &quot;itself.&quot;], [&quot;A&quot;, &quot;a&quot;, &quot;candle&quot;, &quot;lights&quot;, &quot;others&quot;, &quot;and&quot;, &quot;consumes&quot;, &quot;itself.&quot;]]</span><br><span class="line"># 按word分，还可以按char、char_wb处理：</span><br><span class="line"># vectorizer &#x3D; TfidfVectorizer(min_df&#x3D;1, analyzer&#x3D;&#39;word&#39;)</span><br><span class="line"># 也可以使用自定义的分词</span><br><span class="line">vectorizer &#x3D; TfidfVectorizer(analyzer&#x3D;lambda x:[w for w in x if w not in stop_words])</span><br><span class="line">tf_idf_matrix_candidates &#x3D; vectorizer.fit_transform(choices)</span><br><span class="line">tf_idf_matrix_queries &#x3D; tf_idf_matrix_candidates[-1]</span><br><span class="line">tf_idf_matrix_candidates &#x3D; tf_idf_matrix_candidates[:-1]</span><br><span class="line"># vectorizer.get_feature_names()可以看到所有的token</span><br></pre></td></tr></table></figure>
<p>其次使用sparse_dot_topn找到相似的字符串：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from scipy.sparse import csr_matrix</span><br><span class="line">import sparse_dot_topn.sparse_dot_topn as ct</span><br><span class="line"></span><br><span class="line">def awesome_cossim_top(A, B, ntop, lower_bound&#x3D;0):</span><br><span class="line">    # force A and B as a CSR matrix.</span><br><span class="line">    # If they have already been CSR,there is no overhead</span><br><span class="line">    A &#x3D; A.tocsr()</span><br><span class="line">    B &#x3D; B.tocsr()</span><br><span class="line">    M, _ &#x3D; A.shape</span><br><span class="line">    _, N &#x3D; B.shape</span><br><span class="line">    idx_dtype &#x3D; np.int32</span><br><span class="line">    nnz_max &#x3D; M*ntop</span><br><span class="line">    indptr &#x3D; np.zeros(M+1,dtype&#x3D;idx_dtype)</span><br><span class="line">    indices &#x3D; np.zeros(nnz_max,dtype&#x3D;idx_dtype)</span><br><span class="line">    data &#x3D; np.zeros(nnz_max,dtype&#x3D;A.dtype)</span><br><span class="line">    ct.sparse_dot_topn(M, N, np.asarray(A.indptr,dtype&#x3D;idx_dtype), np.asarray(A.indices,dtype&#x3D;idx_dtype), A.data, np.asarray(B.indptr,dtype&#x3D;idx_dtype), np.asarray(B.indices,dtype&#x3D;idx_dtype), B.data, ntop, lower_bound, indptr, indices, data)</span><br><span class="line">    return csr_matrix((data,indices,indptr),shape&#x3D;(M,N))</span><br><span class="line">    </span><br><span class="line">matches &#x3D; awesome_cossim_top(tf_idf_matrix_candidates, tf_idf_matrix_queries.transpose(),1,0.0).todense()</span><br><span class="line">matches &#x3D; np.squeeze(matches)</span><br><span class="line">match_score_index &#x3D; np.argsort(-matches)</span><br></pre></td></tr></table></figure>
<h1 id="alignment"><a href="#alignment" class="headerlink" title="alignment"></a>alignment</h1><p>alignment主要用于字符串之间对齐，其使用方法如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from alignment.sequence import Sequence</span><br><span class="line">from alignment.vocabulary import Vocabulary</span><br><span class="line">from alignment.sequencealigner import SimpleScoring, GlobalSequenceAligner</span><br><span class="line"># Create sequences to be aligned.</span><br><span class="line">a &#x3D; Sequence(&#39;what a beautiful day&#39;.split())</span><br><span class="line">b &#x3D; Sequence(&#39;what a disappointingly bad day&#39;.split())</span><br><span class="line"># Create a vocabulary and encode the sequences.</span><br><span class="line">v &#x3D; Vocabulary()</span><br><span class="line">aEncoded &#x3D; v.encodeSequence(a)</span><br><span class="line">bEncoded &#x3D; v.encodeSequence(b)</span><br><span class="line"># Create a scoring and align the sequences using global aligner.</span><br><span class="line">scoring &#x3D; SimpleScoring(1, -1)</span><br><span class="line">aligner &#x3D; GlobalSequenceAligner(scoring, -1)</span><br><span class="line">score, encodeds &#x3D; aligner.align(aEncoded, bEncoded, backtrace&#x3D;True)</span><br><span class="line"># Iterate over optimal alignments and print them.</span><br><span class="line">for encoded in encodeds:</span><br><span class="line">    alignment &#x3D; v.decodeSequenceAlignment(encoded)</span><br><span class="line">    print(alignment)</span><br><span class="line">    print(&#39;Alignment score:&#39;, alignment.score)</span><br><span class="line">    print(&#39;Percent identity:&#39;, alignment.percentIdentity())</span><br></pre></td></tr></table></figure>
<h1 id="strsimpy"><a href="#strsimpy" class="headerlink" title="strsimpy"></a>strsimpy</h1><p>这是一个用于计算各种字符串距离的包。其使用方法如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from strsimpy.levenshtein import Levenshtein</span><br><span class="line">levenshtein &#x3D; Levenshtein()</span><br><span class="line">print(levenshtein.distance(&#39;My string&#39;, &#39;My $string&#39;))</span><br><span class="line">from strsimpy.normalized_levenshtein import NormalizedLevenshtein</span><br><span class="line">normalized_levenshtein &#x3D; NormalizedLevenshtein()</span><br><span class="line">print(normalized_levenshtein.distance(&#39;My string&#39;, &#39;My $string&#39;))</span><br><span class="line">print(normalized_levenshtein.similarity(&#39;My string&#39;, &#39;My $string&#39;))</span><br><span class="line"></span><br><span class="line"># 带权重的编辑距离</span><br><span class="line">from strsimpy.weighted_levenshtein import WeightedLevenshtein</span><br><span class="line">from strsimpy.weighted_levenshtein import CharacterSubstitutionInterface</span><br><span class="line">class CharacterSubstitution(CharacterSubstitutionInterface):</span><br><span class="line">    def cost(self, c0, c1):</span><br><span class="line">        if c0&#x3D;&#x3D;&#39;t&#39; and c1&#x3D;&#x3D;&#39;r&#39;:</span><br><span class="line">            return 0.5</span><br><span class="line">        return 1.0</span><br><span class="line">weighted_levenshtein &#x3D; WeightedLevenshtein(CharacterSubstitution())</span><br><span class="line">print(weighted_levenshtein.distance(&#39;String1&#39;, &#39;String2&#39;))</span><br><span class="line">from strsimpy.damerau import Damerau</span><br><span class="line">damerau &#x3D; Damerau()</span><br><span class="line">print(damerau.distance(&#39;ABCDEF&#39;, &#39;ABDCEF&#39;))</span><br><span class="line"></span><br><span class="line"># 最优化对齐后的编辑距离</span><br><span class="line">from strsimpy.optimal_string_alignment import OptimalStringAlignment</span><br><span class="line">optimal_string_alignment &#x3D; OptimalStringAlignment()</span><br><span class="line">print(optimal_string_alignment.distance(&#39;CA&#39;, &#39;ABC&#39;))</span><br><span class="line"></span><br><span class="line">from strsimpy.jaro_winkler import JaroWinkler</span><br><span class="line">jarowinkler &#x3D; JaroWinkler()</span><br><span class="line">print(jarowinkler.similarity(&#39;My string&#39;, &#39;My tsring&#39;))</span><br><span class="line"></span><br><span class="line"># 最长公共子序列</span><br><span class="line">from strsimpy.longest_common_subsequence import LongestCommonSubsequence</span><br><span class="line">lcs &#x3D; LongestCommonSubsequence()</span><br><span class="line">print(lcs.distance(&#39;AGCAT&#39;, &#39;GAC&#39;))</span><br><span class="line"></span><br><span class="line">from strsimpy.metric_lcs import MetricLCS</span><br><span class="line">metric_lcs &#x3D; MetricLCS()</span><br><span class="line">s1 &#x3D; &#39;ABCDEFG&#39;</span><br><span class="line">s2 &#x3D; &#39;ABCDEFHJKL&#39;</span><br><span class="line">print(metric_lcs.distance(s1, s2))</span><br><span class="line"></span><br><span class="line"># ngram</span><br><span class="line">from strsimpy.ngram import NGram</span><br><span class="line">twogram &#x3D; NGram(2)</span><br><span class="line">print(twogram.distance(&#39;ABCD&#39;, &#39;ABTUIO&#39;))</span><br><span class="line">s1 &#x3D; &#39;Adobe CreativeSuite 5 Master Collection from cheap 4zp&#39;</span><br><span class="line">s2 &#x3D; &#39;Adobe CreativeSuite 5 Master Collection from cheap d1x&#39;</span><br><span class="line">fourgram &#x3D; NGram(4)</span><br><span class="line">print(fourgram.distance(s1, s2))</span><br><span class="line"></span><br><span class="line">from strsimpy.qgram import QGram</span><br><span class="line">qgram &#x3D; QGram(2)</span><br><span class="line">print(qgram.distance(&#39;ABCD&#39;, &#39;ABCE&#39;))</span><br><span class="line"></span><br><span class="line">from strsimpy.cosine import Cosine</span><br><span class="line">cosine &#x3D; Cosine(2)</span><br><span class="line">s0 &#x3D; &#39;My first string&#39;</span><br><span class="line">s1 &#x3D; &#39;My other string...&#39;</span><br><span class="line">p0 &#x3D; cosine.get_profile(s0)</span><br><span class="line">p1 &#x3D; cosine.get_profile(s1)</span><br><span class="line">print(cosine.similarity_profiles(p0, p1))</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title>大话交叉熵损失函数</title>
    <url>/2020/03/10/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<p>使用keras进行二分类时，常使用binary_crossentropy作为损失函数。那么它的原理是什么，跟categorical_crossentropy、sparse_categorical_crossentropy有什么区别？在进行文本分类时，如何选择损失函数，有哪些优化损失函数的方式？本文将从原理到实现进行一一介绍。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a" target="_blank" rel="noopener">https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a</a></p>
</blockquote>
<h1 id="binary-crossentropy"><a href="#binary-crossentropy" class="headerlink" title="binary_crossentropy"></a>binary_crossentropy</h1><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>假设我们想做一个二分类，输入有10个点：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x &#x3D; [-2.2, -1.4, -0.8, 0.2, 0.4, 0.8, 1.2, 2.2, 2.9, 4.6]</span><br></pre></td></tr></table></figure>
<p>输出有两类，分别为红色、绿色：<br><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5/1.png" alt="图片"></p>
<p>我们可以将问题描述成“这个点是绿色的吗?”，或者“这个点是绿色的概率是多少?”。理想情况下，绿点的概率是1.0，而红点的（是绿色的）概率是0.0。从而，绿色就是正样本，红色就是负样本。</p>
<p>如果我们拟合一个模型来执行这种分类，它将预测我们每个点的绿色概率。那么我们如何评估预测概率的好坏呢？这就是损失函数的意义，</p>
<p>Binary CrossEntorpy的计算如下：</p>
<p>$H_{p}(q)=-\frac{1}{N} \sum_{i=1}^{N} y_{i} \cdot \log \left(p\left(y_{i}\right)\right)+\left(1-y_{i}\right) \cdot \log \left(1-p\left(y_{i}\right)\right)$</p>
<p>其中y是标签(1代表绿色点，0代表红色点)，p(y)是所有N个点都是绿色的预测概率。看到这个计算式，发现对于每一个绿点(y=1)它增加了log(p(y))的损失（概率越大，增加的越小），也就是它是绿色的概率。下面我们可视化地看一下这个损失函数。</p>
<p>假设我们训练一个逻辑回归模型来进行分类，那么训练出的函数趋近于一个sigmoid曲线，曲线上每个点表示对于每个x是绿色点的概率：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5/2.png" alt="图片"></p>
<p>那么对于这些绿色的点，他们预测为绿色的概率是多少呢？实际下面图片中绿色的bar：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5/3.png" alt="图片"></p>
<p>那么红色点预测为红色的概率是多少呢？实际就是下面图片中红色的bar：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5/4.png" alt="图片"></p>
<p>我们把图片绘制得更好看一下，如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5/5.png" alt="图片"></p>
<p>因为我们要计算损失，我们需要惩罚错误的预测。如果与正例相关的概率是1.0，我们需要它的损失为零。相反，如果概率很低，比如0.01，我们需要它的损失是巨大的！取概率的(负)对数非常适合我们的目的(由于0.0和1.0之间的值的对数是负的，我们取负对数来获得正的损失值)。下面这个图展示了当正例的概率逐渐趋近于0时loss的变化：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5/6.png" alt="图片"></p>
<p>下面这个图表示了，我们使用负对数时每个点的损失，我们计算其平均值，就是binary cross entropy了！</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5/7.png" alt="图片"></p>
<h2 id="keras实现"><a href="#keras实现" class="headerlink" title="keras实现"></a>keras实现</h2><p>tf2.1的bce用法如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bce &#x3D; tf.keras.losses.BinaryCrossentropy()</span><br><span class="line">loss &#x3D; bce([0., 0., 1., 1.], [1., 1., 1., 0.])</span><br><span class="line">print(&#39;Loss: &#39;, loss.numpy())  # Loss: 11.522857</span><br></pre></td></tr></table></figure>
<p>或者：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">model &#x3D; tf.keras.Model(inputs, outputs)</span><br><span class="line">model.compile(&#39;sgd&#39;, loss&#x3D;tf.keras.losses.BinaryCrossentropy())</span><br></pre></td></tr></table></figure>
<p>具体实现如下（tensorflow.python.keras/losses）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class BinaryCrossentropy(LossFunctionWrapper):</span><br><span class="line">    def __init__(self, from_logits&#x3D;False,</span><br><span class="line">                  label_smoothing&#x3D;0,</span><br><span class="line">                  reduction&#x3D;losses_utils.ReductionV2.AUTO,</span><br><span class="line">                  name&#x3D;&#39;binary_crossentropy&#39;):</span><br><span class="line">        super(BinaryCrossentropy, self).__init__(</span><br><span class="line">              binary_crossentropy,</span><br><span class="line">              name&#x3D;name,</span><br><span class="line">              reduction&#x3D;reduction,</span><br><span class="line">              from_logits&#x3D;from_logits,</span><br><span class="line">              label_smoothing&#x3D;label_smoothing)</span><br><span class="line">        self.from_logits &#x3D; from_logits</span><br><span class="line"></span><br><span class="line">def binary_crossentropy(y_true, y_pred, from_logits&#x3D;False, label_smoothing&#x3D;0):</span><br><span class="line">    y_pred &#x3D; ops.convert_to_tensor_v2(y_pred)</span><br><span class="line">    y_true &#x3D; math_ops.cast(y_true, y_pred.dtype)</span><br><span class="line">    label_smoothing &#x3D; ops.convert_to_tensor_v2(label_smoothing, dtype&#x3D;K.floatx())</span><br><span class="line">    </span><br><span class="line">    def _smooth_labels():</span><br><span class="line">      return y_true * (1.0 - label_smoothing) + 0.5 * label_smoothing</span><br><span class="line">    </span><br><span class="line">    y_true &#x3D; smart_cond.smart_cond(label_smoothing, _smooth_labels, lambda: y_true)</span><br><span class="line">    return K.mean(K.binary_crossentropy(y_true, y_pred, from_logits&#x3D;from_logits), axis&#x3D;-1)</span><br></pre></td></tr></table></figure>
<p>在上面代码中，如果from_logits=True，则认为y_predit是tensor（可以认为是[0,1]之间的概率值），使用from_logits=True可以更稳定一些。label_smoothing在[0,1]之间。reduction的默认值是AUTO，表示根据上下文确定；如果是SUM_OVER_BATCH_SIZE表示整个batch的结果相加。<br>其中K.binary_crossentropy实现如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def binary_crossentropy(target, output, from_logits&#x3D;False):</span><br><span class="line">  if from_logits:</span><br><span class="line">    return nn.sigmoid_cross_entropy_with_logits(labels&#x3D;target, logits&#x3D;output)</span><br><span class="line"></span><br><span class="line">  if not isinstance(output, (ops.EagerTensor, variables_module.Variable)):</span><br><span class="line">    output &#x3D; _backtrack_identity(output)</span><br><span class="line">    if output.op.type &#x3D;&#x3D; &#39;Sigmoid&#39;:</span><br><span class="line">      assert len(output.op.inputs) &#x3D;&#x3D; 1</span><br><span class="line">      output &#x3D; output.op.inputs[0]</span><br><span class="line">      return nn.sigmoid_cross_entropy_with_logits(labels&#x3D;target, logits&#x3D;output)</span><br><span class="line"></span><br><span class="line">  epsilon_ &#x3D; _constant_to_tensor(epsilon(), output.dtype.base_dtype)</span><br><span class="line">  output &#x3D; clip_ops.clip_by_value(output, epsilon_, 1. - epsilon_)</span><br><span class="line"></span><br><span class="line">  bce &#x3D; target * math_ops.log(output + epsilon())</span><br><span class="line">  bce +&#x3D; (1 - target) * math_ops.log(1 - output + epsilon())</span><br><span class="line">  return -bce</span><br></pre></td></tr></table></figure>
<p>sigmoid_cross_entropy_with_logits实现如下：（该函数适用于不同类标签之间相互独立的情况，例如一个图片可以既包含大象也包含狗）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def sigmoid_cross_entropy_with_logits(_sentinel&#x3D;None, labels&#x3D;None, logits&#x3D;None, name&#x3D;None):</span><br><span class="line">    zeros &#x3D; array_ops.zeros_like(logits, dtype&#x3D;logits.dtype)</span><br><span class="line">    cond &#x3D; (logits &gt;&#x3D; zeros)</span><br><span class="line">    relu_logits &#x3D; array_ops.where(cond, logits, zeros)</span><br><span class="line">    neg_abs_logits &#x3D; array_ops.where(cond, -logits, logits)</span><br><span class="line">    return math_ops.add(relu_logits - logits * labels, math_ops.log1p(math_ops.exp(neg_abs_logits)), name&#x3D;name)</span><br></pre></td></tr></table></figure>
<p>对于上面代码，解释如下：<br>对于x=logits, z=labels，logistic损失定义为</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))</span><br><span class="line">&#x3D; z * -log(1 &#x2F; (1 + exp(-x))) + (1 - z) * -log(exp(-x) &#x2F; (1 + exp(-x)))</span><br><span class="line">&#x3D; z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))</span><br><span class="line">&#x3D; z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))</span><br><span class="line">&#x3D; (1 - z) * x + log(1 + exp(-x))</span><br><span class="line">&#x3D; x - x * z + log(1 + exp(-x))</span><br></pre></td></tr></table></figure>
<p>对于x&lt;0，为了防止exp(-x)溢出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x - x * z + log(1 + exp(-x))</span><br><span class="line">&#x3D; log(exp(x)) - x * z + log(1 + exp(-x))</span><br><span class="line">&#x3D; - x * z + log(1 + exp(x))</span><br></pre></td></tr></table></figure>
<p>为了保证稳定和不溢出，在实现过程中使用了如下等式：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">max(x, 0) - x * z + log(1 + exp(-abs(x)))</span><br></pre></td></tr></table></figure>
<h1 id="categorical-crossentropy"><a href="#categorical-crossentropy" class="headerlink" title="categorical_crossentropy"></a>categorical_crossentropy</h1><h2 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h2><p>CrossEntropy可用于多分类任务，且label且one-hot形式。它的计算式如下：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5/8.png" alt="图片"></p>
<h2 id="keras实现-1"><a href="#keras实现-1" class="headerlink" title="keras实现"></a>keras实现</h2><p>tf2.1的ce用法如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">y_true &#x3D; [[0, 1, 0], [0, 0, 1]]</span><br><span class="line">y_pred &#x3D; [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]</span><br><span class="line">cce &#x3D; tf.keras.losses.CategoricalCrossentropy()</span><br><span class="line"># Using &#39;auto&#39;&#x2F;&#39;sum_over_batch_size&#39; reduction type.</span><br><span class="line">cce(y_true, y_pred).numpy()</span><br><span class="line"># Calling with &#39;sample_weight&#39;.</span><br><span class="line">cce(y_true, y_pred, sample_weight&#x3D;tf.constant([0.3, 0.7])).numpy()</span><br><span class="line"># Using &#39;sum&#39; reduction type.</span><br><span class="line">cce &#x3D; tf.keras.losses.CategoricalCrossentropy(reduction&#x3D;tf.keras.losses.Reduction.NONE)</span><br><span class="line">cce(y_true, y_pred).numpy()</span><br><span class="line"># Usage with the &#96;compile&#96; API</span><br><span class="line">model &#x3D; tf.keras.Model(inputs, outputs)</span><br><span class="line">model.compile(&#39;sgd&#39;, loss&#x3D;tf.keras.losses.CategoricalCrossentropy())</span><br></pre></td></tr></table></figure>
<p>具体实现如下（tensorflow.python.keras/losses）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class CategoricalCrossentropy(LossFunctionWrapper):</span><br><span class="line">    def __init__(self,</span><br><span class="line">                   from_logits&#x3D;False,</span><br><span class="line">                   label_smoothing&#x3D;0,</span><br><span class="line">                   reduction&#x3D;losses_utils.ReductionV2.AUTO,</span><br><span class="line">                   name&#x3D;&#39;categorical_crossentropy&#39;):</span><br><span class="line">        super(CategoricalCrossentropy, self).__init__(</span><br><span class="line">                categorical_crossentropy,</span><br><span class="line">                name&#x3D;name,</span><br><span class="line">                reduction&#x3D;reduction,</span><br><span class="line">                from_logits&#x3D;from_logits,</span><br><span class="line">                label_smoothing&#x3D;label_smoothing)</span><br><span class="line"></span><br><span class="line">    def categorical_crossentropy(y_true,</span><br><span class="line">                                 y_pred,</span><br><span class="line">                                 from_logits&#x3D;False,</span><br><span class="line">                                 label_smoothing&#x3D;0):</span><br><span class="line">        y_pred &#x3D; ops.convert_to_tensor_v2(y_pred)</span><br><span class="line">        y_true &#x3D; math_ops.cast(y_true, y_pred.dtype)</span><br><span class="line">        label_smoothing &#x3D; ops.convert_to_tensor_v2(label_smoothing, dtype&#x3D;K.floatx())</span><br><span class="line">        def _smooth_labels():</span><br><span class="line">            num_classes &#x3D; math_ops.cast(array_ops.shape(y_true)[1], y_pred.dtype)</span><br><span class="line">            return y_true * (1.0 - label_smoothing) + (label_smoothing &#x2F; num_classes)</span><br><span class="line">        y_true &#x3D; smart_cond.smart_cond(label_smoothing, _smooth_labels, lambda: y_true)</span><br><span class="line">        return K.categorical_crossentropy(y_true, y_pred, from_logits&#x3D;from_logits)</span><br></pre></td></tr></table></figure>
<p>其中K.categorical_crossentropy实现如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def categorical_crossentropy(target, output, from_logits&#x3D;False, axis&#x3D;-1):</span><br><span class="line">    if from_logits:</span><br><span class="line">        return nn.softmax_cross_entropy_with_logits_v2(</span><br><span class="line">            labels&#x3D;target, logits&#x3D;output, axis&#x3D;axis)</span><br><span class="line">    if not isinstance(output, (ops.EagerTensor, variables_module.Variable)):</span><br><span class="line">        output &#x3D; _backtrack_identity(output)</span><br><span class="line">        if output.op.type &#x3D;&#x3D; &#39;Softmax&#39;:</span><br><span class="line">            output &#x3D; output.op.inputs[0]</span><br><span class="line">            return nn.softmax_cross_entropy_with_logits_v2(</span><br><span class="line">                labels&#x3D;target, logits&#x3D;output, axis&#x3D;axis)</span><br><span class="line">    # scale preds so that the class probas of each sample sum to 1</span><br><span class="line">    output &#x3D; output &#x2F; math_ops.reduce_sum(output, axis, True)</span><br><span class="line">    # Compute cross entropy from probabilities.</span><br><span class="line">    epsilon_ &#x3D; _constant_to_tensor(epsilon(), output.dtype.base_dtype)</span><br><span class="line">    output &#x3D; clip_ops.clip_by_value(output, epsilon_, 1. - epsilon_)</span><br><span class="line">    return -math_ops.reduce_sum(target * math_ops.log(output), axis)</span><br></pre></td></tr></table></figure>
<h1 id="sparse-categorical-crossentropy"><a href="#sparse-categorical-crossentropy" class="headerlink" title="sparse_categorical_crossentropy"></a>sparse_categorical_crossentropy</h1><h2 id="原理-2"><a href="#原理-2" class="headerlink" title="原理"></a>原理</h2><p>跟categorical_crossentropy的区别是其标签不是one-hot，而是integer。比如在categorical_crossentropy是[1,0,0]，在sparse_categorical_crossentropy中是3.</p>
<h2 id="keras实现-2"><a href="#keras实现-2" class="headerlink" title="keras实现"></a>keras实现</h2><p>tf2.1中使用方法如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">y_true &#x3D; [1, 2]</span><br><span class="line">y_pred &#x3D; [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]</span><br><span class="line">loss &#x3D; tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)</span><br></pre></td></tr></table></figure>
<h1 id="其他技巧"><a href="#其他技巧" class="headerlink" title="其他技巧"></a>其他技巧</h1><h2 id="focal-loss"><a href="#focal-loss" class="headerlink" title="focal loss"></a>focal loss</h2><blockquote>
<p>参考：<br><a href="https://ldzhangyx.github.io/2018/11/16/focal-loss/" target="_blank" rel="noopener">https://ldzhangyx.github.io/2018/11/16/focal-loss/</a></p>
</blockquote>
<p>Focal Loss的出现是为了解决训练集正负样本极度不平衡的情况，通过reshape标准交叉熵损失解决类别不均衡（Class Imbalance）,这样它就能降低容易分类的样例的比重（Well-classified Examples）。这个方法专注训练在Hard Examples的稀疏集合上，能够防止大量的Easy Negatives在训练中压倒训练器。其公式为：</p>
<p>$F L\left(p_{t}\right)=-\left(1-p_{t}\right)^{\gamma} \log \left(p_{t}\right)​$</p>
<p>其中参数为0的时候，Focal Loss退化为交叉熵CE。当这个参数不同时，对loss的影响如下：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5/0.png" alt="图片"></p>
<p>p_t越大，FL越小，其对总体loss所做的贡献就越小；反过来说，p_t越小（小于0.5的情况也就是被误分类），越能反映在总体loss上。</p>
<p>Focal Loss的tensorflow api：<a href="https://www.tensorflow.org/addons/api_docs/python/tfa/losses/SigmoidFocalCrossEntropy" target="_blank" rel="noopener">https://www.tensorflow.org/addons/api_docs/python/tfa/losses/SigmoidFocalCrossEntropy</a></p>
<h2 id="label-smooth"><a href="#label-smooth" class="headerlink" title="label smooth"></a>label smooth</h2><blockquote>
<p>参考：<br><a href="https://towardsdatascience.com/what-is-label-smoothing-108debd7ef06" target="_blank" rel="noopener">https://towardsdatascience.com/what-is-label-smoothing-108debd7ef06</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/76587755" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/76587755</a></p>
</blockquote>
<p>在使用深度学习模型进行分类任务时，我们通常会遇到以下问题：overfit和over confidence。Overfit问题得到了很好的研究，可以通过earlystop、dropout、正则化等方法来解决。另一方面，我们over confidence的工具较少。标签平滑是一种正则化技术，解决了这两个问题。</p>
<p>Label Smooth将y_hot和均匀分布的混合来代替一个hot编码的标签向量y_hot:</p>
<p>$y_{-} l s=(1-\alpha) * y_{-} h o t+\alpha / K$</p>
<p>K是标签类的数目，α是一个决定平滑的超参数。如果α= 0，我们获得最初的一个原始的y_hot编码。如果α= 1，我们得到均匀分布。</p>
<p>当损失函数为交叉熵时，使用标签平滑，模型将softmax函数应用于倒数第二层的logit向量z，计算其输出概率p。在这种情况下，交叉熵损失函数相对于logit的梯度为：</p>
<p>$\nabla C E=p-y=\operatorname{softmax}(z)-y$</p>
<p>其中y是标签分布，并且：</p>
<ul>
<li>梯度下降会使p尽可能接近y</li>
<li>梯度在-1和1之间有界</li>
</ul>
<p>一个标准的ont-hot希望有更大的logit gaps输入到里面。直观地说，较大的logit gap加上有界的梯度会使模型的自适应性降低，并且对其预测过于自信。相反，平滑的标签鼓励小的logit差距，可以得到更好的模型校准，并防止过度自信的预测。</p>
<p>下面我们使用一个例子说明：假设我们有K = 3类，我们的标签属于第一类。令[a, b, c]为logit向量。如果我们不使用标签平滑，那么标签向量就是一个one-hot向量[1,0,0]。我们的模型将a≫b和a≫c。例如,应用softmax分对数向量(10,0,0)给(0.9999,0,0)的4位小数。</p>
<p>如果我们使用标签的平滑与α= 0.1,平滑标签向量≈(0.9333,0.0333,0.0333)。logit向量[3.3322,0,0]在softmax之后将经过平滑处理的标签向量近似为小数点后4位，并且它的差距更小。这就是为什么我们称平滑标签为一种正则化技术，因为它可以防止最大的logit变得比其他的更大。</p>
<p>更形象地说，对于label_smoothing=0.2，则意味着标签0的概率是0.1，标签1的概率是0.9。</p>
<p>另一种解释（来自<a href="https://zhuanlan.zhihu.com/p/76587755" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/76587755</a>）：</p>
<p>​    在常见的多分类问题中，先经过softmax处理后进行交叉熵计算，原理很简单可以将计算loss理解为，为了使得网络对测试集预测的概率分布和其真实分布接近，常用的做法是使用one-hot对真实标签进行编码，作者认为这种将标签强制one-hot的方式使网络过于自信会导致过拟合，因此软化这种编码方式：$q^{\prime}(k | x)=(1-\epsilon) \delta_{k, y}+\epsilon u(k)​$</p>
<p>​    等号左侧：是一种新的预测的分布；等号右侧：前半部分是对原分布乘一个权重，$\epsilon$是一个超参，需要自己设定，取值在0到1范围内。后半部分u是一个均匀分布，k表示模型的类别数。</p>
<p>​    由以上公式可以看出，这种方式使label有$\epsilon$概率来自于均匀分布， $1-\epsilon$概率来自于原分布。这就相当于在原label上增加噪声，让模型的预测值不要过度集中于概率较高的类别，把一些概率放在概率较低的类别。因此，交叉熵可以替换为：$H\left(q^{\prime}, p\right)=-\sum_{k=1}^{K} \log p(k) q^{\prime}(k)=(1-\epsilon) H(q, p)+\epsilon H(u, p)$</p>
<p>​    可以理解为：loss为对<strong>“预测的分布与真实分布”</strong>及<strong>“预测分布与先验分布（均匀分布）”</strong>的惩罚。个人理解：label smooth的思路“做软化、防止过拟合、增加扰动”是好的，个人认为用均匀分布做先验分布有待商榷。</p>
]]></content>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：《Reformer: The Efficient Transformer》</title>
    <url>/2020/03/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E3%80%8AReformer-The-Efficient-Transformer%E3%80%8B/</url>
    <content><![CDATA[<p>本论文为谷歌近期发表的对Transformer改进的一篇论文，论文名字中的Efficient Transformer解释了论文的主要目的。过去一些基于Transformer结构的论文，一看到模型的总参数量就让人望而生畏，有些模型在我们的单卡GPU上根本跑不起来，因此就看了一下这篇论文。论文感觉比较偏工程，了解下它的大致思想就好。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://arxiv.org/pdf/2001.04451.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2001.04451.pdf</a><br><a href="https://github.com/google/trax/blob/master/trax/models/reformer/reformer.py" target="_blank" rel="noopener">https://github.com/google/trax/blob/master/trax/models/reformer/reformer.py</a><br><a href="https://zhuanlan.zhihu.com/p/92153420" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/92153420</a></p>
</blockquote>
<h1 id="论文摘要"><a href="#论文摘要" class="headerlink" title="论文摘要"></a>论文摘要</h1><p>为了解决Transformer模型在处理长序列时的GPU资源消耗问题，提出了更省内存和更快的Transformer模型结构。其改进主要有两点：</p>
<ul>
<li>使用locality-sensitive hashing代替dot-product attention，使得计算复杂度由 $O(L^2)$直接降为$O(LlogL)$，其中L为序列长度</li>
<li>使用reversible residual layers来代替传统的残差层，使得训练过程中对激活函数的值的存储由N次降低为1次，其中N是层数。</li>
</ul>
<h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h1><p>比较大的Transformer模型里的每一层有0.5Billion的参数，最多可达到64层。并且随着序列长度增加，单个文本train example需要能处理11k左右的token。对于音乐、图像等数据，序列可能会更长，因此有些模型只能在大型GPU集群中进行并行训练。受GPU显存限制，有的模型也很难在单个GPU机器上进行微调。</p>
<p>因此会有这样一个疑问，这么大的Transformer模型到底在哪里消耗了这么多资源？我们不妨计算一下：</p>
<ul>
<li>每层0.5Billion的参数需要2GB的存储</li>
<li>使用1024 embedding size和8 batch size训练的64k token的激活函数值需要64K <em> 1K </em> 8 = 0.5Billion的参数，即2GB的存储</li>
<li>N层网络需要将激活值存储N次(为了back-propagation时进行计算)</li>
<li>Feed-forward层的维度通常要比d_model大很多</li>
<li>对于序列长度L来说计算attention所需要的时空复杂度为O(L^2)</li>
</ul>
<p>具体计算过程如下：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/reformer1.png" alt="图片"></p>
<ul>
<li><p>Transformer Block</p>
<p>$h_{m i d}=\text { LayerNorm }\left(h_{i n}+\text { MultiHead }\left(h_{i n}\right)\right)$</p>
<p>$h_{\text {out}}=\text {LayerNorm }\left(h_{\text {mid}}+\mathrm{FFN}\left(h_{\text {mid}}\right)\right)$</p>
</li>
<li><p>Multi-head Attention</p>
</li>
</ul>
<p>$\begin{array}{l}\text {head}_{i}=\text { Attention }\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right) \ \text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V \ \text { MultiHead }(Q, K, V)=\text {Concat}\left(\text {head}_{1}, \ldots, \text {head}_{h}\right) W^{O}\end{array}$</p>
<ul>
<li>Attention输入：<ul>
<li>Q: (batch_size, seq_q, d_model)</li>
<li>K: (batch_size, seq_k, d_model)</li>
<li>V: (batch_size, seq_k, d_model)</li>
</ul>
</li>
<li>Attention输出：<ul>
<li>(batch_size, q_seq_len, d_model)</li>
</ul>
</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/reformer2.png" alt="图片"></p>
<ul>
<li><p>Feed-Forward</p>
<p>$\operatorname{FFN}(h)=\operatorname{ReLU}\left(h W_{1}+b_{1}\right) W_{2}+b_{2}$</p>
</li>
<li><p>FFN输入：</p>
<ul>
<li>(batch_size, q_seq_len, d_model)</li>
</ul>
</li>
<li><p>FFN输出：</p>
<ul>
<li>(batch_size, q_seq_len, d_ff)</li>
</ul>
</li>
</ul>
<p>本论文提出了Reformer模型，使用了下面方法解决了内存和速度的问题：</p>
<ul>
<li>Reversible layers：整个模型只需保存一次activations，使因层数导致的内存问题解决</li>
<li>FFN层分块并行处理：降低d_ff产生的内存消耗</li>
<li>局部敏感哈希(locality-sensitive hashing)：代替dot-product attention带来的O(L^2)计算和内存复杂度，使得能处理更长的序列</li>
</ul>
<h1 id="局部敏感哈希"><a href="#局部敏感哈希" class="headerlink" title="局部敏感哈希"></a>局部敏感哈希</h1><blockquote>
<p>参考：<br><a href="https://arxiv.org/abs/1509.02897" target="_blank" rel="noopener">https://arxiv.org/abs/1509.02897</a><br><a href="https://www.cnblogs.com/maybe2030/p/4953039.html" target="_blank" rel="noopener">https://www.cnblogs.com/maybe2030/p/4953039.html</a></p>
</blockquote>
<p>Attention计算中最耗时和消耗内存的是QK^T([batch size, length, length])。我们其实关注的是softmax(QK^T)，而softmax的取值主要被其中较大的元素主导，因此对Q的每个向量qi，只需要关注K中哪个向量最接近qi。比如说如果K的长度是64K，对于每个qi，我们只需要关注其中跟qi距离最近的32或64个kj。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/reformer3.png" alt="图片"></p>
<p>我们首先想到的是 locality-sensitive hashing，其特点是对于每个向量x，在经过哈希函数h(x)后，在原来的空间中挨的近的向量有更大的概率获得相同的哈希值。就像上面这张图，经过旋转(映射)后，距离远得点(第一行)有很大概率分到不同得桶中，而距离近得点(第二行)很大概率分到相同得桶中。</p>
<p>在实现时我们使用了一个随机产生的大小为(dk, b/2)的矩阵R，定义$h(x)=\arg{\max }([x R ;-x R])​$为哈希函数，这样所有x，可以把它们分配到b个哈希桶里。具体的计算和证明在另一片论文(Practical and Optimal LSH for Angular Distance)中。</p>
<p>下面这张图说明了LSH具体的计算流程：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/reformer4.png" alt="图片"></p>
<p>在上图中，不同的颜色表示不同的哈希值，相似的词则具有相同的颜色。分配哈希值后，序列重新排列，将具有相同哈希值的元素放在一起，再分为多个片段（或多个区块）以实现并行处理。然后在这些短得多的区块（及其相近邻块以覆盖溢出）内应用注意力，从而大大降低计算负载。</p>
<p>上图右侧（a-b）是和传统注意力的比较。(a)表明传统的注意力是很稀疏的，也就是说大多数的字符其实都不用关注；(b) k和q根据它们的哈希桶（注意随机的那个矩阵R是共享的）排序好，然后再使用。</p>
<p>由于哈希桶的大小很可能不均匀，所以我们首先令$k_{j}=\frac{q_{i}}{\left|q_{i}\right|}​$来保证$h\left(k_{j}\right)=h\left(q_{j}\right)​$，然后再从小到大给Q的哈希桶排序，在每个桶内部，按照位置先后排序。这实际上定义了一个置换$i \mapsto s_{i}​$。在排序后的注意力矩阵中，来自同一个哈希桶的(q,k)对会聚集在矩阵的对角(上图右c)。最后，把它们分组，每组m个，在各组内相互关注。</p>
<p>为了进一步减小桶分布不均的情况，可以用不同的哈希函数进行多轮哈希。下表是几种注意力方式的时空复杂度：(l: 序列长度，b: batch_size, $n_h$: num of heads, $n_c$: num of LSH chunk, $n_r$: num of hash repetition)<img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/reformer5.png" alt="图片"></p>
<h1 id="可逆层"><a href="#可逆层" class="headerlink" title="可逆层"></a>可逆层</h1><blockquote>
<p>参考：<br><a href="https://arxiv.org/pdf/1707.04585.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1707.04585.pdf</a><br><a href="https://zhuanlan.zhihu.com/p/60479586" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/60479586</a><br><a href="https://www.cnblogs.com/gczr/p/12181354.html" target="_blank" rel="noopener">https://www.cnblogs.com/gczr/p/12181354.html</a></p>
</blockquote>
<p>通过LSH可以将attention的复杂度减少为序列长度的线性级，但是参数量占的复杂度依旧很高，我们想要进一步减少。在上面表中我们看出，每一层的输入前都至少有$b \cdot l \cdot d_{\text {model}}$的激活输出值，$n_l$层则至少有个$b \cdot l \cdot d_{\bmod e l} \cdot n_{l}$。而且光是FFN层就会产生$b \cdot l \cdot d_{f f} \cdot n_{l}$的激活输出，对于一些大模型，这个$d_ff$会比较大(4K甚至64K)，甚至消耗掉16GB的内存。因此采用可逆层来解决$n_l$和$d_ff$的问题。</p>
<h2 id="可逆Transformer"><a href="#可逆Transformer" class="headerlink" title="可逆Transformer"></a>可逆Transformer</h2><p>可逆残差网络的前向传播和反向计算过程如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/reformer6.png" alt="图片"></p>
<p>前向：</p>
<p>$\begin{array}{l}y_{1}=x_{1}+\mathcal{F}\left(x_{2}\right) \ y_{2}=x_{2}+\mathcal{G}\left(y_{1}\right)\end{array}​$</p>
<p>逆向：</p>
<p>$\begin{array}{l}x_{2}=y_{2}-\mathcal{G}\left(y_{1}\right) \ x_{1}=y_{1}-\mathcal{F}\left(x_{2}\right)\end{array}$</p>
<p>在典型的残差网络中，通过网络传递的输入将会向堆栈中的每一层不断添加至向量。相反，可逆层中每个层有两组激活。一组遵循刚才描述的标准过程，从一层逐步更新到下一层，但是另一组仅捕获第一层的变更。因此，若要反向运行网络，只需简单地减去每一层应用的激活。</p>
<p>简单来说，可逆层将输入分成两部分，使得每一层的值可以由它下一层的输出推导出来。因此整个网络只需要存储最后一层的值即可。</p>
<p>具体的解释可参考论文：The Reversible Residual Network: Backpropagation Without Storing Activations.</p>
<p>在Transformer中我们这样应用可逆层：</p>
<p>$Y_{1}=X_{1}+\text { Attention }\left(X_{2}\right)​$</p>
<p>$Y_{2}=X_{2}+\text { FeedForward }\left(Y_{1}\right)$</p>
<h2 id="FF层分组"><a href="#FF层分组" class="headerlink" title="FF层分组"></a>FF层分组</h2><p>由于FFN层的计算不依赖于位置信息，可以将计算进行分块处理：$Y_{2}=\left[Y_{2}^{(1)} ; \ldots ; Y_{2}^{(c)}\right]=\left[X_{2}^{(1)}+\text { FeedForward }\left(Y_{1}^{(1)}\right) ; \ldots ; X_{2}^{(c)}+\text { FeedForward }\left(Y_{1}^{(c)}\right)\right]$</p>
<p>论文中特别强调，虽然通过分块和可逆层使得激活值是独立于层数的，但是对参数来说可不是这样，参数会随着层的增长而增长。好在我们可以利用CPU的内存，在逐层计算时将暂不使用的参数存储到CPU内存中，当需要时再交换回来。虽说从GPU到CPU的传输是比较慢的，但这对于Reformer来说，其batch_size * lenth已经达到可以忽略到这种参数传输的成本。</p>
<p>下表是所有变体的复杂度：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/reformer7.png" alt="图片"></p>
<h1 id="实验效果"><a href="#实验效果" class="headerlink" title="实验效果"></a>实验效果</h1><blockquote>
<p>参考：<br><a href="https://colab.research.google.com/github/google/trax/blob/master/trax/models/reformer/image_generation.ipynb" target="_blank" rel="noopener">https://colab.research.google.com/github/google/trax/blob/master/trax/models/reformer/image_generation.ipynb</a><br><a href="https://colab.research.google.com/github/google/trax/blob/master/trax/models/reformer/text_generation.ipynb" target="_blank" rel="noopener">https://colab.research.google.com/github/google/trax/blob/master/trax/models/reformer/text_generation.ipynb</a><br><a href="https://colab.research.google.com/github/google/trax/blob/master/trax/intro.ipynb" target="_blank" rel="noopener">https://colab.research.google.com/github/google/trax/blob/master/trax/intro.ipynb</a></p>
</blockquote>
<p>论文中的实验结论主要是为了证实Reformer可以更高效，且对精度几乎没有损失。这里贴一张Colab上对Reformer应用的效果图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/reformer8.png" alt="图片"></p>
<p>上图使用Reformer逐像素生成全画幅图像。</p>
]]></content>
      <tags>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>PLY教程及例子</title>
    <url>/2020/03/03/PLY%E6%95%99%E7%A8%8B%E5%8F%8A%E4%BE%8B%E5%AD%90/</url>
    <content><![CDATA[<p>最近需要重改语音助手中的计算器模块，打算用yacc&amp;lex实现，在这里记录一下学习和使用过程。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://github.com/PChou/python-lex-yacc" target="_blank" rel="noopener">https://github.com/PChou/python-lex-yacc</a></p>
</blockquote>
<h1 id="PLY教程"><a href="#PLY教程" class="headerlink" title="PLY教程"></a>PLY教程</h1><h2 id="PLY简介"><a href="#PLY简介" class="headerlink" title="PLY简介"></a>PLY简介</h2><p>PLY 是纯粹由 Python 实现的 Lex 和 yacc（流行的编译器构建工具）。PLY 的设计目标是尽可能的沿袭传统 lex 和 yacc 工具的工作方式，包括支持 LALR(1)分析法、提供丰富的输入验证、错误报告和诊断。因此，如果你曾经在其他编程语言下使用过 yacc，你应该能够很容易的迁移到 PLY 上。</p>
<p>PLY 包含两个独立的模块：lex.py 和 yacc.py，都定义在 ply 包下。lex.py 模块用来将输入字符通过一系列的正则表达式分解成标记序列，yacc.py 通过一些上下文无关的文法来识别编程语言语法。yacc.py 使用 LR 解析法，并使用 LALR(1)算法（默认）或者 SLR 算法生成分析表。</p>
<p>这两个工具是为了一起工作的。lex.py 通过向外部提供token()方法作为接口，方法每次会从输入中返回下一个有效的标记。yacc.py 将会不断的调用这个方法来获取标记并匹配语法规则。yacc.py 的功能通常是生成抽象语法树(AST)，不过，这完全取决于用户，如果需要，yacc.py 可以直接用来完成简单的翻译。</p>
<p>就像相应的 unix 工具，yacc.py 提供了大多数你期望的特性，其中包括：丰富的错误检查、语法验证、支持空产生式、错误的标记、通过优先级规则解决二义性。事实上，传统 yacc 能够做到的 PLY 都应该支持。</p>
<p>yacc.py 与 Unix 下的 yacc 的主要不同之处在于，yacc.py 没有包含一个独立的代码生成器，而是在 PLY 中依赖反射来构建词法分析器和语法解析器。不像传统的 lex/yacc 工具需要一个独立的输入文件，并将之转化成一个源文件，Python 程序必须是一个可直接可用的程序，这意味着不能有额外的源文件和特殊的创建步骤（像是那种执行 yacc 命令来生成 Python 代码）。又由于生成分析表开销较大，PLY 会缓存生成的分析表，并将它们保存在独立的文件中，除非源文件有变化，会重新生成分析表，否则将从缓存中直接读取。</p>
<h2 id="LEX简介"><a href="#LEX简介" class="headerlink" title="LEX简介"></a>LEX简介</h2><p>lex.py是用来将输入字符串标记化。例如，假设你正在设计一个编程语言，用户的输入字符串如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x &#x3D; 3 + 42 * (s - t)</span><br></pre></td></tr></table></figure>
<p>标记器将字符串分割成独立的标记：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#39;x&#39;,&#39;&#x3D;&#39;, &#39;3&#39;, &#39;+&#39;, &#39;42&#39;, &#39;*&#39;, &#39;(&#39;, &#39;s&#39;, &#39;-&#39;, &#39;t&#39;, &#39;)&#39;</span><br></pre></td></tr></table></figure><br>标记通常用一组名字来命名和表示：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#39;ID&#39;,&#39;EQUALS&#39;,&#39;NUMBER&#39;,&#39;PLUS&#39;,&#39;NUMBER&#39;,&#39;TIMES&#39;,&#39;LPAREN&#39;,&#39;ID&#39;,&#39;MINUS&#39;,&#39;ID&#39;,&#39;RPAREN&#39;</span><br></pre></td></tr></table></figure><br>将标记名和标记值本身组合起来：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(&#39;ID&#39;,&#39;x&#39;), (&#39;EQUALS&#39;,&#39;&#x3D;&#39;), (&#39;NUMBER&#39;,&#39;3&#39;),(&#39;PLUS&#39;,&#39;+&#39;), (&#39;NUMBER&#39;,&#39;42), (&#39;TIMES&#39;,&#39;*&#39;),(&#39;LPAREN&#39;,&#39;(&#39;), (&#39;ID&#39;,&#39;s&#39;),(&#39;MINUS&#39;,&#39;-&#39;),(&#39;ID&#39;,&#39;t&#39;), (&#39;RPAREN&#39;,&#39;)</span><br></pre></td></tr></table></figure></p>
<h3 id="LEX例子"><a href="#LEX例子" class="headerlink" title="LEX例子"></a>LEX例子</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import ply.lex as lex</span><br><span class="line"></span><br><span class="line"># List of token names.   This is always required</span><br><span class="line">tokens &#x3D; (</span><br><span class="line">   &#39;NUMBER&#39;,</span><br><span class="line">   &#39;PLUS&#39;,</span><br><span class="line">   &#39;MINUS&#39;,</span><br><span class="line">   &#39;TIMES&#39;,</span><br><span class="line">   &#39;DIVIDE&#39;,</span><br><span class="line">   &#39;LPAREN&#39;,</span><br><span class="line">   &#39;RPAREN&#39;,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># Regular expression rules for simple tokens</span><br><span class="line">t_PLUS    &#x3D; r&#39;\+&#39;</span><br><span class="line">t_MINUS   &#x3D; r&#39;-&#39;</span><br><span class="line">t_TIMES   &#x3D; r&#39;\*&#39;</span><br><span class="line">t_DIVIDE  &#x3D; r&#39;&#x2F;&#39;</span><br><span class="line">t_LPAREN  &#x3D; r&#39;\(&#39;</span><br><span class="line">t_RPAREN  &#x3D; r&#39;\)&#39;</span><br><span class="line"></span><br><span class="line"># A regular expression rule with some action code</span><br><span class="line">def t_NUMBER(t):</span><br><span class="line">    r&#39;\d+&#39;</span><br><span class="line">    t.value &#x3D; int(t.value)    </span><br><span class="line">    return t</span><br><span class="line"></span><br><span class="line"># Define a rule so we can track line numbers</span><br><span class="line">def t_newline(t):</span><br><span class="line">    r&#39;\n+&#39;</span><br><span class="line">    t.lexer.lineno +&#x3D; len(t.value)</span><br><span class="line"></span><br><span class="line"># A string containing ignored characters (spaces and tabs)</span><br><span class="line">t_ignore  &#x3D; &#39; \t&#39;</span><br><span class="line"></span><br><span class="line"># Error handling rule</span><br><span class="line">def t_error(t):</span><br><span class="line">    print(&quot;Illegal character &#39;%s&#39;&quot; % t.value[0])</span><br><span class="line">    t.lexer.skip(1)</span><br><span class="line"></span><br><span class="line"># Build the lexer</span><br><span class="line">lexer &#x3D; lex.lex()</span><br></pre></td></tr></table></figure>
<p>为了使 lexer 工作，你需要给定一个输入，并传递给input()方法。然后，重复调用token()方法来获取标记序列：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Test it out</span><br><span class="line">data &#x3D; &#39;&#39;&#39;</span><br><span class="line">3 + 4 * 10</span><br><span class="line">  + -20 *2</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line"></span><br><span class="line"># Give the lexer some input</span><br><span class="line">lexer.input(data)</span><br><span class="line"></span><br><span class="line"># Tokenize</span><br><span class="line">for tok in lexer:</span><br><span class="line">    if not tok: break      # No more input</span><br><span class="line">    print(tok.type, tok.value, tok.lineno, tok.lexpos)</span><br></pre></td></tr></table></figure><br>程序执行，将给出如下输出：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">LexToken(NUMBER,3,2,1)</span><br><span class="line">LexToken(PLUS,&#39;+&#39;,2,3)</span><br><span class="line">LexToken(NUMBER,4,2,5)</span><br><span class="line">LexToken(TIMES,&#39;*&#39;,2,7)</span><br><span class="line">LexToken(NUMBER,10,2,10)</span><br><span class="line">LexToken(PLUS,&#39;+&#39;,3,14)</span><br><span class="line">LexToken(MINUS,&#39;-&#39;,3,16)</span><br><span class="line">LexToken(NUMBER,20,3,18)</span><br><span class="line">LexToken(TIMES,&#39;*&#39;,3,20)</span><br><span class="line">LexToken(NUMBER,2,3,21)</span><br></pre></td></tr></table></figure><br>tok.type和tok.value属性表示标记本身的类型和值。tok.line和tok.lexpos属性包含了标记的位置信息，tok.lexpos表示标记相对于输入串起始位置的偏移。</p>
<h3 id="标记列表"><a href="#标记列表" class="headerlink" title="标记列表"></a>标记列表</h3><p>词法分析器必须提供一个标记的列表，这个列表将所有可能的标记告诉分析器，用来执行各种验证，同时也提供给 yacc.py 作为终结符。在上面的例子中，标记列表是由tokens指定的。</p>
<h3 id="标记的规则"><a href="#标记的规则" class="headerlink" title="标记的规则"></a>标记的规则</h3><p>每种标记用一个正则表达式规则来表示，每个规则需要以”t_”开头声明，表示该声明是对标记的规则定义。对于简单的标记，可以定义成这样（在 Python 中使用 raw string 能比较方便的书写正则表达式）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">t_PLUS &#x3D; r&#39;\+&#39;</span><br></pre></td></tr></table></figure>
<p>这里，紧跟在 t_ 后面的单词，必须跟标记列表中的某个标记名称对应。如果需要执行动作的话，规则可以写成一个方法。例如，下面的规则匹配数字字串，并且将匹配的字符串转化成 Python 的整型：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def t_NUMBER(t):</span><br><span class="line">    r&#39;\d+&#39;</span><br><span class="line">    t.value &#x3D; int(t.value)</span><br><span class="line">    return t</span><br></pre></td></tr></table></figure><br>如果使用方法的话，正则表达式写成方法的文档字符串。方法总是需要接受一个 LexToken 实例的参数，该实例有一个 t.type 的属性（字符串表示）来表示标记的类型名称，t.value 是标记值（匹配的实际的字符串），t.lineno 表示当前在源输入串中的作业行，t.lexpos 表示标记相对于输入串起始位置的偏移。默认情况下，t.type 是以t_开头的变量或方法的后面部分。方法可以在方法体里面修改这些属性。但是，如果这样做，应该返回结果 token，否则，标记将被丢弃。<br>在 lex 内部，lex.py 用re模块处理模式匹配，在构造最终的完整的正则式的时候，用户提供的规则按照下面的顺序加入：</p>
<ul>
<li>所有由方法定义的标记规则，按照他们的出现顺序依次加入</li>
<li>由字符串变量定义的标记规则按照其正则式长度倒序后，依次加入（长的先入）</li>
<li>顺序的约定对于精确匹配是必要的。比如，如果你想区分‘=’和‘==’，你需要确保‘==’优先检查。如果用字符串来定义这样的表达式的话，通过将较长的正则式先加入，可以帮助解决这个问题。用方法定义标记，可以显示地控制哪个规则优先检查。</li>
</ul>
<p>为了处理保留字，你应该写一个单一的规则来匹配这些标识，并在方法里面作特殊的查询：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">reserved &#x3D; &#123;</span><br><span class="line">   &#39;if&#39; : &#39;IF&#39;,</span><br><span class="line">   &#39;then&#39; : &#39;THEN&#39;,</span><br><span class="line">   &#39;else&#39; : &#39;ELSE&#39;,</span><br><span class="line">   &#39;while&#39; : &#39;WHILE&#39;,</span><br><span class="line">   ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">tokens &#x3D; [&#39;LPAREN&#39;,&#39;RPAREN&#39;,...,&#39;ID&#39;] + list(reserved.values())</span><br><span class="line"></span><br><span class="line">def t_ID(t):</span><br><span class="line">    r&#39;[a-zA-Z_][a-zA-Z_0-9]*&#39;</span><br><span class="line">    t.type &#x3D; reserved.get(t.value,&#39;ID&#39;)    # Check for reserved words</span><br></pre></td></tr></table></figure>
<pre><code>return t
</code></pre><p>这样做可以大大减少正则式的个数，并稍稍加快处理速度。注意：你应该避免为保留字编写单独的规则，例如，如果你像下面这样写：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">t_FOR   &#x3D; r&#39;for&#39;</span><br><span class="line">t_PRINT &#x3D; r&#39;print&#39;</span><br></pre></td></tr></table></figure><br>但是，这些规则照样也能够匹配以这些字符开头的单词，比如’forget’或者’printed’，这通常不是你想要的。</p>
<h3 id="标记的值"><a href="#标记的值" class="headerlink" title="标记的值"></a>标记的值</h3><p>标记被 lex 返回后，它们的值被保存在value属性中。正常情况下，value 是匹配的实际文本。事实上，value 可以被赋为任何 Python 支持的类型。例如，当扫描到标识符的时候，你可能不仅需要返回标识符的名字，还需要返回其在符号表中的位置，可以像下面这样写：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def t_ID(t):</span><br><span class="line">    ...</span><br><span class="line">    # Look up symbol table information and return a tuple</span><br><span class="line">    t.value &#x3D; (t.value, symbol_lookup(t.value))</span><br><span class="line">    ...</span><br><span class="line">    return t</span><br></pre></td></tr></table></figure>
<p>需要注意的是，不推荐用其他属性来保存值，因为 yacc.py 模块只会暴露出标记的 value属 性，访问其他属性会变得不自然。如果想保存多种属性，可以将元组、字典、或者对象实例赋给 value。</p>
<h3 id="丢弃标记"><a href="#丢弃标记" class="headerlink" title="丢弃标记"></a>丢弃标记</h3><p>想丢弃像注释之类的标记，只要不返回 value 就行了，像这样：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def t_COMMENT(t):</span><br><span class="line">    r&#39;\#.*&#39;</span><br><span class="line">    pass</span><br><span class="line">    # No return value. Token discarded</span><br></pre></td></tr></table></figure>
<p>为标记声明添加”ignore_”前缀同样可以达到目的：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">t_ignore_COMMENT &#x3D; r&#39;\#.*&#39;</span><br></pre></td></tr></table></figure><br>如果有多种文本需要丢弃，建议使用方法来定义规则，因为方法能够提供更精确的匹配优先级控制（方法根据出现的顺序，而字符串的正则表达式依据正则表达式的长度）</p>
<h3 id="行号和位置信息"><a href="#行号和位置信息" class="headerlink" title="行号和位置信息"></a>行号和位置信息</h3><p>默认情况下，lex.py 对行号一无所知。因为 lex.py 根本不知道何为”行”的概念（换行符本身也作为文本的一部分）。不过，可以通过写一个特殊的规则来记录行号：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Define a rule so we can track line numbers</span><br><span class="line">def t_newline(t):</span><br><span class="line">    r&#39;\n+&#39;</span><br><span class="line">    t.lexer.lineno +&#x3D; len(t.value)</span><br></pre></td></tr></table></figure>
<p>在这个规则中，当前 lexer 对象 t.lexer 的 lineno 属性被修改了，而且空行被简单的丢弃了，因为没有任何的返回。<br>lex.py 也不自动做列跟踪。但是，位置信息被记录在了每个标记对象的lexpos属性中，这样，就有可能来计算列信息了。例如：每当遇到新行的时候就重置列值：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Compute column. input is the input text string token is a token instance</span><br><span class="line">def find_column(input,token):</span><br><span class="line">    last_cr &#x3D; input.rfind(&#39;\n&#39;,0,token.lexpos)</span><br><span class="line">    if last_cr &lt; 0:</span><br><span class="line">        last_cr &#x3D; 0</span><br><span class="line">    column &#x3D; (token.lexpos - last_cr) + 1</span><br><span class="line">    return column</span><br></pre></td></tr></table></figure>
<p>通常，计算列的信息是为了指示上下文的错误位置，所以只在必要时有用。</p>
<h3 id="忽略字符"><a href="#忽略字符" class="headerlink" title="忽略字符"></a>忽略字符</h3><p>t_ignore规则比较特殊，是lex.py所保留用来忽略字符的，通常用来跳过空白或者不需要的字符。虽然可以通过定义像t_newline()这样的规则来完成相同的事情，不过使用t_ignore能够提供较好的词法分析性能，因为相比普通的正则式，它被特殊化处理了。用PLY写一个简单计算器</p>
<h3 id="字面字符"><a href="#字面字符" class="headerlink" title="字面字符"></a>字面字符</h3><p>字面字符可以通过在词法模块中定义一个literals变量做到，例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">literals &#x3D; [ &#39;+&#39;,&#39;-&#39;,&#39;*&#39;,&#39;&#x2F;&#39; ]</span><br></pre></td></tr></table></figure>
<p>或者<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">literals &#x3D; &quot;+-*&#x2F;&quot;</span><br></pre></td></tr></table></figure><br>字面字符是指单个字符，表示把字符本身作为标记，标记的type和value都是字符本身。不过，字面字符是在其他正则式之后被检查的，因此如果有规则是以这些字符开头的，那么这些规则的优先级较高。</p>
<h3 id="错误处理"><a href="#错误处理" class="headerlink" title="错误处理"></a>错误处理</h3><p>最后，在词法分析中遇到非法字符时，t_error()用来处理这类错误。这种情况下，t.value包含了余下还未被处理的输入字串，在之前的例子中，错误处理方法是这样的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Error handling rule</span><br><span class="line">def t_error(t):</span><br><span class="line">    print &quot;Illegal character &#39;%s&#39;&quot; % t.value[0]</span><br><span class="line">    t.lexer.skip(1)</span><br></pre></td></tr></table></figure>
<p>这个例子中，我们只是简单的输出不合法的字符，并且通过调用t.lexer.skip(1)跳过一个字符。</p>
<h3 id="构建和使用-lexer"><a href="#构建和使用-lexer" class="headerlink" title="构建和使用 lexer"></a>构建和使用 lexer</h3><p>函数lex.lex()使用 Python 的反射机制读取调用上下文中的正则表达式，来创建 lexer。lexer 一旦创建好，有两个方法可以用来控制 lexer 对象：</p>
<ul>
<li>lexer.input(data) 重置 lexer 和输入字串</li>
<li>lexer.token() 返回下一个 LexToken 类型的标记实例，如果进行到输入字串的尾部时将返回None</li>
</ul>
<p>推荐直接在 lex() 函数返回的 lexer 对象上调用上述接口，尽管也可以向下面这样用模块级别的 lex.input() 和 lex.token()：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lex.lex()</span><br><span class="line">lex.input(sometext)</span><br><span class="line">while 1:</span><br><span class="line">    tok &#x3D; lex.token()</span><br><span class="line">    if not tok: break</span><br><span class="line">    print tok</span><br></pre></td></tr></table></figure>
<p>在这个例子中，lex.input() 和 lex.token() 是模块级别的方法，在 lex 模块中，input() 和 token() 方法绑定到最新创建的 lexer 对象的对应方法上。最好不要这样用，因为这种接口可能不知道在什么时候就失效（例如垃圾回收）。</p>
<h3 id="TOKEN-装饰器"><a href="#TOKEN-装饰器" class="headerlink" title="@TOKEN 装饰器"></a>@TOKEN 装饰器</h3><p>在一些应用中，你可能需要定义一系列辅助的记号来构建复杂的正则表达式，例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">digit            &#x3D; r&#39;([0-9])&#39;</span><br><span class="line">nondigit         &#x3D; r&#39;([_A-Za-z])&#39;</span><br><span class="line">identifier       &#x3D; r&#39;(&#39; + nondigit + r&#39;(&#39; + digit + r&#39;|&#39; + nondigit + r&#39;)*)&#39;        </span><br><span class="line"></span><br><span class="line">def t_ID(t):</span><br><span class="line">    # want docstring to be identifier above. ?????</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p>在这个例子中，我们希望 ID 的规则引用上面的已有的变量。然而，使用文档字符串无法做到，为了解决这个问题，你可以使用@TOKEN装饰器：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from ply.lex import TOKEN</span><br><span class="line"></span><br><span class="line">@TOKEN(identifier)</span><br><span class="line">def t_ID(t):</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><br>装饰器可以将 identifier 关联到 t_ID() 的文档字符串上以使 lex.py 正常工作，一种等价的做法是直接给文档字符串赋值：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def t_ID(t):</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">t_ID.__doc__ &#x3D; identifier</span><br></pre></td></tr></table></figure></p>
<h3 id="优化模式"><a href="#优化模式" class="headerlink" title="优化模式"></a>优化模式</h3><p>为了提高性能，你可能希望使用 Python 的优化模式（比如，使用 -o 选项执行 Python）。然而，这样的话，Python 会忽略文档字串，这是 lex.py 的特殊问题，可以通过在创建 lexer 的时候使用 optimize 选项：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lexer &#x3D; lex.lex(optimize&#x3D;1)</span><br></pre></td></tr></table></figure>
<p>接着，用 Python 常规的模式运行，这样，lex.py 会在当前目录下创建一个 lextab.py 文件，这个文件会包含所有的正则表达式规则和词法分析阶段的分析表。然后，lextab.py 可以被导入用来构建 lexer。这种方法大大改善了词法分析程序的启动时间，而且可以在 Python 的优化模式下工作。<br>想要更改生成的文件名，使用如下参数：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lexer &#x3D; lex.lex(optimize&#x3D;1,lextab&#x3D;&quot;footab&quot;)</span><br></pre></td></tr></table></figure>
<p>在优化模式下执行，需要注意的是 lex 会被禁用大多数的错误检查。因此，建议只在确保万事俱备准备发布最终代码时使用。</p>
<h3 id="调试"><a href="#调试" class="headerlink" title="调试"></a>调试</h3><p>如果想要调试，可以使 lex() 运行在调试模式：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lexer &#x3D; lex.lex(debug&#x3D;1)</span><br></pre></td></tr></table></figure>
<p>这将打出一些调试信息，包括添加的规则、最终的正则表达式和词法分析过程中得到的标记。除此之外，lex.py 有一个简单的主函数，不但支持对命令行参数输入的字串进行扫描，还支持命令行参数指定的文件名：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;</span><br><span class="line">     lex.runmain()</span><br></pre></td></tr></table></figure></p>
<h3 id="其他方式定义词法规则"><a href="#其他方式定义词法规则" class="headerlink" title="其他方式定义词法规则"></a>其他方式定义词法规则</h3><p>上面的例子，词法分析器都是在单个的 Python 模块中指定的。如果你想将标记的规则放到不同的模块，使用 module 关键字参数。例如，你可能有一个专有的模块，包含了标记的规则：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># module: tokrules.py</span><br><span class="line"># This module just contains the lexing rules</span><br><span class="line"></span><br><span class="line"># List of token names.   This is always required</span><br><span class="line">tokens &#x3D; (</span><br><span class="line">   &#39;NUMBER&#39;,</span><br><span class="line">   &#39;PLUS&#39;,</span><br><span class="line">   &#39;MINUS&#39;,</span><br><span class="line">   &#39;TIMES&#39;,</span><br><span class="line">   &#39;DIVIDE&#39;,</span><br><span class="line">   &#39;LPAREN&#39;,</span><br><span class="line">   &#39;RPAREN&#39;,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># Regular expression rules for simple tokens</span><br><span class="line">t_PLUS    &#x3D; r&#39;\+&#39;</span><br><span class="line">t_MINUS   &#x3D; r&#39;-&#39;</span><br><span class="line">t_TIMES   &#x3D; r&#39;\*&#39;</span><br><span class="line">t_DIVIDE  &#x3D; r&#39;&#x2F;&#39;</span><br><span class="line">t_LPAREN  &#x3D; r&#39;\(&#39;</span><br><span class="line">t_RPAREN  &#x3D; r&#39;\)&#39;</span><br><span class="line"></span><br><span class="line"># A regular expression rule with some action code</span><br><span class="line">def t_NUMBER(t):</span><br><span class="line">    r&#39;\d+&#39;</span><br><span class="line">    t.value &#x3D; int(t.value)    </span><br><span class="line">    return t</span><br><span class="line"></span><br><span class="line"># Define a rule so we can track line numbers</span><br><span class="line">def t_newline(t):</span><br><span class="line">    r&#39;\n+&#39;</span><br><span class="line">    t.lexer.lineno +&#x3D; len(t.value)</span><br><span class="line"></span><br><span class="line"># A string containing ignored characters (spaces and tabs)</span><br><span class="line">t_ignore  &#x3D; &#39; \t&#39;</span><br><span class="line"></span><br><span class="line"># Error handling rule</span><br><span class="line">def t_error(t):</span><br><span class="line">    print &quot;Illegal character &#39;%s&#39;&quot; % t.value[0]</span><br></pre></td></tr></table></figure>
<pre><code>t.lexer.skip(1)
</code></pre><p>现在，如果你想要从不同的模块中构建分析器，应该这样：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import tokrules</span><br><span class="line">import ply.lex as lex</span><br><span class="line"></span><br><span class="line">lexer &#x3D; lex.lex(module&#x3D;tokrules)</span><br><span class="line">lexer.input(&quot;3 + 4&quot;)</span><br><span class="line">for tok in lexer:</span><br><span class="line">    if not tok: break      # No more input</span><br><span class="line">    print(tok.type, tok.value, tok.lineno, tok.lexpos)</span><br></pre></td></tr></table></figure><br>module选项也可以指定类型的实例，例如：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import ply.lex as lex</span><br><span class="line"></span><br><span class="line">class MyLexer:</span><br><span class="line">    # List of token names.   This is always required</span><br><span class="line">    tokens &#x3D; (</span><br><span class="line">       &#39;NUMBER&#39;,</span><br><span class="line">       &#39;PLUS&#39;,</span><br><span class="line">       &#39;MINUS&#39;,</span><br><span class="line">       &#39;TIMES&#39;,</span><br><span class="line">       &#39;DIVIDE&#39;,</span><br><span class="line">       &#39;LPAREN&#39;,</span><br><span class="line">       &#39;RPAREN&#39;,</span><br><span class="line">    )</span><br><span class="line">    # Regular expression rules for simple tokens</span><br><span class="line">    t_PLUS    &#x3D; r&#39;\+&#39;</span><br><span class="line">    t_MINUS   &#x3D; r&#39;-&#39;</span><br><span class="line">    t_TIMES   &#x3D; r&#39;\*&#39;</span><br><span class="line">    t_DIVIDE  &#x3D; r&#39;&#x2F;&#39;</span><br><span class="line">    t_LPAREN  &#x3D; r&#39;\(&#39;</span><br><span class="line">    t_RPAREN  &#x3D; r&#39;\)&#39;</span><br><span class="line">    # A regular expression rule with some action code</span><br><span class="line">    # Note addition of self parameter since we&#39;re in a class</span><br><span class="line">    def t_NUMBER(self,t):</span><br><span class="line">        r&#39;\d+&#39;</span><br><span class="line">        t.value &#x3D; int(t.value)    </span><br><span class="line">        return t</span><br><span class="line">    # Define a rule so we can track line numbers</span><br><span class="line">    def t_newline(self,t):</span><br><span class="line">        r&#39;\n+&#39;</span><br><span class="line">        t.lexer.lineno +&#x3D; len(t.value)</span><br><span class="line">    # A string containing ignored characters (spaces and tabs)</span><br><span class="line">    t_ignore  &#x3D; &#39; \t&#39;</span><br><span class="line">    # Error handling rule</span><br><span class="line">    def t_error(self,t):</span><br><span class="line">        print(&quot;Illegal character &#39;%s&#39;&quot; % t.value[0])</span><br><span class="line">        t.lexer.skip(1)</span><br><span class="line">    # Build the lexer</span><br><span class="line">    def build(self,**kwargs):</span><br><span class="line">        self.lexer &#x3D; lex.lex(module&#x3D;self, **kwargs)</span><br><span class="line">    </span><br><span class="line">    # Test it output</span><br><span class="line">    def test(self,data):</span><br><span class="line">        self.lexer.input(data)</span><br><span class="line">        while True:</span><br><span class="line">             tok &#x3D; self.lexer.token()</span><br><span class="line">             if not tok: break</span><br><span class="line">             print(tok)</span><br><span class="line"># Build the lexer and try it out</span><br><span class="line">m &#x3D; MyLexer()</span><br><span class="line">m.build()           # Build the lexer</span><br><span class="line">m.test(&quot;3 + 4&quot;)     # Test it</span><br></pre></td></tr></table></figure><br>当从类中定义 lexer，你需要创建类的实例，而不是类本身。这是因为，lexer 的方法只有被绑定（bound-methods）对象后才能使 PLY 正常工作。<br>当给 lex() 方法使用 module 选项时，PLY 使用dir()方法，从对象中获取符号信息，因为不能直接访问对象的<strong>dict</strong>属性。（译者注：可能是因为兼容性原因，<strong>dict</strong>这个方法可能不存在）</p>
<p>最后，如果你希望保持较好的封装性，但不希望什么东西都写在类里面，lexers 可以在闭包中定义，例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import ply.lex as lex</span><br><span class="line"></span><br><span class="line"># List of token names.   This is always required</span><br><span class="line">tokens &#x3D; (</span><br><span class="line">  &#39;NUMBER&#39;,</span><br><span class="line">  &#39;PLUS&#39;,</span><br><span class="line">  &#39;MINUS&#39;,</span><br><span class="line">  &#39;TIMES&#39;,</span><br><span class="line">  &#39;DIVIDE&#39;,</span><br><span class="line">  &#39;LPAREN&#39;,</span><br><span class="line">  &#39;RPAREN&#39;,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">def MyLexer():</span><br><span class="line">    # Regular expression rules for simple tokens</span><br><span class="line">    t_PLUS    &#x3D; r&#39;\+&#39;</span><br><span class="line">    t_MINUS   &#x3D; r&#39;-&#39;</span><br><span class="line">    t_TIMES   &#x3D; r&#39;\*&#39;</span><br><span class="line">    t_DIVIDE  &#x3D; r&#39;&#x2F;&#39;</span><br><span class="line">    t_LPAREN  &#x3D; r&#39;\(&#39;</span><br><span class="line">    t_RPAREN  &#x3D; r&#39;\)&#39;</span><br><span class="line"></span><br><span class="line">    # A regular expression rule with some action code</span><br><span class="line">    def t_NUMBER(t):</span><br><span class="line">        r&#39;\d+&#39;</span><br><span class="line">        t.value &#x3D; int(t.value)    </span><br><span class="line">        return t</span><br><span class="line"></span><br><span class="line">    # Define a rule so we can track line numbers</span><br><span class="line">    def t_newline(t):</span><br><span class="line">        r&#39;\n+&#39;</span><br><span class="line">        t.lexer.lineno +&#x3D; len(t.value)</span><br><span class="line"></span><br><span class="line">    # A string containing ignored characters (spaces and tabs)</span><br><span class="line">    t_ignore  &#x3D; &#39; \t&#39;</span><br><span class="line"></span><br><span class="line">    # Error handling rule</span><br><span class="line">    def t_error(t):</span><br><span class="line">        print &quot;Illegal character &#39;%s&#39;&quot; % t.value[0]</span><br><span class="line">        t.lexer.skip(1)</span><br><span class="line"></span><br><span class="line">    # Build the lexer from my environment and return it    </span><br><span class="line">    return lex.lex()</span><br></pre></td></tr></table></figure>
<h3 id="额外状态维护"><a href="#额外状态维护" class="headerlink" title="额外状态维护"></a>额外状态维护</h3><p>在你的词法分析器中，你可能想要维护一些状态。这可能包括模式设置，符号表和其他细节。例如，假设你想要跟踪NUMBER标记的出现个数。</p>
<p>一种方法是维护一个全局变量：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">num_count &#x3D; 0</span><br><span class="line">def t_NUMBER(t):</span><br><span class="line">    r&#39;\d+&#39;</span><br><span class="line">    global num_count</span><br><span class="line">    num_count +&#x3D; 1</span><br><span class="line">    t.value &#x3D; int(t.value)    </span><br><span class="line">    return t</span><br></pre></td></tr></table></figure>
<p>如果你不喜欢全局变量，另一个记录信息的地方是 lexer 对象内部。可以通过当前标记的 lexer 属性访问：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def t_NUMBER(t):</span><br><span class="line">    r&#39;\d+&#39;</span><br><span class="line">    t.lexer.num_count +&#x3D; 1     # Note use of lexer attribute</span><br><span class="line">    t.value &#x3D; int(t.value)    </span><br><span class="line">    return t</span><br><span class="line"></span><br><span class="line">lexer &#x3D; lex.lex()</span><br><span class="line">lexer.num_count &#x3D; 0            # Set the initial count</span><br></pre></td></tr></table></figure><br>上面这样做的优点是当同时存在多个 lexer 实例的情况下，简单易行。不过这看上去似乎是严重违反了面向对象的封装原则。lexer 的内部属性（除了 lineno ）都是以 lex 开头命名的（lexdata、lexpos）。因此，只要不以 lex 开头来命名属性就很安全的。<br>如果你不喜欢给 lexer 对象赋值，你可以自定义你的 lexer 类型，就像前面看到的那样：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class MyLexer:</span><br><span class="line">    ...</span><br><span class="line">    def t_NUMBER(self,t):</span><br><span class="line">        r&#39;\d+&#39;</span><br><span class="line">        self.num_count +&#x3D; 1</span><br><span class="line">        t.value &#x3D; int(t.value)    </span><br><span class="line">        return t</span><br><span class="line"></span><br><span class="line">    def build(self, **kwargs):</span><br><span class="line">        self.lexer &#x3D; lex.lex(object&#x3D;self,**kwargs)</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.num_count &#x3D; 0</span><br></pre></td></tr></table></figure>
<p>如果你的应用会创建很多 lexer 的实例，并且需要维护很多状态，上面的类可能是最容易管理的。<br>状态也可以用闭包来管理，比如，在 Python3 中：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def MyLexer():</span><br><span class="line">    num_count &#x3D; 0</span><br><span class="line">    ...</span><br><span class="line">    def t_NUMBER(t):</span><br><span class="line">        r&#39;\d+&#39;</span><br><span class="line">        nonlocal num_count</span><br><span class="line">        num_count +&#x3D; 1</span><br><span class="line">        t.value &#x3D; int(t.value)    </span><br><span class="line">        return t</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<h3 id="Lexer-克隆"><a href="#Lexer-克隆" class="headerlink" title="Lexer 克隆"></a>Lexer 克隆</h3><p>如果有必要的话，lexer 对象可以通过clone()方法来复制：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lexer &#x3D; lex.lex()</span><br><span class="line">...</span><br><span class="line">newlexer &#x3D; lexer.clone()</span><br></pre></td></tr></table></figure>
<p>当 lexer 被克隆后，复制品能够精确的保留输入串和内部状态，不过，新的 lexer 可以接受一个不同的输出字串，并独立运作起来。这在几种情况下也许有用：当你在编写的解析器或编译器涉及到递归或者回退处理时，你需要扫描先前的部分，你可以clone并使用复制品，或者你在实现某种预编译处理，可以 clone 一些 lexer 来处理不同的输入文件。<br>创建克隆跟重新调用 lex.lex() 的不同点在于，PLY 不会重新构建任何的内部分析表或者正则式。当 lexer 是用类或者闭包创建的，需要注意类或闭包本身的的状态。换句话说你要注意新创建的 lexer 会共享原始 lexer 的这些状态，比如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">m &#x3D; MyLexer()</span><br><span class="line">a &#x3D; lex.lex(object&#x3D;m)      # Create a lexer</span><br><span class="line"></span><br><span class="line">b &#x3D; a.clone()              # Clone the lexer</span><br></pre></td></tr></table></figure>
<h3 id="Lexer-的内部状态"><a href="#Lexer-的内部状态" class="headerlink" title="Lexer 的内部状态"></a>Lexer 的内部状态</h3><p>lexer 有一些内部属性在特定情况下有用：</p>
<ul>
<li>lexer.lexpos。这是一个表示当前分析点的位置的整型值。如果你修改这个值的话，这会改变下一个 token() 的调用行为。在标记的规则方法里面，这个值表示紧跟匹配字串后面的第一个字符的位置，如果这个值在规则中修改，下一个返回的标记将从新的位置开始匹配</li>
<li>lexer.lineno。表示当前行号。PLY 只是声明这个属性的存在，却永远不更新这个值。如果你想要跟踪行号的话，你需要自己添加代码（ 4.6 行号和位置信息）</li>
<li>lexer.lexdata。当前 lexer 的输入字串，这个字符串就是 input() 方法的输入字串，更改它可能是个糟糕的做法，除非你知道自己在干什么。</li>
<li>lexer.lexmatch。PLY 内部调用 Python 的 re.match() 方法得到的当前标记的原始的 Match 对象，该对象被保存在这个属性中。如果你的正则式中包含分组的话，你可以通过这个对象获得这些分组的值。注意：这个属性只在有标记规则定义的方法中才有效。<h3 id="基于条件的扫描和启动条件"><a href="#基于条件的扫描和启动条件" class="headerlink" title="基于条件的扫描和启动条件"></a>基于条件的扫描和启动条件</h3>在高级的分析器应用程序中，使用状态化的词法扫描是很有用的。比如，你想在出现特定标记或句子结构的时候触发开始一个不同的词法分析逻辑。PLY 允许 lexer 在不同的状态之间转换。每个状态可以包含一些自己独特的标记和规则等。这是基于 GNU flex 的“启动条件”来实现的，关于 flex 详见 <a href="http://flex.sourceforge.net/manual/Start-Conditions.html#Start-Conditions" target="_blank" rel="noopener">http://flex.sourceforge.net/manual/Start-Conditions.html#Start-Conditions</a></li>
</ul>
<p>要使用 lex 的状态，你必须首先声明。通过在 lex 模块中声明”states”来做到：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">states &#x3D; (</span><br><span class="line">   (&#39;foo&#39;,&#39;exclusive&#39;),</span><br><span class="line">   (&#39;bar&#39;,&#39;inclusive&#39;),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>这个声明中包含有两个状态：’foo’和’bar’。状态可以有两种类型：’排他型’和’包容型’。排他型的状态会使得 lexer 的行为发生完全的改变：只有能够匹配在这个状态下定义的规则的标记才会返回；包容型状态会将定义在这个状态下的规则添加到默认的规则集中，进而，只要能匹配这个规则集的标记都会返回。<br>一旦声明好之后，标记规则的命名需要包含状态名：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">t_foo_NUMBER &#x3D; r&#39;\d+&#39;                      # Token &#39;NUMBER&#39; in state &#39;foo&#39;        </span><br><span class="line">t_bar_ID     &#x3D; r&#39;[a-zA-Z_][a-zA-Z0-9_]*&#39;   # Token &#39;ID&#39; in state &#39;bar&#39;</span><br><span class="line"></span><br><span class="line">def t_foo_newline(t):</span><br><span class="line">    r&#39;\n&#39;</span><br><span class="line">    t.lexer.lineno +&#x3D; 1</span><br></pre></td></tr></table></figure>
<p>一个标记可以用在多个状态中，只要将多个状态名包含在声明中：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">t_foo_bar_NUMBER &#x3D; r&#39;\d+&#39;         # Defines token &#39;NUMBER&#39; in both state &#39;foo&#39; and &#39;bar&#39;</span><br></pre></td></tr></table></figure><br>同样的，在任何状态下都生效的声明可以在命名中使用ANY：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">t_ANY_NUMBER &#x3D; r&#39;\d+&#39;         # Defines a token &#39;NUMBER&#39; in all states</span><br></pre></td></tr></table></figure><br>不包含状态名的情况下，标记被关联到一个特殊的状态INITIAL，比如，下面两个声明是等价的：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">t_NUMBER &#x3D; r&#39;\d+&#39;</span><br><span class="line">t_INITIAL_NUMBER &#x3D; r&#39;\d+&#39;</span><br></pre></td></tr></table></figure><br>特殊的t_ignore()和t_error()也可以用状态关联：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">t_foo_ignore &#x3D; &quot; \t\n&quot;       # Ignored characters for state &#39;foo&#39;</span><br><span class="line">def t_bar_error(t):          # Special error handler for state &#39;bar&#39;</span><br><span class="line">    pass</span><br></pre></td></tr></table></figure><br>词法分析默认在INITIAL状态下工作，这个状态下包含了所有默认的标记规则定义。对于不希望使用“状态”的用户来说，这是完全透明的。在分析过程中，如果你想要改变词法分析器的这种的状态，使用begin()方法：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def t_begin_foo(t):</span><br><span class="line">    r&#39;start_foo&#39;</span><br><span class="line">    t.lexer.begin(&#39;foo&#39;)             # Starts &#39;foo&#39; state</span><br></pre></td></tr></table></figure><br>使用 begin() 切换回初始状态：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def t_foo_end(t):</span><br><span class="line">    r&#39;end_foo&#39;</span><br><span class="line">    t.lexer.begin(&#39;INITIAL&#39;)        # Back to the initial state</span><br></pre></td></tr></table></figure><br>状态的切换可以使用栈：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def t_begin_foo(t):</span><br><span class="line">    r&#39;start_foo&#39;</span><br><span class="line">    t.lexer.push_state(&#39;foo&#39;)             # Starts &#39;foo&#39; state</span><br><span class="line"></span><br><span class="line">def t_foo_end(t):</span><br><span class="line">    r&#39;end_foo&#39;</span><br><span class="line">    t.lexer.pop_state()                   # Back to the previous state</span><br></pre></td></tr></table></figure><br>当你在面临很多状态可以选择进入，而又仅仅想要回到之前的状态时，状态栈比较有用。<br>举个例子会更清晰。假设你在写一个分析器想要从一堆 C 代码中获取任意匹配的闭合的大括号里面的部分：这意味着，当遇到起始括号’{‘，你需要读取与之匹配的’}’以上的所有部分。并返回字符串。使用通常的正则表达式几乎不可能，这是因为大括号可以嵌套，而且可以有注释，字符串等干扰。因此，试图简单的匹配第一个出现的’}’是不行的。这里你可以用lex的状态来做到：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Declare the state</span><br><span class="line">states &#x3D; (</span><br><span class="line">  (&#39;ccode&#39;,&#39;exclusive&#39;),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># Match the first &#123;. Enter ccode state.</span><br><span class="line">def t_ccode(t):</span><br><span class="line">    r&#39;\&#123;&#39;</span><br><span class="line">    t.lexer.code_start &#x3D; t.lexer.lexpos        # Record the starting position</span><br><span class="line">    t.lexer.level &#x3D; 1                          # Initial brace level</span><br><span class="line">    t.lexer.begin(&#39;ccode&#39;)                     # Enter &#39;ccode&#39; state</span><br><span class="line"></span><br><span class="line"># Rules for the ccode state</span><br><span class="line">def t_ccode_lbrace(t):     </span><br><span class="line">    r&#39;\&#123;&#39;</span><br><span class="line">    t.lexer.level +&#x3D;1                </span><br><span class="line"></span><br><span class="line">def t_ccode_rbrace(t):</span><br><span class="line">    r&#39;\&#125;&#39;</span><br><span class="line">    t.lexer.level -&#x3D;1</span><br><span class="line"></span><br><span class="line">    # If closing brace, return the code fragment</span><br><span class="line">    if t.lexer.level &#x3D;&#x3D; 0:</span><br><span class="line">         t.value &#x3D; t.lexer.lexdata[t.lexer.code_start:t.lexer.lexpos+1]</span><br><span class="line">         t.type &#x3D; &quot;CCODE&quot;</span><br><span class="line">         t.lexer.lineno +&#x3D; t.value.count(&#39;\n&#39;)</span><br><span class="line">         t.lexer.begin(&#39;INITIAL&#39;)           </span><br><span class="line">         return t</span><br><span class="line"></span><br><span class="line"># C or C++ comment (ignore)    </span><br><span class="line">def t_ccode_comment(t):</span><br><span class="line">    r&#39;(&#x2F;\*(.|\n)*?*&#x2F;)|(&#x2F;&#x2F;.*)&#39;</span><br><span class="line">    pass</span><br><span class="line"></span><br><span class="line"># C string</span><br><span class="line">def t_ccode_string(t):</span><br><span class="line">   r&#39;\&quot;([^\\\n]|(\\.))*?\&quot;&#39;</span><br><span class="line"></span><br><span class="line"># C character literal</span><br><span class="line">def t_ccode_char(t):</span><br><span class="line">   r&#39;\&#39;([^\\\n]|(\\.))*?\&#39;&#39;</span><br><span class="line"></span><br><span class="line"># Any sequence of non-whitespace characters (not braces, strings)</span><br><span class="line">def t_ccode_nonspace(t):</span><br><span class="line">   r&#39;[^\s\&#123;\&#125;\&#39;\&quot;]+&#39;</span><br><span class="line"></span><br><span class="line"># Ignored characters (whitespace)</span><br><span class="line">t_ccode_ignore &#x3D; &quot; \t\n&quot;</span><br><span class="line"></span><br><span class="line"># For bad characters, we just skip over it</span><br><span class="line">def t_ccode_error(t):</span><br></pre></td></tr></table></figure>
<pre><code>t.lexer.skip(1)
</code></pre><p>这个例子中，第一个’{‘使得 lexer 记录了起始位置，并且进入新的状态’ccode’。一系列规则用来匹配接下来的输入，这些规则只是丢弃掉标记（不返回值），如果遇到闭合右括号，t_ccode_rbrace 规则收集其中所有的代码（利用先前记录的开始位置），并保存，返回的标记类型为’CCODE’，与此同时，词法分析的状态退回到初始状态。</p>
<h3 id="其他问题"><a href="#其他问题" class="headerlink" title="其他问题"></a>其他问题</h3><ul>
<li>lexer 需要输入的是一个字符串。好在大多数机器都有足够的内存，这很少导致性能的问题。这意味着，lexer 现在还不能用来处理文件流或者 socket 流。这主要是受到 re 模块的限制。</li>
<li>lexer 支持用 Unicode 字符描述标记的匹配规则，也支持输入字串包含 Unicode</li>
<li>如果你想要向re.compile()方法提供 flag，使用 reflags 选项：lex.lex(reflags=re.UNICODE)</li>
<li>由于 lexer 是全部用 Python 写的，性能很大程度上取决于 Python 的 re 模块，即使已经尽可能的高效了。当接收极其大量的输入文件时表现并不尽人意。如果担忧性能，你可以升级到最新的 Python，或者手工创建分析器，或者用 C 语言写 lexer 并做成扩展模块。</li>
</ul>
<p>如果你要创建一个手写的词法分析器并计划用在 yacc.py 中，只需要满足下面的要求：</p>
<ul>
<li>需要提供一个 token() 方法来返回下一个标记，如果没有可用的标记了，则返回 None。</li>
<li>token() 方法必须返回一个 tok 对象，具有 type 和 valu e属性。如果行号需要跟踪的话，标记还需要定义 lineno 属性。<h2 id="语法分析基础"><a href="#语法分析基础" class="headerlink" title="语法分析基础"></a>语法分析基础</h2>‘语法’通常用 BNF 范式来表达。例如，如果想要分析简单的算术表达式，你应该首先写下无二义的文法：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">expression : expression + term</span><br><span class="line">           | expression - term</span><br><span class="line">           | term</span><br><span class="line"></span><br><span class="line">term       : term * factor</span><br><span class="line">           | term &#x2F; factor</span><br><span class="line">           | factor</span><br><span class="line"></span><br><span class="line">factor     : NUMBER</span><br><span class="line">           | ( expression )</span><br></pre></td></tr></table></figure>
<p>在这个文法中，像NUMBER,+,-,*,/的符号被称为终结符，对应原始的输入。类似term，factor等称为非终结符，它们由一系列终结符或其他规则的符号组成，用来指代语法规则。<br>通常使用一种叫语法制导翻译的技术来指定某种语言的语义。在语法制导翻译中，符号及其属性出现在每个语法规则后面的动作中。每当一个语法被识别，动作就能够描述需要做什么。比如，对于上面给定的文法，想要实现一个简单的计算器，应该写成下面这样：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Grammar                             Action</span><br><span class="line">--------------------------------    -------------------------------------------- </span><br><span class="line">expression0 : expression1 + term    expression0.val &#x3D; expression1.val + term.val</span><br><span class="line">            | expression1 - term    expression0.val &#x3D; expression1.val - term.val</span><br><span class="line">            | term                  expression0.val &#x3D; term.val</span><br><span class="line"></span><br><span class="line">term0       : term1 * factor        term0.val &#x3D; term1.val * factor.val</span><br><span class="line">            | term1 &#x2F; factor        term0.val &#x3D; term1.val &#x2F; factor.val</span><br><span class="line">            | factor                term0.val &#x3D; factor.val</span><br><span class="line"></span><br><span class="line">factor      : NUMBER                factor.val &#x3D; int(NUMBER.lexval)</span><br></pre></td></tr></table></figure>
<pre><code>        | ( expression )        factor.val = expression.val
</code></pre><p>一种理解语法指导翻译的好方法是将符号看成对象。与符号相关的值代表了符号的“状态”（比如上面的 val 属性），语义行为用一组操作符号及符号值的函数或者方法来表达。<br>Yacc 用的分析技术是著名的 LR 分析法或者叫移进-归约分析法。LR 分析法是一种自下而上的技术：首先尝试识别右部的语法规则，每当右部得到满足，相应的行为代码将被触发执行，当前右边的语法符号将被替换为左边的语法符号。（归约）</p>
<p>LR 分析法一般这样实现：将下一个符号进栈，然后结合栈顶的符号和后继符号（译者注：下一个将要输入符号），与文法中的某种规则相比较。具体的算法可以在编译器的手册中查到，下面的例子展现了如果通过上面定义的文法，来分析 3 + 5 * ( 10 - 20 ) 这个表达式，$ 用来表示输入结束，action 里面的 Shift 就是进栈动作，简称移进；Reduce 是归约：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Step Symbol Stack           Input Tokens            Action</span><br><span class="line">---- ---------------------  ---------------------   -------------------------------</span><br><span class="line">1                           3 + 5 * ( 10 - 20 )$    Shift 3</span><br><span class="line">2    3                        + 5 * ( 10 - 20 )$    Reduce factor : NUMBER</span><br><span class="line">3    factor                   + 5 * ( 10 - 20 )$    Reduce term   : factor</span><br><span class="line">4    term                     + 5 * ( 10 - 20 )$    Reduce expr : term</span><br><span class="line">5    expr                     + 5 * ( 10 - 20 )$    Shift +</span><br><span class="line">6    expr +                     5 * ( 10 - 20 )$    Shift 5</span><br><span class="line">7    expr + 5                     * ( 10 - 20 )$    Reduce factor : NUMBER</span><br><span class="line">8    expr + factor                * ( 10 - 20 )$    Reduce term   : factor</span><br><span class="line">9    expr + term                  * ( 10 - 20 )$    Shift *</span><br><span class="line">10   expr + term *                  ( 10 - 20 )$    Shift (</span><br><span class="line">11   expr + term * (                  10 - 20 )$    Shift 10</span><br><span class="line">12   expr + term * ( 10                  - 20 )$    Reduce factor : NUMBER</span><br><span class="line">13   expr + term * ( factor              - 20 )$    Reduce term : factor</span><br><span class="line">14   expr + term * ( term                - 20 )$    Reduce expr : term</span><br><span class="line">15   expr + term * ( expr                - 20 )$    Shift -</span><br><span class="line">16   expr + term * ( expr -                20 )$    Shift 20</span><br><span class="line">17   expr + term * ( expr - 20                )$    Reduce factor : NUMBER</span><br><span class="line">18   expr + term * ( expr - factor            )$    Reduce term : factor</span><br><span class="line">19   expr + term * ( expr - term              )$    Reduce expr : expr - term</span><br><span class="line">20   expr + term * ( expr                     )$    Shift )</span><br><span class="line">21   expr + term * ( expr )                    $    Reduce factor : (expr)</span><br><span class="line">22   expr + term * factor                      $    Reduce term : term * factor</span><br><span class="line">23   expr + term                               $    Reduce expr : expr + term</span><br><span class="line">24   expr                                      $    Reduce expr</span><br></pre></td></tr></table></figure>
<p>25                                             $    Success!</p>
<p>在分析表达式的过程中，一个相关的自动状态机和后继符号决定了下一步应该做什么。如果下一个标记看起来是一个有效语法（产生式）的一部分（通过栈上的其他项判断这一点），那么这个标记应该进栈。如果栈顶的项可以组成一个完整的右部语法规则，一般就可以进行“归约”，用产生式左边的符号代替这一组符号。当归约发生时，相应的行为动作就会执行。如果输入标记既不能移进也不能归约的话，就会发生语法错误，分析器必须进行相应的错误恢复。分析器直到栈空并且没有另外的输入标记时，才算成功。 需要注意的是，这是基于一个有限自动机实现的，有限自动器被转化成分析表。分析表的构建比较复杂，超出了本文的讨论范围。不过，这构建过程的微妙细节能够解释为什么在上面的例子中，解析器选择在步骤 9 将标记转移到堆栈中，而不是按照规则 expr : expr + term 做归约。</p>
<h2 id="Yacc简介"><a href="#Yacc简介" class="headerlink" title="Yacc简介"></a>Yacc简介</h2><p>ply.yacc 模块实现了 PLY 的分析功能，‘yacc’是‘Yet Another Compiler Compiler’的缩写并保留了其作为 Unix 工具的名字。</p>
<h3 id="一个例子"><a href="#一个例子" class="headerlink" title="一个例子"></a>一个例子</h3><p>假设你希望实现上面的简单算术表达式的语法分析：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Yacc example</span><br><span class="line"></span><br><span class="line">import ply.yacc as yacc</span><br><span class="line"></span><br><span class="line"># Get the token map from the lexer.  This is required.</span><br><span class="line">from calclex import tokens</span><br><span class="line"></span><br><span class="line">def p_expression_plus(p):</span><br><span class="line">    &#39;expression : expression PLUS term&#39;</span><br><span class="line">    p[0] &#x3D; p[1] + p[3]</span><br><span class="line"></span><br><span class="line">def p_expression_minus(p):</span><br><span class="line">    &#39;expression : expression MINUS term&#39;</span><br><span class="line">    p[0] &#x3D; p[1] - p[3]</span><br><span class="line"></span><br><span class="line">def p_expression_term(p):</span><br><span class="line">    &#39;expression : term&#39;</span><br><span class="line">    p[0] &#x3D; p[1]</span><br><span class="line"></span><br><span class="line">def p_term_times(p):</span><br><span class="line">    &#39;term : term TIMES factor&#39;</span><br><span class="line">    p[0] &#x3D; p[1] * p[3]</span><br><span class="line"></span><br><span class="line">def p_term_div(p):</span><br><span class="line">    &#39;term : term DIVIDE factor&#39;</span><br><span class="line">    p[0] &#x3D; p[1] &#x2F; p[3]</span><br><span class="line"></span><br><span class="line">def p_term_factor(p):</span><br><span class="line">    &#39;term : factor&#39;</span><br><span class="line">    p[0] &#x3D; p[1]</span><br><span class="line"></span><br><span class="line">def p_factor_num(p):</span><br><span class="line">    &#39;factor : NUMBER&#39;</span><br><span class="line">    p[0] &#x3D; p[1]</span><br><span class="line"></span><br><span class="line">def p_factor_expr(p):</span><br><span class="line">    &#39;factor : LPAREN expression RPAREN&#39;</span><br><span class="line">    p[0] &#x3D; p[2]</span><br><span class="line"></span><br><span class="line"># Error rule for syntax errors</span><br><span class="line">def p_error(p):</span><br><span class="line">    print &quot;Syntax error in input!&quot;</span><br><span class="line"></span><br><span class="line"># Build the parser</span><br><span class="line">parser &#x3D; yacc.yacc()</span><br><span class="line"></span><br><span class="line">while True:</span><br><span class="line">   try:</span><br><span class="line">       s &#x3D; raw_input(&#39;calc &gt; &#39;)</span><br><span class="line">   except EOFError:</span><br><span class="line">       break</span><br><span class="line">   if not s: continue</span><br><span class="line">   result &#x3D; parser.parse(s)</span><br></pre></td></tr></table></figure>
<p>   print result</p>
<p>在这个例子中，每个语法规则被定义成一个 Python 的方法，方法的文档字符串描述了相应的上下文无关文法，方法的语句实现了对应规则的语义行为。每个方法接受一个单独的 p 参数，p 是一个包含有当前匹配语法的符号的序列，p[i] 与语法符号的对应关系如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_expression_plus(p):</span><br><span class="line">    &#39;expression : expression PLUS term&#39;</span><br><span class="line">    #   ^            ^        ^    ^</span><br><span class="line">    #  p[0]         p[1]     p[2] p[3]</span><br><span class="line"></span><br><span class="line">    p[0] &#x3D; p[1] + p[3]</span><br></pre></td></tr></table></figure><br>其中，p[i] 的值相当于词法分析模块中对 p.value 属性赋的值，对于非终结符的值，将在归约时由 p[0] 的赋值决定，这里的值可以是任何类型，当然，大多数情况下只是 Python 的简单类型、元组或者类的实例。在这个例子中，我们依赖这样一个事实：NUMBER 标记的值保存的是整型值，所有规则的行为都是得到这些整型值的算术运算结果，并传递结果。<br>在 yacc 中定义的第一个语法规则被默认为起始规则（这个例子中的第一个出现的 expression 规则）。一旦起始规则被分析器归约，而且再无其他输入，分析器终止，最后的值将返回（这个值将是起始规则的p[0]）。注意：也可以通过在 yacc() 中使用 start 关键字参数来指定起始规则。</p>
<p>p_error(p) 规则用于捕获语法错误。详见处理语法错误部分。</p>
<p>为了构建分析器，需要调用 yacc.yacc() 方法。这个方法查看整个当前模块，然后试图根据你提供的文法构建 LR 分析表。由于分析表的得出相对开销较大（尤其包含大量的语法的情况下），分析表被写入当前目录的一个叫 parsetab.py 的文件中。除此之外，会生成一个调试文件 parser.out。在接下来的执行中，yacc 直到发现文法发生变化，才会重新生成分析表和 parsetab.py 文件，否则 yacc 会从 parsetab.py 中加载分析表。注：如果有必要的话这里输出的文件名是可以改的。</p>
<p>如果在你的文法中有任何错误的话，yacc.py 会产生调试信息，而且可能抛出异常。一些可以被检测到的错误如下：</p>
<ul>
<li>方法重复定义（在语法文件中具有相同名字的方法）</li>
<li>二义文法产生的移进-归约和归约-归约冲突</li>
<li>指定了错误的文法</li>
<li>不可终止的递归（规则永远无法终结）</li>
<li>未使用的规则或标记</li>
<li>未定义的规则或标记</li>
</ul>
<p>这个例子的最后部分展示了如何执行由 yacc() 方法创建的分析器。你只需要简单的调用 parse()，并将输入字符串作为参数就能运行分析器。它将运行所有的语法规则，并返回整个分析的结果，这个结果就是在起始规则中赋给 p[0] 的值。</p>
<h3 id="将语法规则合并"><a href="#将语法规则合并" class="headerlink" title="将语法规则合并"></a>将语法规则合并</h3><p>如果语法规则类似的话，可以合并到一个方法中。例如，考虑前面例子中的两个规则：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_expression_plus(p):</span><br><span class="line">    &#39;expression : expression PLUS term&#39;</span><br><span class="line">    p[0] &#x3D; p[1] + p[3]</span><br><span class="line"></span><br><span class="line">def p_expression_minus(t):</span><br><span class="line">    &#39;expression : expression MINUS term&#39;</span><br><span class="line">    p[0] &#x3D; p[1] - p[3]</span><br></pre></td></tr></table></figure>
<p>比起写两个方法，你可以像下面这样写在一个方法里面：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_expression(p):</span><br><span class="line">    &#39;&#39;&#39;expression : expression PLUS term</span><br><span class="line">                  | expression MINUS term&#39;&#39;&#39;</span><br><span class="line">    if p[2] &#x3D;&#x3D; &#39;+&#39;:</span><br><span class="line">        p[0] &#x3D; p[1] + p[3]</span><br><span class="line">    elif p[2] &#x3D;&#x3D; &#39;-&#39;:</span><br><span class="line">        p[0] &#x3D; p[1] - p[3]</span><br></pre></td></tr></table></figure><br>总之，方法的文档字符串可以包含多个语法规则。所以，像这样写也是合法的（尽管可能会引起困惑）：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_binary_operators(p):</span><br><span class="line">    &#39;&#39;&#39;expression : expression PLUS term</span><br><span class="line">                  | expression MINUS term</span><br><span class="line">       term       : term TIMES factor</span><br><span class="line">                  | term DIVIDE factor&#39;&#39;&#39;</span><br><span class="line">    if p[2] &#x3D;&#x3D; &#39;+&#39;:</span><br><span class="line">        p[0] &#x3D; p[1] + p[3]</span><br><span class="line">    elif p[2] &#x3D;&#x3D; &#39;-&#39;:</span><br><span class="line">        p[0] &#x3D; p[1] - p[3]</span><br><span class="line">    elif p[2] &#x3D;&#x3D; &#39;*&#39;:</span><br><span class="line">        p[0] &#x3D; p[1] * p[3]</span><br><span class="line">    elif p[2] &#x3D;&#x3D; &#39;&#x2F;&#39;:</span><br><span class="line">        p[0] &#x3D; p[1] &#x2F; p[3]</span><br></pre></td></tr></table></figure><br>如果所有的规则都有相似的结构，那么将语法规则合并才是个不错的注意（比如，产生式的项数相同）。不然，语义动作可能会变得复杂。不过，简单情况下，可以使用len()方法区分，比如：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_expressions(p):</span><br><span class="line">    &#39;&#39;&#39;expression : expression MINUS expression</span><br><span class="line">                  | MINUS expression&#39;&#39;&#39;</span><br><span class="line">    if (len(p) &#x3D;&#x3D; 4):</span><br><span class="line">        p[0] &#x3D; p[1] - p[3]</span><br><span class="line">    elif (len(p) &#x3D;&#x3D; 3):</span><br><span class="line">        p[0] &#x3D; -p[2]</span><br></pre></td></tr></table></figure><br>如果考虑解析的性能，你应该避免像这些例子一样在一个语法规则里面用很多条件来处理。因为，每次检查当前究竟匹配的是哪个语法规则的时候，实际上重复做了分析器已经做过的事（分析器已经准确的知道哪个规则被匹配了）。为每个规则定义单独的方法，可以消除这点开销。</p>
<h3 id="字面字符-1"><a href="#字面字符-1" class="headerlink" title="字面字符"></a>字面字符</h3><p>如果愿意，可以在语法规则里面使用单个的字面字符，例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_binary_operators(p):</span><br><span class="line">    &#39;&#39;&#39;expression : expression &#39;+&#39; term</span><br><span class="line">                  | expression &#39;-&#39; term</span><br><span class="line">       term       : term &#39;*&#39; factor</span><br><span class="line">                  | term &#39;&#x2F;&#39; factor&#39;&#39;&#39;</span><br><span class="line">    if p[2] &#x3D;&#x3D; &#39;+&#39;:</span><br><span class="line">        p[0] &#x3D; p[1] + p[3]</span><br><span class="line">    elif p[2] &#x3D;&#x3D; &#39;-&#39;:</span><br><span class="line">        p[0] &#x3D; p[1] - p[3]</span><br><span class="line">    elif p[2] &#x3D;&#x3D; &#39;*&#39;:</span><br><span class="line">        p[0] &#x3D; p[1] * p[3]</span><br><span class="line">    elif p[2] &#x3D;&#x3D; &#39;&#x2F;&#39;:</span><br><span class="line">        p[0] &#x3D; p[1] &#x2F; p[3]</span><br></pre></td></tr></table></figure>
<p>字符必须像’+’那样使用单引号。除此之外，需要将用到的字符定义单独定义在 lex 文件的literals列表里：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Literals.  Should be placed in module given to lex()</span><br><span class="line">literals &#x3D; [&#39;+&#39;,&#39;-&#39;,&#39;*&#39;,&#39;&#x2F;&#39; ]</span><br></pre></td></tr></table></figure><br>字面的字符只能是单个字符。因此，像’&lt;=’或者’==’都是不合法的，只能使用一般的词法规则（例如 t_EQ = r’==’)。</p>
<h3 id="空产生式"><a href="#空产生式" class="headerlink" title="空产生式"></a>空产生式</h3><p>yacc.py 可以处理空产生式，像下面这样做：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_empty(p):</span><br><span class="line">    &#39;empty :&#39;</span><br><span class="line">    pass</span><br></pre></td></tr></table></figure>
<p>现在可以使用空匹配，只要将’empty’当成一个符号使用：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_optitem(p):</span><br><span class="line">    &#39;optitem : item&#39;</span><br><span class="line">    &#39;        | empty&#39;</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><br>注意：你可以将产生式保持’空’，来表示空匹配。然而，我发现用一个’empty’规则并用其来替代’空’，更容易表达意图，并有较好的可读性。</p>
<h3 id="改变起始符号"><a href="#改变起始符号" class="headerlink" title="改变起始符号"></a>改变起始符号</h3><p>默认情况下，在 yacc 中的第一条规则是起始语法规则（顶层规则）。可以用 start 标识来改变这种行为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">start &#x3D; &#39;foo&#39;</span><br><span class="line">def p_bar(p):</span><br><span class="line">    &#39;bar : A B&#39;</span><br><span class="line"></span><br><span class="line"># This is the starting rule due to the start specifier above</span><br><span class="line">def p_foo(p):</span><br><span class="line">    &#39;foo : bar X&#39;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>用 start 标识有助于在调试的时候将大型的语法规则分成小部分来分析。也可把 start 符号作为yacc的参数：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yacc.yacc(start&#x3D;&#39;foo&#39;)</span><br></pre></td></tr></table></figure></p>
<h3 id="处理二义文法"><a href="#处理二义文法" class="headerlink" title="处理二义文法"></a>处理二义文法</h3><p>上面例子中，对表达式的文法描述用一种特别的形式规避了二义文法。然而，在很多情况下，这样的特殊文法很难写，或者很别扭。一个更为自然和舒服的语法表达应该是这样的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">expression : expression PLUS expression</span><br><span class="line">           | expression MINUS expression</span><br><span class="line">           | expression TIMES expression</span><br><span class="line">           | expression DIVIDE expression</span><br><span class="line">           | LPAREN expression RPAREN</span><br><span class="line">           | NUMBER</span><br></pre></td></tr></table></figure>
<p>不幸的是，这样的文法是存在二义性的。举个例子，如果你要解析字符串”3 <em> 4 + 5”，操作符如何分组并没有指明，究竟是表示”(3 </em> 4) + 5”还是”3 <em> (4 + 5)”呢？<br>如果在 yacc.py 中存在二义文法，会输出”移进归约冲突”或者”归约归约冲突”。在分析器无法确定是将下一个符号移进栈还是将当前栈中的符号归约时会产生移进归约冲突。例如，对于”3 </em> 4 + 5”，分析器内部栈是这样工作的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Step Symbol Stack           Input Tokens            Action</span><br><span class="line">---- ---------------------  ---------------------   -------------------------------</span><br><span class="line">1    $                                3 * 4 + 5$    Shift 3</span><br><span class="line">2    $ 3                                * 4 + 5$    Reduce : expression : NUMBER</span><br><span class="line">3    $ expr                             * 4 + 5$    Shift *</span><br><span class="line">4    $ expr *                             4 + 5$    Shift 4</span><br><span class="line">5    $ expr * 4                             + 5$    Reduce: expression : NUMBER</span><br><span class="line">6    $ expr * expr                          + 5$    SHIFT&#x2F;REDUCE CONFLICT ????</span><br></pre></td></tr></table></figure>
<p>两种选择对于上面的上下文无关文法而言都是合法的。<br>默认情况下，所有的移进归约冲突会倾向于使用移进来处理。因此，对于上面的例子，分析器总是会将’+’进栈，而不是做归约。虽然在很多情况下，这个策略是合适的（像”if-then”和”if-then-else”），但这对于算术表达式是不够的。事实上，对于上面的例子，将’+’进栈是完全错误的，应当先将expr * expr归约，因为乘法的优先级要高于加法。</p>
<p>为了解决二义文法，尤其是对表达式文法，yacc.py 允许为标记单独指定优先级和结合性。需要像下面这样增加一个 precedence 变量：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">precedence &#x3D; (</span><br><span class="line">    (&#39;left&#39;, &#39;PLUS&#39;, &#39;MINUS&#39;),</span><br><span class="line">    (&#39;left&#39;, &#39;TIMES&#39;, &#39;DIVIDE&#39;),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>这样的定义说明 PLUS/MINUS 标记具有相同的优先级和左结合性，TIMES/DIVIDE 具有相同的优先级和左结合性。在 precedence 声明中，标记的优先级从低到高。因此，这个声明表明 TIMES/DIVIDE（他们较晚加入 precedence）的优先级高于 PLUS/MINUS。<br>由于为标记添加了数字表示的优先级和结合性的属性，所以，对于上面的例子，将会得到：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PLUS      : level &#x3D; 1,  assoc &#x3D; &#39;left&#39;</span><br><span class="line">MINUS     : level &#x3D; 1,  assoc &#x3D; &#39;left&#39;</span><br><span class="line">TIMES     : level &#x3D; 2,  assoc &#x3D; &#39;left&#39;</span><br><span class="line">DIVIDE    : level &#x3D; 2,  assoc &#x3D; &#39;left&#39;</span><br></pre></td></tr></table></figure>
<p>随后这些值被附加到语法规则的优先级和结合性属性上，这些值由最右边的终结符的优先级和结合性决定：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">expression : expression PLUS expression                 # level &#x3D; 1, left</span><br><span class="line">           | expression MINUS expression                # level &#x3D; 1, left</span><br><span class="line">           | expression TIMES expression                # level &#x3D; 2, left</span><br><span class="line">           | expression DIVIDE expression               # level &#x3D; 2, left</span><br><span class="line">           | LPAREN expression RPAREN                   # level &#x3D; None (not specified)</span><br></pre></td></tr></table></figure><br>           | NUMBER                                     # level = None (not specified)</p>
<p>当出现移进归约冲突时，分析器生成器根据下面的规则解决二义文法：</p>
<ul>
<li>如果当前的标记的优先级高于栈顶规则的优先级，移进当前标记</li>
<li>如果栈顶规则的优先级更高，进行归约</li>
<li>如果当前的标记与栈顶规则的优先级相同，如果标记是左结合的，则归约，否则，如果是右结合的则移进</li>
<li>如果没有优先级可以参考，默认对于移进归约冲突执行移进</li>
</ul>
<p>比如，当解析到”expression PLUS expression”这个语法时，下一个标记是 TIMES，此时将执行移进，因为 TIMES 具有比 PLUS 更高的优先级；当解析到”expression TIMES expression”，下一个标记是 PLUS，此时将执行归约，因为 PLUS 的优先级低于 TIMES。</p>
<p>如果在使用前三种技术解决已经归约冲突后，yacc.py 将不会报告语法中的冲突或者错误（不过，会在 parser.out 这个调试文件中输出一些信息）。</p>
<p>使用 precedence 指定优先级的技术会带来一个问题，有时运算符的优先级需要基于上下文。例如，考虑”3 + 4 * -5”中的一元的’-‘。数学上讲，一元运算符应当拥有较高的优先级。然而，在我们的 precedence 定义中，MINUS 的优先级却低于 TIMES。为了解决这个问题，precedene 规则中可以包含”虚拟标记”：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">precedence &#x3D; (</span><br><span class="line">    (&#39;left&#39;, &#39;PLUS&#39;, &#39;MINUS&#39;),</span><br><span class="line">    (&#39;left&#39;, &#39;TIMES&#39;, &#39;DIVIDE&#39;),</span><br><span class="line">    (&#39;right&#39;, &#39;UMINUS&#39;),            # Unary minus operator</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>在语法文件中，我们可以这么表示一元算符：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_expr_uminus(p):</span><br><span class="line">    &#39;expression : MINUS expression %prec UMINUS&#39;</span><br><span class="line">    p[0] &#x3D; -p[2]</span><br></pre></td></tr></table></figure><br>在这个例子中，%prec UMINUS 覆盖了默认的优先级（MINUS 的优先级），将 UMINUS 指代的优先级应用在该语法规则上。<br>起初，UMINUS 标记的例子会让人感到困惑。UMINUS 既不是输入的标记也不是语法规则，你应当将其看成 precedence 表中的特殊的占位符。当你使用 %prec 宏时，你是在告诉 yacc，你希望表达式使用这个占位符所表示的优先级，而不是正常的优先级。</p>
<p>还可以在 precedence 表中指定”非关联”。这表明你不希望链式运算符。比如，假如你希望支持比较运算符’&lt;’和’&gt;’，但是你不希望支持 a &lt; b &lt; c，只要简单指定规则如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">precedence &#x3D; (</span><br><span class="line">    (&#39;nonassoc&#39;, &#39;LESSTHAN&#39;, &#39;GREATERTHAN&#39;),  # Nonassociative operators</span><br><span class="line">    (&#39;left&#39;, &#39;PLUS&#39;, &#39;MINUS&#39;),</span><br><span class="line">    (&#39;left&#39;, &#39;TIMES&#39;, &#39;DIVIDE&#39;),</span><br><span class="line">    (&#39;right&#39;, &#39;UMINUS&#39;),            # Unary minus operator</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>此时，当输入形如 a &lt; b &lt; c 时，将产生语法错误，却不影响形如 a &lt; b 的表达式。<br>对于给定的符号集，存在多种语法规则可以匹配时会产生归约/归约冲突。这样的冲突往往很严重，而且总是通过匹配最早出现的语法规则来解决。归约/归约冲突几乎总是相同的符号集合具有不同的规则可以匹配，而在这一点上无法抉择，比如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">assignment :  ID EQUALS NUMBER</span><br><span class="line">           |  ID EQUALS expression</span><br><span class="line">           </span><br><span class="line">expression : expression PLUS expression</span><br><span class="line">           | expression MINUS expression</span><br><span class="line">           | expression TIMES expression</span><br><span class="line">           | expression DIVIDE expression</span><br><span class="line">           | LPAREN expression RPAREN</span><br><span class="line">           | NUMBER</span><br></pre></td></tr></table></figure>
<p>这个例子中，对于下面这两条规则将产生归约/归约冲突：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">assignment  : ID EQUALS NUMBER</span><br><span class="line">expression  : NUMBER</span><br></pre></td></tr></table></figure><br>比如，对于”a = 5”，分析器不知道应当按照 assignment : ID EQUALS NUMBER 归约，还是先将 5 归约成 expression，再归约成 assignment : ID EQUALS expression。<br>应当指出的是，只是简单的查看语法规则是很难减少归约/归约冲突。如果出现归约/归约冲突，yacc()会帮助打印出警告信息：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">WARNING: 1 reduce&#x2F;reduce conflict</span><br><span class="line">WARNING: reduce&#x2F;reduce conflict in state 15 resolved using rule (assignment -&gt; ID EQUALS NUMBER)</span><br><span class="line">WARNING: rejected rule (expression -&gt; NUMBER)</span><br></pre></td></tr></table></figure>
<p>上面的信息标识出了冲突的两条规则，但是，并无法指出究竟在什么情况下会出现这样的状态。想要发现问题，你可能需要结合语法规则和parser.out调试文件的内容。</p>
<h3 id="parser-out调试文件"><a href="#parser-out调试文件" class="headerlink" title="parser.out调试文件"></a>parser.out调试文件</h3><p>使用 LR 分析算法跟踪移进/归约冲突和归约/归约冲突是件乐在其中的事。为了辅助调试，yacc.py 在生成分析表时会创建出一个调试文件叫 parser.out：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Unused terminals:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Grammar</span><br><span class="line"></span><br><span class="line">Rule 1     expression -&gt; expression PLUS expression</span><br><span class="line">Rule 2     expression -&gt; expression MINUS expression</span><br><span class="line">Rule 3     expression -&gt; expression TIMES expression</span><br><span class="line">Rule 4     expression -&gt; expression DIVIDE expression</span><br><span class="line">Rule 5     expression -&gt; NUMBER</span><br><span class="line">Rule 6     expression -&gt; LPAREN expression RPAREN</span><br><span class="line"></span><br><span class="line">Terminals, with rules where they appear</span><br><span class="line"></span><br><span class="line">TIMES                : 3</span><br><span class="line">error                : </span><br><span class="line">MINUS                : 2</span><br><span class="line">RPAREN               : 6</span><br><span class="line">LPAREN               : 6</span><br><span class="line">DIVIDE               : 4</span><br><span class="line">PLUS                 : 1</span><br><span class="line">NUMBER               : 5</span><br><span class="line"></span><br><span class="line">Nonterminals, with rules where they appear</span><br><span class="line"></span><br><span class="line">expression           : 1 1 2 2 3 3 4 4 6 0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Parsing method: LALR</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">state 0</span><br><span class="line"></span><br><span class="line">    S&#39; -&gt; . expression</span><br><span class="line">    expression -&gt; . expression PLUS expression</span><br><span class="line">    expression -&gt; . expression MINUS expression</span><br><span class="line">    expression -&gt; . expression TIMES expression</span><br><span class="line">    expression -&gt; . expression DIVIDE expression</span><br><span class="line">    expression -&gt; . NUMBER</span><br><span class="line">    expression -&gt; . LPAREN expression RPAREN</span><br><span class="line"></span><br><span class="line">    NUMBER          shift and go to state 3</span><br><span class="line">    LPAREN          shift and go to state 2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">state 1</span><br><span class="line"></span><br><span class="line">    S&#39; -&gt; expression .</span><br><span class="line">    expression -&gt; expression . PLUS expression</span><br><span class="line">    expression -&gt; expression . MINUS expression</span><br><span class="line">    expression -&gt; expression . TIMES expression</span><br><span class="line">    expression -&gt; expression . DIVIDE expression</span><br><span class="line"></span><br><span class="line">    PLUS            shift and go to state 6</span><br><span class="line">    MINUS           shift and go to state 5</span><br><span class="line">    TIMES           shift and go to state 4</span><br><span class="line">    DIVIDE          shift and go to state 7</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">state 2</span><br><span class="line"></span><br><span class="line">    expression -&gt; LPAREN . expression RPAREN</span><br><span class="line">    expression -&gt; . expression PLUS expression</span><br><span class="line">    expression -&gt; . expression MINUS expression</span><br><span class="line">    expression -&gt; . expression TIMES expression</span><br><span class="line">    expression -&gt; . expression DIVIDE expression</span><br><span class="line">    expression -&gt; . NUMBER</span><br><span class="line">    expression -&gt; . LPAREN expression RPAREN</span><br><span class="line"></span><br><span class="line">    NUMBER          shift and go to state 3</span><br><span class="line">    LPAREN          shift and go to state 2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">state 3</span><br><span class="line"></span><br><span class="line">    expression -&gt; NUMBER .</span><br><span class="line"></span><br><span class="line">    $               reduce using rule 5</span><br><span class="line">    PLUS            reduce using rule 5</span><br><span class="line">    MINUS           reduce using rule 5</span><br><span class="line">    TIMES           reduce using rule 5</span><br><span class="line">    DIVIDE          reduce using rule 5</span><br><span class="line">    RPAREN          reduce using rule 5</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">state 4</span><br><span class="line"></span><br><span class="line">    expression -&gt; expression TIMES . expression</span><br><span class="line">    expression -&gt; . expression PLUS expression</span><br><span class="line">    expression -&gt; . expression MINUS expression</span><br><span class="line">    expression -&gt; . expression TIMES expression</span><br><span class="line">    expression -&gt; . expression DIVIDE expression</span><br><span class="line">    expression -&gt; . NUMBER</span><br><span class="line">    expression -&gt; . LPAREN expression RPAREN</span><br><span class="line"></span><br><span class="line">    NUMBER          shift and go to state 3</span><br><span class="line">    LPAREN          shift and go to state 2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">state 5</span><br><span class="line"></span><br><span class="line">    expression -&gt; expression MINUS . expression</span><br><span class="line">    expression -&gt; . expression PLUS expression</span><br><span class="line">    expression -&gt; . expression MINUS expression</span><br><span class="line">    expression -&gt; . expression TIMES expression</span><br><span class="line">    expression -&gt; . expression DIVIDE expression</span><br><span class="line">    expression -&gt; . NUMBER</span><br><span class="line">    expression -&gt; . LPAREN expression RPAREN</span><br><span class="line"></span><br><span class="line">    NUMBER          shift and go to state 3</span><br><span class="line">    LPAREN          shift and go to state 2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">state 6</span><br><span class="line"></span><br><span class="line">    expression -&gt; expression PLUS . expression</span><br><span class="line">    expression -&gt; . expression PLUS expression</span><br><span class="line">    expression -&gt; . expression MINUS expression</span><br><span class="line">    expression -&gt; . expression TIMES expression</span><br><span class="line">    expression -&gt; . expression DIVIDE expression</span><br><span class="line">    expression -&gt; . NUMBER</span><br><span class="line">    expression -&gt; . LPAREN expression RPAREN</span><br><span class="line"></span><br><span class="line">    NUMBER          shift and go to state 3</span><br><span class="line">    LPAREN          shift and go to state 2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">state 7</span><br><span class="line"></span><br><span class="line">    expression -&gt; expression DIVIDE . expression</span><br><span class="line">    expression -&gt; . expression PLUS expression</span><br><span class="line">    expression -&gt; . expression MINUS expression</span><br><span class="line">    expression -&gt; . expression TIMES expression</span><br><span class="line">    expression -&gt; . expression DIVIDE expression</span><br><span class="line">    expression -&gt; . NUMBER</span><br><span class="line">    expression -&gt; . LPAREN expression RPAREN</span><br><span class="line"></span><br><span class="line">    NUMBER          shift and go to state 3</span><br><span class="line">    LPAREN          shift and go to state 2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">state 8</span><br><span class="line"></span><br><span class="line">    expression -&gt; LPAREN expression . RPAREN</span><br><span class="line">    expression -&gt; expression . PLUS expression</span><br><span class="line">    expression -&gt; expression . MINUS expression</span><br><span class="line">    expression -&gt; expression . TIMES expression</span><br><span class="line">    expression -&gt; expression . DIVIDE expression</span><br><span class="line"></span><br><span class="line">    RPAREN          shift and go to state 13</span><br><span class="line">    PLUS            shift and go to state 6</span><br><span class="line">    MINUS           shift and go to state 5</span><br><span class="line">    TIMES           shift and go to state 4</span><br><span class="line">    DIVIDE          shift and go to state 7</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">state 9</span><br><span class="line"></span><br><span class="line">    expression -&gt; expression TIMES expression .</span><br><span class="line">    expression -&gt; expression . PLUS expression</span><br><span class="line">    expression -&gt; expression . MINUS expression</span><br><span class="line">    expression -&gt; expression . TIMES expression</span><br><span class="line">    expression -&gt; expression . DIVIDE expression</span><br><span class="line"></span><br><span class="line">    $               reduce using rule 3</span><br><span class="line">    PLUS            reduce using rule 3</span><br><span class="line">    MINUS           reduce using rule 3</span><br><span class="line">    TIMES           reduce using rule 3</span><br><span class="line">    DIVIDE          reduce using rule 3</span><br><span class="line">    RPAREN          reduce using rule 3</span><br><span class="line"></span><br><span class="line">  ! PLUS            [ shift and go to state 6 ]</span><br><span class="line">  ! MINUS           [ shift and go to state 5 ]</span><br><span class="line">  ! TIMES           [ shift and go to state 4 ]</span><br><span class="line">  ! DIVIDE          [ shift and go to state 7 ]</span><br><span class="line"></span><br><span class="line">state 10</span><br><span class="line"></span><br><span class="line">    expression -&gt; expression MINUS expression .</span><br><span class="line">    expression -&gt; expression . PLUS expression</span><br><span class="line">    expression -&gt; expression . MINUS expression</span><br><span class="line">    expression -&gt; expression . TIMES expression</span><br><span class="line">    expression -&gt; expression . DIVIDE expression</span><br><span class="line"></span><br><span class="line">    $               reduce using rule 2</span><br><span class="line">    PLUS            reduce using rule 2</span><br><span class="line">    MINUS           reduce using rule 2</span><br><span class="line">    RPAREN          reduce using rule 2</span><br><span class="line">    TIMES           shift and go to state 4</span><br><span class="line">    DIVIDE          shift and go to state 7</span><br><span class="line"></span><br><span class="line">  ! TIMES           [ reduce using rule 2 ]</span><br><span class="line">  ! DIVIDE          [ reduce using rule 2 ]</span><br><span class="line">  ! PLUS            [ shift and go to state 6 ]</span><br><span class="line">  ! MINUS           [ shift and go to state 5 ]</span><br><span class="line"></span><br><span class="line">state 11</span><br><span class="line"></span><br><span class="line">    expression -&gt; expression PLUS expression .</span><br><span class="line">    expression -&gt; expression . PLUS expression</span><br><span class="line">    expression -&gt; expression . MINUS expression</span><br><span class="line">    expression -&gt; expression . TIMES expression</span><br><span class="line">    expression -&gt; expression . DIVIDE expression</span><br><span class="line"></span><br><span class="line">    $               reduce using rule 1</span><br><span class="line">    PLUS            reduce using rule 1</span><br><span class="line">    MINUS           reduce using rule 1</span><br><span class="line">    RPAREN          reduce using rule 1</span><br><span class="line">    TIMES           shift and go to state 4</span><br><span class="line">    DIVIDE          shift and go to state 7</span><br><span class="line"></span><br><span class="line">  ! TIMES           [ reduce using rule 1 ]</span><br><span class="line">  ! DIVIDE          [ reduce using rule 1 ]</span><br><span class="line">  ! PLUS            [ shift and go to state 6 ]</span><br><span class="line">  ! MINUS           [ shift and go to state 5 ]</span><br><span class="line"></span><br><span class="line">state 12</span><br><span class="line"></span><br><span class="line">    expression -&gt; expression DIVIDE expression .</span><br><span class="line">    expression -&gt; expression . PLUS expression</span><br><span class="line">    expression -&gt; expression . MINUS expression</span><br><span class="line">    expression -&gt; expression . TIMES expression</span><br><span class="line">    expression -&gt; expression . DIVIDE expression</span><br><span class="line"></span><br><span class="line">    $               reduce using rule 4</span><br><span class="line">    PLUS            reduce using rule 4</span><br><span class="line">    MINUS           reduce using rule 4</span><br><span class="line">    TIMES           reduce using rule 4</span><br><span class="line">    DIVIDE          reduce using rule 4</span><br><span class="line">    RPAREN          reduce using rule 4</span><br><span class="line"></span><br><span class="line">  ! PLUS            [ shift and go to state 6 ]</span><br><span class="line">  ! MINUS           [ shift and go to state 5 ]</span><br><span class="line">  ! TIMES           [ shift and go to state 4 ]</span><br><span class="line">  ! DIVIDE          [ shift and go to state 7 ]</span><br><span class="line"></span><br><span class="line">state 13</span><br><span class="line"></span><br><span class="line">    expression -&gt; LPAREN expression RPAREN .</span><br><span class="line"></span><br><span class="line">    $               reduce using rule 6</span><br><span class="line">    PLUS            reduce using rule 6</span><br><span class="line">    MINUS           reduce using rule 6</span><br><span class="line">    TIMES           reduce using rule 6</span><br><span class="line">    DIVIDE          reduce using rule 6</span><br></pre></td></tr></table></figure>
<pre><code>RPAREN          reduce using rule 6
</code></pre><p>文件中出现的不同状态，代表了有效输入标记的所有可能的组合，这是依据文法规则得到的。当得到输入标记时，分析器将构造一个栈，并找到匹配的规则。每个状态跟踪了当前输入进行到语法规则中的哪个位置，在每个规则中，’.’表示当前分析到规则的哪个位置，而且，对于在当前状态下，输入的每个有效标记导致的动作也被罗列出来。当出现移进/归约或归约/归约冲突时，被忽略的规则前面会添加!，就像这样：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">! TIMES           [ reduce using rule 2 ]</span><br><span class="line">  ! DIVIDE          [ reduce using rule 2 ]</span><br><span class="line">  ! PLUS            [ shift and go to state 6 ]</span><br><span class="line">  ! MINUS           [ shift and go to state 5 ]</span><br></pre></td></tr></table></figure>
<p>通过查看这些规则并结合一些实例，通常能够找到大部分冲突的根源。应该强调的是，不是所有的移进归约冲突都是不好的，想要确定解决方法是否正确，唯一的办法就是查看 parser.out。</p>
<h3 id="处理语法错误"><a href="#处理语法错误" class="headerlink" title="处理语法错误"></a>处理语法错误</h3><p>如果你创建的分析器用于产品，处理语法错误是很重要的。一般而言，你不希望分析器在遇到错误的时候就抛出异常并终止，相反，你需要它报告错误，尽可能的恢复并继续分析，一次性的将输入中所有的错误报告给用户。这是一些已知语言编译器的标准行为，例如 C,C++,Java。在 PLY 中，在语法分析过程中出现错误，错误会被立即检测到（分析器不会继续读取源文件中错误点后面的标记）。然而，这时，分析器会进入恢复模式，这个模式能够用来尝试继续向下分析。LR 分析器的错误恢复是个理论与技巧兼备的问题，yacc.py 提供的错误机制与 Unix 下的 yacc 类似，所以你可以从诸如 O’Reilly 出版的《Lex and yacc》的书中找到更多的细节。</p>
<p>当错误发生时，yacc.py 按照如下步骤进行：</p>
<ul>
<li>第一次错误产生时，用户定义的 p_error()方法会被调用，出错的标记会作为参数传入；如果错误是因为到达文件结尾造成的，传入的参数将为 None。随后，分析器进入到“错误恢复”模式，该模式下不会在产生p_error()调用，直到它成功的移进 3 个标记，然后回归到正常模式。</li>
<li>如果在 p_error() 中没有指定恢复动作的话，这个导致错误的标记会被替换成一个特殊的 error 标记。</li>
<li>如果导致错误的标记已经是 error 的话，原先的栈顶的标记将被移除。</li>
<li>如果整个分析栈被放弃，分析器会进入重置状态，并从他的初始状态开始分析。</li>
<li>如果此时的语法规则接受 error 标记，error 标记会移进栈。</li>
<li>如果当前栈顶是 error 标记，之后的标记将被忽略，直到有标记能够导致 error 的归约。<ul>
<li>根据 error 规则恢复和再同步</li>
</ul>
</li>
</ul>
<p>最佳的处理语法错误的做法是在语法规则中包含 error 标记。例如，假设你的语言有一个关于 print 的语句的语法规则：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_statement_print(p):</span><br><span class="line">     &#39;statement : PRINT expr SEMI&#39;</span><br><span class="line">     ...</span><br></pre></td></tr></table></figure>
<p>为了处理可能的错误表达式，你可以添加一条额外的语法规则：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_statement_print_error(p):</span><br><span class="line">     &#39;statement : PRINT error SEMI&#39;</span><br><span class="line">     print &quot;Syntax error in print statement. Bad expression&quot;</span><br></pre></td></tr></table></figure><br>这样（expr 错误时），error 标记会匹配任意多个分号之前的标记（分号是SEMI指代的字符）。一旦找到分号，规则将被匹配，这样 error 标记就被归约了。<br>这种类型的恢复有时称为”分析器再同步”。error 标记扮演了表示所有错误标记的通配符的角色，而紧随其后的标记扮演了同步标记的角色。</p>
<p>重要的一个说明是，通常 error 不会作为语法规则的最后一个标记，像这样：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_statement_print_error(p):</span><br><span class="line">    &#39;statement : PRINT error&#39;</span><br><span class="line">    print &quot;Syntax error in print statement. Bad expression&quot;</span><br></pre></td></tr></table></figure>
<p>这是因为，第一个导致错误的标记会使得该规则立刻归约，进而使得在后面还有错误标记的情况下，恢复变得困难。</p>
<ul>
<li>悲观恢复模式</li>
</ul>
<p>另一个错误恢复方法是采用“悲观模式”：该模式下，开始放弃剩余的标记，直到能够达到一个合适的恢复机会。</p>
<p>悲观恢复模式都是在 p_error() 方法中做到的。例如，这个方法在开始丢弃标记后，直到找到闭合的’}’，才重置分析器到初始化状态：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_error(p):</span><br><span class="line">    print &quot;Whoa. You are seriously hosed.&quot;</span><br><span class="line">    # Read ahead looking for a closing &#39;&#125;&#39;</span><br><span class="line">    while 1:</span><br><span class="line">        tok &#x3D; yacc.token()             # Get the next token</span><br><span class="line">        if not tok or tok.type &#x3D;&#x3D; &#39;RBRACE&#39;: break</span><br><span class="line">    yacc.restart()</span><br></pre></td></tr></table></figure>
<p>下面这个方法简单的抛弃错误的标记，并告知分析器错误被接受了：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_error(p):</span><br><span class="line">    print &quot;Syntax error at token&quot;, p.type</span><br><span class="line">    # Just discard the token and tell the parser it&#39;s okay.</span><br><span class="line">    yacc.errok()</span><br></pre></td></tr></table></figure><br>在p_error()方法中，有三个可用的方法来控制分析器的行为：</p>
<ul>
<li>yacc.errok() 这个方法将分析器从恢复模式切换回正常模式。这会使得不会产生 error 标记，并重置内部的 error 计数器，而且下一个语法错误会再次产生 p_error() 调用</li>
<li>yacc.token() 这个方法用于得到下一个标记</li>
<li>yacc.restart() 这个方法抛弃当前整个分析栈，并重置分析器为起始状态</li>
</ul>
<p>注意：这三个方法只能在p_error()中使用，不能用在其他任何地方。</p>
<p>p_error()方法也可以返回标记，这样能够控制将哪个标记作为下一个标记返回给分析器。这对于需要同步一些特殊标记的时候有用，比如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_error(p):</span><br><span class="line">    # Read ahead looking for a terminating &quot;;&quot;</span><br><span class="line">    while 1:</span><br><span class="line">        tok &#x3D; yacc.token()             # Get the next token</span><br><span class="line">        if not tok or tok.type &#x3D;&#x3D; &#39;SEMI&#39;: break</span><br><span class="line">    yacc.errok()</span><br><span class="line"></span><br><span class="line">    # Return SEMI to the parser as the next lookahead token</span><br><span class="line">    return tok</span><br></pre></td></tr></table></figure>
<ul>
<li>从产生式中抛出错误</li>
</ul>
<p>如果有需要的话，产生式规则可以主动的使分析器进入恢复模式。这是通过抛出SyntaxError异常做到的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_production(p):</span><br><span class="line">    &#39;production : some production ...&#39;</span><br><span class="line">    raise SyntaxError</span><br></pre></td></tr></table></figure>
<p>raise SyntaxError 错误的效果就如同当前的标记是错误标记一样。因此，当你这么做的话，最后一个标记将被弹出栈，当前的下一个标记将是 error 标记，分析器进入恢复模式，试图归约满足 error 标记的规则。此后的步骤与检测到语法错误的情况是完全一样的，p_error() 也会被调用。<br>手动设置错误有个重要的方面，就是 p_error() 方法在这种情况下不会调用。如果你希望记录错误，确保在抛出 SyntaxError 错误的产生式中实现。</p>
<p>注：这个功能是为了模仿 yacc 中的YYERROR宏的行为</p>
<ul>
<li>错误恢复总结</li>
</ul>
<p>对于通常的语言，使用 error 规则和再同步标记可能是最合理的手段。这是因为你可以将语法设计成在一个相对容易恢复和继续分析的点捕获错误。悲观恢复模式只在一些十分特殊的应用中有用，这些应用往往需要丢弃掉大量输入，再寻找合理的同步点。</p>
<h3 id="行号和位置的跟踪"><a href="#行号和位置的跟踪" class="headerlink" title="行号和位置的跟踪"></a>行号和位置的跟踪</h3><p>位置跟踪通常是个设计编译器时的技巧性玩意儿。默认情况下，PLY 跟踪所有标记的行号和位置，这些信息可以这样得到：</p>
<ul>
<li>p.lineno(num) 返回第 num 个符号的行号</li>
<li>p.lexpos(num) 返回第 num 个符号的词法位置偏移<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_expression(p):</span><br><span class="line">    &#39;expression : expression PLUS expression&#39;</span><br><span class="line">    p.lineno(1)        # Line number of the left expression</span><br><span class="line">    p.lineno(2)        # line number of the PLUS operator</span><br><span class="line">    p.lineno(3)        # line number of the right expression</span><br><span class="line">    ...</span><br><span class="line">    start,end &#x3D; p.linespan(3)    # Start,end lines of the right expression</span><br><span class="line">    starti,endi &#x3D; p.lexspan(3)   # Start,end positions of right expression</span><br></pre></td></tr></table></figure>
注意：lexspan() 方法只会返回的结束位置是最后一个符号的起始位置。<br>虽然，PLY 对所有符号的行号和位置的跟踪很管用，但经常是不必要的。例如，你仅仅是在错误信息中使用行号，你通常可以仅仅使用关键标记的信息，比如：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_bad_func(p):</span><br><span class="line">    &#39;funccall : fname LPAREN error RPAREN&#39;</span><br><span class="line">    # Line number reported from LPAREN token</span><br><span class="line">    print &quot;Bad function call at line&quot;, p.lineno(2)</span><br></pre></td></tr></table></figure>
<p>类似的，为了改善性能，你可以有选择性的将行号信息在必要的时候进行传递，这是通过 p.set_lineno() 实现的，例如：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_fname(p):</span><br><span class="line">    &#39;fname : ID&#39;</span><br><span class="line">    p[0] &#x3D; p[1]</span><br><span class="line">    p.set_lineno(0,p.lineno(1))</span><br></pre></td></tr></table></figure><br>对于已经完成分析的规则，PLY 不会保留行号信息，如果你是在构建抽象语法树而且需要行号，你应该确保行号保留在树上。</p>
<h3 id="构造抽象语法树"><a href="#构造抽象语法树" class="headerlink" title="构造抽象语法树"></a>构造抽象语法树</h3><p>yacc.py 没有构造抽像语法树的特殊方法。不过，你可以自己很简单的构造出来。</p>
<p>一个最为简单的构造方法是为每个语法规则创建元组或者字典，并传递它们。有很多中可行的方案，下面是一个例子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_expression_binop(p):</span><br><span class="line">    &#39;&#39;&#39;expression : expression PLUS expression</span><br><span class="line">                  | expression MINUS expression</span><br><span class="line">                  | expression TIMES expression</span><br><span class="line">                  | expression DIVIDE expression&#39;&#39;&#39;</span><br><span class="line"></span><br><span class="line">    p[0] &#x3D; (&#39;binary-expression&#39;,p[2],p[1],p[3])</span><br><span class="line"></span><br><span class="line">def p_expression_group(p):</span><br><span class="line">    &#39;expression : LPAREN expression RPAREN&#39;</span><br><span class="line">    p[0] &#x3D; (&#39;group-expression&#39;,p[2])</span><br><span class="line"></span><br><span class="line">def p_expression_number(p):</span><br><span class="line">    &#39;expression : NUMBER&#39;</span><br><span class="line">    p[0] &#x3D; (&#39;number-expression&#39;,p[1])</span><br></pre></td></tr></table></figure>
<p>另一种方法可以是为不同的抽象树节点创建一系列的数据结构，并赋值给 p[0]：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Expr: pass</span><br><span class="line"></span><br><span class="line">class BinOp(Expr):</span><br><span class="line">    def __init__(self,left,op,right):</span><br><span class="line">        self.type &#x3D; &quot;binop&quot;</span><br><span class="line">        self.left &#x3D; left</span><br><span class="line">        self.right &#x3D; right</span><br><span class="line">        self.op &#x3D; op</span><br><span class="line"></span><br><span class="line">class Number(Expr):</span><br><span class="line">    def __init__(self,value):</span><br><span class="line">        self.type &#x3D; &quot;number&quot;</span><br><span class="line">        self.value &#x3D; value</span><br><span class="line"></span><br><span class="line">def p_expression_binop(p):</span><br><span class="line">    &#39;&#39;&#39;expression : expression PLUS expression</span><br><span class="line">                  | expression MINUS expression</span><br><span class="line">                  | expression TIMES expression</span><br><span class="line">                  | expression DIVIDE expression&#39;&#39;&#39;</span><br><span class="line"></span><br><span class="line">    p[0] &#x3D; BinOp(p[1],p[2],p[3])</span><br><span class="line"></span><br><span class="line">def p_expression_group(p):</span><br><span class="line">    &#39;expression : LPAREN expression RPAREN&#39;</span><br><span class="line">    p[0] &#x3D; p[2]</span><br><span class="line"></span><br><span class="line">def p_expression_number(p):</span><br><span class="line">    &#39;expression : NUMBER&#39;</span><br><span class="line">    p[0] &#x3D; Number(p[1])</span><br></pre></td></tr></table></figure><br>这种方式的好处是在处理复杂语义时比较简单：类型检查、代码生成、以及其他针对树节点的功能。<br>为了简化树的遍历，可以创建一个通用的树节点结构，例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Node:</span><br><span class="line">    def __init__(self,type,children&#x3D;None,leaf&#x3D;None):</span><br><span class="line">         self.type &#x3D; type</span><br><span class="line">         if children:</span><br><span class="line">              self.children &#x3D; children</span><br><span class="line">         else:</span><br><span class="line">              self.children &#x3D; [ ]</span><br><span class="line">         self.leaf &#x3D; leaf</span><br><span class="line"> </span><br><span class="line">def p_expression_binop(p):</span><br><span class="line">    &#39;&#39;&#39;expression : expression PLUS expression</span><br><span class="line">                  | expression MINUS expression</span><br><span class="line">                  | expression TIMES expression</span><br><span class="line">                  | expression DIVIDE expression&#39;&#39;&#39;</span><br><span class="line">    p[0] &#x3D; Node(&quot;binop&quot;, [p[1],p[3]], p[2])</span><br></pre></td></tr></table></figure>
<h3 id="嵌入式动作"><a href="#嵌入式动作" class="headerlink" title="嵌入式动作"></a>嵌入式动作</h3><p>yacc 使用的分析技术只允许在规则规约后执行动作。假设有如下规则：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_foo(p):</span><br><span class="line">    &quot;foo : A B C D&quot;</span><br><span class="line">    print &quot;Parsed a foo&quot;, p[1],p[2],p[3],p[4]</span><br></pre></td></tr></table></figure>
<p>方法只会在符号 A,B,C和D 都完成后才能执行。可是有的时候，在中间阶段执行一小段代码是有用的。假如，你想在 A 完成后立即执行一些动作，像下面这样用空规则：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_foo(p):</span><br><span class="line">    &quot;foo : A seen_A B C D&quot;</span><br><span class="line">    print &quot;Parsed a foo&quot;, p[1],p[3],p[4],p[5]</span><br><span class="line">    print &quot;seen_A returned&quot;, p[2]</span><br><span class="line">def p_seen_A(p):</span><br><span class="line">    &quot;seen_A :&quot;</span><br><span class="line">    print &quot;Saw an A &#x3D; &quot;, p[-1]   # Access grammar symbol to left</span><br><span class="line">    p[0] &#x3D; some_value            # Assign value to seen_A</span><br></pre></td></tr></table></figure><br>在这个例子中，空规则 seen_A 将在 A 移进分析栈后立即执行。p[-1] 指代的是在分析栈上紧跟在 seen_A 左侧的符号。在这个例子中，是 A 符号。像其他普通的规则一样，在嵌入式行为中也可以通过为 p[0] 赋值来返回某些值。<br>使用嵌入式动作可能会导致移进归约冲突，比如，下面的语法是没有冲突的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_foo(p):</span><br><span class="line">    &quot;&quot;&quot;foo : abcd</span><br><span class="line">           | abcx&quot;&quot;&quot;</span><br><span class="line">def p_abcd(p):</span><br><span class="line">    &quot;abcd : A B C D&quot;</span><br><span class="line">def p_abcx(p):</span><br><span class="line">    &quot;abcx : A B C X&quot;</span><br></pre></td></tr></table></figure>
<p>可是，如果像这样插入一个嵌入式动作：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_foo(p):</span><br><span class="line">    &quot;&quot;&quot;foo : abcd</span><br><span class="line">           | abcx&quot;&quot;&quot;</span><br><span class="line">def p_abcd(p):</span><br><span class="line">    &quot;abcd : A B C D&quot;</span><br><span class="line">def p_abcx(p):</span><br><span class="line">    &quot;abcx : A B seen_AB C X&quot;</span><br><span class="line">def p_seen_AB(p):</span><br><span class="line">    &quot;seen_AB :&quot;</span><br></pre></td></tr></table></figure><br>会产生移进归约冲，只是由于对于两个规则 abcd 和 abcx 中的 C，分析器既可以根据 abcd 规则移进，也可以根据 abcx 规则先将空的 seen_AB 归约。<br>嵌入动作的一般用于分析以外的控制，比如为本地变量定义作用于。对于 C 语言：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_statements_block(p):</span><br><span class="line">    &quot;statements: LBRACE new_scope statements RBRACE&quot;&quot;&quot;</span><br><span class="line">    # Action code</span><br><span class="line">    ...</span><br><span class="line">    pop_scope()        # Return to previous scope</span><br><span class="line"></span><br><span class="line">def p_new_scope(p):</span><br><span class="line">    &quot;new_scope :&quot;</span><br><span class="line">    # Create a new scope for local variables</span><br><span class="line">    s &#x3D; new_scope()</span><br><span class="line">    push_scope(s)</span><br></pre></td></tr></table></figure>
<p>在这个例子中，new_scope 作为嵌入式行为，在左大括号{之后立即执行。可以是调正内部符号表或者其他方面。statements_block 一完成，代码可能会撤销在嵌入动作时的操作（比如，pop_scope())</p>
<h3 id="Yacc-的其他"><a href="#Yacc-的其他" class="headerlink" title="Yacc 的其他"></a>Yacc 的其他</h3><ul>
<li>默认的分析方法是 LALR，使用 SLR 请像这样运行 yacc()：yacc.yacc(method=”SLR”) 注意：LRLR 生成的分析表大约要比 SLR 的大两倍。解析的性能没有本质的区别，因为代码是一样的。由于 LALR 能力稍强，所以更多的用于复杂的语法。</li>
<li>默认情况下，yacc.py 依赖 lex.py 产生的标记。不过，可以用一个等价的词法标记生成器代替： yacc.parse(lexer=x) 这个例子中，x 必须是一个 Lexer 对象，至少拥有 x.token() 方法用来获取标记。如果将输入字串提供给 yacc.parse()，lexer 还必须具有 x.input() 方法。</li>
<li>默认情况下，yacc 在调试模式下生成分析表（会生成 parser.out 文件和其他东西），使用 yacc.yacc(debug=0) 禁用调试模式。</li>
<li>改变 parsetab.py 的文件名：yacc.yacc(tabmodule=”foo”)</li>
<li>改变 parsetab.py 的生成目录：yacc.yacc(tabmodule=”foo”,outputdir=”somedirectory”)</li>
<li>不生成分析表：yacc.yacc(write_tables=0)。注意：如果禁用分析表生成，yacc()将在每次运行的时候重新构建分析表（这里耗费的时候取决于语法文件的规模）</li>
<li>想在分析过程中输出丰富的调试信息，使用：yacc.parse(debug=1)</li>
<li>yacc.yacc()方法会返回分析器对象，如果你想在一个程序中支持多个分析器：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">p &#x3D; yacc.yacc()</span><br><span class="line">...</span><br><span class="line">p.parse()</span><br></pre></td></tr></table></figure>
注意：yacc.parse() 方法只绑定到最新创建的分析器对象上。</li>
<li>由于生成生成 LALR 分析表相对开销较大，先前生成的分析表会被缓存和重用。判断是否重新生成的依据是对所有的语法规则和优先级规则进行 MD5 校验，只有不匹配时才会重新生成。生成分析表是合理有效的办法，即使是面对上百个规则和状态的语法。对于复杂的编程语言，像 C 语言，在一些慢的机器上生成分析表可能要花费 30-60 秒，请耐心。</li>
<li>由于 LR 分析过程是基于分析表的，分析器的性能很大程度上取决于语法的规模。最大的瓶颈可能是词法分析器和语法规则的复杂度。<h2 id="多个语法和词法分析器"><a href="#多个语法和词法分析器" class="headerlink" title="多个语法和词法分析器"></a>多个语法和词法分析器</h2>在高级的分析器程序中，你可能同时需要多个语法和词法分析器。依照规则行事不会有问题。不过，你需要小心确定所有东西都正确的绑定(hooked up)了。首先，保证将 lex() 和 yacc() 返回的对象保存起来：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lexer  &#x3D; lex.lex()       # Return lexer object</span><br><span class="line">parser &#x3D; yacc.yacc()     # Return parser object</span><br></pre></td></tr></table></figure>
<p>接着，在解析时，确保给 parse() 方法一个正确的 lexer 引用：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">parser.parse(text,lexer&#x3D;lexer)</span><br></pre></td></tr></table></figure><br>如果遗漏这一步，分析器会使用最新创建的 lexer 对象，这可能不是你希望的。<br>词法器和语法器的方法中也可以访问这些对象。在词法器中，标记的 lexer 属性指代的是当前触发规则的词法器对象：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def t_NUMBER(t):</span><br><span class="line">   r&#39;\d+&#39;</span><br><span class="line">   ...</span><br><span class="line">   print t.lexer           # Show lexer object</span><br></pre></td></tr></table></figure>
<p>在语法器中，lexer 和 parser 属性指代的是对应的词法器对象和语法器对象<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_expr_plus(p):</span><br><span class="line">   &#39;expr : expr PLUS expr&#39;</span><br><span class="line">   ...</span><br><span class="line">   print p.parser          # Show parser object</span><br><span class="line">   print p.lexer           # Show lexer object</span><br></pre></td></tr></table></figure><br>如果有必要，lexe r对象和 parser 对象都可以附加其他属性。例如，你想要有不同的解析器状态，可以为 parser 对象附加更多的属性，并在后面用到它们。</p>
<h2 id="使用Python的优化模式"><a href="#使用Python的优化模式" class="headerlink" title="使用Python的优化模式"></a>使用Python的优化模式</h2><p>由于 PLY 从文档字串中获取信息，语法解析和词法分析信息必须通过正常模式下的 Python 解释器得到（不带 有-O 或者 -OO 选项）。不过，如果你像这样指定 optimize 模式：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lex.lex(optimize&#x3D;1)</span><br><span class="line">yacc.yacc(optimize&#x3D;1)</span><br></pre></td></tr></table></figure>
<p>PLY 可以在下次执行，在 Python 的优化模式下执行。但你必须确保第一次执行是在 Python 的正常模式下进行，一旦词法分析表和语法分析表生成一次后，在 Python 优化模式下执行，PLY 会使用生成好的分析表而不再需要文档字串。</p>
<h2 id="高级调试"><a href="#高级调试" class="headerlink" title="高级调试"></a>高级调试</h2><p>调试一个编译器不是件容易的事情。PLY 提供了一些高级的调试能力，这是通过 Python 的l ogging 模块实现的，下面两节介绍这一主题：</p>
<h3 id="调试-lex-和-yacc-命令"><a href="#调试-lex-和-yacc-命令" class="headerlink" title="调试 lex() 和 yacc() 命令"></a>调试 lex() 和 yacc() 命令</h3><p>lex() 和 yacc() 命令都有调试模式，可以通过 debug 标识实现：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lex.lex(debug&#x3D;True)</span><br><span class="line">yacc.yacc(debug&#x3D;True)</span><br></pre></td></tr></table></figure>
<p>正常情况下，调试不仅输出标准错误，对于 yacc()，还会给出 parser.out 文件。这些输出可以通过提供 logging 对象来精细的控制。下面这个例子增加了对调试信息来源的输出：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Set up a logging object</span><br><span class="line">import logging</span><br><span class="line">logging.basicConfig(</span><br><span class="line">    level &#x3D; logging.DEBUG,</span><br><span class="line">    filename &#x3D; &quot;parselog.txt&quot;,</span><br><span class="line">    filemode &#x3D; &quot;w&quot;,</span><br><span class="line">    format &#x3D; &quot;%(filename)10s:%(lineno)4d:%(message)s&quot;</span><br><span class="line">)</span><br><span class="line">log &#x3D; logging.getLogger()</span><br><span class="line"></span><br><span class="line">lex.lex(debug&#x3D;True,debuglog&#x3D;log)</span><br><span class="line">yacc.yacc(debug&#x3D;True,debuglog&#x3D;log)</span><br></pre></td></tr></table></figure><br>如果你提供一个自定义的 logger，大量的调试信息可以通过分级来控制。典型的是将调试信息分为 DEBUG,INFO,或者 WARNING 三个级别。<br>PLY 的错误和警告信息通过日志接口提供，可以从 errorlog 参数中传入日志对象</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lex.lex(errorlog&#x3D;log)</span><br><span class="line">yacc.yacc(errorlog&#x3D;log)</span><br></pre></td></tr></table></figure>
<p>如果想完全过滤掉警告信息，你除了可以使用带级别过滤功能的日志对象，也可以使用 lex 和 yacc 模块都内建的 Nulllogger 对象。例如：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yacc.yacc(errorlog&#x3D;yacc.NullLogger())</span><br></pre></td></tr></table></figure></p>
<h3 id="运行时调试"><a href="#运行时调试" class="headerlink" title="运行时调试"></a>运行时调试</h3><p>为分析器指定 debug 选项，可以激活语法分析器的运行时调试功能。这个选项可以是整数（表示对调试功能是开还是关），也可以是 logger 对象。例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">log &#x3D; logging.getLogger()</span><br><span class="line">parser.parse(input,debug&#x3D;log)</span><br></pre></td></tr></table></figure>
<p>如果传入日志对象的话，你可以使用其级别过滤功能来控制内容的输出。INFO 级别用来产生归约信息；DEBUG 级别会显示分析栈的信息、移进的标记和其他详细信息。ERROR 级别显示分析过程中的错误相关信息。<br>对于每个复杂的问题，你应该用日志对象，以便输出重定向到文件中，进而方便在执行结束后检查。</p>
<h1 id="写一个计算器"><a href="#写一个计算器" class="headerlink" title="写一个计算器"></a>写一个计算器</h1><h3 id="Lex文件"><a href="#Lex文件" class="headerlink" title="Lex文件"></a>Lex文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from ply import lex</span><br><span class="line"></span><br><span class="line"># Define &#96;tokens&#96;, a list of token names.</span><br><span class="line">tokens &#x3D; ( &#39;PLUS&#39;, &#39;MINUS&#39;, &#39;MULT&#39;, &#39;DIV&#39;, &#39;EXPONENT&#39;, \</span><br><span class="line">        &#39;LPAREN&#39;, &#39;RPAREN&#39;, &#39;AB&#39;, &#39;NUMBER&#39;, \</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"># Define &#96;t_ignore&#96; to ignore unnecessary characters between tokens, such as whitespaces.</span><br><span class="line">t_ignore &#x3D; &quot; \t&quot;</span><br><span class="line"></span><br><span class="line"># Define functions representing regular expression rules for each token.</span><br><span class="line"># The name of functions must be like &#96;t_&lt;token_name&gt;&#96;.</span><br><span class="line"># Functions accept one argument, which is a parsed token.</span><br><span class="line">#    t.type  : name of token</span><br><span class="line">#    t.value : string of parsed token </span><br><span class="line">#    t.lineno: line number of token</span><br><span class="line">#    t.lexpos: position of token from the beginning of input string</span><br><span class="line"></span><br><span class="line">def t_PLUS(t):</span><br><span class="line">    r&#39;\+&#39; # regular expression for the token</span><br><span class="line">    return t</span><br><span class="line"></span><br><span class="line">def t_MINUS(t):</span><br><span class="line">    r&#39;\-&#39;</span><br><span class="line">    return t</span><br><span class="line"></span><br><span class="line"># The order of declaration is also the order of rules the lexer uses.</span><br><span class="line"># That is why &#96;t_EXPONENT&#96; must be before &#96;t_MULT&#96;.</span><br><span class="line">def t_EXPONENT(t):</span><br><span class="line">    r&#39;\*\*&#39;</span><br><span class="line">    return t</span><br><span class="line"></span><br><span class="line">def t_MULT(t):</span><br><span class="line">    r&#39;\*&#39;</span><br><span class="line">    return t</span><br><span class="line"></span><br><span class="line">def t_DIV(t):</span><br><span class="line">    r&#39;&#x2F;&#39;</span><br><span class="line">    return t</span><br><span class="line"></span><br><span class="line">def t_LPAREN(t):</span><br><span class="line">    r&#39;\(&#39;</span><br><span class="line">    return t</span><br><span class="line"></span><br><span class="line">def t_RPAREN(t):</span><br><span class="line">    r&#39;\)&#39;</span><br><span class="line">    return t</span><br><span class="line"></span><br><span class="line">def t_AB(t):</span><br><span class="line">    r&#39;ab&#39;</span><br><span class="line">    return t</span><br><span class="line"></span><br><span class="line">def t_NUMBER(t):</span><br><span class="line">    r&#39;[0-9]+&#39;</span><br><span class="line">    t.value &#x3D; int(t.value)</span><br><span class="line">    return t</span><br><span class="line"></span><br><span class="line"># To count correct line number</span><br><span class="line">def t_newline(t):</span><br><span class="line">    r&#39;\n+&#39;</span><br><span class="line">    t.lexer.lineno +&#x3D; t.value.count(&quot;\n&quot;)</span><br><span class="line">    # return None, so this newlines will not be in the parsed token list.</span><br><span class="line"></span><br><span class="line"># Special function for error handling</span><br><span class="line">def t_error(t):</span><br><span class="line">    print(&quot;illegal character &#39;%s&#39;&quot; % (t.value[0]))</span><br><span class="line">    t.lexer.skip(1)</span><br><span class="line"></span><br><span class="line"># Generate a lexer by &#96;lex.lex()&#96;</span><br><span class="line">lexer &#x3D; lex.lex()</span><br><span class="line"></span><br><span class="line">def test_lexer(input_string):</span><br><span class="line">    lexer.input(input_string)</span><br><span class="line">    result &#x3D; []</span><br><span class="line">    while True:</span><br><span class="line">        tok &#x3D; lexer.token()</span><br><span class="line">        if not tok:</span><br><span class="line">            break</span><br><span class="line">        result &#x3D; result + [(tok.type, tok.value)]</span><br><span class="line">    return result</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    print(test_lexer(&#39;1 + 2&#39;))</span><br><span class="line">    print(test_lexer(&#39;1 + 20 * 3 - 10 &#x2F; -2 * (1 + 3)&#39;))</span><br><span class="line">    print(test_lexer(&#39;1 ** 2&#39;))</span><br><span class="line">    print(test_lexer(&#39;ab 5 + ab -2 * ab (1 - 2)&#39;))</span><br></pre></td></tr></table></figure>
<h3 id="Yacc文件"><a href="#Yacc文件" class="headerlink" title="Yacc文件"></a>Yacc文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from ply import yacc</span><br><span class="line">from calclexer import tokens, lexer</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">Grammars:</span><br><span class="line">S -&gt; E</span><br><span class="line">E -&gt; E + E</span><br><span class="line">E -&gt; E - E</span><br><span class="line">E -&gt; E * E</span><br><span class="line">E -&gt; E &#x2F; E</span><br><span class="line">E -&gt; E ** E</span><br><span class="line">E -&gt; N</span><br><span class="line">E -&gt; +N</span><br><span class="line">E -&gt; -N</span><br><span class="line">E -&gt; ab E</span><br><span class="line">E -&gt; (E)</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"># (optional) Define precedence and associativity of operators.</span><br><span class="line"># The format is ( (&#39;left&#39; or &#39;right&#39;, &lt;token name&gt;, ...), (...) ).</span><br><span class="line"># &lt;token name&gt; is expected to be defined in the lexer definition.</span><br><span class="line"># The latter has the the higher precedence (e.g. &#39;MULT&#39; and &#39;DIV&#39; have the higher precedence than &#39;PLUS&#39; and &#39;MINUS&#39;).</span><br><span class="line"># &#39;UPLUS&#39; and &#39;UMINUS&#39; are defined as aliases to override precedence (see &#96;p_expr_um_num&#96;)</span><br><span class="line">precedence &#x3D; ( \</span><br><span class="line">        (&#39;left&#39;, &#39;PLUS&#39;, &#39;MINUS&#39;), \</span><br><span class="line">        (&#39;left&#39;, &#39;MULT&#39;, &#39;DIV&#39;), \</span><br><span class="line">        (&#39;right&#39;, &#39;EXPONENT&#39;), \</span><br><span class="line">        (&#39;right&#39;, &#39;UPLUS&#39;, &#39;UMINUS&#39;, &#39;AB&#39;), \</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"># Parsing rules</span><br><span class="line"># Functions should be start with &#96;p_&#96;.</span><br><span class="line"># The first rule will be the starting rule of parsing (?).</span><br><span class="line"></span><br><span class="line"># S -&gt; E</span><br><span class="line">def p_statement(p):</span><br><span class="line">    &#39;statement : expr&#39;</span><br><span class="line">    p[0] &#x3D; p[1]</span><br><span class="line"></span><br><span class="line"># E -&gt; E + E</span><br><span class="line">def p_expr_plus(p):</span><br><span class="line">    &#39;expr : expr PLUS expr&#39;</span><br><span class="line">    p[0] &#x3D; p[1] + p[3]</span><br><span class="line"></span><br><span class="line"># E -&gt; E - E</span><br><span class="line">def p_expr_minus(p):</span><br><span class="line">    &#39;expr : expr MINUS expr&#39;</span><br><span class="line">    p[0] &#x3D; p[1] - p[3]</span><br><span class="line"></span><br><span class="line"># E -&gt; E * E</span><br><span class="line">def p_expr_mult(p):</span><br><span class="line">    &#39;expr : expr MULT expr&#39;</span><br><span class="line">    p[0] &#x3D; p[1] * p[3]</span><br><span class="line"></span><br><span class="line"># E -&gt; E &#x2F; E</span><br><span class="line">def p_expr_div(p):</span><br><span class="line">    &#39;expr : expr DIV expr&#39;</span><br><span class="line">    p[0] &#x3D; p[1] &#x2F; p[3]</span><br><span class="line"></span><br><span class="line"># E -&gt; E ** E</span><br><span class="line">def p_expr_exponent(p):</span><br><span class="line">    &#39;expr : expr EXPONENT expr&#39;</span><br><span class="line">    p[0] &#x3D; p[1] ** p[3]</span><br><span class="line"></span><br><span class="line"># E -&gt; N</span><br><span class="line">def p_expr_num(p):</span><br><span class="line">    &#39;expr : NUMBER&#39;</span><br><span class="line">    p[0] &#x3D; p[1]</span><br><span class="line"></span><br><span class="line"># E -&gt; +N</span><br><span class="line">def p_expr_up_num(p):</span><br><span class="line">    &#39;expr : PLUS NUMBER %prec UPLUS&#39; # override precedence of PLUS by &#96;%prec UPLUS&#96;</span><br><span class="line">    p[0] &#x3D; p[2]</span><br><span class="line"></span><br><span class="line"># E -&gt; -N</span><br><span class="line">def p_expr_um_num(p):</span><br><span class="line">    &#39;expr : MINUS NUMBER %prec UMINUS&#39; # override precedence of MINUS by &#96;%prec UMINUS&#96;</span><br><span class="line">    p[0] &#x3D; -p[2]</span><br><span class="line"></span><br><span class="line"># E -&gt; ab E</span><br><span class="line">def p_expr_ab(p):</span><br><span class="line">    &#39;expr : AB expr&#39;</span><br><span class="line">    p[0] &#x3D; abs(p[2])</span><br><span class="line"></span><br><span class="line"># E -&gt; ( E )</span><br><span class="line">def p_expr_paren(p):</span><br><span class="line">    &#39;expr : LPAREN expr RPAREN&#39;</span><br><span class="line">    p[0] &#x3D; p[2]</span><br><span class="line"></span><br><span class="line"># Rule for error handling</span><br><span class="line">def p_error(t):</span><br><span class="line">    print(&quot;syntax error at &#39;%s&#39;&quot; % (t.value))</span><br><span class="line"></span><br><span class="line"># Generate a LALR parser</span><br><span class="line">parser &#x3D; yacc.yacc()</span><br><span class="line"></span><br><span class="line">def parse(input_string):</span><br><span class="line">    lexer.input(input_string)</span><br><span class="line">    parse_tree &#x3D; parser.parse(input_string, lexer&#x3D;lexer)</span><br><span class="line">    return parse_tree</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    assert parse(&#39;1 + 2 + 3&#39;) &#x3D;&#x3D; 6</span><br><span class="line">    assert parse(&#39;1 + 2 * 3 * 4&#39;) &#x3D;&#x3D; 25</span><br><span class="line">    assert parse(&#39;3 * 4 - 10 &#x2F; 2 + 5&#39;) &#x3D;&#x3D; 12</span><br><span class="line">    assert parse(&#39;-3 * (+4 - 10) &#x2F; -2 + 5&#39;) &#x3D;&#x3D; -4</span><br><span class="line">    assert parse(&#39;1 + 2 ** 3 ** 2&#39;) &#x3D;&#x3D; 513</span><br><span class="line">    assert parse(&#39;ab (1 - 2  -3)&#39;) &#x3D;&#x3D; 4</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>工具使用</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark入门</title>
    <url>/2020/02/26/Spark%E5%85%A5%E9%97%A8/</url>
    <content><![CDATA[<p>最近需要用spark比较多，重新学习一下。今天先学习一些基础。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://classroom.udacity.com/courses/ud2002" target="_blank" rel="noopener">https://classroom.udacity.com/courses/ud2002</a></p>
</blockquote>
<h1 id="Spark处理数据"><a href="#Spark处理数据" class="headerlink" title="Spark处理数据"></a>Spark处理数据</h1><h2 id="函数式编程"><a href="#函数式编程" class="headerlink" title="函数式编程"></a>函数式编程</h2><p>首先用下图来看一下，函数式编程和过程式编程的区别。<img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/maj1.png" alt="图片"></p>
<p>函数式编程非常适合分布式系统。Python并不是函数编程语言，但使用PySparkAPI 可以让你编写Spark程序，并确保你的代码使用了函数式编程。在底层，Python 代码使用 py4j 来调用 Java 虚拟机(JVM)。</p>
<p>假设有下面一段代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">log_of_songs &#x3D; [</span><br><span class="line">        &quot;Despacito&quot;,</span><br><span class="line">        &quot;Nice for what&quot;,</span><br><span class="line">        &quot;No tears left to cry&quot;,</span><br><span class="line">        &quot;Despacito&quot;,</span><br><span class="line">        &quot;Havana&quot;,</span><br><span class="line">        &quot;In my feelings&quot;,</span><br><span class="line">        &quot;Nice for what&quot;,</span><br><span class="line">        &quot;Despacito&quot;,</span><br><span class="line">        &quot;All the stars&quot;</span><br><span class="line">]</span><br><span class="line">play_count &#x3D; 0</span><br><span class="line">def count_plays(song_title):</span><br><span class="line">    global play_count</span><br><span class="line">    for song in log_of_songs:</span><br><span class="line">        if song &#x3D;&#x3D; song_title:</span><br><span class="line">            play_count &#x3D; play_count + 1</span><br><span class="line">    return play_count</span><br></pre></td></tr></table></figure>
<p>调用两次count_plays(“Despacito”)会得到不同的结果，这是因为play_count是作为全局变量，在函数内部进行了修改。解决这个问题可以采用如下代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def count_plays(song_title, play_count):</span><br><span class="line">    for song in log_of_songs:</span><br><span class="line">        if song &#x3D;&#x3D; song_title:</span><br><span class="line">            play_count &#x3D; play_count + 1</span><br><span class="line">    return play_count</span><br></pre></td></tr></table></figure>
<p>这就是Spark解决问题的方式。<br>在Spark中我们使用Pure Function（纯函数），就像面包制造厂，不同的面包机器之间是互不干扰的，且不会损坏原材料。Spark会在函数执行前，将数据复制多分，以输入到不同函数中。为了防止内存溢出，Spark会在代码中建立一个数据的有向无环图，在运行前检查是否有必要对某一分数据进行复制。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/maj2.png" alt="图片"></p>
<h2 id="运行时参数设置"><a href="#运行时参数设置" class="headerlink" title="运行时参数设置"></a>运行时参数设置</h2><blockquote>
<p>参考：<br><a href="https://tech.meituan.com/2016/04/29/spark-tuning-basic.html" target="_blank" rel="noopener">https://tech.meituan.com/2016/04/29/spark-tuning-basic.html</a><br><a href="https://spark.apache.org/docs/1.6.1/running-on-yarn.html" target="_blank" rel="noopener">https://spark.apache.org/docs/1.6.1/running-on-yarn.html</a></p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line">    --master yarn \</span><br><span class="line">    --deploy-mode cluster  \</span><br><span class="line">    --num-executors 100 \</span><br><span class="line">    --driver-memory 2g \</span><br><span class="line">    --executor-memory 14g \</span><br><span class="line">    --executor-cores 6 \</span><br><span class="line">    --conf spark.default.parallelism&#x3D;1000 \</span><br><span class="line">    --conf spark.storage.memoryFraction&#x3D;0.2 \</span><br><span class="line">    --conf spark.shuffle.memoryFraction&#x3D;0.6 \</span><br><span class="line">    --conf spark.executor.extraJavaOptions&#x3D;&#39;-Dlog4j.configuration&#x3D;log4j.properties&#39; \</span><br><span class="line">    --driver-java-options -Dlog4j.configuration&#x3D;log4j.properties \</span><br><span class="line">    python文件  \</span><br></pre></td></tr></table></figure>
<ul>
<li>spark-submit: which spark-submit 查看该命令是 spark 系统的还是 pyspark 包自带的，应该使用 spark 系统的<ul>
<li>master:</li>
<li>standaloone: spark 自带的集群资源管理器</li>
<li>yarn</li>
<li>local: 本地运行</li>
</ul>
</li>
<li>deploy-mode：<ul>
<li>client: driver 在本机上，能够直接使用本机文件系统</li>
<li>cluster: driver 指不定在哪台机器上，不能读取本机文件系统</li>
</ul>
</li>
<li>spark 运行时配置：主要的有运行内存和节点数量:<ul>
<li>num_executors</li>
<li>spark_driver_memory</li>
<li>spark_executor_memory</li>
</ul>
</li>
<li>addFiles 与 —files（将需要使用的文件分发到每台机器上）：<ul>
<li>addFiles()：能够分发到每台机器上,包括 driver 上</li>
<li>—files: 只能分发到 executor 上</li>
</ul>
</li>
<li>引用其他模块的问题：<ul>
<li>第三方库：需要将第三方库打包上传供使用</li>
<li>自己的模块：也需要打包上传,以供使用</li>
</ul>
</li>
<li>运行下面前请确认<ul>
<li>export SPARK_HOME=…../spark-1.6.2-bin-hadoop2.6</li>
<li>export PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH</li>
<li>export JAVA_HOME=…/jdk1.8.0_60</li>
<li>PYSPARK_PYTHON=./NLTK/conda-env/bin/python spark-submit —conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./NLTK/conda-env/bin/python —master yarn-cluster —archives conda-env.zip#NLTK clean_step_two.py</li>
</ul>
</li>
</ul>
<h2 id="Maps和Lambda"><a href="#Maps和Lambda" class="headerlink" title="Maps和Lambda"></a>Maps和Lambda</h2><blockquote>
<p>lambda函数起源：<br><a href="http://palmstroem.blogspot.com/2012/05/lambda-calculus-for-absolute-dummies.html" target="_blank" rel="noopener">http://palmstroem.blogspot.com/2012/05/lambda-calculus-for-absolute-dummies.html</a></p>
</blockquote>
<p>Maps会复制原始数据，并把副本数据按照Maps中的函数进行转换。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from pyspark import SparkContext</span><br><span class="line">log_of_songs &#x3D; [</span><br><span class="line">    &quot;Despacito&quot;,</span><br><span class="line">    &quot;Nice for what&quot;,</span><br><span class="line">    &quot;No tears left to cry&quot;,</span><br><span class="line">    &quot;Despacito&quot;,</span><br><span class="line">    &quot;Havana&quot;,</span><br><span class="line">    &quot;In my feelings&quot;,</span><br><span class="line">    &quot;Nice for what&quot;,</span><br><span class="line">    &quot;Despacito&quot;,</span><br><span class="line">    &quot;All the stars&quot;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">def convert_song_to_lowercase(song):</span><br><span class="line">    return song.lower()</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">    conf &#x3D; SparkConf()</span><br><span class="line">    conf.setAppName(&quot;Testing&quot;).setMaster(&quot;local&quot;)</span><br><span class="line">    sc &#x3D; SparkContext(conf&#x3D;conf)</span><br><span class="line">    sc.setLogLevel(&quot;WARN&quot;)</span><br><span class="line"></span><br><span class="line">    # parallelize将对象分配到不同节点上</span><br><span class="line">    distributed_song_log &#x3D; sc.parallelize(log_of_songs)</span><br><span class="line">    # 定义不同节点的所有数据执行convert_song_to_lowercase的操作</span><br><span class="line">    # 但此时spark还未执行，它在等待所有定义结束后，看是否可以优化某些操作</span><br><span class="line">    distributed_song_log.map(convert_song_to_lowercase)</span><br><span class="line">    # 如果想强制spark执行，则可以使用collect，则会将所有数据汇总</span><br><span class="line">    # 注意此时spark并没有改变原始数据的大小写，它将原始数据进行了拷贝，再做的处理</span><br><span class="line">    distributed_song_log.collect()</span><br><span class="line">    # 也可以使用python的匿名函数进行map</span><br><span class="line">    distributed_song_log.map(lambda song: song.lower()).collect()</span><br></pre></td></tr></table></figure>
<h2 id="Data-Frame"><a href="#Data-Frame" class="headerlink" title="Data Frame"></a>Data Frame</h2><p>数据处理有两种方式，一种使用Data Frame和Python进行命令式编程，另一种使用SQL进行声明式编程。命令式编程关注的是”How”，声明式编程关注的是”What”。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/maj3.png" alt="图片"></p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/maj4.png" alt="图片"></p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/maj5.png" alt="图片"></p>
<h3 id="Data-Frame的读取和写入"><a href="#Data-Frame的读取和写入" class="headerlink" title="Data Frame的读取和写入"></a>Data Frame的读取和写入</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import pyspark</span><br><span class="line">from pyspark import SparkConf</span><br><span class="line">from pyspark.sql import SparkSession</span><br><span class="line"></span><br><span class="line">spark &#x3D; SparkSession \</span><br><span class="line">    .builder \</span><br><span class="line">    .appName(&quot;Our first Python Spark SQL example&quot;) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"># 检查一下是否生效了。</span><br><span class="line">spark.sparkContext.getConf().getAll()</span><br><span class="line">path &#x3D; &quot;data&#x2F;sparkify_log_small.json&quot;</span><br><span class="line">user_log &#x3D; spark.read.json(path)</span><br><span class="line">user_log.printSchema()</span><br><span class="line">user_log.describe()</span><br><span class="line">user_log.show(n&#x3D;1)</span><br><span class="line"># 取数据的前5条</span><br><span class="line">user_log.take(5)</span><br><span class="line">out_path &#x3D; &quot;data&#x2F;sparkify_log_small.csv&quot;</span><br><span class="line">user_log.write.save(out_path, format&#x3D;&quot;csv&quot;, header&#x3D;True)</span><br><span class="line"># 读取另一个daraframe</span><br><span class="line">user_log_2 &#x3D; spark.read.csv(out_path, header&#x3D;True)</span><br><span class="line">user_log_2.printSchema()</span><br><span class="line">user_log_2.take(2)</span><br><span class="line">user_log_2.select(&quot;userID&quot;).show()</span><br></pre></td></tr></table></figure>
<h3 id="Data-Frame数据处理"><a href="#Data-Frame数据处理" class="headerlink" title="Data Frame数据处理"></a>Data Frame数据处理</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from pyspark.sql import SparkSession</span><br><span class="line">from pyspark.sql.functions import udf</span><br><span class="line">from pyspark.sql.types import StringType</span><br><span class="line">from pyspark.sql.types import IntegerType</span><br><span class="line">from pyspark.sql.functions import desc</span><br><span class="line">from pyspark.sql.functions import asc</span><br><span class="line">from pyspark.sql.functions import sum as Fsum</span><br><span class="line"></span><br><span class="line">import datetime</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">%matplotlib inline</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">spark &#x3D; SparkSession \</span><br><span class="line">    .builder \</span><br><span class="line">    .appName(&quot;Wrangling Data&quot;) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line">path &#x3D; &quot;data&#x2F;sparkify_log_small.json&quot;</span><br><span class="line">user_log &#x3D; spark.read.json(path)</span><br><span class="line"># 数据搜索</span><br><span class="line">user_log.take(5)</span><br><span class="line">user_log.printSchema()</span><br><span class="line">user_log.describe().show()</span><br><span class="line">user_log.describe(&quot;artist&quot;).show()</span><br><span class="line">user_log.describe(&quot;sessionId&quot;).show()</span><br><span class="line">user_log.count()</span><br><span class="line">user_log.select(&quot;page&quot;).dropDuplicates().sort(&quot;page&quot;).show()</span><br><span class="line">user_log.select([&quot;userId&quot;, &quot;firstname&quot;, &quot;page&quot;, &quot;song&quot;]).where(user_log.userId &#x3D;&#x3D; &quot;1046&quot;).collect()</span><br><span class="line"># 按小时统计数据</span><br><span class="line">get_hour &#x3D; udf(lambda x: datetime.datetime.fromtimestamp(x &#x2F; 1000.0). hour)</span><br><span class="line">user_log &#x3D; user_log.withColumn(&quot;hour&quot;, get_hour(user_log.ts))</span><br><span class="line">user_log.head()</span><br><span class="line">songs_in_hour &#x3D; user_log.filter(user_log.page &#x3D;&#x3D; &quot;NextSong&quot;).groupby(user_log.hour).count().orderBy(user_log.hour.cast(&quot;float&quot;))</span><br><span class="line">songs_in_hour.show()</span><br><span class="line">songs_in_hour_pd &#x3D; songs_in_hour.toPandas()</span><br><span class="line">songs_in_hour_pd.hour &#x3D; pd.to_numeric(songs_in_hour_pd.hour)</span><br><span class="line">plt.scatter(songs_in_hour_pd[&quot;hour&quot;], songs_in_hour_pd[&quot;count&quot;])</span><br><span class="line">plt.xlim(-1, 24);</span><br><span class="line">plt.ylim(0, 1.2 * max(songs_in_hour_pd[&quot;count&quot;]))</span><br><span class="line">plt.xlabel(&quot;Hour&quot;)</span><br><span class="line">plt.ylabel(&quot;Songs played&quot;);</span><br><span class="line"></span><br><span class="line"># 删除空值的行</span><br><span class="line">user_log_valid &#x3D; user_log.dropna(how &#x3D; &quot;any&quot;, subset &#x3D; [&quot;userId&quot;, &quot;sessionId&quot;])</span><br><span class="line">user_log_valid.count()</span><br><span class="line">user_log.select(&quot;userId&quot;).dropDuplicates().sort(&quot;userId&quot;).show()</span><br><span class="line">user_log_valid &#x3D; user_log_valid.filter(user_log_valid[&quot;userId&quot;] !&#x3D; &quot;&quot;)</span><br><span class="line">user_log_valid.count()</span><br><span class="line"># 降级服务的用户</span><br><span class="line">user_log_valid.filter(&quot;page &#x3D; &#39;Submit Downgrade&#39;&quot;).show()</span><br><span class="line">user_log.select([&quot;userId&quot;, &quot;firstname&quot;, &quot;page&quot;, &quot;level&quot;, &quot;song&quot;]).where(user_log.userId &#x3D;&#x3D; &quot;1138&quot;).collect()</span><br><span class="line">flag_downgrade_event &#x3D; udf(lambda x: 1 if x &#x3D;&#x3D; &quot;Submit Downgrade&quot; else 0, IntegerType())</span><br><span class="line">user_log_valid &#x3D; user_log_valid.withColumn(&quot;downgraded&quot;, flag_downgrade_event(&quot;page&quot;))</span><br><span class="line">user_log_valid.head()</span><br><span class="line">from pyspark.sql import Window</span><br><span class="line">windowval &#x3D; Window.partitionBy(&quot;userId&quot;).orderBy(desc(&quot;ts&quot;)).rangeBetween(Window.unboundedPreceding, 0)</span><br><span class="line">user_log_valid &#x3D; user_log_valid.withColumn(&quot;phase&quot;, Fsum(&quot;downgraded&quot;).over(windowval))</span><br><span class="line">user_log_valid.select([&quot;userId&quot;, &quot;firstname&quot;, &quot;ts&quot;, &quot;page&quot;, &quot;level&quot;, &quot;phase&quot;]).where(user_log.userId &#x3D;&#x3D; &quot;1138&quot;).sort(&quot;ts&quot;).collect()</span><br></pre></td></tr></table></figure>
<h2 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from pyspark.sql import SparkSession</span><br><span class="line">from pyspark.sql.functions import udf</span><br><span class="line">from pyspark.sql.types import StringType</span><br><span class="line">from pyspark.sql.types import IntegerType</span><br><span class="line">from pyspark.sql.functions import desc</span><br><span class="line">from pyspark.sql.functions import asc</span><br><span class="line">from pyspark.sql.functions import sum as Fsum</span><br><span class="line"></span><br><span class="line">import datetime</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">%matplotlib inline</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">spark &#x3D; SparkSession \</span><br><span class="line">    .builder \</span><br><span class="line">    .appName(&quot;Data wrangling with Spark SQL&quot;) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line">path &#x3D; &quot;data&#x2F;sparkify_log_small.json&quot;</span><br><span class="line">user_log &#x3D; spark.read.json(path)</span><br><span class="line"></span><br><span class="line">user_log.take(1)</span><br><span class="line"># 下面的代码创建了一个临时视图，你可以使用该视图运行 SQL 查询</span><br><span class="line">user_log.createOrReplaceTempView(&quot;user_log_table&quot;)</span><br><span class="line">spark.sql(&quot;SELECT * FROM user_log_table LIMIT 2&quot;).show()</span><br><span class="line">spark.sql(&#39;&#39;&#39;</span><br><span class="line">          SELECT * </span><br><span class="line">          FROM user_log_table </span><br><span class="line">          LIMIT 2</span><br><span class="line">          &#39;&#39;&#39;</span><br><span class="line">          ).show()</span><br><span class="line">spark.sql(&#39;&#39;&#39;</span><br><span class="line">          SELECT COUNT(*) </span><br><span class="line">          FROM user_log_table </span><br><span class="line">          &#39;&#39;&#39;</span><br><span class="line">          ).show()</span><br><span class="line">spark.sql(&#39;&#39;&#39;</span><br><span class="line">          SELECT userID, firstname, page, song</span><br><span class="line">          FROM user_log_table </span><br><span class="line">          WHERE userID &#x3D;&#x3D; &#39;1046&#39;</span><br><span class="line">          &#39;&#39;&#39;</span><br><span class="line">          ).collect()</span><br><span class="line">spark.sql(&#39;&#39;&#39;</span><br><span class="line">          SELECT DISTINCT page</span><br><span class="line">          FROM user_log_table </span><br><span class="line">          ORDER BY page ASC</span><br><span class="line">          &#39;&#39;&#39;</span><br><span class="line">          ).show()</span><br><span class="line"></span><br><span class="line"># 自定义函数</span><br><span class="line">spark.udf.register(&quot;get_hour&quot;, lambda x: int(datetime.datetime.fromtimestamp(x &#x2F; 1000.0).hour))</span><br><span class="line">spark.sql(&#39;&#39;&#39;</span><br><span class="line">          SELECT *, get_hour(ts) AS hour</span><br><span class="line">          FROM user_log_table </span><br><span class="line">          LIMIT 1</span><br><span class="line">          &#39;&#39;&#39;</span><br><span class="line">          ).collect()</span><br><span class="line">songs_in_hour &#x3D; spark.sql(&#39;&#39;&#39;</span><br><span class="line">          SELECT get_hour(ts) AS hour, COUNT(*) as plays_per_hour</span><br><span class="line">          FROM user_log_table</span><br><span class="line">          WHERE page &#x3D; &quot;NextSong&quot;</span><br><span class="line">          GROUP BY hour</span><br><span class="line">          ORDER BY cast(hour as int) ASC</span><br><span class="line">          &#39;&#39;&#39;</span><br><span class="line">          )</span><br><span class="line">songs_in_hour.show()</span><br><span class="line"># 用 Pandas 转换数据</span><br><span class="line">songs_in_hour_pd &#x3D; songs_in_hour.toPandas()</span><br><span class="line">print(songs_in_hour_pd)</span><br></pre></td></tr></table></figure>
<h2 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h2><blockquote>
<p>参考：<br><a href="https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html" target="_blank" rel="noopener">https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html</a><br><a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/rdd-programming-guide.html</a></p>
</blockquote>
<p>不管使用Pyspark还是其他语言，Spark的底层都会通过Catalyst转成执行DAG序列：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/spark1.png" alt="图片"></p>
<p>DAG在底层使用RDD对象进行操作。</p>
<h1 id="Spark中的机器学习"><a href="#Spark中的机器学习" class="headerlink" title="Spark中的机器学习"></a>Spark中的机器学习</h1><h2 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from pyspark.sql import SparkSession</span><br><span class="line">from pyspark.ml.feature import RegexTokenizer, VectorAssembler, Normalizer, StandardScaler</span><br><span class="line">from pyspark.sql.functions import udf</span><br><span class="line">from pyspark.sql.types import IntegerType</span><br><span class="line"></span><br><span class="line">import re</span><br><span class="line"># create a SparkSession: note this step was left out of the screencast</span><br><span class="line">spark &#x3D; SparkSession.builder \</span><br><span class="line">    .master(&quot;local&quot;) \</span><br><span class="line">    .appName(&quot;Word Count&quot;) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line"># 如何读取数据集</span><br><span class="line">stack_overflow_data &#x3D; &#39;Train_onetag_small.json&#39;</span><br><span class="line">df &#x3D; spark.read.json(stack_overflow_data)</span><br><span class="line">df.head()</span><br><span class="line"># 把字符串分为单独的单词。Spark有一个[Tokenizer]（https:&#x2F;&#x2F;spark.apache.org&#x2F;docs&#x2F;latest&#x2F;ml-features.html#tokenizer） 类以及RegexTokenizer。 后者在分词时有更大的自由度。</span><br><span class="line"># split the body text into separate words</span><br><span class="line">regexTokenizer &#x3D; RegexTokenizer(inputCol&#x3D;&quot;Body&quot;, outputCol&#x3D;&quot;words&quot;, pattern&#x3D;&quot;\\W&quot;)</span><br><span class="line">df &#x3D; regexTokenizer.transform(df)</span><br><span class="line">df.head()</span><br><span class="line"># count the number of words in each body tag</span><br><span class="line">body_length &#x3D; udf(lambda x: len(x), IntegerType())</span><br><span class="line">df &#x3D; df.withColumn(&quot;BodyLength&quot;, body_length(df.words))</span><br><span class="line"># count the number of paragraphs and links in each body tag</span><br><span class="line">number_of_paragraphs &#x3D; udf(lambda x: len(re.findall(&quot;&lt;&#x2F;p&gt;&quot;, x)), IntegerType())</span><br><span class="line">number_of_links &#x3D; udf(lambda x: len(re.findall(&quot;&lt;&#x2F;a&gt;&quot;, x)), IntegerType())</span><br><span class="line">df &#x3D; df.withColumn(&quot;NumParagraphs&quot;, number_of_paragraphs(df.Body))</span><br><span class="line">df &#x3D; df.withColumn(&quot;NumLinks&quot;, number_of_links(df.Body))</span><br><span class="line">df.head(2)</span><br><span class="line"># 将内容长度，段落数和内容中的链接数合并为一个向量</span><br><span class="line">assembler &#x3D; VectorAssembler(inputCols&#x3D;[&quot;BodyLength&quot;, &quot;NumParagraphs&quot;, &quot;NumLinks&quot;], outputCol&#x3D;&quot;NumFeatures&quot;)</span><br><span class="line">df &#x3D; assembler.transform(df)</span><br><span class="line">df.head()</span><br><span class="line"># 归一化向量</span><br><span class="line">scaler &#x3D; Normalizer(inputCol&#x3D;&quot;NumFeatures&quot;, outputCol&#x3D;&quot;ScaledNumFeatures&quot;)</span><br><span class="line">df &#x3D; scaler.transform(df)</span><br><span class="line">df.head(2)</span><br><span class="line"># 缩放向量</span><br><span class="line">scaler2 &#x3D; StandardScaler(inputCol&#x3D;&quot;NumFeatures&quot;, outputCol&#x3D;&quot;ScaledNumFeatures2&quot;, withStd&#x3D;True)</span><br><span class="line">scalerModel &#x3D; scaler2.fit(df)</span><br><span class="line">df &#x3D; scalerModel.transform(df)</span><br><span class="line">df.head(2)</span><br></pre></td></tr></table></figure>
<h2 id="文本特征"><a href="#文本特征" class="headerlink" title="文本特征"></a>文本特征</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from pyspark.sql import SparkSession</span><br><span class="line">from pyspark.ml.feature import RegexTokenizer, CountVectorizer, \</span><br><span class="line">    IDF, StringIndexer</span><br><span class="line">from pyspark.sql.functions import udf</span><br><span class="line">from pyspark.sql.types import IntegerType</span><br><span class="line"></span><br><span class="line">import re</span><br><span class="line"># create a SparkSession: note this step was left out of the screencast</span><br><span class="line">spark &#x3D; SparkSession.builder \</span><br><span class="line">    .master(&quot;local&quot;) \</span><br><span class="line">    .appName(&quot;Word Count&quot;) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line"># 如何读取数据集</span><br><span class="line">stack_overflow_data &#x3D; &#39;Train_onetag_small.json&#39;</span><br><span class="line">df &#x3D; spark.read.json(stack_overflow_data)</span><br><span class="line">df.head()</span><br><span class="line"></span><br><span class="line"># 分词将字符串拆分为单独的单词。Spark 有一个[Tokenizer] （https:&#x2F;&#x2F;spark.apache.org&#x2F;docs&#x2F;latest&#x2F;ml-features.html#tokenizer） 类和RegexTokenizer。后者在分词时有更大的自由度 。</span><br><span class="line"># split the body text into separate words</span><br><span class="line">regexTokenizer &#x3D; RegexTokenizer(inputCol&#x3D;&quot;Body&quot;, outputCol&#x3D;&quot;words&quot;, pattern&#x3D;&quot;\\W&quot;)</span><br><span class="line">df &#x3D; regexTokenizer.transform(df)</span><br><span class="line">df.head()</span><br><span class="line"></span><br><span class="line"># CountVectorizer</span><br><span class="line"># find the term frequencies of the words</span><br><span class="line">cv &#x3D; CountVectorizer(inputCol&#x3D;&quot;words&quot;, outputCol&#x3D;&quot;TF&quot;, vocabSize&#x3D;1000)</span><br><span class="line">cvmodel &#x3D; cv.fit(df)</span><br><span class="line">df &#x3D; cvmodel.transform(df)</span><br><span class="line">df.take(1)</span><br><span class="line"># show the vocabulary in order of </span><br><span class="line">cvmodel.vocabulary</span><br><span class="line"># show the last 10 terms in the vocabulary</span><br><span class="line">cvmodel.vocabulary[-10:]</span><br><span class="line"></span><br><span class="line"># 逆文本频率指数（Inter-document Frequency ）</span><br><span class="line">idf &#x3D; IDF(inputCol&#x3D;&quot;TF&quot;, outputCol&#x3D;&quot;TFIDF&quot;)</span><br><span class="line">idfModel &#x3D; idf.fit(df)</span><br><span class="line">df &#x3D; idfModel.transform(df)</span><br><span class="line">df.head()</span><br><span class="line"></span><br><span class="line"># StringIndexer</span><br><span class="line">indexer &#x3D; StringIndexer(inputCol&#x3D;&quot;oneTag&quot;, outputCol&#x3D;&quot;label&quot;)</span><br><span class="line">df &#x3D; indexer.fit(df).transform(df)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>一首小诗：做最好的自己</title>
    <url>/2020/02/16/%E9%9A%8F%E6%84%9F%E4%B8%80%E7%AF%87/</url>
    <content><![CDATA[<p>今天看一个纪录片《人生第一次》时听到的小诗，来自美国诗人、短片小说作家——道格拉斯·马拉赫。</p>
<a id="more"></a>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=475218187&auto=1&height=66"></iframe>

<h4 id="如果你不能成为山顶上的高松，"><a href="#如果你不能成为山顶上的高松，" class="headerlink" title="如果你不能成为山顶上的高松，"></a>如果你不能成为山顶上的高松，</h4><h4 id="那就当棵山谷里的小树吧，"><a href="#那就当棵山谷里的小树吧，" class="headerlink" title="那就当棵山谷里的小树吧，"></a>那就当棵山谷里的小树吧，</h4><h4 id="但要当棵溪边最好的小树。"><a href="#但要当棵溪边最好的小树。" class="headerlink" title="但要当棵溪边最好的小树。"></a>但要当棵溪边最好的小树。</h4><p><br/></p>
<h4 id="如果你不能成为一棵大树，"><a href="#如果你不能成为一棵大树，" class="headerlink" title="如果你不能成为一棵大树，"></a>如果你不能成为一棵大树，</h4><h4 id="那就当丛小灌木；"><a href="#那就当丛小灌木；" class="headerlink" title="那就当丛小灌木；"></a>那就当丛小灌木；</h4><h4 id="如果你不能成为一丛小灌木，"><a href="#如果你不能成为一丛小灌木，" class="headerlink" title="如果你不能成为一丛小灌木，"></a>如果你不能成为一丛小灌木，</h4><h4 id="那就当一片小草地。"><a href="#那就当一片小草地。" class="headerlink" title="那就当一片小草地。"></a>那就当一片小草地。</h4><p><br/></p>
<h4 id="如果你不能是一只香獐，"><a href="#如果你不能是一只香獐，" class="headerlink" title="如果你不能是一只香獐，"></a>如果你不能是一只香獐，</h4><h4 id="那就当尾小鲈鱼，"><a href="#那就当尾小鲈鱼，" class="headerlink" title="那就当尾小鲈鱼，"></a>那就当尾小鲈鱼，</h4><h4 id="但要当湖里最活泼的小鲈鱼。"><a href="#但要当湖里最活泼的小鲈鱼。" class="headerlink" title="但要当湖里最活泼的小鲈鱼。"></a>但要当湖里最活泼的小鲈鱼。</h4><p><br/></p>
<h4 id="我们不能全是船长，"><a href="#我们不能全是船长，" class="headerlink" title="我们不能全是船长，"></a>我们不能全是船长，</h4><h4 id="必须有人也是水手。"><a href="#必须有人也是水手。" class="headerlink" title="必须有人也是水手。"></a>必须有人也是水手。</h4><p><br/></p>
<h4 id="这里有许多事让我们去做，"><a href="#这里有许多事让我们去做，" class="headerlink" title="这里有许多事让我们去做，"></a>这里有许多事让我们去做，</h4><h4 id="有大事，有小事，"><a href="#有大事，有小事，" class="headerlink" title="有大事，有小事，"></a>有大事，有小事，</h4><h4 id="但最重要的是我们身旁的事。"><a href="#但最重要的是我们身旁的事。" class="headerlink" title="但最重要的是我们身旁的事。"></a>但最重要的是我们身旁的事。</h4><p><br/></p>
<h4 id="如果你不能成为大道，"><a href="#如果你不能成为大道，" class="headerlink" title="如果你不能成为大道，"></a>如果你不能成为大道，</h4><h4 id="那就当一条小路，"><a href="#那就当一条小路，" class="headerlink" title="那就当一条小路，"></a>那就当一条小路，</h4><h4 id="如果你不能成为太阳，"><a href="#如果你不能成为太阳，" class="headerlink" title="如果你不能成为太阳，"></a>如果你不能成为太阳，</h4><h4 id="那就当一颗星星。"><a href="#那就当一颗星星。" class="headerlink" title="那就当一颗星星。"></a>那就当一颗星星。</h4><p><br/></p>
<h4 id="决定成败的不是你的身材"><a href="#决定成败的不是你的身材" class="headerlink" title="决定成败的不是你的身材"></a>决定成败的不是你的身材</h4><h4 id="而是做一个最好的你。"><a href="#而是做一个最好的你。" class="headerlink" title="而是做一个最好的你。"></a>而是做一个最好的你。</h4><p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/suigan1.png" alt="图片"></p>
]]></content>
      <tags>
        <tag>小诗</tag>
      </tags>
  </entry>
  <entry>
    <title>机器翻译检测</title>
    <url>/2020/02/13/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A3%80%E6%B5%8B/</url>
    <content><![CDATA[<p>因为本周要做一个机器翻译检测的任务，因此搜到了几篇论文，看一下大概有哪些思路。论文基本上只简单扫了一眼，简单介绍一下其中的3篇。</p>
<a id="more"></a>
<blockquote>
<p>参考：</p>
<p>Machine Translation Detection from Monolingual Web-Text</p>
<p>Automatic Detection of Machine Translated Text and Translation Quality</p>
<p>Detecting Machine-Translated Paragraphs by Matching Similar Words</p>
<p>Automatic Detection of Translated Text and its Impact on Machine Translation</p>
<p>BLEU: a Method for Automatic Evaluation of Machine Translation</p>
<p>Building a Web-based parallel corpus and filtering out machinetranslated text</p>
<p>Machine Translation Detection from MonolingualWeb-Text</p>
<p>Machine Translationness: a Concept for Machine Translation Evaluation and Detection</p>
<p>MT Detection in Web-Scraped Parallel Corpora</p>
<p>On the Features of Translationese</p>
<p>Translationese and Its Dialects</p>
</blockquote>
<h2 id="Machine-Translation-Detection-from-Monolingual-Web-Text"><a href="#Machine-Translation-Detection-from-Monolingual-Web-Text" class="headerlink" title="Machine Translation Detection from Monolingual Web-Text"></a>Machine Translation Detection from Monolingual Web-Text</h2><p>首先强调的是，这篇论文检测的是SMT机器翻译。看到论文摘要时我想到，针对不同的机器翻译模型，检测的机制也是不一样的，要有这点意识。这篇论文关注到的是SMT系统中“phrase salad”现象，并使用单语语料就可以达到95.8%的准确率。</p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul>
<li>SMT翻译中的‘phrase salad’现象，指的是翻译结果的每个短语单独拿出来是对的，但组合到一起是错的</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mtcheck1.png" alt="图片"></p>
<ul>
<li>比如上面这个例子，not only后面应该有but also，但这个短语在SMT翻译系统里只有一半被翻译了</li>
<li>使用了一个分类器对句子是否是‘phrase salad’进行检测，使用到的特征包括两个，一个是语言模型，另外是一些人们常用但对SMT来说难以生成的短语</li>
</ul>
<h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><p>基于SMT翻译的特点，在特征选择时主要考虑3点：句子流畅度、语法正确度、短语完整度。从人工翻译提取到的特征表达了它和人工产生句子的相似性，从机器翻译中提取到的特征表达了它和机器翻译句子的相似性。</p>
<h3 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h3><ul>
<li>Fluency Feature<ul>
<li>使用两个语言模型，f(w,H)和f(w,MT)，前者表示人工翻译的语言模型，后者是机器翻译的语言模型</li>
</ul>
</li>
<li>Grammaticality Feature<ul>
<li>使用POS语言模型，f(pos,H)和f(pos, MT)，前者表示人工翻译的POS序列的语言模型，后者是机器翻译的</li>
<li>对提取出的function word的POS语言模型，f(fw, H)和f(fw, MT)<ul>
<li>function word: <ul>
<li>Prepositions: of, at, in, without, between</li>
<li>Pro<a href="https://www.baidu.com/s?wd=no&amp;tn=SE_PcZhidaonwhc_ngpagmjz&amp;rsv_dl=gh_pc_zhidao" target="_blank" rel="noopener">no</a>uns: he, they, anybody, it, one</li>
<li>Determiners: the, a, that, my, more, much, either,neither</li>
<li>Conjunctions: and, that, when, while, although, or</li>
<li>Auxiliary verbs: be (is, am, are), have, got, do</li>
<li>Particles: <a href="https://www.baidu.com/s?wd=no&amp;tn=SE_PcZhidaonwhc_ngpagmjz&amp;rsv_dl=gh_pc_zhidao" target="_blank" rel="noopener">no</a>, <a href="https://www.baidu.com/s?wd=no&amp;tn=SE_PcZhidaonwhc_ngpagmjz&amp;rsv_dl=gh_pc_zhidao" target="_blank" rel="noopener">no</a>t, nor, as</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Gappy-Phrase Feature<ul>
<li>中间有间隔的短语：如not only * but also</li>
<li>使用character级别的LM衡量</li>
<li>Sequential Pattern Mining<ul>
<li>使用sequential pattern挖掘的方法找到所有Gappy-Phrase</li>
</ul>
</li>
<li>使用的信息增益进行的短语选择，但是没看懂是如何计算特征的</li>
</ul>
</li>
<li>最后使用SVM进行分类</li>
<li>其他可考虑的feature：<ul>
<li>Translationese and its dialects论文</li>
<li>On the features of translationese论文（比较学术）</li>
<li>average token length</li>
<li>type-token ratio</li>
</ul>
</li>
</ul>
<h2 id="Automatic-Detection-of-Machine-Translated-Text-and-Translation-Quality"><a href="#Automatic-Detection-of-Machine-Translated-Text-and-Translation-Quality" class="headerlink" title="Automatic Detection of Machine Translated Text and Translation Quality"></a>Automatic Detection of Machine Translated Text and Translation Quality</h2><p>这篇论文也是使用的单语语料训练的分类器，用来进行机器翻译的检测和翻译质量的评估。这篇文章的重点强调的是，通过分类器检测的方法，只能对质量特别差的翻译系统起作用，如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mtcheck2.png" alt="图片"></p>
<p>可以看出来，翻译系统越好，检测的准确率就越低。</p>
<h3 id="特征选择-1"><a href="#特征选择-1" class="headerlink" title="特征选择"></a>特征选择</h3><ul>
<li>本文使用的特征都是二元特征，即是否存在POS ngram和467个function word。</li>
<li>从4个方面构建特征：word、lemma、pos、mixed</li>
</ul>
<h2 id="Detecting-Machine-Translated-Paragraphs-by-Matching-Similar-Words"><a href="#Detecting-Machine-Translated-Paragraphs-by-Matching-Similar-Words" class="headerlink" title="Detecting Machine-Translated Paragraphs by Matching Similar Words"></a>Detecting Machine-Translated Paragraphs by Matching Similar Words</h2><p>这篇文章主要是检查段落级别的机器翻译，方法是通过计算word的match情况，和段落的coherence来检测机器翻译，整体流程如下：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mtcheck3.png" alt="图片"></p>
<h3 id="计算相似词"><a href="#计算相似词" class="headerlink" title="计算相似词"></a>计算相似词</h3><p>先把段落内所有词打上POS标签，依次计算和其他词的相似度（如果POS相同则保留）。能够看出人工翻译的整体相似度比较低，机器翻译的相似度高一些，如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mtcheck4.png" alt="图片"></p>
<h3 id="计算Coherence"><a href="#计算Coherence" class="headerlink" title="计算Coherence"></a>计算Coherence</h3><p>基于POS对统计根据的均值和方差。如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mtcheck5.png" alt="图片"></p>
<h3 id="进行分类"><a href="#进行分类" class="headerlink" title="进行分类"></a>进行分类</h3><p>使用SVM分类。</p>
]]></content>
      <tags>
        <tag>机器翻译</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：《Solving and Generating Chinese Character Riddles》</title>
    <url>/2020/02/11/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E3%80%8ASolving-and-Generating-Chinese-Character-Riddles%E3%80%8B/</url>
    <content><![CDATA[<p>之前想给语音助手增加一个猜字谜的功能，这两天不忙就读了一下这篇机器解谜语的论文。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://www.aclweb.org/anthology/D16-1081.pdf" target="_blank" rel="noopener">https://www.aclweb.org/anthology/D16-1081.pdf</a></p>
</blockquote>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>解谜语类似于下面的过程：</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/solgene1.png" alt="图片"></p>
<ul>
<li>解谜的pipeline<ul>
<li>解题过程<ul>
<li>学习谜语中的短语和部首的对齐关系</li>
<li>学习谜语和rule的关系</li>
<li>使用上面两个关系，用算法得到候选答案</li>
<li>使用Ranking-SVM来对candidate进行排序</li>
</ul>
</li>
<li>生成谜语过程<ul>
<li>使用模版方法</li>
<li>使用替代的方法</li>
<li>使用Ranking-SVM来对candidate进行排序</li>
</ul>
</li>
<li>整体过程如下图：</li>
</ul>
</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/solgene2.png" alt="图片"></p>
<h2 id="Phrase-Radical-Alignments-and-Rules"><a href="#Phrase-Radical-Alignments-and-Rules" class="headerlink" title="Phrase-Radical Alignments and Rules"></a>Phrase-Radical Alignments and Rules</h2><h3 id="Alignment"><a href="#Alignment" class="headerlink" title="Alignment"></a>Alignment</h3><ul>
<li>希望将“千里”和“马”进行对齐</li>
<li>方法一<ul>
<li>将谜语分词$\left(w_{1}, w_{2}, \ldots, w_{n}\right)$</li>
<li>将答案分成不同部首$\left(r_{1}, r_{2}, \ldots, r_{m}\right)$</li>
<li>统计对齐$\left(\left[w_{i}, w_{j}\right], r_{k}\right)(i, j \in[1, n], k \in[1, m])$</li>
</ul>
</li>
<li>方法二<ul>
<li>谜语中两个连续字符$\left(w_{1}, w_{2}\right)$，如果w1是w2的部首，且w2的其余部分r出现在答案q中，则$\left(\left(w_{1}, w_{2}\right), r\right)$是一个对齐</li>
</ul>
</li>
<li>统计所有的对齐，并过滤掉出现频次小于3的</li>
<li>特别常见的对齐如下图：</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/solgne3.png" alt="图片"></p>
<h3 id="Rules"><a href="#Rules" class="headerlink" title="Rules"></a>Rules</h3><ul>
<li>总结了6类规则</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/solgne4.png" alt="图片"></p>
<ul>
<li>对于$\left(\left[w_{1}, w_{n}\right], r\right)$，如果r是wi的部首，则$\left(w_{1}, \dots, w_{i-1},(.), w_{i+1}, \dots, w_{n}\right)$就是一个潜在的规则，我们从数据中最终总结193条规则，归纳为上面6类</li>
<li>1000个汉字有至少1个alignment，27个汉字有至少100个alignment</li>
</ul>
<h2 id="Riddle-Solving-and-Generation"><a href="#Riddle-Solving-and-Generation" class="headerlink" title="Riddle Solving and Generation"></a>Riddle Solving and Generation</h2><h3 id="Solving-Chinese-Character-Riddles"><a href="#Solving-Chinese-Character-Riddles" class="headerlink" title="Solving Chinese Character Riddles"></a>Solving Chinese Character Riddles</h3><ul>
<li>解谜算法的伪代码如下</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/solgne5.png" alt="图片"></p>
<ul>
<li>以“上岗必戴安全帽”为例，“上岗”通过规则“上(up) (.)”和山对齐，“必” 和 “戴”跟自己对齐，“安全帽”因为analogical shape和“宀”对齐，最终得到结果“密”</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/solgene6.png" alt="图片"></p>
<ul>
<li>对答案进行排序，排序时使用的特征如下：</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/solgene7.png" alt="图片"></p>
<h3 id="Generating-Chinese-Character-Riddles"><a href="#Generating-Chinese-Character-Riddles" class="headerlink" title="Generating Chinese Character Riddles"></a>Generating Chinese Character Riddles</h3><ul>
<li>基于模板的方法</li>
<li>基于替换的方法</li>
<li>对候选的description进行排序，排序使用的特征如下：</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/solgene8.png" alt="图片"></p>
<h3 id="Ranking-Model"><a href="#Ranking-Model" class="headerlink" title="Ranking Model"></a>Ranking Model</h3><ul>
<li>score的计算：$\text { Score }(c)=\sum_{i=1}^{m} \lambda_{i} * g_{i}(c)$，其中c表示一个候选，gi(c)表示c的第i个特征，m是特征的总数，$\lambda_{i}​$表示特征的权重</li>
<li>使用Ranking SVM算法求解特征权重参数</li>
</ul>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><ul>
<li>数据：从网络上爬取的7w+谜语，3k+的笔画，古代诗词和对联等用于训练语言模型</li>
<li>使用准确率评价解谜的效果，使用人工评测来评价生成谜题</li>
</ul>
]]></content>
      <tags>
        <tag>论文阅读</tag>
        <tag>机器猜字谜</tag>
      </tags>
  </entry>
  <entry>
    <title>论文粗读:《Generating Sentences by Editing Prototypes》</title>
    <url>/2020/02/11/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94%E3%80%8AGenerating-Sentences-by-Editing-Prototypes%E3%80%8B/</url>
    <content><![CDATA[<p>因为想做一个根据不同年龄段的人生成不同故事内容的demo，所以阅读了这篇文本风格转换的论文。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://arxiv.org/abs/1709.08878" target="_blank" rel="noopener">https://arxiv.org/abs/1709.08878</a><br><a href="https://github.com/kelvinguu/neural-editor" target="_blank" rel="noopener">https://github.com/kelvinguu/neural-editor</a></p>
</blockquote>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h3><ul>
<li>通过从训练集中挑选一个prototype sentence，产生一个能捕捉到句子相似度等句子级别信息的latent edit vector，并通过这个向量生成句子</li>
<li>模型的整体框架如下图，这个模型的由来是基于人们的一个经验，通常人们写一个复杂的句子时，都是根据一个简单的句子，逐步修改而来的</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/gsep1.png" alt="图片"></p>
<ul>
<li>目标函数：最大化生成模型的log likelihood</li>
<li>使用locality sensitive hashing寻找相似的句子</li>
</ul>
<h2 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h2><h3 id="解决问题分两步"><a href="#解决问题分两步" class="headerlink" title="解决问题分两步"></a>解决问题分两步</h3><ul>
<li>从语料库里选择一句话<ul>
<li>prototype distribution: p(x’) （uniform over X）</li>
<li>以p(x’)的概率从语料库中随机选择一个prototype sentence</li>
</ul>
</li>
<li>把这句话进行修改<ul>
<li>以edit prior: p(z)的概率sample出一个edit vector: z （实际是对edit type进行编码）</li>
<li>将z和x’送入到$p_{\text {edit }}\left(x | x^{\prime}, z\right)$的神经网络中，产生新的句子x</li>
</ul>
</li>
</ul>
<p>整体公式如下：</p>
<p>$p(x)=\sum_{x^{\prime} \in \mathcal{X}} p\left(x | x^{\prime}\right) p\left(x^{\prime}\right)$</p>
<p>$p\left(x | x^{\prime}\right)=\mathbb{E}_{z \sim p(z)}\left[p_{\mathrm{edit}}\left(x | x^{\prime}, z\right)\right]$</p>
<h3 id="模型满足两个条件"><a href="#模型满足两个条件" class="headerlink" title="模型满足两个条件"></a>模型满足两个条件</h3><ul>
<li>Semantic smoothness：一次edit只能对文本进行小改动；多次edit可以产生大的改动</li>
<li>Consistent edit behavior：对edit有类型的控制，对不同句子同一类型的edit应该产生相似的效果</li>
</ul>
<h2 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h2><h3 id="对p-x-进行近似"><a href="#对p-x-进行近似" class="headerlink" title="对p(x)进行近似"></a>对p(x)进行近似</h3><ul>
<li>大多数的prototype x都是不相关的，即p(x|x’)非常小；因此我们只考虑和x有非常高的lexical overlap的prototype x’</li>
<li>定义一个lexical similarity neighborhor $\mathcal{N}(x) \stackrel{\text { def }}{=}\left\{x^{\prime} \in \mathcal{X}: d_{J}\left(x, x^{\prime}\right)&lt;0.5\right\}$，其中dj是x和x’的Jaccard距离</li>
<li>利用neighborhood prototypes和Jensen不等式求解</li>
</ul>
<p>$\begin{aligned} \log p(x) &amp;=\log \left[\sum_{x^{\prime} \in \mathcal{X}} p\left(x | x^{\prime}\right) p\left(x^{\prime}\right)\right] \ &amp; \geq \log \left[\sum_{x^{\prime} \in \mathcal{N}(x)} p\left(x | x^{\prime}\right) p\left(x^{\prime}\right)\right] \end{aligned}$</p>
<p>$\begin{array}{l}{=\log \left[|\mathcal{N}(x)|^{-1} \sum_{x^{\prime} \in \mathcal{N}(x)} p\left(x | x^{\prime}\right)\right]+R(x)} \ {\geq|\mathcal{N}(x)|^{-1} \sum_{x^{\prime} \in \mathcal{N}(x)} \log p\left(x | x^{\prime}\right)+R(x)} \ {\underbrace{x^{\prime} \in \mathcal{N}(x)}_{\text {def }_{\mathrm{LEX}}(x)}+R(x)}\end{array}$</p>
<ul>
<li>$\begin{array}{l}{p\left(x^{\prime}\right)=1 /|\mathcal{X}|} \ {\mathrm{R}(\mathrm{x})=\log (|\mathcal{N}(x)| /|\mathcal{X}|)}\end{array}$</li>
<li>$|\mathcal{N}(x)|​$是跟x相关的常数，x的邻居使用locality sensitive hashing (LSH) and minhashing进行预先的计算</li>
</ul>
<h3 id="对log-p-x-x’-进行近似"><a href="#对log-p-x-x’-进行近似" class="headerlink" title="对log p(x|x’)进行近似"></a>对log p(x|x’)进行近似</h3><ul>
<li><p>使用蒙特卡洛对$z \sim p(z)​$进行采样时可能会产生比较高的方差，因为$p_{\text {edit }}\left(x | x^{\prime}, z\right)​$对于大部分从p(z) sample出来的z来说都输出0，只对一部分不常见的值输出较大的值</p>
</li>
<li><p>使用inverse neural editor：$q\left(z | x^{\prime}, x\right)$</p>
<ul>
<li>对prototype x’和修正后的句子x，生成一个x’到x的转换的edit vector z，这个z在重要的值上会有较大的概率</li>
</ul>
</li>
<li><p>使用evidence lower bound（ELBO）来计算log p(x|x’)</p>
<p>$\begin{aligned} \log p\left(x | x^{\prime}\right) \geq &amp; \underbrace{\mathbb{E}_{z \sim q\left(z | x^{\prime}, x\right)}\left[\log p_{\text {edit }}\left(x | x^{\prime}, z\right)\right]}_{\mathcal{L}_{\text {gen }}} \ &amp;-\underbrace{\operatorname{KL}\left(q\left(z | x^{\prime}, x\right) | p(z)\right)}_{\mathcal{E}_{\mathrm{KL}}^{L}} \ \stackrel{\text { def }}{=} \operatorname{ELBO}\left(x, x^{\prime}\right) \end{aligned}$</p>
</li>
<li><p>q(z|x’,x)可以看成是VAE的encoder，pedit(x|x’,z)可以看成是VAE的decoder</p>
</li>
</ul>
<h3 id="目标函数-sum-x-prime-in-mathcal-N-x-operatorname-ELBO-left-x-x-prime-right"><a href="#目标函数-sum-x-prime-in-mathcal-N-x-operatorname-ELBO-left-x-x-prime-right" class="headerlink" title="目标函数 $\sum_{x^{\prime} \in \mathcal{N}(x)} \operatorname{ELBO}\left(x, x^{\prime}\right)$"></a>目标函数 $\sum_{x^{\prime} \in \mathcal{N}(x)} \operatorname{ELBO}\left(x, x^{\prime}\right)$</h3><ul>
<li>参数：$\Theta=\left(\Theta_{p}, \Theta_{q}\right)$，包含neural editor的参数和inverse neural editor的参数</li>
</ul>
<h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><h3 id="Neural-editor-p-text-edit-left-x-x-prime-z-right"><a href="#Neural-editor-p-text-edit-left-x-x-prime-z-right" class="headerlink" title="Neural editor $p_{\text {edit }}\left(x | x^{\prime}, z\right)$"></a>Neural editor $p_{\text {edit }}\left(x | x^{\prime}, z\right)$</h3><ul>
<li>input: prototype x’</li>
<li>output: revised sentence x</li>
<li>seq2seq<ul>
<li>encoder: 3层双向LSTM，使用Glove词向量初始化</li>
<li>decoder: 3层包含attention的LSTM<ul>
<li>最上面一层的hidden state用来和encoder输出的hidden state一起算attention</li>
<li>将attention向量和z向量concate一起，再送入softmax中</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Edit-prior-p-z"><a href="#Edit-prior-p-z" class="headerlink" title="Edit prior $p(z)$"></a>Edit prior $p(z)$</h3><ul>
<li>edit vector z的sample方法<ul>
<li>先采样其scalar length：$z_{\text {norm }} \sim  \operatorname{Unif}(0,10)​$</li>
<li>再采样其direction:  在uniform distribution中采样一个zdir向量</li>
<li>$z=z_{\text {norm }} \cdot z_{\text {dir }}$</li>
<li>这样做是为了方便计算KL散度</li>
</ul>
</li>
</ul>
<h3 id="Inverse-neural-editor-q-left-z-x-prime-x-right"><a href="#Inverse-neural-editor-q-left-z-x-prime-x-right" class="headerlink" title="Inverse neural editor $q\left(z | x^{\prime}, x\right)$"></a>Inverse neural editor $q\left(z | x^{\prime}, x\right)$</h3><ul>
<li>假设x’和x只差了一个word，那么edit vector z跟word的词向量应该是一样的，那么多个word的插入就相当于多个word的词向量的和，删除同理</li>
<li>加入到x’的词的集合：$I=x \backslash x^{\prime}​$</li>
<li>从x’中删除的词的集合：$D=x^{\prime}\backslash  x$</li>
<li>x’和x的差异：$f\left(x, x^{\prime}\right)=\sum_{w \in I} \Phi(w) \oplus \sum_{w \in D} \Phi(w)$<ul>
<li>$\Phi(w)$表示w的词向量，它同时也是inverse neural editor q的参数，使用300维的Glove向量初始化</li>
<li>$\oplus$表示concate操作</li>
</ul>
</li>
<li>认为q是在f的基础上加入噪声获得的（先旋转，在rescale）：</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/gsep2.png" alt="图片"></p>
<ul>
<li>$\begin{array}{l}{f_{\text {norm }}=|f|} \ {f_{\text {dir }}=f / f_{\text {norm }}}\end{array}$</li>
<li>$\operatorname{vMF}(v ; \mu, \kappa)​$表示点v的unit空间中的vMF分布，参数包含mean vector$\mu​$和concentration parameter $ \kappa​$</li>
<li><p>因此可得：$\begin{aligned} q\left(z_{\text {dir }} | x^{\prime}, x\right) &amp;=\operatorname{vMF}\left(z_{\text {dir }} ; f_{\text {dir }}, \kappa\right) \ q\left(z_{\text {norm }} | x^{\prime}, x\right) &amp;=\text { Unif }\left(z_{\text {norn }} ;\left[\tilde{f}_{\text {norn }}, \tilde{f}_{\text {nom }}+\epsilon\right]\right) \end{aligned}$</p>
</li>
<li><p>其中 $\tilde{f}_{\text {norm }}=\min \left(f_{\text {norm }}, 10-\epsilon\right)$</p>
</li>
<li>最终 $z=z_{\mathrm{dir}} \cdot z_{\mathrm{norm}}$</li>
</ul>
<h2 id="Details"><a href="#Details" class="headerlink" title="Details"></a>Details</h2><h3 id="计算目标函数的梯度-nabla-Theta-q-operatorname-ELBO-left-x-x-prime-right-nabla-Theta-q-mathcal-L-mathrm-gen-nabla-Theta-q-mathcal-L-mathrm-KL"><a href="#计算目标函数的梯度-nabla-Theta-q-operatorname-ELBO-left-x-x-prime-right-nabla-Theta-q-mathcal-L-mathrm-gen-nabla-Theta-q-mathcal-L-mathrm-KL" class="headerlink" title="计算目标函数的梯度$\nabla_{\Theta_{q}} \operatorname{ELBO}\left(x, x^{\prime}\right)=\nabla_{\Theta_{q}} \mathcal{L}_{\mathrm{gen}}-\nabla_{\Theta_{q}} \mathcal{L}_{\mathrm{KL}}$"></a>计算目标函数的梯度$\nabla_{\Theta_{q}} \operatorname{ELBO}\left(x, x^{\prime}\right)=\nabla_{\Theta_{q}} \mathcal{L}_{\mathrm{gen}}-\nabla_{\Theta_{q}} \mathcal{L}_{\mathrm{KL}}$</h3><ul>
<li>使用重参数计算：$\nabla_{\Theta_{q}} \mathcal{L}_{\mathrm{gen}}$<ul>
<li>将z~q(z|x’,x)重写为$z=h(\alpha)$</li>
<li>$\begin{aligned} \nabla \Theta_{q} \mathcal{L}_{\mathrm{gen}} &amp;=\nabla \Theta_{q} \mathbb{E}_{z \sim q\left(z | x^{\prime}, x\right)}\left[\log p_{\text {edit }}\left(x | x^{\prime}, z\right)\right] \ &amp;=\mathbb{E}_{\alpha \sim p(\alpha)}\left[\nabla \Theta_{q} \log p_{\text {edit }}\left(x | x^{\prime}, h(\alpha)\right)\right] \end{aligned}$</li>
</ul>
</li>
<li>计算$\nabla_{\Theta_{q}} \mathcal{L}_{\mathrm{KL}}$<ul>
<li>$\begin{aligned} \mathcal{L}_{\mathrm{KL}} &amp;=\mathrm{KL}\left(q\left(z_{\text {norm }} | x^{\prime}, x\right) | p\left(z_{\text {norm }}\right)\right) \ &amp;+\mathrm{KL}\left(q\left(z_{\text {dir }} | x^{\prime}, x\right) | p\left(z_{\text {dir }}\right)\right) \end{aligned}$</li>
<li>$\begin{array}{l}{\operatorname{KL}(\operatorname{vMF}(\mu, \kappa) | \operatorname{vMF}(\mu, 0))=\kappa \frac{I_{d / 2}(\kappa)+I_{d / 2-1}(\kappa) \frac{d-2}{2 \kappa}}{I_{d / 2-1}(\kappa)-\frac{d-2}{2 \kappa}}} \ {-\log \left(I_{d / 2-1}(\kappa)\right)-\log (\Gamma(d / 2))} \ {+\log (\kappa)(d / 2-1)-(d-2) \log (2) / 2}\end{array}$</li>
</ul>
</li>
</ul>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="Datsets"><a href="#Datsets" class="headerlink" title="Datsets"></a>Datsets</h3><ul>
<li>Yelp</li>
<li>One BillionWord Language Model Benchmark</li>
<li>使用Spacy将NER的词替换成其NER category</li>
<li>将出现频次小于10000的词用UNK替换</li>
</ul>
<h3 id="Generative-Modeling"><a href="#Generative-Modeling" class="headerlink" title="Generative Modeling"></a>Generative Modeling</h3><ul>
<li>对比几个生成模型的效果（KENLM语言模型、自回归语言模型）</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/gsep3.png" alt="图片"></p>
<ul>
<li>使用neural editor可以生成跟prototype很不一样的句子</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/gsep4.png" alt="图片"></p>
<h3 id="Human-Evaluation"><a href="#Human-Evaluation" class="headerlink" title="Human Evaluation"></a>Human Evaluation</h3><ul>
<li>降低softmax temperture有助于产生更符合语法的句子，但也会产生short and generic sentence<ul>
<li>从corpus中sample出prototype sentence可以增加生成句子的多样性，因此即便temperature设置为0，也不会影响句子多样子，这是比传统的NLM强的地方</li>
</ul>
</li>
<li>从grammaticality 和 plausibility两方面进行评测</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/gsep5.png" alt="图片"></p>
<h3 id="Semantics-of-NeuralEditor"><a href="#Semantics-of-NeuralEditor" class="headerlink" title="Semantics of NeuralEditor"></a>Semantics of NeuralEditor</h3><ul>
<li>跟sentence variational autoencoder (SVAE)模型对比，SVAE将句子映射到semantic空间向量，再从向量还原句子</li>
<li>semantic smoothness<ul>
<li>smoothness表示每一个小的edit只能对句子有一点点改变</li>
<li>我们先从corpus中选择一个prototype sentence，然后不断对它使用neural editor，并让人工对semantic的变化进行打分。</li>
<li>对比实验有两个，一个是SVAE，一个是从corpus中根据cosine相似度选择出的句子</li>
</ul>
</li>
<li>consistent edit behavior</li>
</ul>
]]></content>
      <tags>
        <tag>论文阅读</tag>
        <tag>文本风格迁移</tag>
      </tags>
  </entry>
  <entry>
    <title>Separable Convolution</title>
    <url>/2020/02/06/Separable-Convolution/</url>
    <content><![CDATA[<p>阅读论文《The Evolved Transformer》时遇到了separable convolution的概念，因此找了相关资料学习了一下。</p>
<blockquote>
<p>参考：<br><a href="https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-al" target="_blank" rel="noopener">https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-al</a></p>
</blockquote>
<a id="more"></a>
<p>在讲Separable Convolution前先了解下常用的卷积网络的定义。</p>
<h2 id="Convolutional-Neural-Networks"><a href="#Convolutional-Neural-Networks" class="headerlink" title="Convolutional Neural Networks"></a>Convolutional Neural Networks</h2><blockquote>
<p>参考：<br><a href="https://towardsdatascience.com/what-is-a-neural-network-6010edabde2b" target="_blank" rel="noopener">https://towardsdatascience.com/what-is-a-neural-network-6010edabde2b</a></p>
</blockquote>
<p>卷积网络中最重要的是卷积核，通过卷积核在图像每个区域的运算，得到图像不同的特征，如下图（可以在<a href="http://setosa.io/ev/image-kernels/" target="_blank" rel="noopener">http://setosa.io/ev/image-kernels/</a>中更好得体验）：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn1.png" alt="图片"></p>
<p>上面这张图使用outliine卷积核，实际中可以使用sharp等不同功能的卷积核以达到不同效果。</p>
<p>对于神经网络的每一层而言，可以使用多个卷积和得到不同的特征图，并将这些特征图一起输入到下一层网络。最终这些特征供给最后一层的分类器进行匹配，得到分类结果。下面的动画展示了这个过程：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn1.gif" alt="图片"></p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn2.gif" alt="图片"></p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn3.gif" alt="图片"></p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn4.gif" alt="图片"></p>
<p>Separable Convolution可以分成spatial separable convolution和depthwise separable convolution。</p>
<p>对于12x12x3的图像，5x5x3的卷积核，能产生8x8x1的输出：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn2.png" alt="图片"></p>
<p>假设我们想要8x8x256的输出，则需要使用256个卷积核来创造256个8x8x1的图像，把他们叠加在一起产生8x8x256的输出：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn3.png" alt="图片"></p>
<p>即12x12x3 — (5x5x3x256) — &gt;12x12x256</p>
<h2 id="Spatial-Separable-Convolutions"><a href="#Spatial-Separable-Convolutions" class="headerlink" title="Spatial Separable Convolutions"></a>Spatial Separable Convolutions</h2><p>Spatial separable convolution将卷积分成两部分，最常见的是把3x3的kernel分解成3x1和1x3的kernel，如：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn4.png" alt="图片"></p>
<p>通过这种方式，原本一次卷积要算9次乘法，现在只需要6次。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn5.png" alt="图片"></p>
<p>还有一个Sobel kernel（用来检测边）也是用的这种方法：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn6.png" alt="图片"></p>
<p>但spatial separable convolution存在的问题是，不是所有kernel都能转换成2个小的kernel。</p>
<h2 id="Depthwise-Separable-Convolutions"><a href="#Depthwise-Separable-Convolutions" class="headerlink" title="Depthwise Separable Convolutions"></a>Depthwise Separable Convolutions</h2><p>由于卷积并不使用矩阵相乘，为了减少计算量，可以将卷积的过程分成两部分：a depthwise convolution and a pointwise convolution. </p>
<h3 id="depthwise-convolution"><a href="#depthwise-convolution" class="headerlink" title="depthwise convolution"></a>depthwise convolution</h3><p>首先，我们使用3个5x5x1的卷积核产生8x8x3的图像：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn7.png" alt="图片"></p>
<h3 id="pointwise-convolution"><a href="#pointwise-convolution" class="headerlink" title="pointwise convolution"></a>pointwise convolution</h3><p>其次，使用1x1x3 的卷积核对每个像素计算，得到8x8x1 的图像：</p>
<p><img src="http://q503tsu73.bkt.clouddn.com/sc8.png?e=1580956603&amp;token=05Ii263bPN3Z-CT3JPRaRfWi5sXIj8pwX6V1bN2j:BxS4Xqtt2YsqJ5GJVFVlnK9D3xw=&amp;attname=" alt="图片"></p>
<p>使用256个1x1x3 的卷积核，则恶意产生8x8x256的图像：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn9.png" alt="图片"></p>
<p>可以看到，整个过程由原来的12x12x3 — (5x5x3x256) →12x12x256，变成12x12x3 — (5x5x1x1) — &gt; (1x1x3x256) — &gt;12x12x256</p>
<h2 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h2><p>主要就是减少了计算量，原先是256个5x5x3的卷积核移动8x8次，即需要256x3x5x5x8x8=1,228,800次乘法计算。使用depthwise convolution，有3个5x5x1的卷积核移动8x8次，需要3x5x5x8x8 = 4,800次乘法计算。使用pointwise convolution，有256个1x1x3的卷积核移动8x8次，需要256x1x1x3x8x8=49,152次乘法计算，加起来共有53,952次计算。</p>
<h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><ul>
<li>keras doc：<a href="https://keras.io/layers/convolutional/" target="_blank" rel="noopener">https://keras.io/layers/convolutional/</a></li>
<li><a href="https://github.com/alexandrosstergiou/keras-DepthwiseConv3D" target="_blank" rel="noopener">https://github.com/alexandrosstergiou/keras-DepthwiseConv3D</a></li>
<li>[<a href="https://github.com/sara-kassani/Depthwise-Separable-Convolutional-Neural-Network-for-Skin-Lesion-Classification](" target="_blank" rel="noopener">https://github.com/sara-kassani/Depthwise-Separable-Convolutional-Neural-Network-for-Skin-Lesion-Classification](</a></li>
</ul>
]]></content>
      <tags>
        <tag>Convolution</tag>
        <tag>卷积网络</tag>
      </tags>
  </entry>
  <entry>
    <title>论文粗读:《The Evolved Transformer》</title>
    <url>/2020/02/05/%E8%AE%BA%E6%96%87%E7%B2%97%E8%AF%BB%E2%80%94%E2%80%94%E3%80%8AThe-Evolved-Transformer%E3%80%8B/</url>
    <content><![CDATA[<p>今天阅读了论文《The Evolved Transformer》，该论文使用了神经架构搜索方法找到了一个更优的transformer结构。下面是阅读过程中的笔记。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://arxiv.org/pdf/1901.11117.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1901.11117.pdf</a><br><a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/evolved_transformer.py" target="_blank" rel="noopener">https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/evolved_transformer.py</a><br><a href="https://blog.csdn.net/jasonzhoujx/article/details/88875469" target="_blank" rel="noopener">https://blog.csdn.net/jasonzhoujx/article/details/88875469</a></p>
</blockquote>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h3><ul>
<li>神经架构搜索<ul>
<li>tournament selection architecture search </li>
<li>warm start</li>
<li>Progressive Dynamic Hurdles（PDH）</li>
</ul>
</li>
<li>搜索出了一个新的transformer架构：Evolved Transformer</li>
</ul>
<h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><h3 id="搜索空间"><a href="#搜索空间" class="headerlink" title="搜索空间"></a>搜索空间</h3><ul>
<li>encoder stackable cell<ul>
<li>6个NASNet-style block<ul>
<li>左右两个block将输入的hidden state转成左右两个hidden state再归并成为一个新的hidden state，作为self-attention的输入</li>
</ul>
</li>
</ul>
</li>
<li>decoder stackable cell<ul>
<li>8个NASNet-style block</li>
</ul>
</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/et1.png" alt="图片"></p>
<ul>
<li>搜索空间branch<ul>
<li>Input：分支可以从输入池中选择一个隐藏状态作为当前block的输入。单元中的第i个block可以从[0, i]个隐藏状态中进行选择，其中第j个隐藏状态表示该cell中第j个block的输出，第0个候选项为单元的输入。</li>
<li>Normalization：归一化项提供了两个选项， [LAYER NORMALIZATION (Ba et al., 2016), NONE]</li>
<li>Layer：构造一个神经网络层，提供的选项包括：<ul>
<li>标准卷积</li>
<li>深度可分离卷积</li>
<li>LIGHTWEIGHT 卷积</li>
<li>n头注意力层</li>
<li>GATED LINEAR UNIT</li>
<li>ATTEND TO ENCODER（decoder专用）</li>
<li>全等无操作</li>
<li>Dead Branch，切断输出</li>
</ul>
</li>
<li>Relative Output Dimension：决定神经网络层输出的维度。</li>
<li>Activation：搜索中激活函数的选项有[SWISH, RELU, LEAKY RELU, NON]</li>
<li>Combiner Function：表征的是左枝和右枝的结合方式，包括{ADDITION、CONCATENATION、MULTIPLICATION}。如果左右枝最终输出形状不同，则需要使用padding进行填充。短的向量向长的向量对齐，当使用加法进行结合时使用0填充，当使用乘法进行结合时使用1填充。</li>
<li>Number of cells：纵向叠加的cell的数量，搜索范围是[1,6]</li>
</ul>
</li>
</ul>
<h3 id="演进过程"><a href="#演进过程" class="headerlink" title="演进过程"></a>演进过程</h3><ul>
<li>锦标赛选择（Tournament Selection）：<ul>
<li>tournament selection算法是一种遗传算法，首先随机生成一批个体, 这些个体是一个个由不同组件组成的完整的模型，我们在目标任务上训练这些个体并在验证集上面计算他们的表现。</li>
<li>首先在初始种群中进行采样产生子种群，从子种群中选出适应性（fitness）最高的个体作为亲本（parent）。被选中的亲本进行突变——也就是将网络模型中的一些组件改变为其他的组件——以产生子模型，然后在对这些子模型分配适应度（fitness），在训练集和测试集上进行训练和验证。</li>
<li>对种群重新进行采样，用通过评估的子模型代替子种群中的fitness的个体以生成新的种群。</li>
<li>重复上面的步骤，直到种群中出现超过给定指标的模型。</li>
</ul>
</li>
<li>渐进式动态障碍（Progressive Dynamic Hurdle）：<ul>
<li>实验使用的训练集是WMT14英语到德语的机器翻译数据集，完整的训练和验证过程需要很长的时间，如果在所有的子模型上进行完整的训练和验证过程将会耗费很大的计算资源。因此论文中使用渐进式动态障碍的方法来提前停止一些没有前景的模型的训练，转而将更多的计算资源分配那些当前表现更好的子模型。具体来说就是让当前表现最好的一些模型多训练一些step。</li>
<li>假设当前种群经过一次锦标赛选择，生成了m个子模型并且加入到了种群中，这时候计算整个种群fitness的平均值h0，下一次锦标赛选择将会以h0作为对照，生成的另外m个fitness超过h0的子模型可以继续训练s1个step，接着进行种群中的所有的其他个体会继续训练s1个step，然后在新的种群中生成h1，以此类推知道种群中所有的个体的训练step都达到一个指定值。</li>
<li>如果一个子模型是由第iii次锦标赛选择之后的亲本生成的，那么验证的过程将会进行iii次。第一次为该模型分配s0次的训练step并且在验证集上进行验证，若验证的fitness大于h0则再分配s1次训练step，再验证，再与h1比较，只有子样本通过h0,h1,…,hi次比较才能作为新的个体加入到新的种群中。</li>
</ul>
</li>
</ul>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><ul>
<li><p>机器翻译</p>
<ul>
<li>在初始的10K step使用0.01的learning rate</li>
<li><p>Transformer</p>
<ul>
<li>inverse-square-root decay to 0 at 300K steps：$l r=s t e p^{-0.00303926^{\circ}}-.962392$</li>
</ul>
</li>
<li><p>Evolved Transformer</p>
<ul>
<li>single-cycle cosine decay</li>
</ul>
</li>
<li>every decay was paired with the same constant 0.01 warmup.</li>
<li>大模型使用高一点的dropout（0.3），小模型使用0.2 dropout</li>
<li>beam-size=6, lenth-penalty=0.6, max-output=50</li>
</ul>
</li>
<li>语言模型<ul>
<li>跟机器翻译差不多，去掉了label smooth, intra-attention dropout=0.0</li>
</ul>
</li>
<li>Search Configuration<ul>
<li>populatino=100</li>
<li>mutation=2.5%</li>
<li>fitness: negative log perplexity</li>
</ul>
</li>
</ul>
<h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><ul>
<li>最终搜索出来的模型结构</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/et2.png" alt="图片"></p>
<ul>
<li>embedding_size=768, 6 encoder, 6 decoder</li>
<li>attention_head=16</li>
<li>ET比Transformer可以在更小的模型上达到更好的效果，当模型增大时两者的差距就不大了（可能因为模型越大越容易过拟合，而且单独增加embedding_size可能不起作用，需要和depth共同增加）</li>
</ul>
]]></content>
      <tags>
        <tag>Transformer</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读:《Towards a Human-like Open-Domain Chatbot》</title>
    <url>/2020/02/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94%E3%80%8ATowards-a-Human-like-Open-Domain-Chatbot%E3%80%8B/</url>
    <content><![CDATA[<p>今天阅读了谷歌最新出的一篇论文，《Towards a Human-like Open-Domain Chatbot》，主要提出了端到端对话机器人的一种评测方法和模型框架。下面是阅读过程中的笔记。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html" target="_blank" rel="noopener">https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html</a><br><a href="https://arxiv.org/pdf/2001.09977.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2001.09977.pdf</a><br><a href="https://github.com/google-research/google-research/tree/master/meena" target="_blank" rel="noopener">https://github.com/google-research/google-research/tree/master/meena</a></p>
</blockquote>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="开放的chatbot-API总结"><a href="#开放的chatbot-API总结" class="headerlink" title="开放的chatbot API总结"></a>开放的chatbot API总结</h3><ul>
<li>cleverbot API: <a href="https://www.cleverbot.com/api/" target="_blank" rel="noopener">https://www.cleverbot.com/api/</a><ul>
<li><a href="https://github.com/plasticuproject/cleverbotfree" target="_blank" rel="noopener">https://github.com/plasticuproject/cleverbotfree</a></li>
</ul>
</li>
<li>xiaobing: <a href="https://www.msxiaobing.com/" target="_blank" rel="noopener">https://www.msxiaobing.com/</a></li>
<li>mitsuku: <a href="https://www.pandorabots.com/mitsuku/" target="_blank" rel="noopener">https://www.pandorabots.com/mitsuku/</a><ul>
<li><a href="https://github.com/hanwenzhu/mitsuku-api" target="_blank" rel="noopener">https://github.com/hanwenzhu/mitsuku-api</a></li>
</ul>
</li>
</ul>
<h3 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h3><ul>
<li>模型架构：Evolved Transformer<ul>
<li>模型输入：多轮对话（最多7轮）</li>
<li>模型输出：回复</li>
<li>最佳模型：2.6B参数，10.2PPL，8K BPE subword vocabulary, 训练数据40B words</li>
</ul>
</li>
<li>评测指标<ul>
<li>PPL</li>
<li>SSA（Sensibleness and Specificity Average）用来评估<ul>
<li>whether make sense</li>
<li>whether specific</li>
</ul>
</li>
<li>人工评测使用static（1477个多轮对话）和interactive（想说啥就说啥）两种数据集，发现SSA和PPL在这两个数据集上高度相关</li>
<li>模型在评测集的表现：<ul>
<li>0.72的SSA</li>
<li>经过filtering mechanism 和 tuned decoding后有0.79的SSA，相比于人提供的0.86SSA的回复已经很接近了</li>
</ul>
</li>
</ul>
</li>
<li>方法的局限性<ul>
<li>评测数据集的局限性，不能解决所有领域的问题</li>
</ul>
</li>
</ul>
<h2 id="对话机器人的评价"><a href="#对话机器人的评价" class="headerlink" title="对话机器人的评价"></a>对话机器人的评价</h2><h3 id="人工进行评测时的参考标准"><a href="#人工进行评测时的参考标准" class="headerlink" title="人工进行评测时的参考标准"></a>人工进行评测时的参考标准</h3><ul>
<li>Sensibleness<ul>
<li>common sense</li>
<li>logical coherence</li>
<li>consistency</li>
<li>人工评测时对于可打的标签：confusing, illogical, out of context, factually wrong, make sense</li>
<li>缺陷：对于安全的回答，如I don’t know，无法区分</li>
</ul>
</li>
<li>Specificity<ul>
<li>A: I love tennis.   B: That’s nice 应该被标记为not specific，如果 B：Me too, I can’t get enough of Roger Federer!则被标记为specific</li>
<li>已经被标记为not sensible的直接标记为not specific</li>
</ul>
</li>
<li>SSA<ul>
<li>可以使用Sensibleness和Specificity标记在所有responses的比例来作为参考标准</li>
<li>使用SSA将Sensibleness和Specificity的比例进行了结合</li>
</ul>
</li>
</ul>
<h3 id="可进行对比的几个开源chatbot框架"><a href="#可进行对比的几个开源chatbot框架" class="headerlink" title="可进行对比的几个开源chatbot框架"></a>可进行对比的几个开源chatbot框架</h3><ul>
<li>基于RNN：<a href="https://github.com/lukalabs/cakechat" target="_blank" rel="noopener">https://github.com/lukalabs/cakechat</a></li>
<li>基于Transformer: <a href="https://github.com/microsoft/DialoGPT" target="_blank" rel="noopener">https://github.com/microsoft/DialoGPT</a><ul>
<li>762M参数的模型效果更好一些</li>
<li>dialogpt没有公开其解码和MMI-reranking的过程，gpt2bot实现了解码：<a href="https://github.com/polakowo/gpt2bot" target="_blank" rel="noopener">https://github.com/polakowo/gpt2bot</a></li>
<li>附加一个中文的基于DialoGPT开发的闲聊模型<ul>
<li><a href="https://github.com/yangjianxin1/GPT2-chitchat" target="_blank" rel="noopener">https://github.com/yangjianxin1/GPT2-chitchat</a></li>
<li><a href="https://blog.csdn.net/kingsonyoung/article/details/103803067" target="_blank" rel="noopener">https://blog.csdn.net/kingsonyoung/article/details/103803067</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="构建静态评测集"><a href="#构建静态评测集" class="headerlink" title="构建静态评测集"></a>构建静态评测集</h3><ul>
<li>从单轮开始：<a href="http://ai.stanford.edu/~quocle/QAresults.pdf" target="_blank" rel="noopener">http://ai.stanford.edu/~quocle/QAresults.pdf</a></li>
<li>增加一些个性化问题，如：Do you like cats?<ul>
<li>A: Do you like movies?; B: Yeah. I like sci-fi mostly; A: Really? Which is your favorite?期待I love Back to the Future这样的回答，对于I don’t like movies这样的回复应标记为not sensible</li>
</ul>
</li>
</ul>
<h3 id="进行动态评测"><a href="#进行动态评测" class="headerlink" title="进行动态评测"></a>进行动态评测</h3><ul>
<li>机器人以Hi开始，评测人员自由与bot对话，并对每一个bot的回复进行评测。每一个对话至少14轮，至多28轮。</li>
</ul>
<h2 id="Meena-Chatbot"><a href="#Meena-Chatbot" class="headerlink" title="Meena Chatbot"></a>Meena Chatbot</h2><h3 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h3><ul>
<li>来源于public social media</li>
<li>清洗流程<ul>
<li>去掉 subword 数目&lt;=2 或 subword 数目 &gt;= 128</li>
<li>去掉 字母比例&lt;0.7</li>
<li>去掉 包含URL</li>
<li>去掉 作者名字bot</li>
<li>去掉 出现100次以上</li>
<li>去掉 跟上文n-gram重复比例过高</li>
<li>去掉 敏感句子</li>
<li>去掉 括号中内容</li>
<li>当一个句子被删除时，则上文全部被删除</li>
</ul>
</li>
<li>共清洗出867M的(context, response)对</li>
<li>使用sentence piece进行BPE分词，得到8K的BPE vocab</li>
<li>最终语料包含341GB的语料(40B word)</li>
</ul>
<h3 id="模型框架"><a href="#模型框架" class="headerlink" title="模型框架"></a>模型框架</h3><ul>
<li>Evolved Transformer<ul>
<li>2.6B parameter</li>
<li>1 ET encoder + 13 ET decoder</li>
</ul>
</li>
<li>最大的模型可达到10.2的PPL</li>
<li>最大的传统Transformer模型（32层decoder）可达到10.7的PPL</li>
<li>hidden size: 2560</li>
<li>attention head: 32</li>
<li>共享编码、解码、softmax的embedding</li>
<li>编码、解码最长是128</li>
</ul>
<h3 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h3><ul>
<li>使用Adafactor optimizer，初始学习率0.01，在前10k step保持不变，使用inverse square root of the number of steps进行衰减</li>
<li>使用<a href="https://github.com/tensorflow/" target="_blank" rel="noopener">https://github.com/tensorflow/</a>tensor2tensor代码进行训练</li>
</ul>
<h3 id="解码细节"><a href="#解码细节" class="headerlink" title="解码细节"></a>解码细节</h3><ul>
<li><p>为了避免产生乏味的回复，可以使用多种方法进行解码</p>
<ul>
<li>reranking</li>
<li>基于profiles, topics, and styles</li>
<li>强化学习</li>
<li>变分自编吗</li>
</ul>
</li>
<li><p>当PPL足够小时，可以使用sample-and-rank策略进行解码</p>
<ul>
<li><p>使用temperature T随机产生N个独立的候选</p>
<ul>
<li><p>$p_{i}=\frac{\exp \left(z_{i} / T\right)}{\sum_{j} \exp \left(z_{j} / T\right)}$</p>
</li>
<li><p>T=1产生不经过修正的分布</p>
</li>
<li><p>T越大，越容易产生不常见的词，如相关的实体名词，但可能产生错误的词</p>
</li>
<li><p>T越小，越容易产生常见的词，如冠词或介词，虽然安全但不specific</p>
</li>
<li><p>解释1</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">温度是神经网络的超参数，用于在应用softmax之前通过缩放对数来控制预测的随机性。 例如，在TensorFlow的LSTM中，温度代表在计算softmax之前将logit除以多少。</span><br><span class="line"></span><br><span class="line">当温度为1时，我们直接在logits（较早层的未缩放输出）上计算softmax，并使用温度为0.6的模型在logits&#x2F;0.6上计算softmax，从而得出较大的值。 在更大的值上执行softmax可使LSTM 更加自信 （需要较少的输入来激活输出层），但在其样本中也更加保守 （从不太可能的候选样本中进行抽样的可能性较小）。 使用较高的温度会在各个类上产生较软的概率分布，并使RNN更容易被样本“激发”，从而导致更多的多样性和更多的错误 。</span><br><span class="line"></span><br><span class="line">softmax函数通过确保网络输出在每个时间步长都在零到一之间，基于其指数值对候选网络的每次迭代进行归一化。</span><br><span class="line"></span><br><span class="line">因此，温度增加了对低概率候选者的敏感性。</span><br></pre></td></tr></table></figure>
</li>
<li><p>解释2</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">当T很大时，即趋于正无穷时，所有的激活值对应的激活概率趋近于相同（激活概率差异性较小）；而当T很低时，即趋于0时，不同的激活值对应的激活概率差异也就越大。</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
<li><p>发现使用beam-search解码会产生重复且无趣的回复，使用sample-and-rank产生的回复会丰富一些</p>
</li>
<li>使用N=20，T=0.88</li>
<li>response score的计算：logP/T，P是response的likelihood，T是token的个数</li>
<li><p>解码时增加detect cross turn repetitions</p>
<ul>
<li>当两个turn的n-gram重复超过一定比例时，则从候选中删除</li>
</ul>
</li>
<li>增加一个分类层，用来过滤掉敏感回复</li>
</ul>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><h3 id="SSA和PPL是相关的"><a href="#SSA和PPL是相关的" class="headerlink" title="SSA和PPL是相关的"></a>SSA和PPL是相关的</h3><ul>
<li>基本呈线性关系</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/tod1.png" alt="图片"></p>
<h3 id="效果的比较"><a href="#效果的比较" class="headerlink" title="效果的比较"></a>效果的比较</h3><ul>
<li>小冰：呈现出个性化的回复，但有时也会无意义，且经常回复得太平常。小冰另一个特点就是具有同情心，可以在以后的评价指标中考虑这一点。小冰有near-human-level engagingness但not very close to human-level humanness，因此在我们的评测指标上SSA不高。</li>
<li>mitsuku：56%SSA（72%sensibility 40%specifity）, 网站上的对话并不是它参加图灵测试的版本</li>
<li>DialoGPT：48%SSA（57%sensibility 49%specifity）</li>
<li>CleverBot：在interactive评测表现比static上稍微好一些（56% interactive SSA，44% static SSA）。发现cleverbot更擅长将话题引入到它更擅长的领域中，缺少personality</li>
<li>Meena：base（72% SSA），full（79% SSA）</li>
</ul>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>论文阅读</tag>
        <tag>Chatbot</tag>
      </tags>
  </entry>
  <entry>
    <title>各类资源定期汇总</title>
    <url>/2020/02/01/%E5%90%84%E7%B1%BB%E8%B5%84%E6%BA%90%E5%AE%9A%E6%9C%9F%E6%B1%87%E6%80%BB/</url>
    <content><![CDATA[<p>一些学习等资源的总结，不定期更新。</p>
<a id="more"></a>
<h1 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h1><h2 id="视频类资源"><a href="#视频类资源" class="headerlink" title="视频类资源"></a>视频类资源</h2><h3 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h3><ul>
<li>吴恩达机器学习：<a href="https://study.163.com/course/courseMain.htm?courseId=1004570029" target="_blank" rel="noopener">https://study.163.com/course/courseMain.htm?courseId=1004570029</a></li>
<li>统计学习基础：链接: <a href="https://pan.baidu.com/s/1NWE6lEJrSgOFAEVVOW2TmQ" target="_blank" rel="noopener">https://pan.baidu.com/s/1NWE6lEJrSgOFAEVVOW2TmQ</a> 提取码: g7km</li>
<li>林轩田机器学习：<a href="https://www.tinymind.cn/articles/168" target="_blank" rel="noopener">https://www.tinymind.cn/articles/168</a></li>
</ul>
<h3 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h3><ul>
<li>花书：<a href="https://www.bilibili.com/video/av69236102?from=search&amp;seid=4183604428283884229" target="_blank" rel="noopener">https://www.bilibili.com/video/av69236102?from=search&amp;seid=4183604428283884229</a></li>
</ul>
<h3 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h3><ul>
<li>斯坦福自然语言处理：<a href="https://zhuanlan.zhihu.com/p/63199665" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/63199665</a></li>
<li>宗成庆统计自然语言处理：<ul>
<li><a href="https://space.bilibili.com/373951238" target="_blank" rel="noopener">https://space.bilibili.com/373951238</a></li>
<li><a href="https://space.bilibili.com/303667813/video" target="_blank" rel="noopener">https://space.bilibili.com/303667813/video</a></li>
</ul>
</li>
</ul>
<h2 id="CV"><a href="#CV" class="headerlink" title="CV"></a>CV</h2><ul>
<li>CS231n计算机视觉：<a href="https://www.bilibili.com/video/av77752864/" target="_blank" rel="noopener">https://www.bilibili.com/video/av77752864/</a></li>
</ul>
<h3 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h3><ul>
<li>David Silver强化学习：<a href="https://space.bilibili.com/74997410?spm_id_from=333.788.b_765f7570696e666f.2" target="_blank" rel="noopener">https://space.bilibili.com/74997410?spm_id_from=333.788.b_765f7570696e666f.2</a></li>
</ul>
<h2 id="博客类资源"><a href="#博客类资源" class="headerlink" title="博客类资源"></a>博客类资源</h2><ul>
<li>科学空间：<a href="https://kexue.fm/" target="_blank" rel="noopener">https://kexue.fm/</a></li>
</ul>
<h2 id="学习笔记类资源"><a href="#学习笔记类资源" class="headerlink" title="学习笔记类资源"></a>学习笔记类资源</h2><h3 id="机器学习-1"><a href="#机器学习-1" class="headerlink" title="机器学习"></a>机器学习</h3><ul>
<li>吴恩达机器学习笔记：<a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes" target="_blank" rel="noopener">https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes</a></li>
<li>统计学习基础笔记：<a href="https://github.com/SmirkCao/Lihang" target="_blank" rel="noopener">https://github.com/SmirkCao/Lihang</a></li>
<li>百面机器学习：<a href="https://github.com/Relph1119/QuestForMachineLearning-Camp" target="_blank" rel="noopener">https://github.com/Relph1119/QuestForMachineLearning-Camp</a></li>
</ul>
<h3 id="深度学习-1"><a href="#深度学习-1" class="headerlink" title="深度学习"></a>深度学习</h3><ul>
<li>花书笔记：<a href="https://discoverml.github.io/simplified-deeplearning/" target="_blank" rel="noopener">https://discoverml.github.io/simplified-deeplearning/</a><ul>
<li>中文版图书：<a href="https://github.com/exacity/deeplearningbook-chinese" target="_blank" rel="noopener">https://github.com/exacity/deeplearningbook-chinese</a></li>
</ul>
</li>
</ul>
<h3 id="NLP-1"><a href="#NLP-1" class="headerlink" title="NLP"></a>NLP</h3><ul>
<li>斯坦福自然语言处理：<a href="https://zhuanlan.zhihu.com/p/59011576" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/59011576</a></li>
<li>宗成庆统计自然语言处理：<ul>
<li><a href="https://github.com/aicourse/ZMC301-CAS-NLP-2019" target="_blank" rel="noopener">https://github.com/aicourse/ZMC301-CAS-NLP-2019</a></li>
<li><a href="http://www.nlpr.ia.ac.cn/cip/ZongReportandLecture/ReportandLectureIndex.htm" target="_blank" rel="noopener">http://www.nlpr.ia.ac.cn/cip/ZongReportandLecture/ReportandLectureIndex.htm</a></li>
</ul>
</li>
</ul>
<h3 id="CV-1"><a href="#CV-1" class="headerlink" title="CV"></a>CV</h3><ul>
<li>CS231n计算机视觉：<ul>
<li><a href="https://zhuanlan.zhihu.com/p/21930884" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21930884</a></li>
<li><a href="https://github.com/mbadry1/CS231n-2017-Summary" target="_blank" rel="noopener">https://github.com/mbadry1/CS231n-2017-Summary</a></li>
</ul>
</li>
</ul>
<h3 id="强化学习-1"><a href="#强化学习-1" class="headerlink" title="强化学习"></a>强化学习</h3><ul>
<li>David Silver强化学习笔记：<a href="https://zhuanlan.zhihu.com/reinforce" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/reinforce</a></li>
</ul>
<h2 id="工具汇总"><a href="#工具汇总" class="headerlink" title="工具汇总"></a>工具汇总</h2><h3 id="NLP-2"><a href="#NLP-2" class="headerlink" title="NLP"></a>NLP</h3><ul>
<li>jialu: <a href="https://github.com/ownthink/Jiagu" target="_blank" rel="noopener">https://github.com/ownthink/Jiagu</a></li>
</ul>
<h2 id="数据集汇总"><a href="#数据集汇总" class="headerlink" title="数据集汇总"></a>数据集汇总</h2><h3 id="NLP-3"><a href="#NLP-3" class="headerlink" title="NLP"></a>NLP</h3><ul>
<li>百度：<a href="http://ai.baidu.com/broad" target="_blank" rel="noopener">http://ai.baidu.com/broad</a></li>
</ul>
<h2 id="前沿追踪类资源"><a href="#前沿追踪类资源" class="headerlink" title="前沿追踪类资源"></a>前沿追踪类资源</h2><h3 id="NLP-4"><a href="#NLP-4" class="headerlink" title="NLP"></a>NLP</h3><ul>
<li><a href="https://github.com/sebastianruder/NLP-progress" target="_blank" rel="noopener">https://github.com/sebastianruder/NLP-progress</a></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>Hello Edge: Keyword spotting on Microcontrollers论文源码阅读及实验结论</title>
    <url>/2019/04/23/Hello-Edge-Keyword-spotting-on-Microcontrollers%E8%AE%BA%E6%96%87%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E5%8F%8A%E5%AE%9E%E9%AA%8C%E7%BB%93%E8%AE%BA/</url>
    <content><![CDATA[<p>高通的大哥想做语音唤醒实验，我自己对这个也很感兴趣，所以阅读了一下相关论文，并做了一次实验。本篇主要对源码进行解读。</p>
<a id="more"></a>
<h2 id="论文结论"><a href="#论文结论" class="headerlink" title="论文结论"></a>论文结论</h2><ul>
<li>LSTM使用的内存最少</li>
<li>整体DS-CNN &gt; CRNN &gt; GRU &gt; LSTM &gt; Basic_LSTM &gt; CNN &gt; DNN</li>
</ul>
<h2 id="KWS-for-FIXLEN代码阅读"><a href="#KWS-for-FIXLEN代码阅读" class="headerlink" title="KWS_for_FIXLEN代码阅读"></a>KWS_for_FIXLEN代码阅读</h2><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><h4 id="音频相关"><a href="#音频相关" class="headerlink" title="音频相关"></a>音频相关</h4><ul>
<li>words_list：_silence_,_unknown_,yes,no,up,down,left,right,on,off,stop,go<ul>
<li>label_count（How many classes are to be recognized）：12</li>
<li>wanted_words: yes,no,up,down,left,right,on,off,stop,go</li>
</ul>
</li>
<li>sample_rate（需要和提供的wav文件的采样率匹配）：16000 </li>
<li>clip_duration_ms（录音文件的时长）：1000 <ul>
<li>desired_samples （sample_rate * clip_duration_ms / 1000 语音需要的样本点个数）：16000</li>
</ul>
</li>
<li>window_size_ms（帧长）：40.0<ul>
<li>window_size_samples（sample_rate * window_size_ms / 1000）：640</li>
</ul>
</li>
<li>window_stride_ms（帧移）：40.0 帧移<ul>
<li>window_stride_samples（sample_rate * window_stride_ms / 1000）：640</li>
<li>spectrogram_length（0:1 + int((desired_samples - window_size_samples) / window_stride_samples) ? desired_samples - window_size_samples &lt; 0 声音有多少帧）: 25</li>
</ul>
</li>
<li>dct_coefficient_count（对MFCC来说每一帧有多少系数）：10<ul>
<li>fingerprint_size （dct_coefficient_count * spectrogram_length）: 250</li>
</ul>
</li>
<li>background_volume（背景噪声的音量，默认0.1。这是一种Data Augmentation的技术，通过给语音增加噪声来提高模型的泛化能力）</li>
<li>background_frequency（多少比例的训练数据会增加噪声）：0.8</li>
<li>silence_percentage（How much of the training data should be silence）：10.0</li>
<li>unknown_percentage(How much of the training data should be unknown words):10.0</li>
<li>validation_percentage(What percentage of wavs to use as a validation set):10</li>
<li>testing_percentage(What percentage of wavs to use as a test set):10</li>
<li>time_shift_ms（录音都是长度1秒的文件，但是在实际预测的时候用户开始的实际是不固定的，为了模拟这种情况，我们这里会随机的把录音文件往前或者往后平移一段时间，这个参数就是指定平移的范围。默认100(ms)，说明会随机的在[-100,100]之间平移数据）：100.0 <ul>
<li>time_shift_samples（time_shift_ms * sample_rate/ 1000）：16</li>
</ul>
</li>
</ul>
<h4 id="模型相关"><a href="#模型相关" class="headerlink" title="模型相关"></a>模型相关</h4><ul>
<li>网络参数<ul>
<li>model_architecture：single_fc, conv, low_latency_conv, low_latency_svdf, dnn, cnn, basic_lstm, lstm, gru, crnn, ds_cnn</li>
<li>model_size_info：[144, 144, 144]</li>
</ul>
</li>
<li>训练参数<ul>
<li>learning_rate: [0.0005, 0.0001, 0.00002]</li>
<li>how_many_training_steps: [10000,10000,10000]</li>
<li>summaries_dir</li>
<li>train_dir</li>
<li>eval_step_interval: 400</li>
<li>save_step_interval：100</li>
</ul>
</li>
</ul>
<h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><ul>
<li><p>python train.py —mode prepare —dct_coefficient_count 10 —window_size_ms 40 —window_stride_ms 40 —data_dir data/speech_commands</p>
<ul>
<li>如果需要下载原始数据：python train.py —mode prepare —dct_coefficient_count 10 —window_size_ms 40 —window_stride_ms 40 —data_url <a href="http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz" target="_blank" rel="noopener">http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz</a> —data_dir data/speech_commands</li>
<li><p><strong>prepare_data_index</strong>：根据音频文件名哈希到不同集合中（尽量让同一个发声人在一个集合中），将静音根据比例添加到不同集合中，把一些unknown的wav根据比例添加到不同集合中，最后进行集合内shuffle （先忽略background文件夹下的wav）。</p>
<ul>
<li><p>这里涉及到划分训练集、验证集和测试集的一个小技巧。通常我们的训练数据是不断增加的，如果按照随机的按比例划分训练集、验证集和测试集，那幺增加一个新的数据重新划分后有可能把原来的训练集中的数据划分到测试数据里。因为我们的模型可能要求incremental的训练，因此这就相对于把测试数据也拿来训练了。因此我们需要一种“稳定”的划分方法——原来在训练集中的数据仍然在训练数据中。这里我们使用的技巧就是对于文件名进行hash，然后根据hash的结果对总量取模来划分到不同的集合里。这样就能保证同一个数据第一次如果是在训练集合里，那幺它永远都会划分到训练集合里。不过它只能大致保证三个集合的比例而不能绝对的保证比例</p>
</li>
<li><p>另一点是每个集合里都要加入一定比例(silence_percentage)的silence和unknown词</p>
</li>
<li><p><strong>input</strong>：silence_percentage, unknown_percentage, validation_percentage, testing_percentage, wanted_words</p>
</li>
<li><p><strong>output</strong>：data_index: {‘validation’: [{‘label’: word, ‘file’: wav_path}], ‘testing’: […], ‘training’: […]}</p>
<ul>
<li><p>words_list：[_silence_, _unknown_, yes, no, up, down, left, right, on, off, stop, go]</p>
</li>
<li><p>word_to_index（不需要识别的词都映射成1）：{‘marvin’: 1, ‘tree’: 1, ‘learn’: 1, ‘dog’: 1, ‘sheila’: 1, ‘bird’: 1, ‘right’: 7, ‘off’: 9, ‘backward’: 1, ‘six’: 1, ‘two’: 1, ‘no’: 3, ‘yes’: 2, ‘one’: 1, ‘follow’: 1, ‘up’: 4, ‘three’: 1, ‘forward’: 1, ‘happy’: 1, ‘nine’: 1, ‘bed’: 1, ‘zero’: 1, ‘house’: 1, ‘visual’: 1, ‘five’: 1, ‘seven’: 1, ‘cat’: 1, ‘left’: 6, ‘stop’: 10, ‘go’: 11, ‘four’: 1, ‘on’: 8, ‘wow’: 1, ‘down’: 5, ‘eight’: 1}</p>
</li>
</ul>
</li>
</ul>
</li>
<li>prepare_background_data: <ul>
<li>单独处理background_noise文件夹下的音频</li>
<li>input：BACKGROUND_NOISE_DIR_NAME</li>
<li>output:<ul>
<li>background_data: （16000, 0）的float tensor</li>
<li>重要函数：<strong>wav_decoder = contrib_audio.decode_wav(wav_loader, desired_channels=1)</strong> ：将16-bit PCM Wave 文件解码成一维 float tensor</li>
</ul>
</li>
</ul>
</li>
<li>prepare_processing_graph：先将音频解码的tensorflow graph创建好，等session.run的时候执行<ul>
<li>加载一个WAVE文件 —&gt; 解码（(16000,0)的float tensor） —&gt; 缩放音量（element-wise乘，得到还是(16000,0)的float tensor） —&gt; 语音平移 —&gt; 加入背景噪声（(desired_samples,)且在-1.0到1.0的float tensor） —&gt; 计算频谱 （(1, 25, 513)这里第一个维度是通道数，单通道是1）—&gt; 建立一个MFCC指纹（<strong>(1, 25, 10)</strong>）</li>
<li>语音平移：如果是右移，那幺需要在左边补零；如果是左移，则要右边补零。这个padding的量也是在生成batch数据的时候动态产生的，所以也定义为一个placeholder。因为语音的tensor是(16000,1)的，所有padding是一个[2,2]的tensor，不过通常只在第一个维度(时间)padding。比如右移100个点，那幺传入的tensor是[[100,0],[0,0]]。如果是左移，我们除了要padding，还要把左边的部分“切掉“，因此还会传入一个time_shift_offset_placeholder_，如果是右移，那幺这个值是零。比如我们要实现左移100个点，那幺传入的time_shift_padding_placeholder_应该是[[0,100],[0,0]],而time_shift_offset_placeholder_应该是[100]。_</li>
<li>混入噪声：placeholder background_data_placeholder_表示噪声，而background_volume_placeholder_表示混入的音量(比例)，如果background_volume_placeholder_是零就表示没有噪声。把它们乘起来就得到background_mul，然后把它加到sliced_foreground就得到background_add，因为加起来音量可能超过1，所有需要把大于1的变成1，这可以使用clip_by_value函数把音量限制在[-1,1]的区间里</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="模型声明"><a href="#模型声明" class="headerlink" title="模型声明"></a>模型声明</h3><ul>
<li>输入、输出<ul>
<li>input: fingerprint_input（batch_size, fingerprint_size）, model_settings, model_size_info, is_training</li>
<li>output: logits, dropout_prob(if training, 是一个placeholder)</li>
</ul>
</li>
</ul>
<ul>
<li>dnn: create_dnn_model<ul>
<li>参数：<ul>
<li>W1：（fingerprint_size, model_size_info[0]）</li>
<li>b1: (model_size_info[0])</li>
<li>W2: （model_size_info[0], model_size_info[1]）</li>
<li>b2: （model_size_info[1]）</li>
<li>W3：（model_size_info[1], model_size_info[2]）</li>
<li>b3: （model_size_info[2]）</li>
<li>weights: （model_size_info[2], label_count）</li>
<li>bias: （label_count）</li>
<li>总参数个数：250 <em> 144 + 144 + 144 </em> 144 + 144 + 144 <em> 144 + 144+ 144 </em> 12 + 12 = 79644</li>
<li>操作个数：</li>
</ul>
</li>
<li>前向计算过程：fingerprint_input用flow代替, * 表示矩阵乘<ul>
<li>flow  —&gt;（batch_size, fingerprint_size）</li>
<li>flow1 = dropout(relu(flow * W1 + b1 ))  —&gt;（batch_size, model_size_info[0]）</li>
<li>flow2 = dropout(relu(flow1 * W2 + b2 ))  —&gt; （batch_size, model_size_info[1]）</li>
<li>flow3 = dropout(relu(flow2 * W2 + b2 ))  —&gt; （batch_size, model_size_info[2] </li>
<li>logits = flow3 * weights + bias  —&gt; （batch_size, label_count）</li>
</ul>
</li>
</ul>
</li>
<li>conv: create_conv_model  参考【2】<ul>
<li>参数：<ul>
<li>first_weights：（first_filter_height=20, first_filter_width=8, 1, first_filter_count=64）注：卷积核</li>
<li>first_bias：（first_filter_count,）</li>
<li>second_weights: （second_filter_height=10, second_filter_width=4, first_filter_count=64, second_filter_count=64）</li>
<li>second_bias： （second_filter_count,）</li>
<li>final_fc_weights：（13 <em> 5 </em> 64=4160, label_count）</li>
<li>final_fc_bias：（label_count,）</li>
</ul>
</li>
<li>前向计算过程：<ul>
<li>flow  —&gt;（batch_size, fingerprint_size）</li>
<li>fingerprint_4d = reshape(flow) —&gt; (batch_size, spectrogram_length=25, dct_coefficient_count=10, 1) 注：类似图像的[batch_size, in_height, in_width, in_channels]</li>
<li>first_conv = dropout(relu(conv2d(fingerprint_4d, first_weights, [1, 1, 1, 1], ‘SAME’) + first_bias)) —&gt; （batch_size, spectrogram_length=25, dct_coefficient_count=10, first_filter_count=64）卷积</li>
<li>max_pool = max_pool(first_conv, [1, 2, 2, 1], [1, 2, 2, 1], ‘SAME’) —&gt; (batch_size, (25-2 + 2 <em> 1)/2 + 1=13, (10 - 2 + 2</em>1)/2 = 5, first_filter_count=64)</li>
<li>second_conv = dropout(relu(conv2d(max_pool, second_weights, [1,1,1,1], ‘SAME’) + second_bias)) —&gt; （batch_size, 13, 5, 64）</li>
<li>flattened_second_conv = reshape(second_conv) —&gt; (batch_size, 13 <em> 5 </em> 64=4160)</li>
<li>final_fc = flattened_second_conv * final_fc_weights + final_fc_bias —&gt; (batch_size, label_count)</li>
<li>总参数个数：224140 = 20 <em> 8 </em> 64 + 64 + 10 <em> 4 </em> 64 <em> 64 + 64 + 4160 </em> 12 + 12</li>
<li>总操作个数</li>
</ul>
</li>
</ul>
</li>
<li>low_latency_conv：create_low_latency_conv_model 参考【2】相比于conv有更少的参数，但是有准确度的损失<ul>
<li>参数<ul>
<li>first_weights：（spectrogram_length=25, first_filter_width=8, 1, first_filter_count=186）</li>
<li>first_bias：（first_filter_count,）</li>
<li>first_fc_weights：（first_conv_element_count=558, first_fc_output_channels = 128）</li>
<li>first_fc_bias：（first_fc_output_channels, ）</li>
<li>second_fc_weights：（first_fc_output_channels=128, second_fc_output_channels=128）</li>
<li>second_fc_bias：（second_fc_output_channels=128, ）</li>
<li>final_fc_weights：（second_fc_output_channels=128, label_count=12）</li>
<li>final_fc_bias：（label_count,）</li>
<li>总参数个数：126998</li>
<li>总操作个数</li>
</ul>
</li>
<li>前向计算过程<ul>
<li>flow  —&gt;（batch_size, fingerprint_size）</li>
<li>fingerprint_4d = reshape(flow) —&gt; (batch_size, spectrogram_length=25, dct_coefficient_count=10, 1)</li>
<li>first_conv = dropout(relu(conv2d(fingerprint_4d, first_weights, [1, 1, 1, 1], ‘VALID’ )+ first_bias)) —&gt; （batch_size, 1, 3, 186）</li>
<li>flattened_first_conv = reshape(first_conv) —&gt; （batch_size, 3 * 186=558）</li>
<li>first_fc = dropout(flattened_first_conv * first_fc_weights + first_fc_bias) —&gt; （batch_size, first_fc_output_channels = 128）</li>
<li>second_fc = dropout(first_fc * second_fc_weights + second_fc_bias) —&gt; （batch_size, second_fc_output_channels = 128）</li>
<li>final_fc = second_fc * final_fc_weights + final_fc_bias —&gt; (batch_size, label_count)</li>
</ul>
</li>
</ul>
</li>
<li>basic-lstm: 和lstm差了use_peepholes和num_proj，参数个数为43916 = 10 <em> 98 </em> 4 + 98 <em> 98 </em> 4 + 98 <em> 4 + 98 </em> 12 + 12</li>
<li>lstm: model_size_info [0]对应projection size，model_size_info[1]对应memory cells in LSTM<ul>
<li>参数 ?加peephole之后参数也不太对<ul>
<li>lstm_cell: （隐藏层使用了全链接，peephole connections）<ul>
<li>W_f、W_i、W_o、W_c：（dct_coefficient_count,  model_size_info[1]）</li>
<li>U_f、U_i、U_o、U_c: （model_size_info [0], model_size_info[1]）</li>
<li>bias_f、bias_i、bias_o、bias_c: (model_size_info[1])</li>
<li>W_h: （model_size_info[1], model_size_info[0]）</li>
<li>bias_h: （model_size_info[0]）</li>
</ul>
</li>
<li>W_o：（model_size_info[0]=98, label_count=12）</li>
<li>b_o: （label_count=12,）</li>
<li>参数总个数：? 78516 = <strong>5760 + 56448 + 576 + 14112 + 98 + 1176 + 12 = 78182</strong></li>
<li>总操作个数</li>
</ul>
</li>
<li>前向计算过程 * 是矩阵乘法，∘是元素乘<ul>
<li>flow  —&gt;（batch_size, fingerprint_size）</li>
<li>fingerprint_4d = reshape(flow) —&gt; (batch_size, spectrogram_length=25, dct_coefficient_count=10)<ul>
<li>一个时间步的输入<ul>
<li>隐状态h_t-1：（batch_size，model_size_info[0]）</li>
<li>x_t：（batch_size, dct_coefficient_count=10）</li>
<li>c_t-1: （batch_size, model_size_info[1]）</li>
</ul>
</li>
<li>f_t = sigmoid(x_t <em> W_f + h_t-1 </em> U_f + bias_f) —&gt;（batch_size，model_size_info[1]）</li>
<li>i_t = sigmoid(x_t <em> W_i + h_t-1 </em> U_i + bias_i) —&gt; （batch_size，model_size_info[1]）</li>
<li>o_t = sigmoid(x_t <em> W_o + h_t-1 </em> U_o + bias_o ) —&gt; （batch_size，model_size_info[1]）</li>
<li>c’ = tanh(x_t <em> W_c + h_t-1 </em> U_c + bias_c) —&gt; （batch_size，model_size_info[1]）</li>
<li>c_t = f_t ∘ c_t-1 + i_t ∘ c’ —&gt; （batch_size, model_size_info[1]）</li>
<li>h_t = tanh(o_t ∘ c_t) —&gt; （batch_size, model_size_info[1]）</li>
<li>h_t = h_t * W_h + bias_h —&gt; （batch_size, model_size_info[0]）</li>
</ul>
</li>
<li>flow = dynamic_rnn(lstm_cell, fingerprint_4d) 的最后一个输出 —&gt; (batch_size, model_size_info[0]=98)</li>
<li>logits = flow * W_o + b_o —&gt; （batch_size, label_count）</li>
</ul>
</li>
</ul>
</li>
<li><p>gru: </p>
<ul>
<li>参数 model_size_info[0]对应num_layers，model_size_info[1]对应gru_units<ul>
<li>使用layer_normalize</li>
<li>不使用layer_normalize<ul>
<li>W_rx、W_zx、W_hx: （dct_coefficient_count=10, gru_units=154）</li>
<li>W_rh、W_zh、W_hh: （gru_units, gru_units）</li>
<li>bias_r、bias_z、bias_h: （gru_units）</li>
<li>W_o：（gru_units, label_count）</li>
<li>b_o：（label_count）</li>
<li>总参数个数：10 <em> 154 </em> 3 + 154 <em> 154 </em> 3 + 154 <em> 3 + 154 </em> 12 + 12  = 78080 ? 91950</li>
</ul>
</li>
</ul>
</li>
<li><p>前向计算过程 </p>
<ul>
<li>flow  —&gt;（batch_size, fingerprint_size）</li>
<li><p>fingerprint_4d = reshape(flow) —&gt; (batch_size, spectrogram_length=25, dct_coefficient_count=10)</p>
<ul>
<li>一个时间步的输入<ul>
<li>隐状态h_t-1：（batch_size，gru_units）</li>
<li>x_t：（batch_size, dct_coefficient_count=10）</li>
</ul>
</li>
<li><p>使用layer_normalize</p>
<ul>
<li>单向</li>
<li>双向：前向和后向logits的拼接</li>
</ul>
</li>
<li><p>不使用layer_normalize</p>
<ul>
<li>单向<ul>
<li>r_t = sigmoid(x_t <em> W_rx + h_t-1 </em> W_rh + bias_r) —&gt; (batch_size, gru_units)</li>
<li>z_t = sigmoid(x_t <em> W_zx + h_t-1 </em> W_zh + bias_z) —&gt; (batch_size, gru_units)</li>
<li>h’ = tanh(x_t <em> W_hx + (r_t ∘ h_t-1) </em> W_hh + bias_h) —&gt; (batch_size, gru_units)</li>
<li>h_t = (1 - z_t) ∘ h_t-1 + z_t ∘ h’ —&gt;  (batch_size, gru_units)</li>
<li>flow = dynamic_rnn(gru_cell, fingerprint_4d) 的最后一个输出 —&gt; (batch_size, gru_units) </li>
<li>logits = flow * W_o + b_o —&gt; （batch_size, label_count）</li>
</ul>
</li>
<li>双向<ul>
<li>前向和后向logits的拼接</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>crnn</li>
<li>ds_cnn</li>
</ul>
<h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><ul>
<li>定义loss：softmax cross entropy<ul>
<li>定义optimize：Adam</li>
<li>重点是audio_processor.get_data 随机生成一个batch的训练数据的过程<ul>
<li>input：<ul>
<li>how_many ：batch大小，如果是-1则返回所有</li>
<li>offset： 如果是非随机的生成数据，这个参数指定开始的offset</li>
<li>background_frequency： 0.0-1.0之间的值，表示需要混入噪音的数据的比例</li>
<li>background_volume_range ：背景噪音的音量</li>
<li>time_shift ：平移的范围，为[-time_shift, time_shift]</li>
<li>sess ：用于执行前面用于产生数据的Operation，参考prepare_processing_graph函数</li>
</ul>
</li>
<li>output：<ul>
<li>data：</li>
<li>labels</li>
<li>过程看代码中注释</li>
</ul>
</li>
</ul>
</li>
<li>实验结果：<ul>
<li>Final test accuracy = 84.68% (N=3081)：python train.py —model_architecture dnn —model_size_info 144 144 144 —dct_coefficient_count 10 —window_size_ms 40 —window_stride_ms 40 —learning_rate 0.0005,0.0001,0.00002 —how_many_training_steps 10000,10000,10000 —data_dir ./data/speech_commands  —summaries_dir work/DNN/DNN1/retrain_logs —train_dir work/DNN/DNN1/training</li>
<li>python train.py —model_architecture conv —model_size_info 144 144 144 —dct_coefficient_count 10 —window_size_ms 40 —window_stride_ms 40 —learning_rate 0.0005,0.0001,0.00002 —how_many_training_steps 10000,10000,10000 —data_dir ./data/speech_commands  —summaries_dir work/CNN/CNN1/retrain_logs —train_dir work/CNN/CNN1/training</li>
<li>python train.py —model_architecture low_latency_conv —model_size_info 144 144 144 —dct_coefficient_count 10 —window_size_ms 40 —window_stride_ms 40 —learning_rate 0.0005,0.0001,0.00002 —how_many_training_steps 10000,10000,10000 —data_dir ./data/speech_commands  —summaries_dir work/CNN/CNN2/retrain_logs —train_dir work/CNN/CNN2/training</li>
<li>python train.py —model_architecture basic_lstm —model_size_info 98 144 —dct_coefficient_count 10 —window_size_ms 40 —window_stride_ms 40 —learning_rate 0.0005,0.0001,0.00002 —how_many_training_steps 10000,10000,10000 —data_dir ./data/speech_commands  —summaries_dir work/LSTM/LSTM1/retrain_logs —train_dir work/LSTM/LSTM1/training</li>
<li>python train.py —model_architecture lstm —model_size_info 98 —dct_coefficient_count 10 —window_size_ms 40 —window_stride_ms 40 —learning_rate 0.0005,0.0001,0.00002 —how_many_training_steps 10000,10000,10000 —data_dir ./data/speech_commands  —summaries_dir work/LSTM/LSTM2/retrain_logs —train_dir work/LSTM/LSTM2/training</li>
<li>python train.py —model_architecture gru —model_size_info 1 154 —window_size_ms 40 —window_stride_ms 40 —learning_rate 0.0005,0.0001,0.00002 —how_many_training_steps 10000,10000,10000 —data_dir ./data/speech_commands  —summaries_dir work/GRU/retrain_logs —train_dir work/GRU/training</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="模型测试"><a href="#模型测试" class="headerlink" title="模型测试"></a>模型测试</h3><ul>
<li>使用checkpoint<ul>
<li>python test.py —model_architecture dnn —model_size_info 144 144 144 —dct_coefficient_count 10 —window_size_ms 40 —window_stride_ms 40 —checkpoint work/DNN/DNN1/training/best/dnn_8503.ckpt-24000</li>
<li>使用freeze后的图： 可生成在安卓和IOS上可执行的图<ul>
<li>python freeze.py —model_architecture dnn —model_size_info 144 144 144 —dct_coefficient_count 10 —window_size_ms 40 —window_stride_ms 40 —data_dir ./data/speech_commands —checkpoint work/DNN/DNN1/training/best/dnn_8503.ckpt-24000 —output_file work/DNN/DNN1/training/best/dnn.pb<ul>
<li>创建图：create_inference_graph，将wave_data的filepath placeholder经过一系列计算转为reshaped_input，用create_model创建图得到logits, 再用softmax得到输出</li>
<li>load weights：models.load_variables_from_checkpoint</li>
<li>将变量替换为inline constant：graph_util.convert_variables_to_constants</li>
<li>写入目标图文件中：tf.train.write_graph</li>
</ul>
</li>
<li>python test_pb.py —model_architecture dnn —model_size_info 144 144 144 —dct_coefficient_count 10 —window_size_ms 40 —window_stride_ms 40 —data_dir ./data/speech_commands —graph work/DNN/DNN1/training/best/dnn.pb</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>模型单个音频预测（使用pb文件预测）</li>
<li>python predict_pb.py —model_architecture dnn —model_size_info 144 144 144 —dct_coefficient_count 10 —window_size_ms 40 —window_stride_ms 40 —data_dir ./data/speech_commands —graph work/DNN/DNN1/training/best/dnn.pb —test_wave data/speech_commands/yes/377e916b_nohash_0.wav</li>
</ul>
<h2 id="日常思考"><a href="#日常思考" class="headerlink" title="日常思考"></a>日常思考</h2><ul>
<li><p>KWS是否可以做成两阶段的，小模型先粗检（得到一个probability），再进入语音系统里用稍大一点的模型进行细检（根据上一个probability和声学特征）</p>
</li>
<li><ul>
<li>参考：<a href="https://mp.weixin.qq.com/s?__biz=MzAxMzc2NDAxOQ==&amp;mid=2650364421&amp;idx=1&amp;sn=7d0bb873fc8912919313c7f7f93f11e3&amp;chksm=83906ed9b4e7e7cfd86895b8c988da3f9b9658cbd3e59757f330667195e66ba691a437cade7c&amp;mpshare=1&amp;scene=24&amp;srcid=0420VEnXWAfwQt5bzrWKExbk&amp;key=6deb43fc298651d2d979d26548ea622f8207610665103536b676bc530064991d7785e44ddc7d09213a5dd65cfbf8f7e10de1f77f152a738e362c2ae53d33be353a3e56ad09614fcc3c3e752935aee8f9&amp;ascene=0&amp;uin=NjI1NjAyODg0&amp;devicetype=iMac+MacBookPro12%2C1+OSX+OSX+10.12.6+build(16G29" target="_blank" rel="noopener">苹果发布长文，揭秘「Hey Siri」的语音唤醒触发器和它背后的DNN模型</a></li>
</ul>
</li>
<li><p>KWS的安全性问题，是谁都可以唤醒吗？会不会根据说话人的几句话自己进行调整</p>
</li>
<li><ul>
<li>参考：<a href="https://mp.weixin.qq.com/s?__biz=MjM5NDM4NjQyNA==&amp;mid=2650918986&amp;idx=1&amp;sn=647ef8da86aea7ff4782fec1b2cde2f6&amp;chksm=bd7de8dd8a0a61cb30368d6d81a848e43171ea704de0eeb5bf81a394783d707d5445c2f8656f&amp;mpshare=1&amp;scene=24&amp;srcid=0420XnfffJnnWilAASoO14l1&amp;key=6deb43fc298651d21d2ecf5eb271a88dc4d0986ba4728fe4872d7b2f71f761167d621e2d64797ba83534f1e94cabe22a28e29077744a42bdc22d5a2afd384fd0ea33b9b15f9cf0aa286a9f8473858011&amp;ascene=0&amp;uin=NjI1NjAyODg0&amp;devicetype=iMac+MacBookPro12%2C1+OSX+OSX+10.12.6+build(16G29" target="_blank" rel="noopener">思必驰锁屏语音唤醒</a></li>
</ul>
</li>
<li><p>KWS在联网时可以将误检、漏检、正常唤醒的音频数据传到服务端，服务端进行标注并训练，但是模型能动态更新到硬件上吗？如果不行，这就是个一锤子买卖，是不是可以考虑先放到少儿词典这样的app中去收集一些数据</p>
</li>
<li><p>网络的输出：是否要标明开始和结束的位置</p>
</li>
</ul>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li>KWS相关资料<ul>
<li>不变长：<a href="https://github.com/ARM-software/ML-KWS-for-MCU" target="_blank" rel="noopener">代码</a>、<a href="https://arxiv.org/pdf/1711.07128.pdf" target="_blank" rel="noopener">论文</a></li>
<li>变长：<a href="https://github.com/mindorii/kws" target="_blank" rel="noopener">代码</a>、<a href="https://arxiv.org/pdf/1611.09405.pdf" target="_blank" rel="noopener">论文</a></li>
</ul>
</li>
<li>GRU推导：<a href="https://www.cnblogs.com/YiXiaoZhou/p/6075777.html" target="_blank" rel="noopener">GRU(Gated Recurrent Unit) 更新过程推导及简单代码实现</a></li>
<li>LSTM和GRU讲解：[<a href="https://www.cnblogs.com/zyly/p/9029591.html" target="_blank" rel="noopener">使用TensorFlow实现LSTM和GRU网络</a>]</li>
<li>关于卷积的详细讲解：<ul>
<li>卷积：<a href="https://blog.csdn.net/u011630575/article/details/78062452" target="_blank" rel="noopener">tensorflow：卷积函数——tf.nn.conv2d</a></li>
<li>卷积：<a href="https://blog.csdn.net/mao_xiao_feng/article/details/78004522" target="_blank" rel="noopener">tf.nn.conv2d是怎样实现卷积的？</a></li>
<li>池化：<a href="https://blog.csdn.net/u010402786/article/details/51541465" target="_blank" rel="noopener">卷积神经网络中图像池化操作全解析</a></li>
<li>池化：<a href="https://blog.csdn.net/mao_xiao_feng/article/details/53453926" target="_blank" rel="noopener">tf.nn.max_pool实现池化操作</a></li>
</ul>
</li>
<li>tensorflow rnn源码解读：<a href="https://panxiaoxie.cn/2018/09/01/tensorflow-rnn-api-源码阅读/" target="_blank" rel="noopener">Tensorflow RNN API 源码阅读</a></li>
<li><a href="https://flashgene.com/archives/25812.html" target="_blank" rel="noopener">使用Tensorflow识别语音关键词</a></li>
<li><a href="http://www.isca-speech.org/archive/interspeech_2015/papers/i15_1478.pdf" target="_blank" rel="noopener">Convolutional Neural Networks for Small-footprint Keyword Spotting</a></li>
</ul>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>KWS</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：《Lattice CNNs for Matching Based Chinese Question Answering》</title>
    <url>/2019/03/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E3%80%8ALattice-CNNs-for-Matching-Based-Chinese-Question-Answering%E3%80%8B/</url>
    <content><![CDATA[<p>问答系统是普通用户使用知识库最直接的渠道。匹配用户问题这种短文本，通常面临相同语义的单词和表达方式不唯一的挑战。 中文这种还需要额外分词的语言中，这种现象尤为严重。在论文《基于Lattice CNN的中文问答匹配方法（Lattice CNNs for Matching Based Chinese Question Answering）》中，研究者提出一个基于Lattice CNN的模型，能够利用word-lattice中固有的多粒度信息，且具有很强的处理噪声的能力。 对基于文档的问答和基于知识的问答任务进行了广泛的实验，实验结果表明，LCNs模型可以提取word lattice中丰富且有差别的信息，性能优于其他。</p>
<a id="more"></a>
<h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h1><p>短文本匹配受分词效果影响。针对中文或类似语言的匹配文本序列经常会遭受分词的困扰，因为在这种情况下，通常没有完美的中文分词工具可以适合每种情况。 文本匹配通常需要捕获多个粒度的两个序列之间的相关性。 例如，在图1中，示例短语通常被标记为“中国–公民–生活–质量–高”，但是当我们计划将其与“中国人–生活–好”相匹配时，将其细分为“中国人-生计-生活”更有帮助，而不是普通的细分。</p>
<p><img src="https://uploader.shimo.im/f/KIUVXnEHurrHp0a0.png!thumbnail?fileGuid=J8rhpwtJ8CkV8GXD" alt="图片"></p>
<p>融合词级和字级信息的方法，在一定程度上可以缓解不同分词方法之间的不匹配问题，但这些方法仍然受到原有词序结构的影响。 它们通常依赖于一个现有的词的标记，这必须在同一时间做出分词选择，例如，“中国”(China)和“中国人”(Chinese)处理“中国人民”(Chinese people)。 混合只是在它们的框架中的一个位置进行指导。</p>
<p>特定的任务，如问题回答(QA)可以提出进一步的挑战，短文本匹配：</p>
<ul>
<li>基于文档的问答系统(DBQA)。匹配度反映对一个给定的问题，一个句子是他的回答的概率，问题和回答来源不同，因此会存在风格和句法结构都不同的问题。</li>
<li>基于知识的问题回答(KBQA)。一个关键任务是对知识库的谓词短语来匹配问题的关系表达式。</li>
</ul>
<p>最近的研究进展致力于多粒度信息的匹配建模。  Seo等人将单词和字符混合成一个简单的序列(单词级) ，并且Chen等人利用多个卷积内核大小来捕获不同的 n-grams。 但是汉语中的大多数字都可以看作是单独的词，因此直接将汉字与相应的词组合起来可能会失去这些字所能表达的意义。 由于顺序输入的原因，他们要么在处理字符序列时丢失字级信息，要么不得不做出分词选择。</p>
<h1 id="本文方法"><a href="#本文方法" class="headerlink" title="本文方法"></a>本文方法</h1><p>本文提出了一种用于中文问答中短文本匹配的多粒度方法，该方法利用基于Lattice的CNN提取单词格上的句子级特征。具体而言，LCN不再依赖于字符或单词级别序列，而是将单词格作为输入，其中每个可能的单词和字符将被平等对待并具有各自的上下文，以便它们可以在每一层进行交互。 对于每层中的每个单词，LCN可以通过合并方法以不同的粒度捕获不同的上下文单词。</p>
<h2 id="SIAMESE-ARCHITECTURE"><a href="#SIAMESE-ARCHITECTURE" class="headerlink" title="SIAMESE ARCHITECTURE"></a>SIAMESE ARCHITECTURE</h2><p>SIAMESE ARCHITECTURE 及其变体已被广泛应用于句子匹配和基于匹配的问答中，这种结构具有对称分量，可以从不同的输入通道中提取高级特征，这些特征共享相同的矢量空间中的参数和映射输入。 然后对句子表征进行合并，并与输出的相似性进行比较。</p>
<p>例如可以用多层 cnn 学习句子表示。 卷积层之间采用残差连接的方法以丰富特征且易于训练。然后采用池化层总结全局特征以获得句子级别的表示，并通过逐元素乘法进行合并。匹配分数通过多层感知器产生：$s=\sigma\left(\boldsymbol{W}_{2} \operatorname{ReLU}\left(\boldsymbol{W}_{1}\left(\boldsymbol{f}_{q u} \odot \boldsymbol{f}_{c a n}\right)+\boldsymbol{b}_{1}^{T}\right)+\boldsymbol{b}_{2}^{T}\right)$</p>
<p>其中fqu和fcan是问题和候选回答经过CNN编码的特征向量，σ是sigmoid函数W2，W1，b1T,b2T是参数，⊙是逐元素乘法。训练目标是最小化二进制交叉熵：$L=-\sum_{i=1}^{N}\left[y_{i} \log \left(s_{i}\right)+\left(1-y_{i}\right) \log \left(1-s_{i}\right)\right]$</p>
<p>其中，yi第i个训练对的标签。注意，句子表示可以是原始CNN，也可以是Lattice CNN。在原始CN中，卷积核按照顺序扫面每n-gram，并得到一个特征向量，该向量可以看作是中心词的表示，被传递至下一层。但是，每一个词在每一个lattice中可能具有不同粒度的上下文词，并且可以被视为具有相同长度的卷积核的中心。因此，不同于原始CNN，<strong>lattice CNN对于一个词可能产生多个特征向量</strong>，这是将标准CNN直接用于lattice输入的关键挑战。</p>
<p>如下图所示，“citizen”具有四个长度为3的上下文的中心词（China -citizen - life, China - citizen - alive, country - citizen -life, country - citizen - alive）。对于“citizen”，尺寸为3的卷积核会产生4个特征向量：</p>
<p><img src="https://uploader.shimo.im/f/GowtC1TcmsbJWxRu.png!thumbnail?fileGuid=J8rhpwtJ8CkV8GXD" alt="图片"></p>
<p>将”citizen“看作中心词，卷积核尺寸为3时，将会涉及到5个词和四个上下文组成，如上文所述，每个词用不同的颜色标记。然后，使用3个卷积核扫描所有的位置，产生4个3维特征向量。门控权重是通过dense layer根据这些特征向量计算的，反映了不同上下文构成的重要性。中心词的输出向量是他们的权重和，嘈杂上下文通过平滑处理会具有较小的权重。这些在不同上下文的池化操作，允许LCNs在word lattice上工作。</p>
<h2 id="Word-Lattice"><a href="#Word-Lattice" class="headerlink" title="Word Lattice"></a>Word Lattice</h2><p>如下图所示，一个word lattice是一个有向图 $G=<V，E>$ ，$V$是点集，$E$是边集。对于一个中文句子，是一个中文字符序列$S=c_{1:n}$，其所有可被视为单词的字符的子字符串都被视为顶点，即 V={ c_{i:j} |c_{i:j} 是单词}，然后所有相邻单词都根据其在原始句子中的位置，以有向边相连。即$E={e(c_{i:j}，c_{j:k}) | ∀ i，j，k s.t. c_{i:j}，c_{j:k} ∈ V }$</p>
<p>关键问题： 如何决定一个字符序列可以被描述成一个词？作者通过查询现有的词汇来解决这个问题，这些词是百度百科中的频繁词。注意到，大部分中国字符本身都可以被看作词，百度百科的语料库中就会包含这些词。 但这样就会不可避免的对word lattice引入噪声，通过模型的池化过程平滑处理。 构造的图可能会因为存在词汇表以外的词断开，因此，添加$$标签来替换这些词，以连接图。 显然，word lattice 是字符和所有可能的单词的集合。因此不需要进行分词，只需要将所有可能的信息嵌入lattice并且将其输入CNN。 word lattice内在的图结构允许所有可能的单词都被清晰表示。</p>
<p><img src="https://uploader.shimo.im/f/On7kSaZ6rWKH3598.png!thumbnail?fileGuid=J8rhpwtJ8CkV8GXD" alt="图片"></p>
<h2 id="lattice-based-CNN-layer"><a href="#lattice-based-CNN-layer" class="headerlink" title="lattice based CNN layer"></a>lattice based CNN layer</h2><p>采用 lattice based CNN layer 作为输入，而不是标准的 CNN，并利用池化机制合并由多个卷积核在不同上下文产生的特征向量。</p>
<p>形式上，卷积核尺寸为n 的lattice CNN层对词w在word lattice $G=<V,E>$ 下的输出特征向量是：</p>
<p><img src="https://uploader.shimo.im/f/KStboepUiyCCniUa.png!thumbnail?fileGuid=J8rhpwtJ8CkV8GXD" alt="图片"></p>
<p>其中，$f$ 为激活函数，$v_{w_{i}}$ 为该层中词 $w_{i}$ 对应的输入向量，$v_{w_{1}}:…:v_{w_{n}}$ 为这些向量的级联，$g$ 为池化函数，有最大池化、均值池化、门池化。 门池化的公式表示如下所示：</p>
<ul>
<li>$\alpha_{1}, \ldots, \alpha_{t}=\operatorname{softmax}\left\{\boldsymbol{v}_{g}^{T} \boldsymbol{v}_{1}+b_{g}, \ldots, \boldsymbol{v}_{g}^{T} \boldsymbol{v}_{t}+b_{g}\right\}$</li>
<li>$\text { gated-pooling }\left\{\boldsymbol{v}_{1}, \ldots, \boldsymbol{v}_{t}\right\}=\sum_{i=1}^{n} \alpha_{i} \times \boldsymbol{v}_{i}$</li>
</ul>
<p>其中，$v_g$ 和 $b_g$ 为参数，$ɑ_i$ 为 softmax 函数归一化的门控权重。门代表n-gram上下文的重要性，加权和可以控制嘈杂上下文词的传输。必要的时候进行padding。</p>
<p>word lattice可以看作有向图，并通过 Direct Graph Convolutional networks(DGCs)建模，该模型在相邻顶点上采用池化操作，忽略了n-gram的语义结构。但是，对某些情况来说，他们的构想可能和我们的相似。比如，如果我们将LCNs的卷积核尺寸设置为3，使用线性激活函数，假设LCN和DGC的池化是均值池化，则在每一层的每个词处，DGC计算中心词及其一阶邻居的平均值，LCN计算分别计算前一个和后一个词的均值，然后将他们加在中心词上。具体见实验部分。</p>
<p>最后，给定一个已经被构造成word lattice格式的句子，对于lattice的每一个节点，LCN层会产生一个类似于原始CNN的特征向量，这使得堆叠多个LCN以获得更抽象的特征表示更加容易。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="实验目标"><a href="#实验目标" class="headerlink" title="实验目标"></a>实验目标</h2><ul>
<li>word lattice 中的多粒度信息是否有助于基于问答任务的匹配；</li>
<li>LCNs 是否能很好地通过 lattice 捕获多粒度信息；</li>
<li>如何平衡 word lattice 引入的噪声和带有丰富信息的词。</li>
</ul>
<h2 id="数据集选取"><a href="#数据集选取" class="headerlink" title="数据集选取"></a>数据集选取</h2><p>数据集选自 NLPCC-2016 评估任务的中文问答数据集：</p>
<ul>
<li>DBQA：是一个基于文档的问题回答数据集。 在测试集中有8.8 k 的问题和182k 的问句对用于训练，6k 的问题和123k 的问句对用于测试。 平均每个问题有20.6个候选句子和1.04个黄金答案。 问题的平均长度为15.9个字符，每个候选句子平均有38.4个字符。 问句和句子都是自然语言的句子，可能比 KBQA 更多地共享相似的词语和表达方式。 但是候选句子是从网页上提取出来的，而且通常比问句长得多，还有许多不相关的从句。</li>
<li>KBQA：是一种基于知识的关系抽取数据集。 我们按照与[ Lai 等人2017]相同的预处理过程来清理数据集，并将问题中提到的实体替换为特殊标记。 在训练集中有14.3 k 问题，其中问题谓词对为273k，问题谓词对为156k，问题谓词对为9.4 k。 每个问题只包含一个黄金谓词。 每个问题平均有18.1个候选谓词和8.1个字符长度，而一个 KB 谓词平均只有3.4个字符长。 注意，知识库谓词通常是一个简洁的短语，与自然语言问题相比，用词选择有很大的不同，这给解决带来了不同的挑战。</li>
</ul>
<p>我们用来构造 word lattice 的词汇表包含 156k 个单词，其中包括 9.1k 个单字符的单词。 平均而言，每个 DBQA 问题在其 lattice 中包含 22.3 个标记(单词或字符) ，每个 DBQA 候选句子有 55.8 个标记，每个 KBQA 问题有 10.7 个标记，每个 KBQA 谓词包含 5.1 个标记。</p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img src="https://uploader.shimo.im/f/ww9nHeYK5OnuFVCV.png!thumbnail?fileGuid=J8rhpwtJ8CkV8GXD" alt="图片"></p>
]]></content>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer细节思考</title>
    <url>/2019/02/20/Transformer%E7%BB%86%E8%8A%82%E6%80%9D%E8%80%83/</url>
    <content><![CDATA[<p>今天研究一下Transformer的一些细节，总结一下。</p>
<a id="more"></a>
<blockquote>
<p>参考：</p>
<p><a href="https://state-of-art.top/2019/01/17/Transformer%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E7%8E%B0-%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/" target="_blank" rel="noopener">Transformer原理和实现-从入门到精通</a></p>
<p><a href="https://mp.weixin.qq.com/s?__biz=MjM5MzY4NzE3MA==&amp;mid=2247484632&amp;idx=2&amp;sn=9170e3f036098b5d428123abc1ac1520&amp;source=41#wechat_redirect" target="_blank" rel="noopener">绝对干货！NLP预训练模型：从transformer到albert</a></p>
</blockquote>
<h1 id="Transformer图解"><a href="#Transformer图解" class="headerlink" title="Transformer图解"></a>Transformer图解</h1><p>Transformer使用self-attention和position-encoding替代原始的RNN。</p>
<p><img src="https://uploader.shimo.im/f/WVDQjP11Yteuyi8D.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>每一层encoder</p>
<p><img src="https://uploader.shimo.im/f/bpxADrZe0G5t4yKl.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>每一层decoder</p>
<p><img src="https://uploader.shimo.im/f/TCRzJC0pMGDm5MJz.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<h1 id="Transformer具体结构"><a href="#Transformer具体结构" class="headerlink" title="Transformer具体结构"></a>Transformer具体结构</h1><p><img src="https://uploader.shimo.im/f/uXqNpDFaou7fzJlo.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<h2 id="加入Tensor"><a href="#加入Tensor" class="headerlink" title="加入Tensor"></a>加入Tensor</h2><p>输入的句子是一个词(ID)的序列，我们首先通过Embedding把它变成一个连续稠密的向量，如下图所示。</p>
<p><img src="https://uploader.shimo.im/f/7cQDKKuf7RJsUD3D.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>Embedding之后的序列会输入Encoder，首先经过Self-Attention层然后再经过全连接层，如下图所示。</p>
<p><img src="https://uploader.shimo.im/f/QCmk9C9Ee0OH3T6l.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>我们在计算𝑧𝑖时需要依赖所有时刻的输入𝑥1,…,𝑥𝑛，不过我们可以用矩阵运算一下子把所有的𝑧𝑖计算出来。而全连接网络的计算则完全是独立的，计算i时刻的输出只需要输入𝑧𝑖就足够了，因此很容易并行计算。下图更加明确的表达了这一点（注意全连接层每一个时刻的参数共享）。</p>
<p><img src="https://uploader.shimo.im/f/JA9x4GiSygcDqMiU.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<h2 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h2><p>对于输入的每一个向量(第一层是词的Embedding，其它层是前一层的输出)，我们首先需要生成3个新的向量Q、K和V，分别代表查询(Query)向量、Key向量和Value向量。Q表示为了编码当前词，需要去注意(attend to)其它(其实也包括它自己)的词，我们需要有一个查询向量。而Key向量可以认为是这个词的关键的用于被检索的信息，而Value向量是真正的内容。</p>
<p>对于普通的Attention机制，其计算过程如下：</p>
<p><img src="https://uploader.shimo.im/f/EVMl6DTRHZFNfBj1.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>每个向量的Key和Value向量都是它本身，而Q是当前隐状态ℎ𝑡，计算energy的时候我们计算Q(ℎ𝑡)和Key(ℎ¯𝑗)。然后用softmax变成概率，最后把所有的ℎ¯𝑗加权平均得到context向量。</p>
<p>而Transformer使用的self-attention的计算，以t1时刻为例，如下图：</p>
<p><img src="https://uploader.shimo.im/f/OWcXBG6iT4eJlB1M.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>Self-Attention里的Query不是隐状态，并且来自当前输入向量本身，因此叫作Self-Attention。另外Key和Value都不是输入向量，而是输入向量做了一下线性变换。当然理论上这个线性变换矩阵可以是Identity矩阵，也就是使得Key=Value=输入向量。因此可以认为普通的Attention是这里的特例。这样做的好处是模型可以根据数据从输入向量中提取最适合作为Key(可以看成一种索引)和Value的部分。类似的，Query也是对输入向量做一下线性变换，它让系统可以根据任务学习出最适合的Query，从而可以注意到(attend to)特定的内容。</p>
<p>具体的计算过程：比如图中的输入是两个词”thinking”和”machines”，我们对它们进行Embedding(这是第一层，如果是后面的层，直接输入就是向量了)，得到向量𝑥1,𝑥2。接着我们用3个矩阵分别对它们进行变换，得到向量𝑞1,𝑘1,𝑣1和𝑞2,𝑘2,𝑣2。比如𝑞1=𝑥1𝑊𝑄。图中𝑥1的shape是1x4，𝑊𝑄是4x3，得到的𝑞1是1x3。其它的计算也是类似的，为了能够使得Key和Query可以内积，我们要求𝑊𝐾和𝑊𝑄的shape是一样的，但是并不要求𝑊𝑉和它们一定一样(虽然实际论文实现是一样的)。</p>
<p>每个时刻t都计算出𝑄𝑡,𝐾𝑡,𝑉𝑡之后，我们就可以来计算Self-Attention了。以第一个时刻为例，我们首先计算𝑞1和𝑘1,𝑘2的内积，得到score。接下来使用softmax把得分变成概率，注意这里把得分除以8, 即sqrt(𝑑𝑘)之后再计算的softmax，根据论文的说法，这样计算梯度时会更加稳定(stable)。接下来用softmax得到的概率对所有时刻的V求加权平均，这样就可以认为得到的向量根据Self-Attention的概率综合考虑了所有时刻的输入信息</p>
<h3 id="Self-Attention的矩阵运算"><a href="#Self-Attention的矩阵运算" class="headerlink" title="Self-Attention的矩阵运算"></a>Self-Attention的矩阵运算</h3><p>第一步还是计算Q、K和V，不过不是计算某个时刻的𝑞𝑡,𝑘𝑡,𝑣𝑡了，而是一次计算所有时刻的Q、K和V。计算过程如下图所示。这里的输入是一个矩阵，矩阵的第i行表示第i个时刻的输入𝑥𝑖。</p>
<p><img src="https://uploader.shimo.im/f/wAyzetkWzdIE6WcA.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>接下来就是计算Q和K得到score，然后除以sqrt(dk)，然后再softmax，最后加权平均得到输出。全过程如下图所示。</p>
<p><img src="https://uploader.shimo.im/f/7jg3ECgtzDcFQee7.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head-Attention"></a>Multi-Head-Attention</h3><p><img src="https://uploader.shimo.im/f/Gvl4v25HaKoSjmaW.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>对于输入矩阵(time_step, num_input)，每一组Q、K和V都可以得到一个输出矩阵Z(time_step, num_features):</p>
<p><img src="https://uploader.shimo.im/f/K4i3L8oDT17I4zq1.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>但是后面的全连接网络需要的输入是一个矩阵而不是多个矩阵，因此我们可以把多个head输出的Z按照第二个维度拼接起来，但是这样的特征有一些多，因此Transformer又用了一个线性变换(矩阵𝑊𝑂)对它进行了压缩:</p>
<p><img src="https://uploader.shimo.im/f/SxjNbR105DKJ0XLE.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>整体过程：</p>
<p><img src="https://uploader.shimo.im/f/5ybvl5baeI1eC0L5.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>可以Q、K、V在维度上比词嵌入向量更低。他们的维度是64，而词嵌入和编码器的输入/输出向量的维度是512. 但实际上不强求维度更小，这只是一种基于架构上的选择，它可以使多头注意力（multiheaded attention）的大部分计算保持不变。</p>
<h3 id="使用self-attention的好处"><a href="#使用self-attention的好处" class="headerlink" title="使用self-attention的好处"></a>使用self-attention的好处</h3><p>比如我们要翻译如下句子”The animal didn’t cross the street because it was too tired”(这个动物无法穿越马路，因为它太累了)。这里的it到底指代什么呢，是animal还是street？要知道具体的指代，我们需要在理解it的时候同时关注所有的单词，重点是animal、street和tired，然后根据知识(常识)我们知道只有animal才能tired，而street是不能tired的。Self-Attention用Encoder在编码一个词的时候会考虑句子中所有其它的词，从而确定怎么编码当前词。如果把tired换成narrow，那么it就指代的是street了。</p>
<p>而LSTM(即使是双向的)是无法实现上面的逻辑的。为什么呢？比如前向的LSTM，我们在编码it的时候根本没有看到后面是tired还是narrow，所有它无法把it编码成哪个词。而后向的LSTM呢？当然它看到了tired，但是到it的时候它还没有看到animal和street这两个单词，当然就更无法编码it的内容了。</p>
<p>当然多层的LSTM理论上是可以编码这个语义的，它需要下层的LSTM同时编码了animal和street以及tired三个词的语义，然后由更高层的LSTM来把it编码成animal的语义。但是这样模型更加复杂。</p>
<p>但如果使用multo-head的attention，在编码it的时候有一个Attention Head(后面会讲到)注意到了Animal，因此编码后的it有Animal的语义。</p>
<h2 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h2><p>位置编码有很多方法，其中需要考虑的一个重要因素就是需要它编码的是相对位置的关系。比如两个句子：”北京到上海的机票”和”你好，我们要一张北京到上海的机票”。显然加入位置编码之后，两个北京的向量是不同的了，两个上海的向量也是不同的了，但是我们期望Query(北京1)Key(上海1)却是等于Query(北京2)Key(上海2)的。具体的编码算法我们在代码部分再介绍。位置编码加入后的模型如下图所示：</p>
<p><img src="https://uploader.shimo.im/f/SRIZ4VF3sKXfDYIP.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<h2 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h2><p>几乎所有的归一化方法都能起到平滑损失平面的作用，LN也一样。前面我们介绍过Batch Normalization，这个技巧能够让模型收敛的更快。但是Batch Normalization有一个问题——它需要一个minibatch的数据，而且这个minibatch不能太小(比如1)。另外一个问题就是它不能用于RNN，因为同样一个节点在不同时刻的分布是明显不同的。</p>
<p>假设我们的输入是一个minibatch的数据，我们再假设每一个数据都是一个向量，则输入是一个矩阵，每一行是一个训练数据，每一列都是一个特征。BatchNorm是对每个特征进行Normalization，而LayerNorm是对每个样本的不同特征进行Normalization，因此LayerNorm的输入可以是一行(一个样本)。</p>
<p>如下图所示，输入是(3,6)的矩阵，minibatch的大小是3，每个样本有6个特征。BatchNorm会对6个特征维度分别计算出6个均值和方差，然后用这两个均值和方差来分别对6个特征进行Normalization。而LayerNorm是分别对3个样本的6个特征求均值和方差，因此可以得到3个均值和方差，然后用这3个均值和方差对3个样本来做Normalization。</p>
<p><img src="https://uploader.shimo.im/f/TOvA2LPKMM1fykBk.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>BatchNorm看起来比较直观，我们在数据预处理也经常会把输入Normalize成均值为0，方差为1的数据，只不过它引入了可以学习的参数使得模型可以更加需要重新缓慢(不能剧烈)的调整均值和方差。而LayerNorm似乎有效奇怪，比如第一个特征是年龄，第二个特征是身高，把一个人的这两个特征求均值和方差似乎没有什么意义。论文里有一些讨论，都比较抽象。当然把身高和年龄平均并没有什么意义，但是对于其它层的特征，我们通过平均”期望”它们的取值范围大体一致，也可能使得神经网络调整参数更加容易，如果这两个特征实在有很大的差异，模型也可以学习出合适的参数让它来把取值范围缩放到更合适的区间。</p>
<h3 id="LN原理详解"><a href="#LN原理详解" class="headerlink" title="LN原理详解"></a>LN原理详解</h3><p>在Transformer中，LN被一笔带过，但却是不可或缺的一部分。每个子层的输出值为$\text { Layer } N o r m(x+\text { Sublayer }(x))$，这在网络结构图上非常明显（Norm即LN）。基本上所有的规范化技术，都可以概括为如下的公式：</p>
<ul>
<li>调整前：$h_{i}=f\left(a_{i}\right)$</li>
<li>调整后：$h_{i}^{\prime}=f\left(\frac{g_{i}}{\sigma_{i}}\left(a_{i}-u_{i}\right)+b_{i}\right)$</li>
</ul>
<p>对于隐层中某个节点的输出为对激活值 a_i 进行非线性变换 f() 后的h_i ，先使用均值$u$和方差$\sigma_{i}$对 $a_i$ 进行分布调整。如果将其理解成正态分布，就是把“高瘦”和“矮胖”的都调整回正常体型（深粉色），把偏离x=0的拉回中间来（淡紫色）。</p>
<p><img src="https://uploader.shimo.im/f/KOXOhpHIVBFM7fWl.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>这样做的第一个好处（平移）是，可以让激活值落入f()的梯度敏感区间（红色虚线的中间段）。梯度更新幅度变大，模型训练加快。第二个好处是，可以将每一次迭代的数据调整为相同分布（相当于“白化”），消除极端值，提升训练稳定性。</p>
<p>然而，在梯度敏感区内，隐层的输出接近于“线性”，模型表达能力会大幅度下降。引入 gain 因子g_i和 bias 因子b_i，为规范化后的分布再加入一点“个性”。需要注意的是， g_i和 b_i作为模型参数训练得到，$u_i$和$\sigma_{i}$在限定的数据范围内统计得到。BN 和 LN 的差别就在这里，前者在某一个 Batch 内统计某特定神经元节点的输出分布（跨样本），后者在某一次迭代更新中统计同一层内的所有神经元节点的输出分布（同一样本下）。</p>
<p>那么，为什么要舍弃 BN 改用 LN 呢？朴素版的 BN 是为 CNN 任务提出的，需要较大的 BatchSize 来保证统计量的可靠性，并在训练阶段记录全局的$u$和$\sigma$供预测任务使用。对于天然变长的 RNN 任务，需要对每个神经元进行在每个时序的状态进行统计。这不仅把原本非常简单的 BN 流程变复杂，更导致偏长的序列位置统计量不足。相比之下，LN 的使用限制就小很多，不需要在预测中使用训练阶段的统计量，即使 BatchSize = 1 也毫无影响。</p>
<p>个人理解，对于 CNN 图像类任务，每个卷积核可以看做特定的特征抽取器，对其输出做统计是有理可循的；对于 RNN 序列类任务，统计特定时序每个隐层的输出，毫无道理可言——序列中的绝对位置并没有什么显著的相关性。相反，同一样本同一时序同一层内，不同神经元节点处理的是相同的输入，在它们的输出间做统计合理得多。</p>
<p>从上面的分析可以看出，Normalization 通常被放在非线性化函数之前。以 GRU 为例，来看看 LN 是怎么设置的：</p>
<ul>
<li>$\left(\begin{array}{l}<br>\mathbf{z}_{t} \\<br>\mathbf{r}_{t}<br>\end{array}\right)=\mathbf{W}_{h} \mathbf{h}_{t-1}+\mathbf{W}_{x} \mathbf{x}_{t}$</li>
<li>$\hat{\mathbf{h}}_{t}=\tanh \left(\mathbf{W}_{\mathbf{x}_{t}}+\sigma\left(\mathbf{r}_{t}\right) \odot\left(\mathbf{U} \mathbf{h}_{t-1}\right) \mathbf{h}\right.$</li>
<li>$\mathbf{h}_{t}=\left(1-\sigma\left(\mathbf{z}_{t}\right)\right) \mathbf{h}_{t-1}+\sigma\left(\mathbf{z}_{t}\right) \hat{\mathbf{h}}_{\mathbf{t}}$</li>
</ul>
<p>可以看到，总体的原则是在“非线性之前单独处理各个矩阵”。对于 Transformer，主要的非线性部分在 FFN（ReLU） 和 Self-Attention（Softmax） 的内部，已经没有了显式的循环，但这些逐个叠加的同构子层像极了 GRU 和 LSTM 等 RNN 单元。信息的流动由沿着时序变成了穿过子层，把 LN 设置在每个子层的输出位置，意义上已经不再是“落入sigmoid 的梯度敏感空间来加速训练”了，个人认为更重要的是前文提到的“白化”—— 让每个词的向量化数值更加均衡，以消除极端情况对模型的影响，获得更稳定的深层网络结构 —— 就像这些词从 Embdding 层出来时候那样，彼此只有信息的不同，没有能量的多少。在和之前的 TWWT 实验一样的配置中，删除了全部的 LN 层后模型不再收敛。LN 正如 LSTM 中的tanh，它为模型提供非线性以增强表达能力，同时将输出限制在一定范围内。 因此，对于 Transformer 来说，LN 的效果已经不是“有多好“的范畴了，而是“不能没有”。</p>
<h3 id="MLP中的LN"><a href="#MLP中的LN" class="headerlink" title="MLP中的LN"></a>MLP中的LN</h3><p>设H是一层中隐层节点的数量，l是MLP的层数，我们可以计算LN的归一化统计量$u$和$\sigma$：</p>
<ul>
<li>$\mu^{l}=\frac{1}{H} \sum_{i=1}^{H} a_{i}^{l}$</li>
<li>$\sigma^{l}=\sqrt{\frac{1}{H} \sum_{i=1}^{H}\left(a_{i}^{l}-\mu^{l}\right)^{2}}$</li>
</ul>
<p>注意上面统计量的计算是和样本数量没有关系的，它的数量只取决于隐层节点的数量，所以只要隐层节点的数量足够多，我们就能保证LN的归一化统计量足够具有代表性。通过$u^l$和$\sigma^l$可以得到归一化后的值$\hat{\mathbf{a}}^{l}=\frac{\mathbf{a}^{l}-\mu^{l}}{\sqrt{\left(\sigma^{l}\right)^{2}+\epsilon}}$（其中$\epsilon$是一个很小的小数，防止除0）。</p>
<p>在LN中我们也需要一组参数来保证归一化操作不会破坏之前的信息，在LN中这组参数叫做增益（gain）g和偏置（bias) b（等同于BN中的$\gamma$和）$\beta$。假设激活函数为f，最终LN的输出为：$\mathbf{h}^{l}=f\left(\mathbf{g}^{l} \odot \hat{\mathbf{a}}^{l}+\mathbf{b}^{l}\right)$。合并公式并忽略参数l可以得到：$\mathbf{h}=f\left(\frac{\mathbf{g}}{\sqrt{\sigma^{2}+\epsilon}} \odot(\mathbf{a}-\mu)+\mathbf{b}\right)$</p>
<h3 id="RNN中的LN"><a href="#RNN中的LN" class="headerlink" title="RNN中的LN"></a>RNN中的LN</h3><p>在RNN中，我们可以非常简单的在每个时间片中使用LN，而且在任何时间片我们都能保证归一化统计量统计的是H个节点的信息。对于RNN时刻 t 时的节点，其输入是 t-1 时刻的隐层状态 h^{t-1}和 t 时刻的输入数据x_t，可以表示为：$\mathbf{a}^{t}=W_{h h} h^{t-1}+W_{x h} \mathbf{x}^{t}$。</p>
<p>接着我们便可以在$\mathbf{a}^{t}$上采取归一化过程：</p>
<ul>
<li>$\mu^{t}=\frac{1}{H} \sum_{i=1}^{H} a_{i}^{t}$</li>
<li>$\sigma^{t}=\sqrt{\frac{1}{H} \sum_{i=1}^{H}\left(a_{i}^{t}-\mu^{t}\right)^{2}}$</li>
<li>$\mathbf{h}^{t}=f\left(\frac{\mathbf{g}}{\sqrt{\left(\sigma^{t}\right)^{2}+\epsilon}} \odot\left(\mathbf{a}^{t}-\mu^{t}\right)+\mathbf{b}\right)$</li>
</ul>
<h4 id="LSTM中的LN"><a href="#LSTM中的LN" class="headerlink" title="LSTM中的LN"></a>LSTM中的LN</h4><p>LSTM的计算公式为：</p>
<ul>
<li>$\left(\begin{array}{c}<br>\mathbf{f}_{t} \\<br>\mathbf{i}_{t} \\<br>\mathbf{o}_{t} \\<br>\mathbf{g}_{t}<br>\end{array}\right)=\mathbf{W}_{h} \mathbf{h}_{t-1}+\mathbf{W}_{x} \mathbf{x}_{t}+b$</li>
<li>$\mathbf{c}_{t}=\sigma\left(\mathbf{f}_{t}\right) \odot \mathbf{c}_{t-1}+\sigma\left(\mathbf{i}_{t}\right) \odot \tanh \left(\mathbf{g}_{t}\right)$</li>
<li>$\mathbf{h}_{t}=\sigma\left(\mathbf{o}_{t}\right) \odot \tanh \left(\mathbf{c}_{t}\right)$</li>
</ul>
<p>使用了LN后的计算公式为：</p>
<ul>
<li>$\left(\begin{array}{c}<br>\mathbf{f}_{t} \\<br>\mathbf{i}_{t} \\<br>\mathbf{o}_{t} \\<br>g_{t}<br>\end{array}\right)=\quad L N\left(\mathbf{W}_{h} \mathbf{h}_{t-1} ; \boldsymbol{\alpha}_{1}, \boldsymbol{\beta}_{1}\right)+L N\left(\mathbf{W}_{x} \mathbf{x}_{t} ; \boldsymbol{\alpha}_{2}, \boldsymbol{\beta}_{2}\right)+b$</li>
<li>$\mathbf{c}_{t}=\sigma\left(\mathbf{f}_{t}\right) \odot \mathbf{c}_{t-1}+\sigma\left(\mathbf{i}_{t}\right) \odot \tanh \left(\mathbf{g}_{t}\right)$</li>
<li>$\mathbf{h}_{t}=\sigma\left(\mathbf{o}_{t}\right) \odot \tanh \left(L N\left(\mathbf{c}_{t} ; \boldsymbol{\alpha}_{3}, \boldsymbol{\beta}_{3}\right)\right)$</li>
</ul>
<h3 id="对照实验"><a href="#对照实验" class="headerlink" title="对照实验"></a>对照实验</h3><p>这里我们设置了一组对照试验来对比普通网络，BN以及LN在MLP和RNN上的表现。</p>
<h4 id="MLP上的归一化"><a href="#MLP上的归一化" class="headerlink" title="MLP上的归一化"></a>MLP上的归一化</h4><p>这里使用的是MNIST数据集，但是归一化操作只添加到了后面的MLP部分。Keras官方源码中没有LN的实现，我们可以通过<code>pip install keras-layer-normalization</code>进行安装，使用方法如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from keras_layer_normalization import LayerNormalization</span><br><span class="line"># 构建LN CNN网络</span><br><span class="line">model_ln &#x3D; Sequential()</span><br><span class="line">model_ln.add(Conv2D(input_shape &#x3D; (28,28,1), filters&#x3D;6, kernel_size&#x3D;(5,5), padding&#x3D;&#39;valid&#39;, activation&#x3D;&#39;tanh&#39;))</span><br><span class="line">model_ln.add(MaxPool2D(pool_size&#x3D;(2,2), strides&#x3D;2))</span><br><span class="line">model_ln.add(Conv2D(input_shape&#x3D;(14,14,6), filters&#x3D;16, kernel_size&#x3D;(5,5), padding&#x3D;&#39;valid&#39;, activation&#x3D;&#39;tanh&#39;))</span><br><span class="line">model_ln.add(MaxPool2D(pool_size&#x3D;(2,2), strides&#x3D;2))</span><br><span class="line">model_ln.add(Flatten())</span><br><span class="line">model_ln.add(Dense(120, activation&#x3D;&#39;tanh&#39;))</span><br><span class="line">model_ln.add(LayerNormalization()) # 添加LN运算</span><br><span class="line">model_ln.add(Dense(84, activation&#x3D;&#39;tanh&#39;))</span><br><span class="line">model_ln.add(LayerNormalization())</span><br><span class="line">model_ln.add(Dense(10, activation&#x3D;&#39;softmax&#39;))</span><br></pre></td></tr></table></figure>
<p>另外两个对照试验也使用了这个网络结构，不同点在于归一化部分。图3左侧是batchsize=128时得到的收敛曲线，从中我们可以看出BN和LN均能取得加速收敛的效果，且BN的效果要优于LN。图3右侧是batchsize=8是得到的收敛曲线，这时BN反而会减慢收敛速度，验证了我们上面的结论，对比之下LN要轻微的优于无归一化的网络，说明了LN在小尺度批量上的有效性：<br><img src="https://uploader.shimo.im/f/8jLUlejv9kLBLD4n.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"><img src="https://uploader.shimo.im/f/hpHSt6yZW9dhXRKY.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<h4 id="LSTM上的归一化"><a href="#LSTM上的归一化" class="headerlink" title="LSTM上的归一化"></a>LSTM上的归一化</h4><p>另外一组对照实验是基于imdb的二分类任务，使用了glove作为词嵌入。这里设置了无LN的LSTM和带LN的LSTM的作为对照试验，网络结构如下面代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from lstm_ln import LSTM_LN</span><br><span class="line">model_ln &#x3D; Sequential()</span><br><span class="line">model_ln.add(Embedding(max_features,100))</span><br><span class="line">model_ln.add(LSTM_LN(128))</span><br><span class="line">model_ln.add(Dense(1, activation&#x3D;&#39;sigmoid&#39;))</span><br><span class="line">model_ln.summary()</span><br></pre></td></tr></table></figure>
<p>LN对于RNN系列动态网络的收敛加速上的效果是略有帮助的。LN的有点主要体现在两个方面：</p>
<ul>
<li>LN得到的模型更稳定；</li>
<li>LN有正则化的作用，得到的模型更不容易过拟合。</li>
</ul>
<h2 id="残差连接"><a href="#残差连接" class="headerlink" title="残差连接"></a>残差连接</h2><p>每个Self-Attention层都会加一个残差连接，然后是一个LayerNorm层，如下图所示：</p>
<p><img src="https://uploader.shimo.im/f/9JnJi8vn7Izidhmp.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>下图展示了更多细节：输入𝑥1、𝑥2经self-attention层之后变成𝑧1、𝑧2，然后和残差连接的输入𝑥1、𝑥2加起来，然后经过LayerNorm层输出给全连接层。全连接层也是有一个残差连接和一个LayerNorm层，最后再输出给上一层：</p>
<p><img src="https://uploader.shimo.im/f/smiABSb61r8rvfqg.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>这样做的好处不言而喻，避免了梯度消失（求导时多了一个常数项）。</p>
<h2 id="Decoder层"><a href="#Decoder层" class="headerlink" title="Decoder层"></a>Decoder层</h2><p>Decoder和Encoder是类似的，如下图所示，区别在于它多了一个Encoder-Decoder Attention层，这个层的输入除了来自Self-Attention之外还有Encoder最后一层的所有时刻的输出。Encoder-Decoder Attention层的Query来自下一层，而Key和Value则来自Encoder的输出。</p>
<p><img src="https://uploader.shimo.im/f/IuBCl1ZxZG9BKh5z.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>在这个框架下，解码器实际上可看成一个神经网络语言模型，预测的时候，target中的每一个单词是逐个生成的，当前词的生成依赖两方面：一是Encoder的输出，二是target的前面的单词。例如，在生成第一个单词是，不仅依赖于Encoder的输出，还依赖于起始标志[CLS]；生成第二个单词是，不仅依赖Encoder的输出，还依赖起始标志和第一个单词…依此类推。这其实是说，在翻译当前词的时候，是看不到后面的要翻译的词。由上可以看出，这里的mask是动态的。</p>
<h1 id="Transformer细节讨论"><a href="#Transformer细节讨论" class="headerlink" title="Transformer细节讨论"></a>Transformer细节讨论</h1><h2 id="Scaled-Dot-product-Attention"><a href="#Scaled-Dot-product-Attention" class="headerlink" title="Scaled Dot-product Attention"></a>Scaled Dot-product Attention</h2><p><img src="https://uploader.shimo.im/f/wUrUGWRx6fOH6zaP.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>首先， Q 与 K 进行了一个点积操作，这个其实就是我在Attention讲到的 score 操作。然后，有一个 Scale 操作，其实就是为了防止结果过大，除以一个尺度标度 sqrt(dk)（注：主要因为dk比较大时，点乘注意力的表现变差，认为是由于点乘后的值过大，导致softmax函数趋近于边缘，梯度较小）， 其中dk是Q中一个向量的维度。再然后，经过一个Mask操作； Mask操作是将需要隐藏的数据设置为负无穷，这样经过后面的 Softmax 后就接近于 0，这样这些数据就不会对后续结果产生影响了。最后经过一个 Softmax 层， 然后计算 Attention Value。其公式：$\text {Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V$</p>
<p>这里解释一下 Scaled Dot-product Attention 在本文中的应用，也是称为 Self-Attention 的原因所在，这里的Q，K， V 都是一样的，意思就是说，这里是句子对句子自己进行Attention 来查找句子中词之间的关系，这是一件很厉害的事情，回想LSTM是怎么做的，再比较 Self-Attention， 直观的感觉，Self-Attention更能把握住词与词的语义特征，而LSTM对长依赖的句子，往往毫无办法，表征极差。</p>
<p>常用的attention主要有“Add-相加”和“Mul-相乘”两种：</p>
<ul>
<li>$\operatorname{score}\left(h_{j}, s_{i}\right)=<v, \tanh \left(W_{1} h_{j}+W_{2} s_{i}\right)>[A d d]$</li>
<li>$\operatorname{score}\left(h_{j}, s_{i}\right)=\left\langle W_{1} h_{j}, W_{2} s_{i}&gt;\quad[M u l]\right.$</li>
</ul>
<p>矩阵加法的计算更简单，但是外面套着tanh和 v，相当于一个完整的隐层。在整体计算复杂度上两者接近，但考虑到矩阵乘法已经有了非常成熟的加速算法，Transformer采用了Mul形式。</p>
<p>在模型效果上，《Massive Exploration of Neural Machine Translation Architectures》对不同Attention-Dimension (d_k) 下的Add和Mul进行了对比，如下图：</p>
<p><img src="https://uploader.shimo.im/f/I3AO3zusDdNUXzMC.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>可以看到，在$d_k$较小的时候，Add和Mul相差不大；随着$d_k$增大，Add明显超越了Mul。Transformer 设置 d_k=64虽然不在表格的范围内，但是可以推测Add仍略优于Mul。作者认为，$d_k$的增大将点积结果推向了softmax函数的梯度平缓区，影响了训练的稳定性。原话是这么说的：</p>
<blockquote>
<p>We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients.</p>
</blockquote>
<p>因此，Transformer 为“dot-product attention”加了一个前缀“scaled”，即引入一个温度因子（temperature）$\sqrt{d_{k}}$，中文全称“缩放的点积注意力网络”：$\text {Attention}(Q, K, V)=\operatorname{softmax}\left(Q K^{T} / \sqrt{d_{k}}\right) V$</p>
<p>等等，Add也离不开softmax，怎么没有这个问题呢？首先，左侧v的导数就是 tanh的输出，后者本身就是 [-1,1]之间的，一定不会超限。然后，右侧 W_1的导数就是隐层h_j ，必然是sigmoid或者其他激活函数的输出，值也不会太大。</p>
<p>回到Mul。左侧W_1的导数来自于$h_{j}\left(W_{2} s_{i}\right)$。如果s_i分布在(0,1)，那么$W_{2} s_{i}$就会扩展到(0, d_k) ，即点积可能会造成一个非常大的梯度值。</p>
<h2 id="MultiHeadAttention"><a href="#MultiHeadAttention" class="headerlink" title="MultiHeadAttention"></a>MultiHeadAttention</h2><p>Transformer 中使用 Multi-head Attention要注意以下几点：</p>
<p>首先， 在Encoder与Decoder中的黑色框中，采用的都是是 Self-Attention ，Q，K，V 同源。</p>
<p>其次，需要注意的是只有在Decoder中的Muti-head中才有 Mask 操作，而在Encoder中却没有，这是因为我们在预测第t个词时，需要将 t 时刻及以后的词遮住，只对前面的词进行 self-attention。</p>
<p>最后， 在黄色框中， 采用的是传统的Attention思路，Q 来自Decoder层， 而 K， V来自Encoder的输出 。</p>
<h2 id="最后的Linear与Softmax"><a href="#最后的Linear与Softmax" class="headerlink" title="最后的Linear与Softmax"></a>最后的Linear与Softmax</h2><p>一般都会在最后一层加一个前馈神经网络来增加泛化能力，最后用一个 softmax 来进行预测。</p>
<p>线性层的参数个数为d_model vocab_size， 一般来说，vocab_size会比较大，拿20000为例，那么只这层的参数就有51220000个，约为10的8次方，非常惊人。而在词向量那一层，同样也是这个数值，所以，一种比较好的做法是将这两个全连接层的参数共享，会节省不少内存，而且效果也不会差。</p>
<p>注意：在Encoder的输入embedding、Decoder的输入embedding后，增加了1个scale因子，而生成下一词汇的linear transformation前却未添加scale因子。</p>
<h2 id="Position-Embedding"><a href="#Position-Embedding" class="headerlink" title="Position Embedding"></a>Position Embedding</h2><p>在下图中，每一行对应一个词向量的位置编码，所以第一行对应着输入序列的第一个词。每行包含512个值，每个值介于1和-1之间。我们已经对它们进行了颜色编码，所以图案是可见的。20字(行)的位置编码实例，词嵌入大小为512(列)。你可以看到它从中间分裂成两半。这是因为左半部分的值由一个函数(使用正弦)生成，而右半部分由另一个函数(使用余弦)生成。然后将它们拼在一起而得到每一个位置编码向量。这种编码的优点是能够扩展到未知的序列长度(例如，当我们训练出的模型需要翻译远比训练集里的句子更长的句子时)。</p>
<ul>
<li>$P E_{(p o s, 2 i)}=\sin \left(p o s / 10000^{2 i / d_{\text {mod }}}\right)$</li>
<li>$P E_{(p o s, 2 i+1)}=\cos \left(p o s / 10000^{2 i / d_{\text {model }}}\right)$</li>
</ul>
<p><img src="https://uploader.shimo.im/f/Su3SrWtdY1L1R7zr.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>首先，可以证明出每个位置都能获得唯一的编码。其次，i决定了频率的大小，不同的i可以看成是不同的频率空间中的编码，是相互正交的，通过改变i的值，就能得到多维度的编码，类似于词向量的维度。这里2i&lt;=512（$d_{model}$）, 一共512维。想象一下，当2i大于$d_{model}$时会出现什么情况，这时sin函数的周期会变得非常大，函数值会非常接近于0，这显然不是我们希望看到的，因为这样和词向量就不在一个量级了，位置编码的作用被削弱了。另外，值得注意的是，位置编码是不参与训练的，而词向量是参与训练的。作者通过实验发现，位置编码参与训练与否对最终的结果并无影响。</p>
<p>其次，之所以对奇偶位置分别编码，论文中是这样解释的：</p>
<blockquote>
<p>We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of PEpos.</p>
</blockquote>
<p>相隔 k 个词的两个位置 pos 和 pos+k 的位置编码是由 k 的位置编码定义的一个线性变换，推导公式如下：</p>
<ul>
<li>$P E(p o s+k, 2 i)=P E(p o s, 2 i) P E(k, 2 i+1)+P E(p o s, 2 i+1) P E(k, 2 i)$</li>
<li>$P E(p o s+k, 2 i+1)=P E(p o s, 2 i+1) P E(k, 2 i+1)-P E(p o s, 2 i) P E(k, 2 i)$</li>
</ul>
<p>或者下面的推导更清晰一些：</p>
<ul>
<li>$\begin{aligned}<br>P E_{(p o s+k, 2 i)} &amp;=\sin \left((p o s+k) / 10000^{2 i / d_{\text {model}}}\right) \\<br>&amp;=\sin \left(p o s / 10000^{2 i / d_{\text {model}}}\right) \cos \left(k / 10000^{2 i / d_{\text {model}}}\right)+\cos \left(p o s / 10000^{2 i / d_{\text {model}}}\right) \sin \left(k / 10000^{2 i / d_{\text {model}}}\right) \\<br>&amp;=\cos \left(k / 10000^{2 i / d_{\text {model}}}\right) P E_{(p o s, 2 i)}+\sin \left(k / 10000^{2 i / d_{\text {model}}}\right) P E_{(p o s, 2 i+1)}<br>\end{aligned}$</li>
<li>$\begin{aligned}<br>P E_{(p o s+k, 2 i+1)} &amp;=\cos \left((p o s+k) / 10000^{2 i / d_{\text {model}}}\right) \\<br>&amp;=\cos \left(\text {pos} / 10000^{2 i / d_{\text {model}}}\right) \cos \left(k / 10000^{2 i / d_{\text {model}}}\right)-\sin \left(p o s / 10000^{2 i / d_{\text {model}}}\right) \sin \left(k / 10000^{2 i / d_{\text {model}}}\right) \\<br>&amp;=\cos \left(k / 10000^{2 i / d_{\text {model}}}\right) P E_{(p o s, 2 i+1)}-\sin \left(k / 10000^{2 i / d_{\text {model}}}\right) P E_{(\text {pos}, 2 i)}<br>\end{aligned}$</li>
</ul>
<h2 id="前馈网络"><a href="#前馈网络" class="headerlink" title="前馈网络"></a>前馈网络</h2><p>每个encoderLayer中，多头attention后会接一个前馈网络。这个前馈网络其实是两个全连接层，进行了如下操作：$FFN(x)=max(0,xW1+b1)W2+b2$</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">self.w_1 &#x3D; nn.Linear(d_model, d_ff)</span><br><span class="line"># self.w_1 &#x3D; nn.Conv1d(in_features&#x3D;d_model, out_features&#x3D;d_ff, kenerl_size&#x3D;1)</span><br><span class="line">self.w_2 &#x3D; nn.Linear(d_ff, d_model)</span><br><span class="line"># self.w_2 &#x3D; nn.Conv1d(in_features&#x3D;d_ff, out_features&#x3D;d_model, kenerl_size&#x3D;1)</span><br></pre></td></tr></table></figure>
<p>这两层的作用等价于两个 kenerl_size=1的一维卷积操作。<br>FFN 相当于将每个位置的Attention结果映射到一个更大维度的特征空间，然后使用ReLU引入非线性进行筛选，最后恢复回原始维度。需要说明的是，在抛弃了 LSTM 结构后，FFN 中的 ReLU成为了一个主要的能提供非线性变换的单元。</p>
<h2 id="Weight-Tying"><a href="#Weight-Tying" class="headerlink" title="Weight Tying"></a>Weight Tying</h2><p>论文的3.4小节Embeddings and Softmax，有这样一句话：</p>
<blockquote>
<p>In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [29].</p>
</blockquote>
<p>《Using the Output Embedding to Improve Language Models》这篇文章主要介绍的是RNNLM中的Weight Tying技术，不仅能压缩LM的大小，还能显著改善PPL表现。作者为了证明算法的普适性，在NMT任务上也进行了扩展实验。在seq2seq模型中，decoder可以近似地看作RNNLM，它必不可少地有一个embedding矩阵$U \in R^{C \times H}$和一个pre-softmax矩阵$V \in R^{C \times H}$，来完成词表大小C到隐层大小H的尺度转换：$h_{i n}=U^{T} C, \ldots, h_{p r e_{-} s o f t m a x}=V h_{o u t}$</p>
<p>。Weight Tying在操作上非常简单，即令U=V 。在OPEN-NMT的Pytorch实现版本中，仅仅一行代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if model_opt.share_decoder_embeddings:</span><br><span class="line">    generator[0].weight &#x3D; decoder.embeddings.word_lut.weight</span><br></pre></td></tr></table></figure>
<p>Transformer在英法和英德上，混用了源语言和目标语言的词表，因此使用了升级版的TWWT（Three way weight tying），把encoder的embedding层权重，也加入共享：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if model_opt.share_embeddings:</span><br><span class="line">    tgt_emb.word_lut.weight &#x3D; src_emb.word_lut.weight</span><br></pre></td></tr></table></figure>
<p>虽然weight共享了，但是embedding和pre-softmax仍然是两个不同的层，因为bias是彼此独立的。<br>在我个人的理解中，one-hot向量和对U的操作是“指定抽取”，即取出某个单词的向量行；pre-softmax对U的操作是“逐个点积”，对隐层的输出，依次计算其和每个单词向量行的变换结果。虽然具体的操作不同，但在本质上，U和V都是对任一的单词进行向量化表示（H列），然后按词表序stack起来（C行）。因此，两个权重矩阵在语义上是相通的。</p>
<p>也是由于上面两种操作方式的不同，U在反向传播中不如V训练得充分。将两者绑定在一起，缓和了这一问题，可以训练得到质量更高的新矩阵。另外，由于词表大小C通常比模型隐层大小H高出一个数量级，Weight Tying 可以显著减小模型的参数量。这个数据在论文中是28%~51%，我使用Transformer-base在非共享词表的英中方向上进行测试，模型参数量从 131,636,930 减小到 102,177,474，足足的22%。模型更小，收敛更快更容易。</p>
<p>下图是知乎上网友用Transformer-base模型跑的实验结果，绿色是Weight Tying版本，蓝色是基础版本。X轴为step，Y轴为Appromix BLEU。可以看到，两者的收敛速度接近，曲线平稳后绿色明显优于蓝色，峰值相差0.17。</p>
<p><img src="https://uploader.shimo.im/f/wi59MAoPcUomITKw.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<h2 id="Label-Smoothing"><a href="#Label-Smoothing" class="headerlink" title="Label Smoothing"></a>Label Smoothing</h2><p>Label Smoothing是一种正则化技术，核心功能就是防止过拟合，它的全称是 Label Smoothing Regularization（LSR），是 2015 年的经典论文《Rethinking the Inception Architecture for Computer Vision》的一个副产品。作者认为 LSR 可以避免模型 too confident。关于这一点论文里没有详细的解释，不过对于 NMT 任务来说，鼓励模型产生多样化的译文确实会帮助提升 BLEU 值，毕竟标准译文并不是唯一的。</p>
<p>最著名的正则化技术，dropout，通过随机抹掉一些节点来削弱彼此之间的依赖，通常设置在网络内部隐层。作为 dropout 的配合策略，LSR 考虑的是 softmax 层。</p>
<p>假设目标类别为y，任意类别为，ground-truth 分布为q(k)，模型预测分布为p(k)。显然，当k=y时，q(k)=1。当k!=y时，q(k)=0。LSR 为了让模型的输出不要过于贴合单点分布，选择在 gound-truth 中加入噪声。即削弱y的概率，并整体叠加一个独立于训练样例的均匀分布u(k)：$q^{\prime}(k)=(1-\epsilon) q(k)+\epsilon u(k)=(1-\epsilon) q(k)+\epsilon / K$。其中K是 softmax 的类别数。拆开来写可以看得清楚一点：</p>
<ul>
<li>$q^{\prime}(k)=1-\epsilon+\epsilon / K, \quad k=y$</li>
<li>$q^{\prime}(k)=\epsilon / K, \quad k \neq y$</li>
</ul>
<p>所有类别的概率和仍然是归一的。说白了就是把最高点砍掉一点，多出来的概率平均分给所有人。调整之后，交叉熵（损失函数）也随之变化：$H\left(q^{\prime}, p\right)=(1-\epsilon) H(q, p)+\epsilon H(u \cdot p)$。对于两个完全一致的分布，其交叉熵为0。LSR 可以看作是在优化目标中加入了正则项 H(u, p)，在模型输出偏离均匀分布时施以惩罚。</p>
<h2 id="Warmup-amp-Noam学习率更新"><a href="#Warmup-amp-Noam学习率更新" class="headerlink" title="Warmup &amp; Noam学习率更新"></a>Warmup &amp; Noam学习率更新</h2><h3 id="warmup"><a href="#warmup" class="headerlink" title="warmup"></a>warmup</h3><p>论文提出了一个全新的学习率更新公式：$\text {lrate}=d_{\text {model}}^{-0.5} \cdot \min \left(\text {step_num}^{-0.5}, \text {step_num} \cdot \text {warmup_steps}^{-1.5}\right)$</p>
<p>如果把min去掉的话，就变成一个以warmup_steps为分界点的分段函数。在该点之后，$\text { lrate }=d_{\text {model }}^{-0.5} \cdot \text { step_num }^{-0.5}$，是 decay 的部分。常用方法有指数衰减（exponential）、分段常数衰减（piecewise-constant）、反时限衰减（inverse-time）等等。Transformer 采用了负幂的形式，衰减速度先快后慢。</p>
<p>在该点之前，$\text { lrate }=d_{\text {model}}^{-0.5} \cdot \text { step_num } \cdot \text { warmup_steps}^{-1.5}$，是 warmup 的部分。Transformer 采用了线性函数的形式，warmup_steps 越大，斜率越小。</p>
<p><img src="https://uploader.shimo.im/f/bxxdHqj3LdOeu2mt.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>画在图上明显很多。Transformer 的学习率更新公式叫作“noam”，它将 warmup 和 decay 两个部分组合在一起，总体趋势是先增加后减小。</p>
<p>warmup为什么有效? 可参考<a href="https://www.zhihu.com/question/338066667" target="_blank" rel="noopener">https://www.zhihu.com/question/338066667</a>、<a href="https://zhuanlan.zhihu.com/p/45410279" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/45410279</a>。</p>
<h3 id="weight-decay"><a href="#weight-decay" class="headerlink" title="weight-decay"></a>weight-decay</h3><h4 id="指数衰减"><a href="#指数衰减" class="headerlink" title="指数衰减"></a>指数衰减</h4><p>指数衰减学习率是先使用较大的学习率来快速得到一个较优的解，然后随着迭代的继续,逐步减小学习率，使得模型在训练后期更加稳定。Transfomer使用的是指数衰减。指数衰减学习率的公式：$\text { decayed_learning_rate = learning_rate\cdotdecay_rate }^{(\text {global_step} / \text {decay_steps})}$</p>
<p>global_step是计数器,从0计数到训练的迭代次数，learning_rate是初始化的学习率，decayed_learning_rate是随着 global_step递增而衰减。显然，当global_step为初值0时， 有下面等式：$\text { decayed_learning_rate = learning_rate }$。</p>
<p>decay_steps用来控制衰减速度，如果decay_steps大一些, global_step/decay_steps就会增长缓慢一些。从而指数衰减学习率decayed_learning_rate就会衰减得慢一否则学习率很快就会衰减为趋近于0。</p>
<p>具体代码可参考：<a href="https://zhuanlan.zhihu.com/p/29421235" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29421235</a></p>
<h4 id="分段常数衰减"><a href="#分段常数衰减" class="headerlink" title="分段常数衰减"></a>分段常数衰减</h4><pre><code>参考：[https://www.jianshu.com/p/125fe2ab085b](https://www.jianshu.com/p/125fe2ab085b)
</code></pre><h4 id="自然指数衰减"><a href="#自然指数衰减" class="headerlink" title="自然指数衰减"></a>自然指数衰减</h4><p>它与指数衰减方式相似，不同的在于它的衰减底数是e，故而其收敛的速度更快，一般用于相对比较容易训练的网络，便于较快的收敛，其更新规则如下：$\text { decayed_learning_rate = learning_rate } * e^{\frac{-d e c a y_{r a t e}}{g l b b a_{s t e p}}}$</p>
<p>下图为为分段常数衰减、指数衰减、自然指数衰减三种方式的对比图，红色的即为分段常数衰减图，阶梯型曲线。蓝色线为指数衰减图，绿色即为自然指数衰减图，很明可以看到自然指数衰减方式下的学习率衰减程度要大于一般指数衰减方式，有助于更快的收敛。</p>
<p><img src="https://uploader.shimo.im/f/Pw9NQo5Yu503saf7.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<h4 id="多项式衰减"><a href="#多项式衰减" class="headerlink" title="多项式衰减"></a>多项式衰减</h4><p>应用多项式衰减的方式进行更新学习率，这里会给定初始学习率和最低学习率取值，然后将会按照给定的衰减方式将学习率从初始值衰减到最低值,其更新规则如下式所示：</p>
<ul>
<li>$\text { global_step }=\min (\text {global_step, decay_steps})$</li>
<li>$\begin{array}{c}<br>\text { decayed_learning_rate }=(\text {learning_rate}-\text {end_learning_rate}) *\left(1-\frac{\text {global_step}}{\text {decay_steps}}\right)^{\text {powe}} \\<br>+\text {end_learning_rate}<br>\end{array}$</li>
</ul>
<p>需要注意的是，有两个机制，降到最低学习率后，到训练结束可以一直使用最低学习率进行更新，另一个是再次将学习率调高，使用 decay_steps 的倍数，取第一个大于 global_steps 的结果，它是用来防止神经网络在训练的后期由于学习率过小而导致的网络一直在某个局部最小值附近震荡，这样可以通过在后期增大学习率跳出局部极小值。</p>
<p>如下图所示，红色线代表学习率降低至最低后，一直保持学习率不变进行更新，绿色线代表学习率衰减到最低后，又会再次循环往复的升高降低。</p>
<p><img src="https://uploader.shimo.im/f/TWHJqCaUQytQ7SLS.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<h4 id="余弦衰减"><a href="#余弦衰减" class="headerlink" title="余弦衰减"></a>余弦衰减</h4><p>余弦衰减就是采用余弦的相关方式进行学习率的衰减，衰减图和余弦函数相似。其更新机制如下式所示：</p>
<ul>
<li>$\text { global_step }=\min (\text {global_step, decay_steps})$</li>
<li>$\text {cosine_decay}=0.5 <em>\left(1+\cos \left(\pi </em> \frac{\text {global_step}}{\text {decay_steps}}\right)\right)$</li>
<li>$\text { decayed }=(1-\alpha) * \text { cosine_decay }+\alpha$</li>
<li>$\text { decayed_learning_rate = learning_rate * decayed }$</li>
</ul>
<p>如下图所示，红色即为标准的余弦衰减曲线，学习率从初始值下降到最低学习率后保持不变。蓝色的线是线性余弦衰减方式曲线，它是学习率从初始学习率以线性的方式下降到最低学习率值。绿色噪声线性余弦衰减方式：</p>
<p><img src="https://uploader.shimo.im/f/B2zQIXMV8trD31KQ.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<h2 id="Masking"><a href="#Masking" class="headerlink" title="Masking"></a>Masking</h2><h3 id="pad-mask"><a href="#pad-mask" class="headerlink" title="pad mask"></a>pad mask</h3><p>通常也是编码端的mask</p>
<p><img src="https://uploader.shimo.im/f/n9R8KVx3vwWbU3Wu.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<h3 id="sequence-mask"><a href="#sequence-mask" class="headerlink" title="sequence mask"></a>sequence mask</h3><p><img src="https://uploader.shimo.im/f/UcgsvQ46WN33NjqJ.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<h3 id="解码端的mask要同时考虑pad-mask和sequece-mask"><a href="#解码端的mask要同时考虑pad-mask和sequece-mask" class="headerlink" title="解码端的mask要同时考虑pad mask和sequece mask"></a>解码端的mask要同时考虑pad mask和sequece mask</h3><p><img src="https://uploader.shimo.im/f/D2Nwlz6tyDopu7un.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<h2 id=""><a href="#" class="headerlink" title=" "></a> </h2><h2 id="Word-Piece"><a href="#Word-Piece" class="headerlink" title="Word Piece"></a>Word Piece</h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>现在基本性能好一些的NLP模型，例如OpenAI GPT，google的BERT，在数据预处理的时候都会有WordPiece的过程。WordPiece字面理解是把word拆成piece一片一片，其实就是这个意思。</p>
<p>WordPiece的一种主要的实现方式叫做BPE（Byte-Pair Encoding）双字节编码。</p>
<p>BPE的过程可以理解为把一个单词再拆分，使得我们的此表会变得精简，并且寓意更加清晰。</p>
<p>比如”loved”,”loving”,”loves”这三个单词。其实本身的语义都是“爱”的意思，但是如果我们以单词为单位，那它们就算不一样的词，在英语中不同后缀的词非常的多，就会使得词表变的很大，训练速度变慢，训练的效果也不是太好。</p>
<p>BPE算法通过训练，能够把上面的3个单词拆分成”lov”,”ed”,”ing”,”es”几部分，这样可以把词的本身的意思和时态分开，有效的减少了词表的数量。</p>
<h3 id="BPE算法"><a href="#BPE算法" class="headerlink" title="BPE算法"></a>BPE算法</h3><blockquote>
<p>参考：<a href="https://plmsmile.github.io/2017/10/19/subword-units/" target="_blank" rel="noopener">https://plmsmile.github.io/2017/10/19/subword-units/</a></p>
</blockquote>
<p>BPE的大概训练过程：首先将词分成一个一个的字符，然后在词的范围内统计字符对出现的次数，每次将次数最多的字符对保存起来，直到循环次数结束。</p>
<p>我们模拟一下BPE算法。</p>
<ul>
<li>我们原始词表如下：{‘l o w e r ‘: 2, ‘n e w e s t ‘: 6, ‘w i d e s t ‘: 3, ‘l o w ‘: 5}</li>
<li>其中的key是词表的单词拆分层字母，再加代表结尾，value代表词出现的频率。</li>
<li>下面我们每一步在整张词表中找出频率最高相邻序列，并把它合并，依次循环。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">原始词表 &#123;&#39;l o w e r &lt;&#x2F;w&gt;&#39;: 2, &#39;n e w e s t &lt;&#x2F;w&gt;&#39;: 6, &#39;w i d e s t &lt;&#x2F;w&gt;&#39;: 3, &#39;l o w &lt;&#x2F;w&gt;&#39;: 5&#125;</span><br><span class="line">出现最频繁的序列 (&#39;s&#39;, &#39;t&#39;) 9</span><br><span class="line">合并最频繁的序列后的词表 &#123;&#39;n e w e st &lt;&#x2F;w&gt;&#39;: 6, &#39;l o w e r &lt;&#x2F;w&gt;&#39;: 2, &#39;w i d e st &lt;&#x2F;w&gt;&#39;: 3, &#39;l o w &lt;&#x2F;w&gt;&#39;: 5&#125;</span><br><span class="line">出现最频繁的序列 (&#39;e&#39;, &#39;st&#39;) 9</span><br><span class="line">合并最频繁的序列后的词表 &#123;&#39;l o w e r &lt;&#x2F;w&gt;&#39;: 2, &#39;l o w &lt;&#x2F;w&gt;&#39;: 5, &#39;w i d est &lt;&#x2F;w&gt;&#39;: 3, &#39;n e w est &lt;&#x2F;w&gt;&#39;: 6&#125;</span><br><span class="line">出现最频繁的序列 (&#39;est&#39;, &#39;&lt;&#x2F;w&gt;&#39;) 9</span><br><span class="line">合并最频繁的序列后的词表 &#123;&#39;w i d est&lt;&#x2F;w&gt;&#39;: 3, &#39;l o w e r &lt;&#x2F;w&gt;&#39;: 2, &#39;n e w est&lt;&#x2F;w&gt;&#39;: 6, &#39;l o w &lt;&#x2F;w&gt;&#39;: 5&#125;</span><br><span class="line">出现最频繁的序列 (&#39;l&#39;, &#39;o&#39;) 7</span><br><span class="line">合并最频繁的序列后的词表 &#123;&#39;w i d est&lt;&#x2F;w&gt;&#39;: 3, &#39;lo w e r &lt;&#x2F;w&gt;&#39;: 2, &#39;n e w est&lt;&#x2F;w&gt;&#39;: 6, &#39;lo w &lt;&#x2F;w&gt;&#39;: 5&#125;</span><br><span class="line">出现最频繁的序列 (&#39;lo&#39;, &#39;w&#39;) 7</span><br><span class="line">合并最频繁的序列后的词表 &#123;&#39;w i d est&lt;&#x2F;w&gt;&#39;: 3, &#39;low e r &lt;&#x2F;w&gt;&#39;: 2, &#39;n e w est&lt;&#x2F;w&gt;&#39;: 6, &#39;low &lt;&#x2F;w&gt;&#39;: 5&#125;</span><br><span class="line">出现最频繁的序列 (&#39;n&#39;, &#39;e&#39;) 6</span><br><span class="line">合并最频繁的序列后的词表 &#123;&#39;w i d est&lt;&#x2F;w&gt;&#39;: 3, &#39;low e r &lt;&#x2F;w&gt;&#39;: 2, &#39;ne w est&lt;&#x2F;w&gt;&#39;: 6, &#39;low &lt;&#x2F;w&gt;&#39;: 5&#125;</span><br><span class="line">出现最频繁的序列 (&#39;w&#39;, &#39;est&lt;&#x2F;w&gt;&#39;) 6</span><br><span class="line">合并最频繁的序列后的词表 &#123;&#39;w i d est&lt;&#x2F;w&gt;&#39;: 3, &#39;low e r &lt;&#x2F;w&gt;&#39;: 2, &#39;ne west&lt;&#x2F;w&gt;&#39;: 6, &#39;low &lt;&#x2F;w&gt;&#39;: 5&#125;</span><br><span class="line">出现最频繁的序列 (&#39;ne&#39;, &#39;west&lt;&#x2F;w&gt;&#39;) 6</span><br><span class="line">合并最频繁的序列后的词表 &#123;&#39;w i d est&lt;&#x2F;w&gt;&#39;: 3, &#39;low e r &lt;&#x2F;w&gt;&#39;: 2, &#39;newest&lt;&#x2F;w&gt;&#39;: 6, &#39;low &lt;&#x2F;w&gt;&#39;: 5&#125;</span><br><span class="line">出现最频繁的序列 (&#39;low&#39;, &#39;&lt;&#x2F;w&gt;&#39;) 5</span><br><span class="line">合并最频繁的序列后的词表 &#123;&#39;w i d est&lt;&#x2F;w&gt;&#39;: 3, &#39;low e r &lt;&#x2F;w&gt;&#39;: 2, &#39;newest&lt;&#x2F;w&gt;&#39;: 6, &#39;low&lt;&#x2F;w&gt;&#39;: 5&#125;</span><br><span class="line">出现最频繁的序列 (&#39;i&#39;, &#39;d&#39;) 3</span><br><span class="line">合并最频繁的序列后的词表 &#123;&#39;w id est&lt;&#x2F;w&gt;&#39;: 3, &#39;newest&lt;&#x2F;w&gt;&#39;: 6, &#39;low&lt;&#x2F;w&gt;&#39;: 5, &#39;low e r &lt;&#x2F;w&gt;&#39;: 2&#125;</span><br></pre></td></tr></table></figure>
<p>这样我们通过BPE得到了更加合适的词表了，这个词表可能会出现一些不是单词的组合，但是这个本身是有意义的一种形式，加速NLP的学习，提升不同词之间的语义的区分度。</p>
<h3 id="join-BPE"><a href="#join-BPE" class="headerlink" title="join BPE"></a>join BPE</h3><p>为目标语言和原语言一起使用BPE，即联合两种语言的词典去做BPE。提高了源语言和目标语言的分割一致性。训练中一般concat两种语言。</p>
<p>机器翻译时，编码与解码共享wordpiece model，可以处理翻译时目标语言和当前语言直接拷贝的词情况。</p>
<h2 id="阻止训练发散的方法"><a href="#阻止训练发散的方法" class="headerlink" title="阻止训练发散的方法"></a>阻止训练发散的方法</h2><p>降低学习速率、增加warmup_steps以及引入梯度截断。</p>
<h2 id="Dropout使用"><a href="#Dropout使用" class="headerlink" title="Dropout使用"></a>Dropout使用</h2><p>dropout设置在word embedding与position embedding相加之后，以及add+layernorm之前。</p>
<h2 id="Checkpoint-average"><a href="#Checkpoint-average" class="headerlink" title="Checkpoint average"></a>Checkpoint average</h2><p>指将训练一段时间的存储的checkpoint（文中选择为5个或20个），将这些checkpoint的所有模型的参数求平均。</p>
<h2 id="Xavier-初始化"><a href="#Xavier-初始化" class="headerlink" title="Xavier 初始化"></a>Xavier 初始化</h2><p>在官方 tensor2tensor 代码中，可以看到关于 weight 参数初始化的时候，采用了一种叫作xavier_uniform 的方法。也可以简称为 Xavier（读作 [ˈzeɪvjər]） 或者 Glorot，来自于作者 Xavier Glorot 在 2010 年和 Bengio 大神一起发表的《Understanding the difficulty of training deep feedforward neural networks》。</p>
<p>Xavier 是一种均匀初始化。其基线是朴素版本，即对于包含 n_i 个输入单元的网络层 layer_i ，其 weight 的初始值均匀采样自$\left[-1 / \sqrt{\left.\left.\left(n_{i}\right), 1 / \sqrt{(} n_{i}\right)\right]}\right.$。作者发现，在深层神经网络中，朴素版本的均匀初始化和激活函数（sigmoid 与 tanh）配合得不好。具体说来，就是不仅收敛得慢，训练平稳后的模型效果还差。如果使用预训练的模型来初始化网络，则能够明显改善训练过程。因此，作者认为，更换初始化方式是可取的。</p>
<p><img src="https://uploader.shimo.im/f/jhBiphXgghzG5p9R.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>首先来看看 sigmoid。它的线性区在 0.5 附近，偏向 1 或 0 表示则表示饱和。在理想情况下，激活值应当落在表达能力最强的非线性区间，远离线性区与饱和区。然而，在实际训练中，输出层 layer-4 在很长一段时间内都接近下饱和（黑）；其他层由上往下离线性区越来越近（蓝绿红）。虽然从 epoch-100 开始慢慢正常了，但这显然拖慢了收敛过程。</p>
<p><img src="https://uploader.shimo.im/f/kv4jPjIkXP58VeD1.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>然后是正负对称的 tanh。随着训练进行，自下而上（红绿蓝黑青），各层慢慢地都落入了饱和区。明明验证集的效果还达不到预期，模型却“训不动”了。</p>
<p>那么，我们需要什么样的激活值分布呢？一个标准的第 i 层可以分为线性$s^{i}=z^{i} W^{i}+b^{i}$和非线性$z^{i+1}=f\left(s^{i}\right)$两部分。在深层网络中，激活值的方差会自下而上逐层累积， 链式推导可得：$\operatorname{Var}\left[z^{i}\right]=\operatorname{Var}[x] \prod_{j=0}^{i-1} n_{j} \operatorname{Var}\left[W^{j}\right]$</p>
<p>对于反向传播，假定输出落在激活函数的线性区，即$f^{\prime}\left(s_{k}^{i}\right) \approx 1$。同样应用链式法则，各参数的梯度为（d 为总层数）：</p>
<ul>
<li>$\operatorname{Var}\left[\frac{\partial \text { cost }}{s^{i}}\right]=\operatorname{Var}\left[\frac{\partial \text { cost }}{s^{d}}\right] \prod_{j=i}^{d} n_{j+1} \operatorname{Var}\left[W^{j}\right]$</li>
<li>$\operatorname{Var}\left[\frac{\partial \operatorname{cost}}{w^{i}}\right]=\operatorname{Var}\left[\frac{\partial \operatorname{cost}}{s^{i}}\right] \operatorname{Var}\left[z^{i}\right]$</li>
</ul>
<p>现在要回到正题上了。一个稳定的深层网络，要求$\operatorname{Var}\left[z^{i}\right]$和$\operatorname{Var}\left[\partial \operatorname{cost} / s^{i}\right]$在各层保持一致，这样可以让激活值（forward）始终远离饱和区，并且梯度（backward）不会消失或爆炸。根据上面的公式，可以得到约束目标：$\forall i, n_{i} \operatorname{Var}\left[W^{i}\right]=1=n^{i+1} \operatorname{Var}\left[W^{i}\right]$</p>
<p>于方差是和输入单元的个数有关的，所以正向和反向分别使用了$n_i$和$n_{i+1}$。这两个等式显然不能同时成立，只好折中一下，让$\operatorname{Var}\left[W^{i}\right]=2 /\left(n_{i}+n_{i+1}\right)$。补充一条基础知识，对于均匀分布U[a,b] ，其方差为$(b-a)^{2} / 12$。现在开始解方程，让 a=b 并且$(b-a)^{2} / 12=2 /\left(n_{i}+n_{i+1}\right)$，得到 Xavier 均匀分布的最终形式：$U\left[-\sqrt{\frac{6}{n_{i}+n_{i+1}}}, \quad \sqrt{\frac{6}{n_{i}+n_{i+1}}}\right]$</p>
<h2 id="Transformer有哪些缺点"><a href="#Transformer有哪些缺点" class="headerlink" title="Transformer有哪些缺点"></a>Transformer有哪些缺点</h2><ul>
<li>评价不同模型需要考虑的是：<ul>
<li>句法特征提取能力</li>
<li>语义特征提取能力：Transformer &gt;&gt; 原生CNN == 原生RNN</li>
<li>长距离特征捕获能力：Transformer &gt; 原生RNN &gt;&gt; 原生CNN</li>
<li>任务综合特征抽取能力：Transformer最强</li>
<li>并行计算能力及运行效率：Transformer = 原生CNN &gt;&gt; 原生RNN</li>
</ul>
</li>
<li>超长文本：Transformer-XL</li>
<li>计算简化：ALBert</li>
</ul>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>用GitHub+Hexo搭建个人网站</title>
    <url>/2019/02/07/%E7%94%A8GitHub-Hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/</url>
    <content><![CDATA[<p>假期宅在家里，研究了一下用github搭建个人网站，把里面使用到的工具和命令总结一下。相关代码可参考：<a href="https://github.com/majing2019/myblogs" target="_blank" rel="noopener">https://github.com/majing2019/myblogs</a><br><a id="more"></a></p>
<h1 id="安装相关软件"><a href="#安装相关软件" class="headerlink" title="安装相关软件"></a>安装相关软件</h1><blockquote>
<p>参考：<br><a href="https://zhuanlan.zhihu.com/p/26625249" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26625249</a><br><a href="https://zhuanlan.zhihu.com/p/62555815" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/62555815</a></p>
</blockquote>
<ul>
<li>npm install -g hexo-cli</li>
<li>hexo init blog</li>
<li>cd ~/blog</li>
<li>export CC=/usr/bin/clang</li>
<li>export CXX=/usr/bin/clang++</li>
<li>npm install</li>
<li>npm install hexo-server —save</li>
<li>hexo server</li>
<li>在<a href="http://localhost:4000" target="_blank" rel="noopener">http://localhost:4000</a>访问网站首页</li>
<li>npm install hexo-deployer-git —save</li>
</ul>
<h1 id="建立repository"><a href="#建立repository" class="headerlink" title="建立repository"></a>建立repository</h1><blockquote>
<p>参考：<br><a href="https://help.github.com/en/github/working-with-github-pages" target="_blank" rel="noopener">https://help.github.com/en/github/working-with-github-pages</a><br><a href="https://github.community/t5/GitHub-Pages/404-Error/td-p/14331" target="_blank" rel="noopener">https://github.community/t5/GitHub-Pages/404-Error/td-p/14331</a></p>
</blockquote>
<ul>
<li>创建username.github.io的repository</li>
<li>在Settings-&gt;Github Pages中升级账户</li>
</ul>
<h1 id="修改相关配置"><a href="#修改相关配置" class="headerlink" title="修改相关配置"></a>修改相关配置</h1><ul>
<li>修改_config.yml<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: &lt;repository url&gt; #git@github.com:sufaith&#x2F;sufaith.github.io.git</span><br><span class="line">  branch: [branch] #master</span><br><span class="line">  message: [message]</span><br><span class="line">url: majing2019.github.io</span><br></pre></td></tr></table></figure></li>
<li>在source文件夹下创建CNAME文件，内容为二级域名</li>
<li>在~/blog目录下运行hexo generate</li>
<li>hexo clean &amp;&amp; hexo deploy</li>
<li>访问 <a href="https://majing2019.github.io/archives/" target="_blank" rel="noopener">https://majing2019.github.io/archives/</a></li>
</ul>
<h1 id="绑定域名"><a href="#绑定域名" class="headerlink" title="绑定域名"></a>绑定域名</h1><ul>
<li>登录<a href="https://www.aliyun.com/" target="_blank" rel="noopener">https://www.aliyun.com/</a>注册了一个域名majsunflower.cn</li>
<li>添加一个域名解析<ul>
<li>类型CNAME，主机记录www，记录值majing2019.github.io</li>
<li>类型A，主机记录@，记录值是对应的ip地址，可通过ping majing2019.github.io获得</li>
</ul>
</li>
<li>在github仓库中设置custom domain</li>
<li>在blog下创建source/CNAME文件，并写入majsunflower.cn</li>
</ul>
<h1 id="编写自己的个性化网站"><a href="#编写自己的个性化网站" class="headerlink" title="编写自己的个性化网站"></a>编写自己的个性化网站</h1><blockquote>
<p>参考：<br><a href="https://zhwangart.github.io/2018/11/30/Ocean/" target="_blank" rel="noopener">https://zhwangart.github.io/2018/11/30/Ocean/</a></p>
</blockquote>
<h2 id="更换主题"><a href="#更换主题" class="headerlink" title="更换主题"></a>更换主题</h2><ul>
<li>在<a href="https://hexo.io/themes/" target="_blank" rel="noopener">https://hexo.io/themes/</a>中选择一个主题</li>
<li>git clone <a href="https://github.com/zhwangart/hexo-theme-ocean.git" target="_blank" rel="noopener">https://github.com/zhwangart/hexo-theme-ocean.git</a> themes/ocean</li>
<li>修改_config.yml中theme为ocean</li>
</ul>
<h2 id="配置语言"><a href="#配置语言" class="headerlink" title="配置语言"></a>配置语言</h2><ul>
<li>_config.yml中language改为zh-CN</li>
</ul>
<h2 id="评论功能"><a href="#评论功能" class="headerlink" title="评论功能"></a>评论功能</h2><blockquote>
<p>参考：<br><a href="https://zhwangart.github.io/2018/12/06/Gitalk/" target="_blank" rel="noopener">https://zhwangart.github.io/2018/12/06/Gitalk/</a></p>
</blockquote>
<ul>
<li>在<a href="https://github.com/settings/applications/new" target="_blank" rel="noopener">https://github.com/settings/applications/new</a>申请<ul>
<li>后续可在<a href="https://github.com/settings/developers" target="_blank" rel="noopener">https://github.com/settings/developers</a>中修改app相关内容</li>
<li>注意Authorization callback URL在网站绑定域名后需要写域名</li>
</ul>
</li>
<li>填写themes/ocean/_config.yml中gitalk相关字段</li>
</ul>
<h2 id="使用图床"><a href="#使用图床" class="headerlink" title="使用图床"></a>使用图床</h2><blockquote>
<p>参考：<br><a href="https://zhuanlan.zhihu.com/p/26625249" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26625249</a><br><a href="https://blog.csdn.net/qq_36305327/article/details/71578290" target="_blank" rel="noopener">https://blog.csdn.net/qq_36305327/article/details/71578290</a></p>
</blockquote>
<ul>
<li><p>到<a href="https://www.qiniu.com/" target="_blank" rel="noopener">https://www.qiniu.com/</a>上添加对象存储<a href="https://portal.qiniu.com/kodo/bucket/" target="_blank" rel="noopener">https://portal.qiniu.com/kodo/bucket/</a></p>
</li>
<li><p>在markdown中可直接引用图片</p>
</li>
</ul>
<h2 id="添加关于"><a href="#添加关于" class="headerlink" title="添加关于"></a>添加关于</h2><ul>
<li>hexo new page about</li>
<li>使用markdown编写source/about/index.md</li>
</ul>
<h2 id="添加标签"><a href="#添加标签" class="headerlink" title="添加标签"></a>添加标签</h2><ul>
<li>hexo new page tags // 创建标签页面</li>
<li>修改source/tags/index.md为<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: Tags</span><br><span class="line">date: 2019-04-19 17:28:54</span><br><span class="line">type: tags</span><br><span class="line">layout: &quot;tags&quot;</span><br><span class="line">---</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="添加相册"><a href="#添加相册" class="headerlink" title="添加相册"></a>添加相册</h2><ul>
<li>hexo new page gallery</li>
<li>编辑source/gallery/index.md<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: Gallery</span><br><span class="line">albums: [</span><br><span class="line">        [&quot;img_url&quot;,&quot;img_caption&quot;],</span><br><span class="line">        [&quot;img_url&quot;,&quot;img_caption&quot;]</span><br><span class="line">        ]</span><br><span class="line">---</span><br></pre></td></tr></table></figure></li>
<li>如果出现相册加载过慢的问题，可以参考<a href="https://zhwangart.github.io/2019/07/02/Ocean-Issues/" target="_blank" rel="noopener">https://zhwangart.github.io/2019/07/02/Ocean-Issues/</a>解决</li>
</ul>
<h2 id="添加分类"><a href="#添加分类" class="headerlink" title="添加分类"></a>添加分类</h2><ul>
<li>hexo new page categories</li>
</ul>
<h2 id="本地搜索"><a href="#本地搜索" class="headerlink" title="本地搜索"></a>本地搜索</h2><blockquote>
<p>参考：<br><a href="https://github.com/zhwangart/gitalk/issues/7#issuecomment-451877736" target="_blank" rel="noopener">https://github.com/zhwangart/gitalk/issues/7#issuecomment-451877736</a></p>
</blockquote>
<ul>
<li>npm install hexo-generator-searchdb —save</li>
<li>在blog/_config.yml中添加配置<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">search:</span><br><span class="line">  path: search.xml</span><br><span class="line">  field: post</span><br><span class="line">  content: true</span><br></pre></td></tr></table></figure></li>
<li>hexo g</li>
<li><del>修改themes/ocean/layout/_partial/after-footer.ejs中修改如下内容</del><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;% if (theme.local_search.enable)&#123; %&gt;</span><br><span class="line">  &lt;%- js(&#39;&#x2F;js&#x2F;search&#39;) %&gt;</span><br><span class="line">&lt;% &#125; %&gt;</span><br><span class="line"></span><br><span class="line">&lt;%- js(&#39;&#x2F;js&#x2F;ocean&#39;) %&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="用Markdown写文章"><a href="#用Markdown写文章" class="headerlink" title="用Markdown写文章"></a>用Markdown写文章</h2><h3 id="创建文章"><a href="#创建文章" class="headerlink" title="创建文章"></a>创建文章</h3><blockquote>
<p>参考：<br><a href="https://chaxiaoniu.oschina.io/2017/07/10/Markdown-Grammar/" target="_blank" rel="noopener">https://chaxiaoniu.oschina.io/2017/07/10/Markdown-Grammar/</a></p>
</blockquote>
<ul>
<li>hexo new “用GitHub+Hexo搭建个人网站”</li>
<li>文章格式如下<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: 用GitHub+Hexo搭建个人网站 #文章页面上的显示名称，可以任意修改</span><br><span class="line">date: date  #文章生成时间，一般不改，当然也可以任意修改</span><br><span class="line">tags: [Hexo, Ocean] #文章标签，可空。也可以按照你的习惯写分类名字，注意后面有空格，多个标签可以用[]包含，以&#96;,&#96;隔开</span><br><span class="line">categories: [技术] #分类</span><br><span class="line">---</span><br><span class="line">这里是你博客列表显示的摘要文字</span><br><span class="line">&lt;!--more--&gt;</span><br><span class="line">以下是博客的正文，以上面的格式为分隔线</span><br></pre></td></tr></table></figure></li>
<li>如果不希望显示时有目录，需要添加<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">toc: false</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="添加公式"><a href="#添加公式" class="headerlink" title="添加公式"></a>添加公式</h3><blockquote>
<p>参考：<br><a href="https://blog.csdn.net/Aoman_Hao/article/details/81381507" target="_blank" rel="noopener">https://blog.csdn.net/Aoman_Hao/article/details/81381507</a></p>
</blockquote>
<ul>
<li>npm uninstall hexo-renderer-marked —save</li>
<li>npm install hexo-renderer-kramed —save</li>
<li>修改node_modules/hexo-renderer-kramed/lib/renderer.js<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">function formatText(text) &#123;</span><br><span class="line">  &#x2F;&#x2F; Fit kramed&#39;s rule: $$ + \1 + $$</span><br><span class="line">  &#x2F;&#x2F; return text.replace(&#x2F;&#96;\$(.*?)\$&#96;&#x2F;g, &#39;$$$$$1$$$$&#39;);</span><br><span class="line">  return text;&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>npm uninstall hexo-math —save</p>
</li>
<li><p>npm install hexo-renderer-mathjax —save</p>
</li>
<li>修改node_modules/hexo-renderer-mathjax/mathjax.html，注释掉最后一行script并改为<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;script src&#x3D;&quot;https:&#x2F;&#x2F;cdnjs.cloudflare.com&#x2F;ajax&#x2F;libs&#x2F;mathjax&#x2F;2.7.1&#x2F;MathJax.js?config&#x3D;TeX-MML-AM_CHTML&quot;&gt;&lt;&#x2F;script&gt;</span><br></pre></td></tr></table></figure></li>
<li>修改node_modules/kramed/lib/rules/inline.js<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">escape: &#x2F;^\\([&#96;*\[\]()# +\-.!_&gt;])&#x2F;,</span><br><span class="line">em: &#x2F;^\*((?:\*\*|[\s\S])+?)\*(?!\*)&#x2F;,</span><br></pre></td></tr></table></figure></li>
<li>修改themes/ocean/_config.yml增加<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mathjax: true</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="添加文章封面"><a href="#添加文章封面" class="headerlink" title="添加文章封面"></a>添加文章封面</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: Post name</span><br><span class="line">photos: [</span><br><span class="line">        [&quot;img_url&quot;],</span><br><span class="line">        [&quot;img_url&quot;]</span><br><span class="line">        ]</span><br><span class="line">---</span><br></pre></td></tr></table></figure>
<h3 id="添加视频"><a href="#添加视频" class="headerlink" title="添加视频"></a>添加视频</h3><blockquote>
<p>参考：<br><a href="https://blog.csdn.net/u010953692/article/details/79075884" target="_blank" rel="noopener">https://blog.csdn.net/u010953692/article/details/79075884</a><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;iframe height&#x3D;498 width&#x3D;510 src&#x3D;&quot;http:&#x2F;&#x2F;q503tsu73.bkt.clouddn.com&#x2F;IMG_0018.mp4?e&#x3D;1580557032&amp;token&#x3D;05Ii263bPN3Z-CT3JPRaRfWi5sXIj8pwX6V1bN2j:-rUb7zOxk-WfRrhdJtNdOOGfy58&#x3D;&amp;attname&#x3D;&quot; frameborder&#x3D;0 allowfullscreen&gt;&lt;&#x2F;iframe&gt;</span><br></pre></td></tr></table></figure></p>
</blockquote>
<h3 id="文章置顶"><a href="#文章置顶" class="headerlink" title="文章置顶"></a>文章置顶</h3><ul>
<li>npm uninstall hexo-generator-index —save</li>
<li>npm install hexo-generator-index-pin-top —save</li>
<li>在需要置顶的文章上加入<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line"> title: 新增文章置顶</span><br><span class="line"> top: ture</span><br><span class="line"> ---</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="同时部署在Github和Coding上"><a href="#同时部署在Github和Coding上" class="headerlink" title="同时部署在Github和Coding上"></a>同时部署在Github和Coding上</h1><blockquote>
<p>参考：<br><a href="https://tomatoro.cn/archives/3de92cb5.html" target="_blank" rel="noopener">https://tomatoro.cn/archives/3de92cb5.html</a></p>
</blockquote>
<ul>
<li><a href="https://coding.net/" target="_blank" rel="noopener">https://coding.net/</a>上创建devops项目</li>
<li>修改blog/_config.yml中的deploy<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: </span><br><span class="line">    github: git@github.com:majing2019&#x2F;majing2019.github.io.git</span><br><span class="line">    coding: git@e.coding.net:majsunflower&#x2F;myblog.git</span><br><span class="line">  branch: master</span><br><span class="line">  message: my blog</span><br></pre></td></tr></table></figure></li>
<li>将id_rsa.pub的公钥复制到个人账户下，ssh -T git@git.coding.net验证是否成功</li>
<li>hexo deploy -g部署到coding上</li>
<li>配置静态页面即可访问：<a href="http://02ss3u.coding-pages.com/" target="_blank" rel="noopener">https://02ss3u.coding-pages.com/</a></li>
<li>在自定义域名里增加：<a href="http://majsunflower.cn/">majsunflower.cn</a></li>
<li>阿里云<a href="https://homenew.console.aliyun.com/" target="_blank" rel="noopener">https://homenew.console.aliyun.com/</a>中修改域名相关配置，区分境内和境外的访问</li>
</ul>
<h1 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h1><ul>
<li><p>部署：hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</p>
</li>
<li><p>本地测试：hexo server</p>
</li>
</ul>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Ocean</tag>
      </tags>
  </entry>
</search>

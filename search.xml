<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>机器学习基础——支持向量机SVM</title>
    <url>/2021/04/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E2%80%94%E2%80%94%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM/</url>
    <content><![CDATA[<p>使用keras进行二分类时，常使用binary_crossentropy作为损失函数。那么它的原理是什么，跟categorical_crossentropy、sparse_categorical_crossentropy有什么区别？在进行文本分类时，如何选择损失函数，有哪些优化损失函数的方式？本文将从原理到实现进行一一介绍。</p>
<a id="more"></a>
<h1 id="几何定义"><a href="#几何定义" class="headerlink" title="几何定义"></a>几何定义</h1><p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/SVM/1.png" alt="img"></p>
<p>假设空间中的点如上图，我们希望找到一个决策平面，使得两类点能区分开，且点到直线距离和是最近的。我们假设最好的决策平面是$wx+b=1$和$wx+b=-1$（即使$wx+b$不等于1，我们也可以通过等式两边同时除以b使得其等于1），$wx$表示两个向量的点积，$w$表示决策平面的方向。</p>
<p>两个决策边界之间的距离是$\frac{2}{||w||}$，证明如下：</p>
<ul>
<li>假设落在上边界和下边界的两个点分别为$x_1$和$x_2$</li>
<li>$w^{T} x_{1}+b=1$</li>
<li>$w^{T} x_{2}+b=-1$</li>
<li>$\left(w^{T} x_{1}+b\right)-\left(w^{T} x_{2}+b\right)=2$</li>
<li>$w^{T}\left(x_{1}-x_{2}\right)=2$</li>
<li>$d_{1}=d_{2}=\frac{w^{T}\left(x_{1}-x_{2}\right)}{2|w|_{2}}=\frac{2}{2|w|_{2}}=\frac{1}{|w|_{2}}=\frac{\frac{w^{T}}{|w|_{2}}\left(x_{1}-x_{2}\right)}{2}$<ul>
<li>它的物理意义是上图中$x_1$、$x_2$的连线（即红色线）在$w$法向量上的投影向量的长度</li>
<li>$d_{1}+d_{2}=\frac{2}{|w|_{2}}$</li>
</ul>
</li>
</ul>
<h1 id="代数定义"><a href="#代数定义" class="headerlink" title="代数定义"></a>代数定义</h1><p>我们希望决策边界的距离越大越好（max margin）, 由于$w^w$表示两个决策边界距离，因此SVM数学模型如下：$\begin{aligned}\min _{\mathbf{w}, b} &amp; \frac{1}{2} \mathbf{w}^{\top} \mathbf{w} \\\text{s.t. } &amp; y_{i}\left(\mathbf{w}^{\top} \mathbf{x}_{i}+b\right) \geq 1, \quad i=1,\ldots, n\end{aligned}$</p>
<p>当我们已经求解出$w$，则使用下式预测：$h(\mathbf{x})=\operatorname{sign}\left(\mathbf{w}^{\top} \mathbf{x}+b\right)$</p>
<h1 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h1><h2 id="Hard-Constraint"><a href="#Hard-Constraint" class="headerlink" title="Hard Constraint"></a>Hard Constraint</h2><p>Maximize: $\frac{2}{||w||}$，且在$y_i=1$时$w^Tx_i+b\ge 1$，$y_i=-1$时$w^Tx_i+b\le -1$，如下图所示（-1和1是两个决策边界，我们希望所有点落在决策边界的两侧，不允许点出现在两个决策边界中间）：<img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/SVM/SVM/2.png" alt="img"></p>
<p>上面目标函数等价于 Minimize: $||w||$，为了之后的计算方便也可以写成：$||w||^2$。这样的目标函数称做“hard constraint”：Minimize $||w||^2$ s.t. $(w^Tx_i+b)y_i\ge 1$</p>
<h2 id="Soft-Constraint"><a href="#Soft-Constraint" class="headerlink" title="Soft Constraint"></a>Soft Constraint</h2><p>Hard Constraint容易产生过拟合问题，如下图（圆圈圈住的是误差点，右侧的线是hard constaint，但显然左侧是我们想要找到的分界线，因为可能会有很多粉色的点落在右侧线的右边）：<img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/SVM/SVM/3.png" alt="img"> 所以，我们允许一些点落在决策边界中间，只要给这些点加一些惩罚就好，这种就叫“Soft Constraint”：Minimize $||w||^2$ s.t. $(w^Tx_i+b)y_i\ge 1-\xi_i,\xi_i\ge0$，其中$\ni_i$表示可以允许分类起犯多大的错误，那么为了惩罚这个错误，我们把目标函数稍加修改：Minimize $||w||^2+\lambda\sum^n_{i=1}\xi_i$ s.t. $(w^Tx_i+b)y_i\ge 1-\xi_i,\xi_i\ge0$，其中$\lambda$起到平衡关系，当$\lambda$为正无穷时，相当于Hard Constraint，表示不允许犯任何错误。</p>
<h2 id="Hinge-Loss"><a href="#Hinge-Loss" class="headerlink" title="Hinge Loss"></a>Hinge Loss</h2><p>我们将上面式子进行一些变换，可以得到Hinge Loss：</p>
<ul>
<li><p>由$y_{i}\left(\mathbf{w}^{\top} \mathbf{x}_{i}+b\right) \geq 1-\xi_{i}$可得到$\xi_{i} \geq 1-y_{i}\left(\mathrm{w}^{\top} \mathbf{x}_{i}+b\right)$</p>
</li>
<li><p>由于$\xi_{i} \geq 0$可得到$\xi_{i}=\max \left(0,1-y_{i}\left(\mathbf{w}^{\top} \mathbf{x}_{i}+b\right)\right)$，这个式子就叫Hinge Loss，其函数图像如下图：<img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/SVM/SVM/4.png" alt="img">由上图我们可以发现Hinge Loss的几个特点：</p>
<ul>
<li>Convex 凸函数，容易优化</li>
<li>在自变量小于0的部分梯度比较小（恒等于-1），对错误分类的惩罚比较轻<ul>
<li>对比logistic loss，在小于0时指数级增长，损失会变得特别大</li>
</ul>
</li>
<li>在自变量大于等于1的部分，值为0；只要对某个数据分类是正确的，并且正确的可能性足够高，那么就永不着针对这个数据进一步优化了<ul>
<li>对比logistic loss，在大于等于1的时候还是有一些loss的</li>
</ul>
</li>
<li>在自变量等于0处不可导，需要分段求导</li>
<li>在求解最优化时，只有支持向量会参与确定分界线，而且支持向量的个数远小于训练数据的个数<ul>
<li>对于Hard Constraint的SVM来说，只关心支持向量的大小（在分界线上的点），对于其他点不关心</li>
<li>对于Soft Constraint的SVM来说，即关心支持向量，也关心异常点的误差</li>
</ul>
</li>
</ul>
<p>【注】：在吴恩达的机器学习课程中，认为SVM是在logistic regression的基础上，修改了损失函数定义得来的(具体可参考<a href="https://zhuanlan.zhihu.com/p/74764135)：![img](https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/SVM/SVM/5.png" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/74764135)：![img](https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/SVM/SVM/5.png</a>)</p>
<p>也就是说，在逻辑回归中，损失函数定义为：$J(\theta)=\frac{1}{m} \Sigma_{i=1}^{m}\left[-y^{(i)} <em> \log \left(h_{\theta}\left(x^{(i)}\right)\right)-\left(1-y^{(i)}\right) </em> \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]+\frac{\lambda}{2 m} \Sigma_{i=1}^{n} \theta_{j}^{2}$</p>
<p>在SVM中将损失函数定义为：$J(\theta)=C * \Sigma_{i=1}^{m}\left[y^{(i)} \operatorname{cost}_{1}\left(\theta^{T} x^{(i)}\right)+\left(1-y^{(i)}\right) \operatorname{cost}_{0}\left(\theta^{T} x^{(i)}\right)\right]+\frac{1}{2} \Sigma_{i=1}^{n} \theta_{j}^{2}$ </p>
<h1 id="支持多个类别"><a href="#支持多个类别" class="headerlink" title="支持多个类别"></a>支持多个类别</h1><h2 id="OVR-one-versus-rest"><a href="#OVR-one-versus-rest" class="headerlink" title="OVR(one versus rest)"></a>OVR(one versus rest)</h2><p>对于K个类别的情况，训练K个SVM，第j个SVM用于判断任意条数据是属于类别j还是属于类别非j。预测的时候，具有最大值的$w^T_ix$表示给定的数据x属于类别。</p>
<p>比如有3个类别A、B、C，我们训练3个SVM，第一个判断是否是A类，第二个判断是否为B类，第三个判断是否为C类，我们比较3个SVM的概率，选择概率最大的类。</p>
<h2 id="OVO-one-versus-one"><a href="#OVO-one-versus-one" class="headerlink" title="OVO(one versus one)"></a>OVO(one versus one)</h2><p>对于K个类别的情况，训练$K<em>(K-1)/2$个SVM，每一个SVM只用于判读任意条数据是属于K中的特定两个类别。预测的时候，使用$K</em>(K-1)/2$个SVM做$K*(k-1)/2$次预测，使用计票的方式决定数据被分类为哪个类别的次数最多，就认为数据$x$属于此类别。</p>
<p>比如有3个类别A、B、C，我们训练AB、AC、BC3个SVM用于判断是AB中的哪一类、是BC中的哪一类，是AC中的哪一类，选择投票数最多的那一类。</p>
<p>形象解释如下图：<img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/SVM/SVM/6.png" alt="img"> </p>
<h1 id="求解"><a href="#求解" class="headerlink" title="求解"></a>求解</h1><h2 id="二次规划"><a href="#二次规划" class="headerlink" title="二次规划"></a>二次规划</h2><p>二次规划（Quadratic Programming），经典运筹学的最优化问题，可以在多项式时间内求得最优解。</p>
<h2 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h2><p>原问题定义为： $\begin{aligned}\min _{\mathbf{w}, b, \xi \geq 0} &amp; \frac{1}{2} \mathbf{w}^{\top}\mathbf{w}+C \sum_{i} \xi_{i} \\\text { s.t. } y_{i}\left(\mathbf{w}^{\top}\mathbf{x}_{i}+b\right) &amp; \geq 1-\xi_{i}, \quad i=1, \ldots, n \\\xi_{i} &amp; \geq 0\end{aligned}$ </p>
<p>将条件表达式的左侧移动到右侧可得： $1-\xi_{i} - y_{i}\left(\mathbf{w}^{\top} \mathbf{x}_{i}+b\right) \le 0$  $-\xi_i\le0$</p>
<p>将上面两个式子加入到原始SVM中得到： $L(\mathbf{w}, b, \xi, \alpha, \lambda)=\frac{1}{2} \mathbf{w}^{\top} \mathbf{w}+C \sum_{i} \xi_{i}+\sum_{i} \alpha_{i}\left(1-\xi_{i}-y_{i}\left(\mathbf{w}^{\top} \mathbf{x}_{i}+b\right)\right)-\sum_{i} \lambda_{i} \xi_{i}$</p>
<p>其中$\alpha$和$\lambda$称为拉格朗日算子，并将原先最小化问题转化为最大化问题计算目标函数对各参数的导数，可得出：</p>
<ul>
<li>$\frac{\partial L}{\partial b}=0 \quad \rightarrow \quad \sum_{i} \alpha_{i} y_{i}=0$</li>
<li>$\frac{\partial L}{\partial \mathbf{w}}=0 \quad \rightarrow \quad \mathbf{w}=\sum_{i} \alpha_{i} y_{i} \mathbf{x}_{i}$</li>
<li>$\frac{\partial L}{\partial \xi_{i}}=0 \quad \rightarrow \quad C-\alpha_{i}-\lambda_{i}=0$</li>
</ul>
<p>代入$\mathbf{w}=\sum_{i} \alpha_{i} y_{i} \mathbf{x}_{i}$、$\sum_{i} \alpha_{i} y_{i}=0$可以得到：$L(\xi, \alpha, \lambda)=\sum_{i} \alpha_{i}-\frac{1}{2} \sum_{i, j} \alpha_{i} \alpha_{j} y_{i} y_{j} \mathbf{x}_{i}^{\top} \mathbf{x}_{j}+\sum_{i} \xi_{i}\left(C-\alpha_{i}-\lambda_{i}\right)$</p>
<p>再代入$C-\alpha_{i}-\lambda_{i}=0$得到对偶形式：$\begin{aligned}\max _{\alpha \geq 0, \lambda \geq 0} &amp; \sum_{i} \alpha_{i}-\frac{1}{2}\sum_{i, j} \alpha_{i} \alpha_{j} y_{i} y_{j} \mathbf{x}_{i}^{\top} \mathbf{x}_{j} \\\text {s.t. } &amp; \sum_{i} \alpha_{i} y_{i}=0, \quad C-\alpha_{i}-\lambda_{i}=0\end{aligned}$</p>
<p>由于$\lambda_i$唯一需要满足的条件是大于等于0，约束条件$C-\alpha_{i}-\lambda_{i}=0$也可以改为：$\alpha_i\le C$</p>
<p>我们怎么去理解这个表达式呢？<img src="https://uploader.shimo.im/f/Fw3DMTRFYSRVt9WA.png!thumbnail?accessToken=eyJhbGciOiJIUzI1NiIsImtpZCI6ImRlZmF1bHQiLCJ0eXAiOiJKV1QifQ.eyJhdWQiOiJhY2Nlc3NfcmVzb3VyY2UiLCJleHAiOjE2NTAyMDcyMDYsImZpbGVHVUlEIjoicVk5cHZoRDk4cGM5R0tkeSIsImlhdCI6MTY1MDIwNjkwNiwidXNlcklkIjoxNzYxNTM2MH0.xPEhc1mSIcTAk5UXyspcqV9LVnAByz4jnQ-ng9_pzRo" alt="img">注意不同数据点的权重是不同的，但是不同类别的权重一致，即可以有两个A类别和1个B类别的点，但是A类别点的权重和和B类别点的权重和是相等的。</p>
<h2 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h2><p>我们可以使用核函数来替代$x^T_ix_j$：$\begin{array}{l}\max _{\alpha \geq 0} \sum_{i} \alpha_{i}-\frac{1}{2} \sum_{i, j} \alpha_{i}\alpha_{j} y_{i} y_{j} k\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right) \\\text { s.t. }\sum_{i} \alpha_{i} y_{i}=0, \quad \alpha_{i} \leq C, \quad i=1, \ldots, n \\K\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)=\left\langle\Phi\left(\mathbf{x}_{i}\right),\Phi\left(\mathbf{x}_{j}\right)\right\rangle\end{array}$</p>
<h3 id="理解核函数"><a href="#理解核函数" class="headerlink" title="理解核函数"></a>理解核函数</h3><p>吴恩达的机器学习课程里对核函数给出了形象的解释（参考<a href="https://zhuanlan.zhihu.com/p/74764135）。下面这张图的决策边界是一个非线性的。![img](https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/SVM/SVM/8.png" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/74764135）。下面这张图的决策边界是一个非线性的。![img](https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/SVM/SVM/8.png</a>)</p>
<p>我们的假设函数可能是：$\theta_{0}+\theta_{1} x_{1}+\theta_{2} x_{2}+\theta_{3} x_{1} x_{2}+\theta_{4} x_{1}^{2}+\theta_{5} x_{2}^{2}+\ldots$的形式。为了方便，我们可以用一系列新的特征值来替换模型中的每一项，譬如：$f_{1}=x_{1}, f_{2}=x_{2}, f_{3}=x_{1} x_{2}, f_{4}=x_{1}^{2}, f_{5}=x_{2}^{2}$。则假设函数便可以转化为：$h_{\theta}(x)=\theta_{0}+\theta_{1} f_{1}+\theta_{2} f_{2}+\theta_{3} f_{3}+\theta_{4} f_{4}+\theta_{5} f_{5}+\ldots$。这种方法即通过多项式模型的方式构造新特征$f_{1}, f_{2}, f_{3} \ldots f_{n}$，那么有没有其他方式来构造新特征？有，通过核函数即可完成。</p>
<p>我们引入地标（landmark)$l^{(1)}, l^{(2)}, l^{(3)}$，我们可以通过判断样本x和地标间的近似程度来选取新的特征$f_1,f_2,f_3$。<img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/SVM/SVM/9.png" alt="img"></p>
<p>如上图所示，特征$f_1,f_2,f_3$都可以用similarity(x,l)函数来获取，这里的similarity(x,l)函数即被称为—核函数（kernel function）,在本例中我们用核函数中的一种—高斯核函数来举例，即：$f_{i}=\operatorname{similarity}\left(x, l^{(i)}\right)=\exp \left(-\frac{\left|x-l^{(i)}\right|^{2}}{2 \sigma^{2}}\right)$。</p>
<p>地标的作用是什么 ？如果一个样本x距离地标距离接近/等于0，则$f_{i} \simeq \exp (-0)=1$，否则 = 0，于是我们利用样本和地标间的关系来得出了特征f的值。<img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/SVM/SVM/10.png" alt="img"></p>
<p>看上图，可以总结出几点：</p>
<ul>
<li>红色顶点处，即向量 $x=\left\{\begin{array}{l}x_{1} \\x_{2}\end{array}\right\}=\left\{\begin{array}{l}3 \\5\end{array}\right\}$和向量地标向量l重合处，即距离= 0，故此时$f=1$</li>
<li>可以看见$\sigma$越大，图像越宽，同样的x样本向量，譬如$x=\left\{\begin{array}{l}x_{1} \\x_{2}\end{array}\right\}=\left\{\begin{array}{l}4 \\4\end{array}\right\}$，这个样本在图1中就会被判定为$f=0$，而在图3中则可能被判定为$f=1$，即$\sigma$会影响到最终特征值f的判断。即随着 的改变 值改变的速率受到的控制。</li>
</ul>
<p>假设函数值&gt;=0时预测y = 1,否则y = 0，则通过上面的高斯核函数我们可以算出每个样本点x距离地标l的距离，从而算出每个特征f，从而求出每个样本点的预测值y，即可以正确给每个样本分类，从而得到一条决策边界。<img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/SVM/SVM/12.png" alt="img"></p>
<p>例如，$\theta_{0}=-0.5, \theta_{1}=1, \theta_{2}=1, \theta_{3}=0$。对于红色点x，由于其距离地标l1较近，故f1 = 1,同时其距离l2和l3较远，故f2 = f3 = 0，假设函数值= -0.5+1 = 0.5&gt;0故预测其y = 1；对于绿色点x,f2 = 1 ,假设函数值 = -0.5+0+1+0 =0.5故其预测也为1。可以看出此例中存在一条决策边界，如红线划出的范围，在边界以内的样本都是预测y = 1，边界外的都是y = 0。</p>
<p>实际情况下，我们会选取和样本点数量同样多的且值相同的地标。得到新的特征后，我们可以写出代价函数的表达式：$\min _{\theta} C \sum_{i=1}^{m} y^{(i)} \cos t_{1}\left(\theta^{T} f^{(i)}\right)+\left(1-y^{(i)}\right) \cos t_{0}\left(\theta^{T} f^{(i)}\right)+\frac{1}{2} \sum_{j=1}^{n} \theta_{j}^{2}$。</p>
<p>这里可以看到$\theta^{T} f^{(i)}$替代了原来的$\theta^{T} x^{(i)}$。因为f是计算出来的用于模拟x的特征值。在实际计算的时候，$\frac{1}{2} \Sigma_{(j=1)}^{n} \theta_{j}^{2}=\theta^{T} \theta$我们会在之间加一个矩阵M，不同的核函数，M不同，目的在于优化计算和迭代速度。所以最终，正则化项应该是：$\theta^{T} M \theta$。</p>
<h3 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h3><p>如何使用梯度下降法求解svm可以参考：</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/91322308" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/91322308</a></li>
<li><a href="https://www.zhihu.com/question/265751466" target="_blank" rel="noopener">https://www.zhihu.com/question/265751466</a></li>
</ul>
<h3 id="SMO算法"><a href="#SMO算法" class="headerlink" title="SMO算法"></a>SMO算法</h3><p>使用SMO求解SVM可以参考：<a href="https://zhuanlan.zhihu.com/p/29212107" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29212107</a></p>
<h3 id="核技巧"><a href="#核技巧" class="headerlink" title="核技巧"></a>核技巧</h3><p>如果一个算法可以表达为关于一个正定核K1的核函数，那么可以将它转化为关于另外一个正定核K2的函数。即这里$x^T_ix_j$是正定的，$k(x_i, x_j)$也是正定的，相当于改变了衡量两个点相似性的计算公式。</p>
<p>使用核函数的预测公式如下：$b^{3 \text { kuail }} y_{i}-\sum_{j} \alpha_{j} y_{j} k\left(\mathbf{x}_{j}, \mathbf{x}_{i}\right) \quad \forall i \quad C&gt;\alpha_{i}&gt;0$</p>
<p>我们将原始的表示式改写为：$\mathbf{w}^{\top} \phi(\mathbf{x})+b=\sum_{i} \alpha_{i} y_{i} k\left(\mathbf{x}_{i}, \mathbf{x}\right)+b$</p>
<ul>
<li>只有当$x_i$为支持向量的时候，$\alpha_i&gt;0$</li>
<li>当$y_i=1$时$k(x_i, x)&gt;0$；当$y_i=-1$时$k(x_i, x)&lt;0$</li>
</ul>
<h3 id="线性不可分"><a href="#线性不可分" class="headerlink" title="线性不可分"></a>线性不可分</h3><p>核函数常用于处理线性不可分的情况，如下图：<img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/SVM/SVM/13.png" alt="img"></p>
<p>核函数本质上可以将特征映射到更高维：<img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/SVM/SVM/14.png" alt="img"></p>
<p>上图将x-&gt;$x^2$则把线性不可分的一维分布转化为线性可分的二维分布，其映射函数如下：$\begin{array}{l}\Phi: \mathcal{X} \mapsto \hat{\mathcal{X}}=\Phi(\mathbf{x}) \\\Phi\left(\left[x_{i 1}, x_{i 2}\right]\right)=\left[x_{i 1}, x_{i 2}, x_{i 1} x_{i 2}, x_{i 1}^{2},x_{i 2}^{2}\right]\end{array}$</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/SVM/SVM/15.png" alt="img"></p>
<p>上图将平面上不可分的点，映射到三维空间中线性可分的点。</p>
<h3 id="Kernel条件"><a href="#Kernel条件" class="headerlink" title="Kernel条件"></a>Kernel条件</h3><p>Kernel函数需要具备如下条件：有Gram矩阵：$G_{i j}=K\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)$</p>
<ul>
<li>$G_{ij}$为对称矩阵</li>
<li>$G_{ij}$为半正定矩阵，即$\mathbf{z}^{\top} \mathrm{Gz} \geq 0$</li>
</ul>
<h3 id="多项式核"><a href="#多项式核" class="headerlink" title="多项式核"></a>多项式核</h3><p>$K\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)=\left(\left\langle\mathbf{x}_{i}, \mathbf{x}_{j}\right\rangle+c\right)^{d}$</p>
<ul>
<li>$C\ge 0$控制低阶项的强度</li>
<li>特殊情况，当C=0且d=1时它就成为线性核（Linear Kernel），相当于无核函数的SVM一样</li>
</ul>
<p>举个例子，$\mathbf{x}_{i}=\left[x_{i 1}, x_{i 2}\right]$，$\mathbf{x}_{j}=\left[x_{j 1}, x_{j 2}\right]$，定义多项式核函数为：</p>
<p>$\begin{aligned}K\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)&amp;=\left\langle\dot{\mathbf{x}}_{i}, \mathbf{x}_{j}\right\rangle^{2} \\&amp;=\left(x_{i 1} x_{j 1}+x_{i 2} x_{j 2}\right)^{2} \\&amp;=\left(x_{i 1}^{2} x_{j 1}^{2}+x_{i 2}^{2}x_{j 2}^{2}+2 x_{i 1} x_{i 2} x_{j 1} x_{j 2}\right) \\&amp;=\left\langle\Phi\left(\mathbf{x}_{i}\right), \Phi\left(\mathbf{x}_{j}\right)\right\rangle\end{aligned}$</p>
<p>我们可以把核函数看称是将$x_i,x_j$扩展到高维空间，再进行点积，即$\Phi\left(\mathbf{x}_{i}\right)=\left[x_{i 1}^{2}, x_{i 2}^{2}, \sqrt{2} x_{i 1} x_{i 2}\right]$、$\Phi\left(\mathbf{x}_{j}\right)=\left[x_{j 1}^{2}, x_{j 2}^{2}, \sqrt{2} x_{j 1} x_{j 2}\right]$</p>
<h3 id="高斯核"><a href="#高斯核" class="headerlink" title="高斯核"></a>高斯核</h3><p>高斯核也称为Radial Basis Function(RBF) Kernel，其定义如下：$K\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)=\exp \left(-\frac{\left|\mathbf{x}_{i}-\mathbf{x}_{j}\right|_{2}^{2}}{2 \sigma^{2}}\right)$</p>
<p>当$x_i=x_j$时值为1，当$x_i$与$x_j$距离增加，值倾向于0。使用高斯核之前需要将特征正规化。高斯核的参数意义如下图所示：<img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/SVM/SVM/16.png" alt="img"></p>
<p>当$\sigma$比较小的时候，高斯核比较尖，只有当两个点相似度非常高的时候，高斯核值才接近1；当$\sigma$比较大时，如果两个点相似度不高，高斯核的值下降得比较缓慢。举一个具体例子：<img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/SVM/SVM/17.png" alt="img"></p>
<p>上图中，$\alpha=[-0.5,1.0,1.0,0.0]$，如果$\alpha_{0}+\alpha_{1} K\left(x, x_{1}\right)+\alpha_{2} K\left(x, x_{2}\right)+\alpha_{3} K\left(x, x_{3}\right) \geq 0$认为输出1，因为x接近$x_1$，所以$K\left(x, x_{1}\right) \approx 1$，其他情况$\approx 0$：$\begin{array}{l}\alpha_{0}+\alpha_{1} K\left(x, x_{1}\right)+\alpha_{2} K\left(x,x_{2}\right)+\alpha_{3} K\left(x, x_{3}\right) \\=-0.5+1 <em> 1+1 </em> 0+0 * 0=0.5 \geq 0\end{array}$<img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/SVM/SVM/18.png" alt="img"></p>
<p>上图中，因为x’接近$x_3$，$K\left(x’, x_{1}\right) \approx 1$，其他情况$\approx 0$：$\begin{array}{l}\alpha_{0}+\alpha_{1} K\left(x, x_{1}\right)+\alpha_{2} K\left(x,x_{2}\right)+\alpha_{3} K\left(x, x_{3}\right) \\=-0.5+1 <em> 0_{4}+1 </em> 0+0 * 1=-0.5\leq 0\end{array}$<img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/SVM/SVM/19.png" alt="img"></p>
<p>因此，高斯核的分类边界如上图所示。</p>
<h3 id="Sigmoid核"><a href="#Sigmoid核" class="headerlink" title="Sigmoid核"></a>Sigmoid核</h3><p>$K\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)=\tanh \left(\alpha \mathbf{x}_{i}^{\top} \mathbf{x}_{j}+c\right)$</p>
<p>此时，SVM等价于一个没有隐含层的简单神经网络。这也是为什么神经网络比SVM能力强。</p>
<h3 id="Cosine-Similarity核"><a href="#Cosine-Similarity核" class="headerlink" title="Cosine Similarity核"></a>Cosine Similarity核</h3><p>$K\left(\mathbf{x_i,x_j}\right)=\frac{\mathbf{x}_{i}^{\top} \mathbf{x}_{j}}{\left|\mathbf{x}_{i}\right|\left|\mathbf{x}_{j}\right|}$</p>
<p>常用于衡量两段文字的相似性，相当于衡量两个向量的余弦相似度（向量夹角的余弦值）。</p>
<h3 id="Chi-squared-Kernel"><a href="#Chi-squared-Kernel" class="headerlink" title="Chi-squared Kernel"></a>Chi-squared Kernel</h3><p>$K\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)=\frac{\mathbf{x}_{i}^{\top} \mathbf{x}_{j}}{\left|\mathbf{x}_{i}\right|\left|\mathbf{x}_{j}\right|}$</p>
<p>常用于计算机视觉衡量两个概率分布的相似性（例如图片上两个位置的亮度分布）。此时输入数据必须是非负的，并且使用了L1归一化。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>SVM专注于找最优分界线，用于减少过拟合。Kernel Trick的应用使得SVM可以高效的用于非线性可分的情况。其优势是理论非常完美，支持不同Kernel，可用于调参。其缺点是当数据量特别大时，训练比较慢。</p>
<h1 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h1><p>使用SVM做毒蘑菇分类可参考：<a href="https://www.kaggle.com/sunflower2018jing/svm-mushroom使用SVM和PCA做人脸识别：https://www.kaggle.com/sunflower2018jing/svm-face-recognition" target="_blank" rel="noopener">https://www.kaggle.com/sunflower2018jing/svm-mushroom使用SVM和PCA做人脸识别：https://www.kaggle.com/sunflower2018jing/svm-face-recognition</a></p>
</li>
</ul>
]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：《Real-time Attention based Look-alike Model for Recommender System》</title>
    <url>/2021/04/06/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E3%80%8AReal-time-Attention-based-Look-alike-Model-for-Recommender-System%E3%80%8B/</url>
    <content><![CDATA[<p>品牌主在互联网的广告平台精准投放广告时，可以根据平台提供的年龄、地域、性别、地域、商业兴趣标签圈定目标人群。即使广告主对自己的用户非常了解，人工设定标签的方式也不可能非常精确，所以就有了另一种更精准的做法：Look-alike相似人群扩展。</p>
<a id="more"></a>
<p>Look-alike中文叫相似人群扩展，利用品牌提供的种子用户数据，通过模型算法在广告平台的用户大数据库中找到和种子用户相似的用户群。 品牌的种子用户群体往往量比较小，基于种子用户进行广告投放，广告覆盖面小，达不到预期的流量，所以就借助Look-alike能够把投放覆盖的人群放大，同时保证精准定向效果。</p>
<p>Look-alike 是广告领域经典的推荐算法，拥有定向能力强、用户扩展精准等优点。本文在微信看一看的推荐场景下对传统 look-alike 进行了改造，使之更适合高时效性的资讯推荐系统。</p>
<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><h2 id="马太效应"><a href="#马太效应" class="headerlink" title="马太效应"></a>马太效应</h2><p>现阶段，Deep Learning已经在推荐领域中广泛应用，深度模型如 Youtube DNN/DeepFM 等在传统的个性化召回/CTR排序都取得了不错的效果，但仍有一些覆盖效果不佳的场景。对于传统的推荐模型来说，item 的历史行为特征对于 CTR 的影响很大，这也造成模型推荐结果总体趋热（当然大多数场景下热文是大家都爱看的）。因此而来的副作用就是内容体系的马太效应没有得到充分缓解，一些优质的长尾内容，比如运营新闻专题、冷门类目精品等受到打压，无法得到高效的投放。</p>
<p>要解决这个问题，我们不妨思考传统推荐模型的建模思路，大致如下：</p>
<ol>
<li>获取样本：user, item, label。这里以资讯推荐举例，label即是否点击（0/1）。注意，这条样本是最原始的信息，包含这一行为的所有信息量</li>
<li>拆解特征：直接使用行为样本信息损失当然是最低的，但那意味着失去泛化性。所以我们需要引入用户画像、语义信息、统计信息作为user/item的表达</li>
<li>训练权重：拟合样本，学习各特征权重</li>
</ol>
<p>由上可以看到，从1到2有一个信息损失的过程。特别是对于item的历史行为信息，并没有很完整的表示方式。传统模型里一个 point-wise 的样本用 itemid、统计信息（历史点击率/点击次数/标签点击率等等）表征 item 行为，但无论是 itemid 还是点击率/点击次数，都是倾向于头部历史行为丰富 item 的特征，这种现象就称为马太效应。</p>
<h2 id="Look-Alike简介"><a href="#Look-Alike简介" class="headerlink" title="Look-Alike简介"></a>Look-Alike简介</h2><p>综上所述，我们需要的是一个能精准建模 item 历史行为的模型。Look-alike 是广告领域流行的一类方法，其核心思想是针对某个 item，先根据历史行为圈定一部分种子用户，然后通过模型寻找与种子用户相似的人群，为他们推荐该 item。</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/real_time_attention/1.png" alt="图片"></p>
<p>由于 look-alike 充分利用了 item 的所有行为信息，因此在定向挖掘长尾内容受众上具有独特的优势。然而相较于广告系统，我们的资讯推荐有如下差异：</p>
<ul>
<li>内容时效性要求高，一条新闻投放资讯生命周期一般不超过一天。</li>
<li>候选集更新频率高，一天可能有几十上百万条新内容出现。</li>
</ul>
<p>传统的广告 look-alike 包括 similarity based models (LSH/user embedding) 和 regression based models (LR/xgboost/MLP)，在广告系统中都得到过验证，但不太适用于资讯推荐。这些模型往往都是针对每个 item 训练一个模型（或者每个 itemid 训练一个 embedding） ，当 item 候选集增加时，模型都需要首先积累样本，然后重训或增量更新，这对于高时效性高频率更新的资讯推荐系统来说是难以接受的。</p>
<p>总结下来，我们最后需要的模型应具备如下特点：</p>
<ul>
<li>实时扩展用户，无需更新模型，让资讯第一时间触达受众；</li>
<li>保证推荐的准确性和多样性；</li>
<li>支持在线预测</li>
</ul>
<p>基于以上，论文提出了 RALM(Realtime Attention-based Look-alike Model) 模型，它通过 user representation learning 表达用户的兴趣状态，通过 Look-alike learning 学习种子用户群体信息以及目标用户与种子用户群的相似性，从而实现实时且高效的受众用户扩展和内容触达。</p>
<h2 id="Look-Alike主流算法及存在问题"><a href="#Look-Alike主流算法及存在问题" class="headerlink" title="Look-Alike主流算法及存在问题"></a>Look-Alike主流算法及存在问题</h2><p>将受众拓展应用于微信文章推荐，是从文章的角度来描述推荐过程：我们怎么更好地将文章推荐给喜欢该文章的人，而不是基于传统的推荐系统从人的角度来考虑的：怎么为某个人推荐喜欢的文章。受众拓展目前的研究主要关注用户表示和look-alike算法，即用合适的数据结构(一般是向量)来描述用户的偏好特征，再基于look-alike算法找到一批相似的用户。</p>
<h3 id="主流算法"><a href="#主流算法" class="headerlink" title="主流算法"></a>主流算法</h3><p>用户表示基于用户特征，最简单的方式是用一个特征向量来表示每个用户，一般的表示方法向量维度很大并且很稀疏(比如文章数量为N，可以用N维向量表示用户，某一维为1表示用户看过该文章，否则为0)，这类表示不是高效的，有了用户的向量表示后，就可以用向量相似的方法计算相似度了。另外一种可行的获取相似用户的方法是采用LSH或者Kmeans对用户聚类，这样同一类的用户就是相似的用户，这种方法比较粗糙，容易丢失信息。</p>
<p>当前look-alike模型主流的算法主要有两类：基于相似性的方法和基于回归的方法：</p>
<ul>
<li>相似性方法，计算出用户的嵌入向量表示，基于某种距离测量方法(如consine余弦、欧氏距离、内积等)计算种子用户和目标用户之间的相似性。某个目标用户跟一组种子用户的相似性可以取该用户与种子用户相似性的平均值，通过这种方式，只有跟平均值相似的候选用户才能够被选中，而只跟种子用户集中某个或者某些种子用户很相似的候选用户将不会被选中。换句话说，部分种子用户所包含的信息将会丢失。</li>
<li>基于回归的方法，将look-alike问题看成是回归问题。最简单的方式是对每个item(即微信文章)训练一个LR模型，种子用户看成是正样本，通过抽样部分非种子用户作为负样本。这时，与种子用户相似的用户会获得较高的得分(LR预测值)。另外，FM和MLP等方法都曾用于受众拓展问题。所有这些回归类方法本质上都是基于用户特征最大化观察到的种子用户的行为，这类方法最大的问题是需要花费较长的时间针对每个item训练离线模型，另一方面，回归列方法需要积累足够多种子用户作为模型的正样本(对于新的item就无能为力了)，同时当新用户加入时需要重新训练。当频繁有新用户加入时，回归类方法就力不从心了，因此回归类方法不太适合实时的受众拓展场景。</li>
<li>雅虎16年提出了一个结合相似性和回归两种方法的受众拓展方案，首先，对用户进行聚类，对某篇文章，生成待推荐的用户候选集(看过该文章的用户所在聚类的并集就是候选集)。其次，基于LR或者简单的特征选择方法过滤掉不相关的用户。该方法可以解决雅虎海量数据集及大规模受众拓展的问题，虽然该算法可以做实时的look-alike，但该算法相对简称粗暴，精度不够。</li>
</ul>
<h3 id="存在问题及主要困难"><a href="#存在问题及主要困难" class="headerlink" title="存在问题及主要困难"></a>存在问题及主要困难</h3><p>不同于广告，将look-alike应用于微信文章推荐，需要考虑如下三个方面的问题，这3点即是微信文章推荐对受众拓展技术提出的要求。</p>
<ul>
<li>实时性：被推荐的文章是实时产生并加入到微信的文章推荐池中的，由于文章具备时效性，因此，希望推荐算法可以实时地将文章分发出去。对于文章的推荐，这个是一个硬性要求。</li>
<li>有效性：受众扩展方法与主流的基于CTR预估的推荐方法不太一样，是CTR预估的补充策略，我们必须提高受众拓展预测的有效性，并尽最大努力保持预估的性能。同时，实时文章推荐对用户兴趣表示和种子特征表示的准确性和多样性提出了更高的要求。</li>
<li>性能：对于某一篇待推荐文章，有上百万的种子用户，有成千上万的候选用户可作为受众拓展的对象。受众拓展方法必须实时地对上万计的候选用户打分，因此look-alike模型必须足够简单，能够在极短的时间计算出得分并确定最终推荐的用户。</li>
</ul>
<p>一般来说，实时的look-alike模型需要实时计算种子用户与目标用户的相似性，由于种子用户和目标用户表示的低效，最终的效果不尽人意。主要的困难在于：</p>
<ul>
<li>用户表示：为了提高用户兴趣的多样性，需要将尽可能多的用户特征用于用户表示学习，这正是深度学习算法擅长的。深度学习模型虽然可以建模多维度特征，深度学习模型具备学习特征之间的高阶交互和隐含信息的能力，通过深度学习模型我们可以获得用户的稠密嵌入表示，但对于包含强相关和弱相关的多域(multi-fields feature)特征，深度学习的拼接层效果不够理想，对于强相关特征(比如兴趣标签)容易过拟合，对于弱相关特征(比如购买特征)会欠拟合。</li>
<li>种子表示：推荐系统中的种子用户是逐步累积的，包含大量用户，并且可能包含“噪音”用户，怎么表示种子用户是面临的一个有挑战的问题。首先，为了提升鲁棒性，每个种子用户对种子群(后面会提到RALM算法会对种子用户聚类，每一类就是一个种子群)的贡献应该不一样。另一方面，由于种子用户中包含大量用户，目标用户一般只跟种子用户中很少的用户有相似性，因此，我们需要建模局部信息获得适应性。</li>
</ul>
<p>总结一下，对于推荐业务，由于长尾内容包含的内容特征稀少，look-alike方法是一个很好的解决方案，它只依赖于种子用户(点击过该内容的用户)作为输入，而不在意内容本身的特征多少，问题的挑战就变为，怎么选择种子用户以及怎样通过种子用户拓展到更多的其他用户中。</p>
<h1 id="RALM模型架构及工程实践"><a href="#RALM模型架构及工程实践" class="headerlink" title="RALM模型架构及工程实践"></a>RALM模型架构及工程实践</h1><h2 id="RALM简介"><a href="#RALM简介" class="headerlink" title="RALM简介"></a>RALM简介</h2><p>RALM是一个基于相似性的look-alike模型，包含用户表示学习和look-alike模型学习。对于用户表示学习，不是用传统的拼接层(concatenation layer)而是用基于注意融合层(attention merge layer)，这种方法对于多维度(multi-fields)特征有很好的表现。为了优化种子用户的表示学习，look-alike模型基于全局和局部注意单元分别学习种子用户的全局和局部表示。并且使用异步在线训练种子用户聚类的方法减少种子用户规模和注意单元在线预测的时间。</p>
<p>这篇论文的主要贡献有如下3点：</p>
<ul>
<li>提升了用户表示学习的有效性：对于多域用户兴趣表示学习，论文设计了一种引入了注意融合层的深度兴趣网络，这种注意力融合层解决了由强相关特征和弱相关特征分别带来的过拟合和噪音问题。通过在线实验，证明了注意融合层相比拼接层能够更加有效地捕获用户各种不同的兴趣偏好。</li>
<li>提升了种子用户表示学习的鲁棒性和适应性：利用全局注意单元来学习种子用户的全局表示，全局注意单元对单个用户的表示进行加权，并且惩罚噪音用户，这比所有用户权重一样更具有鲁棒性。利用局部注意单元来学习种子用户的局部表示，它对种子用户与目标用户的相关性进行加权。局部注意单元动态地基于目标用户来学习种子用户的表示，对于不同的目标用户，学习到的种子用户表示也不一样，这极大地提升了种子用户表示的表达能力。</li>
<li>实现了一个实时的、高性能的look-alike模型：为了更新最近的种子用户信息，种子用户的局部和全局表示的学习过程必须做到实时。考虑到注意力单元计算的复杂性，论文利用kmeans聚类将种子用户聚为k类。这种处理方法在保证种子用户信息损失最小的情况下，极大地降低了look-alike模型计算的复杂性。同时，当种子用户的向量表示在模型学习过程中微调时，聚类结果也会随着变化。论文引入了种子用户聚类和深度学习look-alike模型迭代训练的方法。基于种子用户到目标用户的的look-alike模型，只需种子用户和目标用户的向量表示灌入预测模型，候选的文章就可以被推荐出来。</li>
</ul>
<h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><p>在微信“看一看精选”中，有好几种类型的候选文章集供受众拓展，比如最新的新闻、人工打标签的高质量文章、长尾有意思的内容等，所有这些内容都是实时产生的，并注入到推荐池中。一般同时又成千上万的候选文章集供受众拓展，对每个候选文章，推荐系统收集点击过候选集的种子用户并实时更新种子用户的聚类结果。</p>
<p>用户向量通过用户表示学习算法离线生成，种子用户的全局和局部向量表示基于种子聚类和离线look-alike模型在线实时计算出。当用户点击精选推荐时，推荐系统的后端服务模型首先获取当前用户的向量表示，然后对每个推荐候选文章迭代计算该用户跟该候选文章的种子用户的look-alike相似性，从而计算出候选推荐文章的得分。</p>
<p>整个推荐过程可以分为三个部分：离线训练、在线异步处理及在线服务，下面分别介绍。整个算法流程见下图：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/real_time_attention/2.png" alt="图片"></p>
<p>[注]: 对比YouTube DNN和word2vec</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/real_time_attention/3.png" alt="图片"><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/real_time_attention/4.png" alt="图片"></p>
<h3 id="离线训练"><a href="#离线训练" class="headerlink" title="离线训练"></a>离线训练</h3><p>Look-Alike的在线服务依赖用户嵌入表示和种子嵌入模型。我们分两个步骤离线训练look-alike受众拓展模型，分别是用户表示学习和look-alike学习。</p>
<h4 id="用户表示学习"><a href="#用户表示学习" class="headerlink" title="用户表示学习"></a>用户表示学习</h4><p>用户表示学习基于深度学习模型构建，利用用户的所有特征作为模型输入，用户在微信的行为作为训练样本，包括读文章、播放视频、购物、播放音乐、订阅等等。用户表示学习模型的输出就是用户的嵌入向量表示，该表示包含了用户多域特征。</p>
<h4 id="look-alike学习"><a href="#look-alike学习" class="headerlink" title="look-alike学习"></a>look-alike学习</h4><p>look-alike学习基于注意力模型和聚类算法，l利用上面获得的用户一致表示作为模型输入，利用受众拓展活动样本作为训练样本，获得look-alike嵌入表示，最终用于look-alike相似性预测。在这一过程中构建全局和局部种子嵌入表示的注意力模型，用于预测种子用户的嵌入表示。</p>
<h3 id="在线异步处理"><a href="#在线异步处理" class="headerlink" title="在线异步处理"></a>在线异步处理</h3><p>在线异步处理的主要目的是实时更新种子的嵌入表示。在受众拓展模型提供服务过程中，种子用户的数量是一直累积的，应用kmeans聚类将所有种子聚为k类。异步处理的工作流分为2步：</p>
<h4 id="用户反馈行为监控"><a href="#用户反馈行为监控" class="headerlink" title="用户反馈行为监控"></a>用户反馈行为监控</h4><p>受众拓展系统通过监控微信用户的实时点击行为来更新候选推荐文章的种子集。种子用户数量的爆发增长会影响聚类的性能，因此该算法只保留最新的3百万点击用户作为某个待推荐文章的种子用户。</p>
<h4 id="种子聚类"><a href="#种子聚类" class="headerlink" title="种子聚类"></a>种子聚类</h4><p>虽然种子是实时更新的，当有新种子加入时，聚类过程不必每次都更新。该系统每隔五分钟运行一次种子聚类过程，将新加入的种子聚类。聚类中心的嵌入表示作为类中种子的初始表示存入数据库中，将会用于在线预测种子的嵌入表示。所有种子的嵌入表示定义为：</p>
<p>$R_{\text {seeds }}=\left\{E_{\text {centroid }_{1}}, E_{\text {centroid }_{2}}, \cdots, E_{\text {centroid }_{k}}\right\}$其中，$E_{\text {centroid }_{k}}$是第k个聚类的嵌入表示。</p>
<h3 id="在线服务"><a href="#在线服务" class="headerlink" title="在线服务"></a>在线服务</h3><p>首先，受众拓展系统获取当前用户的look-alike嵌入表示，其次，对每个候选推荐文章，取该文章的种子用户的聚类中心嵌入表示作为look-alike模型的输入，look-alike模型通过全局注意单元预测种子的全局嵌入表示、通过局部注意单元预测种子的局部嵌入表示。最后，在线服务模块计算look-alike模型的全局和局部相似性(即当前用户与种子用户的全局和局部相似性)得分。对于用户 u 和 种子 s ，look-alike模型的得分为：$\operatorname{score}_{u, s}=\alpha \cdot \operatorname{cosine}\left(E_{u}, E_{\text {global }_{s}}\right)+\beta \cdot \operatorname{cosine}\left(E_{u}, E_{\text {local }_{s}}\right)$这里，$E_{\text {global }_{s}}$是种子的全局嵌入表示，$E_{\text {local }_{s}}$是种子的局部嵌入表示，$\alpha$和$\beta$。</p>
<p>是权重因子。对微信精选取0.3和0.7。Look-alike模型的得分将被用于ctr预估工作流的权重因子。由于RALM基于相似性计算，并且只通过获取高阶(high-level)的嵌入作为输入，在线look-alike服务是简单高效的。</p>
<h2 id="算法原理介绍"><a href="#算法原理介绍" class="headerlink" title="算法原理介绍"></a>算法原理介绍</h2><h3 id="特征处理"><a href="#特征处理" class="headerlink" title="特征处理"></a>特征处理</h3><p>在 RALM 中，我们用 user 特征来表示 item，取代 item 的语义、id、统计类特征，一方面可以得到 item 完整的历史行为表达，另一方面对缺乏丰富统计特征的 item 更加友好。</p>
<p>有很多种特征可以描述用户的兴趣，主要包括类别特征和连续特征两大类。类别特征包括单一的(如性别、地理位置等)和多样的(如用户感兴趣的关键词)特征。对于代表分类特征的值或者一组值，该特征称为特征域 。对于像年龄这些连续特征，预训练好的特征向量先标准化并缩放到0到1之间。在微信中可用特征包括性别、年龄、地理位置、兴趣标签、感兴趣的类别、APP是否登录、媒体id、账号订阅、购物兴趣偏好、搜素兴趣偏好、社交网络关系等。</p>
<h3 id="用户表示模型"><a href="#用户表示模型" class="headerlink" title="用户表示模型"></a>用户表示模型</h3><p>用户的兴趣一般会非常复杂和广泛，用户的年龄、国别、用户读过的文章决定了用户下一篇要读的内容。因此，我们设计一个深度学习模型来学习用户多样的特征，构建用户对内容兴趣的综合表示。该模型包含抽样、模型结构、注意力融合层。</p>
<h4 id="抽样"><a href="#抽样" class="headerlink" title="抽样"></a>抽样</h4><p>我们将用户表示学习看成一个多分类问题：从百万级待推荐文章中选择用户感兴趣的。在计算loss时，为了提升训练速度，采用负采样技术而不是传统的softmax。显然，如果我们随机挑选样本作为负样本，抽样分布将偏离真实情况。借鉴word2vec中NCE负采样的思路，为了获得无偏分布，先将候选推荐item集合按照被点击的次数降序排列，然后计算每个item的概率，该概率依赖刚于讲到的item排序，具体计算公式如下：</p>
<p>$p\left(x_{i}\right)=\frac{\log (k+2)-\log (k+1)}{\log (D+1)}$，这里x_i是第i个item，k是第i个item 的排序，D代表所有item的最大排序。$p(x_i)$代表将item i 选为负样本的概率。由于活跃用户行为决定了最终训练损失，我们限制每个用户选择的样本个数，每个用户最多选择不超过50个正样本，并且抽样使得正负样本比例保持在 1/10。然后利用softmax函数来计算某一次选择c在用户特征为U及item i特征为$X_i$</p>
<p>情况下选择出item i的概率$P\left(c=i \mid U, X_{i}\right)=\frac{e^{x_{i} u}}{\sum_{j \in X} e^{x_{j} u}}$，上式中u是用户的嵌入向量，$x_j$</p>
<p>是item j的嵌入向量。整个训练过程同时利用用户的显式和隐式反馈行为，更多的训练数据可以增强推荐结果的多样性。用户在所有类型内容(文章、视频、网站等)上的行为都会用来作为训练样本，确保囊括了用户的所有兴趣点。</p>
<h4 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h4><p>我们用YouTube DNN作为模型的基础骨架，该模型包含嵌入层、拼接层、MLP层。在嵌入层，将同一field(比如用户点击行为、用户购买行为、年龄、性别等都是不同的field)的所有特征嵌入到固定长度的向量中，然后输入到平均池化层中。当所有field的特征都嵌入后，将它们拼接起来形成稠密向量，再灌入MLP层。最后一层的输出就是用户的嵌入向量表示。item的嵌入是随机初始化的，在训练过程中不断更新。该模型方便整合异质多域特征。</p>
<p>对于用户嵌入 u 和 item i 的嵌入表示x_i</p>
<p>，我们计算$P\left(c=i \mid U, X_{i}\right)$和交叉熵损失$L=-\sum_{j \in X} y_{i} \log P\left(c=i \mid U, X_{i}\right)$。这里$y_{i} \in\{0,1\}$是label，我们利用Adam优化器来训练，求最小值。当loss收敛时，最后一层的输出就是用户的嵌入向量表示。</p>
<h4 id="注意力融合层"><a href="#注意力融合层" class="headerlink" title="注意力融合层"></a>注意力融合层</h4><p>在基础模型中，不同特征域是拼接起来的，类似e_i=(e_{i1};e_{i2};…;e_{ik})。然而，通过观察网络参数的训练过程，我们发现优化过程总是对与用户兴趣很相关的特征(比如兴趣标签)产生过拟合，导致推荐结果由少量的强相关的field决定。弱相关的field(比如购物偏好)，总是欠拟合的，但它们对推荐也至关重要。最终导致的结果就是模型不能完全学习到多域特征，推荐结果缺乏多样性。</p>
<p>为了解决该问题，我们在模型中用注意力融合层而不是拼接层。在基础模型中，拼接让所有用户的兴趣服从同一概率分布。这样，少量对大多数用户产生影响的强相关的field，它们的权重很大，导致出现高维稀疏权重矩阵。注意力单元可以根据用户的上下文特征学习权重的个性化分布，对不同field可以激活神经元的不同部分，在训练过程中强相关和弱相关的field都可以起作用，因此我们采用注意层来学习用户相关的多域权重。</p>
<p>上面图右边是用户表示学习模型，n个field被嵌入到维度为m的向量$h \in \mathbb{R}^{m}$，我们将它们按照第二个维度拼接起来形成矩阵$H \in \mathbb{R}^{n \times m}$，我们按照如下公式计算权重向量：</p>
<ul>
<li>$u=\tanh \left(W_{1} H\right)$</li>
<li>$a_{i}=\frac{e^{W_{2} u_{i}^{T}}}{\sum_{j}^{n} e^{W_{2} u_{j}^{T}}}$</li>
</ul>
<p>这里$W_{1} \in \mathbb{R}^{k \times n}$、$W_{2} \in \mathbb{R}^{k}$是权重矩阵，k 是注意单元的size。</p>
<p>$u \in \mathbb{R}^{n}$是field 的激活单元，$a \in \mathbb{R}^{n}$是field的权重。最后，我们计算融合向量$M \in \mathbb{R}^{m}$：，这个$M=a H$值作为MLP的输入，获得了一致的用户嵌入表示。注意融合层相比拼接层，在学习多域特征上有极大的优势。</p>
<h3 id="Look-Alike模型"><a href="#Look-Alike模型" class="headerlink" title="Look-Alike模型"></a>Look-Alike模型</h3><p>look-alike模型由两个塔构成，左边的塔称为种子塔，将n个种子用户的嵌入$R_{\text {seeds }} \in \mathbb{R}^{n \times m}$作为模型输入，这里m是用户嵌入向量的维数，后面跟着全连接层，作为第一层，它将$n \times m$输入矩阵变换为$n \times h$矩阵，这里h是变换后嵌入向量的维数。之后，一个自我注意单元(self attension unit)和一个一般注意单元(general attention unit)用于池化嵌入向量，最终生成一个h维的向量。右边的塔称为目标塔，将m维向量变换为h维向量。在这两个塔的上面，两个塔输出向量的内积被计算出来，代表种子用户和目标用户的相似性。对于推荐系统来说，相似性代表的是某个item被目标用户点击的概率。</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/real_time_attention/5.png" alt="图片"></p>
<h4 id="变换矩阵"><a href="#变换矩阵" class="headerlink" title="变换矩阵"></a>变换矩阵</h4><p>$m \times h$的权重矩阵被用于从一致的用户嵌入空间到look-alike空间的投影。虽然用户嵌入是从用户的多种行为中学习而来，但将预训练的特征作为模型的输入可能会过拟合。为了避免过拟合，我们用双塔结构共享变换矩阵。模型输出非线性特征之前经过ReLU单元变换，变换后，种子用户被表示为n个维度为h的向量。</p>
<h4 id="局部注意单元"><a href="#局部注意单元" class="headerlink" title="局部注意单元"></a>局部注意单元</h4><p>为了计算种子用户和目标用户的相似性，我们需要将所有种子用户池化为一个向量，平均池化是通常采用的做法。然而，平均池化获得的是所有种子向量的均值，这样公共的信息被保留了，而异常值和个性化信息被忽略了。一般来说，在上百万的种子用户中，只有很少用户的兴趣跟目标用户是匹配的。因此，我们引入局部注意单元，用于激活相对于某个目标用户的局部兴趣，同时自适应地学习种子用户相对于目标用户的个性化表示：$E_{\text {local }_{s}}=E_{s} \operatorname{softmax}\left(\tanh \left(E_{s}^{T} W_{l} E_{u}\right)\right)$</p>
<p>这里$W_{l} \in \mathbb{R}^{h \times h}$是注意力矩阵，$E_{S}$是种子用户，$E_{u}$代表目标用户，$E_{\text {local }_{s}}$是种子用户的局部嵌入。如果某个item有百万级的种子用户，局部注意单元将会花费n*h*h次计算，这里n是百万量级，线上预测肯定会存在问题。为了减少计算的复杂度，我们将种子用户利用Kmeans聚类聚成k类，对于每一类我们计算种子向量的均值作为该类的向量表示，这样我们就获得了k个h维的向量。这时计算复杂度就从n*h*h降到k*h*h，一般k小于100。</p>
<h4 id="全局注意单元"><a href="#全局注意单元" class="headerlink" title="全局注意单元"></a>全局注意单元</h4><p>对于种子用户的全局信息，我们加入自我注意力单元来模拟每个种子用户的全局表示：</p>
<p>$E_{\text {global }_{s}}=E_{s} \operatorname{softmax}\left(E_{s}^{T} \tanh \left(W_{g} E_{s}\right)\right)$这里，$W_{g} \in \mathbb{R}^{s \times n}$是注意力矩阵，s是注意力的维数。$E_{\text {global }_{s}}$代表种子用户Es的全局嵌入表示。由自我注意力获得的全局信息与Es</p>
<p>的兴趣分布相关。有了局部和全局表示E_{local}和E_{glbal}，我们就可以按照如下公式计算种子用户和目标用户的相似性了：$\operatorname{score}_{u, s}=\alpha \cdot \operatorname{cosine}\left(E_{u}, E_{\text {global }_{s}}\right)+\beta \cdot \operatorname{cosine}\left(E_{u}, E_{\text {local }_{s}}\right)$</p>
<h4 id="迭代训练"><a href="#迭代训练" class="headerlink" title="迭代训练"></a>迭代训练</h4><p>在变换和反向传递之后，用户的嵌入表示会改变，为了保持种子聚类和用户表示两个过程的同步，每个epoch之后重新运行一次聚类。因此，我们提出了一个迭代训练过程，一轮一轮交替地训练look-alike模型和种子聚类两个算法。</p>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>我们利用sigmoid交叉熵损失做为损失函数：$L=-\frac{1}{N} \sum_{x, y \in D}(y \log p(x)+(1-y) \log (1-p(x)))$</p>
<p>这里D代表训练集大小，x代表用户嵌入，y是label。p(x)</p>
<p>是通过sigmoid函数计算出的种子用户和目标用户的相似性得分。</p>
<h3 id="冷启动曝光"><a href="#冷启动曝光" class="headerlink" title="冷启动曝光"></a>冷启动曝光</h3><p>虽然 RALM 计算相似度并不需要太多种子用户，但在 item 初次投放时（种子用户为0），我们仍需多 item 做初始曝光以获得最初的种子用户。线上我们使用语义特征与用户画像做了简单的 MLP 预估点击率作为曝光策略，当积累足够量（100以上）种子用户后即可进入 RALM 的正常预测流程。</p>
<h3 id="模型效果"><a href="#模型效果" class="headerlink" title="模型效果"></a>模型效果</h3><h4 id="prec-K"><a href="#prec-K" class="headerlink" title="prec@K"></a>prec@K</h4><h4 id=""><a href="#" class="headerlink" title=""></a><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/real_time_attention/6.png" alt="图片"></h4><h4 id="attention"><a href="#attention" class="headerlink" title="attention"></a>attention</h4><p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/real_time_attention/7.png" alt="图片"></p>
<h4 id="clustering"><a href="#clustering" class="headerlink" title="clustering"></a>clustering</h4><p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/real_time_attention/8.png" alt="图片"></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>这篇论文通过引入RALM算法来解决实时的受众拓展问题，这是一个两阶段的模型，包括用户表示学习和look-like学习。</p>
<p>对于用户表示学习，论文提出了一种基于注意力融合层(attention merge layer)的新的神经网络结构取代经典的拼接层(concatenation layer)，该网络结构大大提升了多特征学习的表达能力。在look-alike模型学习中，针对每个目标用户该论文设计了一个全局注意力单元(global attension unit)和局部注意力单元(local attention unit)来学习种子用户的鲁棒性自适应特征表示。最通过引入种子用户聚类方法，不仅减少了注意力模型预测的时间复杂度还减少了种子表示的信息损失。同时，构建了一个包含训练和在线服务的推荐系统，借助异步处理和种子聚类，在线预测才可以做到实时。</p>
<p>通过在线实验，该方法取得了比传统look-alike模型好得多的效果，特别是在推荐多样性和推荐质量上有较大提升。该模型是第一个应用于推荐系统的实时look-alike模型。</p>
]]></content>
      <tags>
        <tag>Recommend</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：《DIET：Dual Intent and Entity Transformer》</title>
    <url>/2021/04/06/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E3%80%8ADIET%EF%BC%9ADual-Intent-and-Entity-Transformer%E3%80%8B/</url>
    <content><![CDATA[<p>最近开始要重构语音助手项目，重新看了一下rasa。这篇论文是rasa research team发的关于NLU模型的paper，内容比较浅显，论文所述的实验效果较好，不过放在中文实验中目前看上去也不是那么好。</p>
<a id="more"></a>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>大规模预训练语言模型在GLUE和SuperGLUE等在语言理解任务上取得非常好的效果，相比其他预训练方法如分布式表示（GloVe）和纯监督方法都有显着改善。论文引入双重意图和实体 Transformer（DIET）架构，并研究不同的预训练表示在语义理解意图识别任务和实体预测任务上是否有效。</p>
<p>DIET 在复杂的多领域NLU数据集取得state of art的效果，并在其它更简单的数据集上取得很好的性能。出乎意料的是，实验使用大型预训练模型来完成此任务并没有明显的好处，实际上DIET甚至在没有任何预训练词嵌入的情况下也可以取得state of art的结果。 论文的性能最好的模型优于微调的 BERT，训练速度快大约六倍。</p>
<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>数据驱动对话建模的两种常见方法是端到端和模块化系统：</p>
<ul>
<li>模块化方法使用独立的自然语言理解（NLU）和生成（NLG）系统：对话策略接收 NLU 系统的输出，选择下一个系统操作，然后在 NLG 系统生成对应的响应。</li>
<li>在端到端方法中，用户信息直接输入到对话策略中预测下一个系统语句。</li>
</ul>
<p>NLU通常指两个子任务：意图分类和实体识别。如果分别对这些子任务建模可能会导致错误传播，因此只用一个多任务结构应该有益于两个任务之间相互增强。</p>
<p>经过预先训练的大型语言模型在具有挑战性的NLU任务上表现较好。 但是，这类模型的预训练和微调的计算成本都比较大。并且很多对话助手都会使用英语以外的其它语言，有时很难对所有语言都预训练一个大型的语言模型。</p>
<p>论文提出 DIET（Dual Intent and Entity Transformer），这是一种用于意图分类和实体识别的新型多任务体系结构。它能够以即插即用的方式结合语言模型的预训练单词嵌入，并将它们与单词和字符级 n-gram 稀疏特征结合起来。 实验表明，即使没有预训练的嵌入，仅使用单词和字符级 n-gram 稀疏特征，DIET 仍可以在复杂 NLU 数据集上取得state of art的结果。 此外，添加预训练语言模型的单词和句子嵌入，可进一步提高所有任务的整体准确性。 我们性能最好的模型明显优于fine-tune的 BERT，训练速度快六倍。</p>
<h3 id="DIET体系结构"><a href="#DIET体系结构" class="headerlink" title="DIET体系结构"></a>DIET体系结构</h3><p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/diet/1.png" alt="图片"></p>
<p>输入句子被视为token序列，根据特征pipeline的不同，token可以是单词或子词。我们在每个句子的末尾添加一个特殊的分类标记 <strong>CLS</strong>。</p>
<p>每个输入token都具有我们所谓的稀疏特征和/或密集特征：</p>
<ul>
<li>稀疏特征是token级 one-hot 编码和字符级 n-gram ( n ≤ 5) multi-hot 编码。 字符 n-gram 包含许多冗余信息，因此为避免过拟合，我们对这些稀疏特征应用 dropout。</li>
<li>密集特征可以是任何预先训练的词嵌入：ConveRT、BERT或 GloVe。<ul>
<li>由于 ConveRT 还被训练为句子编码器，因此在使用 ConveRT 时，我们将 <strong>CLS</strong> 词符的初始嵌入设置为从 ConveRT 获得的输入句子的句子编码。这在单个单词嵌入信息基础上添加了完整句子的额外上下文信息。 对于开箱即用的</li>
<li>训练 BERT 我们将其设置为 BERT [CLS] 词符对应的输出嵌入</li>
<li>对于GloVe 将其设置为一个句子中词符嵌入的平均值。</li>
</ul>
</li>
</ul>
<p>为了对整个句子中的上下文进行编码，我们使用了两层具有相对位置attention的transformer。</p>
<p>命名体标签序列 yentity 是通过在 transformer 输出序列 a 之上的条件随机场 (CRF) 标记层预测：$L_{\mathrm{E}}=L_{\mathrm{CRF}}\left(\boldsymbol{a}, \boldsymbol{y}_{\text {entity }}\right)$</p>
<p>意图分类是通过Transformer 输出的 __CLS__ 词符$a_{\mathrm{CLS}}$和意图标签$y_{\text {intent }}$被嵌入到一个语义向量空间中后$h_{\mathrm{CLS}}=E\left(a_{\mathrm{CLS}}\right), h_{\mathrm{intent}}=E\left(y_{\text {intent }}\right)$，使用点积损失最大化与目标标签$y_{\text {intent }}^{+}$的相似性并$S_{\mathrm{I}}^{+}=h_{\mathrm{CLS}}^{T} h_{\mathrm{intent}}^{+}$最小化与负样本$y_{\text {intent }}^{-}$的相似性：$L_{\mathrm{I}}=-\left\langle S_{\mathrm{I}}^{+}-\log \left(e^{S_{\mathrm{I}}^{+}}+\sum_{\Omega_{\mathrm{I}}^{-}} e^{S_{\mathrm{I}}^{-}}\right)\right\rangle$</p>
<p>在推理时，点积相似性用于在所有可能的意图标签上排序。</p>
<p>Mask</p>
<p>受Mask语言模型任务的启发，论文添加一个额外的训练目标来预测随机屏蔽的输入词符。 我们在序列中随机选择输入词符的 15％。 对于选定的词符，在70％的情况下我们将输入替换为特殊屏蔽词符 __MASK__ 对应的向量，在 10％ 情况下我们用随机词符的向量替换输入，并在其余的 20％ 情况下保留原始输入。与意图损失类似，我们计算Mask Embedding和实际词Embedding的点击损失：$L_{\mathrm{M}}=-\left\langle S_{\mathrm{M}}^{+}-\log \left(e^{S_{\mathrm{M}}^{+}}+\sum_{\Omega_{\mathrm{M}}^{-}} e^{S_{\mathrm{M}}^{-}}\right)\right\rangle$</p>
<p>Mask Loss可以当作一个正则项，帮助模型从文本中学习更多一般特征，而不仅要从分类中获得区分性。</p>
<p>最终的损失函数为：$L_{\text {total }}=L_{I}+L_{E}+L_{M}$</p>
<p>在训练时，使用balanced batch策略减轻类别不平衡问题，并且在整个训练期间增加批次大小，把它也作为正则化的另一个来源。</p>
<h3 id="实验评估"><a href="#实验评估" class="headerlink" title="实验评估"></a>实验评估</h3><h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><p>使用三个数据集进行评估：NLU-Benchmark、ATIS 和 SNIPS。 我们的实验重点是 NLU-Benchmark 数据集，因为它是这三个中最具挑战性的。 ATIS 和 SNIPS 测试集精度的最先进水平已经接近 100％。</p>
<p>NLU-Benchmark 数据集举例：“schedule a call with Lisa on Monday morning” 标注为场景 calendar、动作 set_event、实体 [event_name:<em>a call with Lisa</em>] 和 [date:<em>Monday morning</em>]。 将场景和动作标签进行连接得到意图标签（例如 calendar_set_event）。该数据集有 25,716 个语句，涵盖多个家庭助理任务，例如播放音乐或日历查询、聊天、以及向机器人发出的命令。 我们将数据分为 10 份。 每一份都有自己的训练集和测试集。总共存在 64 个意图和 54 种实体类型。</p>
<p>ATIS ATIS是 NLU 领域中经过充分研究的数据集。 它由预订机票的人的录音经过标注转录。 开发和测试集分别包含 4,478、500 和 893 个语句。 训练数据集包含 21 个意图和 79 个实体。</p>
<p>SNIPS数据集是从 Snips 个人语音助手收集的。 它包含 13,784 个训练和 700 个测试样本。 训练集中分 700 个样本用作开发集。包含 7 个意图和 39 个实体。</p>
<h4 id="NLU-Benchmark"><a href="#NLU-Benchmark" class="headerlink" title="NLU-Benchmark"></a>NLU-Benchmark</h4><p>使用 NLU-Benchmark 数据集的第一小份来选择超参数。 为此，我们从训练集中随机抽取 250 个语句作为开发集。 我们在一台具有 4 个 CPU，15 GB 内存和一台 NVIDIA Tesla K80 的计算机上训练 200 多个epoch的模型。 使用 Adam进行优化，初始学习率为 0.001。 批次大小从 64 增加到 128。 在 NLU-Benchmark 数据集的第一个小份上训练我们的模型大约需要一个小时。 在推断的时候，我们需要大约 80 毫秒来处理一条语句。</p>
<p>NLU-Benchmark 数据集包含 10 个小份，每个小份具有单独的训练和测试集。 为了获得该模型在该数据集上的整体性能，我们分别训练 10 个模型，每个小份一次，将平均值作为最终得分。 指标采用Micro-averaged precision、召回率和 F1 得分。 意图标签的 True positives、false positives 和 false negatives 的计算方式与其它任何多类分类任务一样。 如果预测范围和正确范围之间重叠，并且其标签匹配，则该实体被视为 true positive。</p>
<p>实验结果如下：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/diet/2.png" alt="图片"></p>
<p>我们性能最好的模型使用稀疏特征，即词符级别的 one-hot 编码和字符 n-gram 的 multi-hot 编码（n ≤ 5）。 这些稀疏特征与 ConveRT 的密集嵌入相结合。 我们性能最好的模型没有使用Mask Loss。</p>
<p>我们在意图方面的表现优于 HERMIT，绝对值超过 2％。 我们的实体 F1 微观平均得分（86.04％）也高于 HERMIT（84.74％）。 HERMIT 报告的实体精度值相似，但是，我们的召回率要高得多（86.13％ 相比 82.04％）。</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/diet/3.png" alt="图片"></p>
<p>为了评估意图分类和命名实体识别这两个任务是否受益于联合优化，我们针对每个任务分别训练了模型（如上图）。 结果表明，与实体识别一起训练时，意图分类的性能略有下降（90.90％ vs 90.18％）。</p>
<p>但是，当单独训练实体时，实体的 micro-averaged F1 分数从 86.04 ％下降到 82.57％。 检查 NLU-Benchmark 数据集，这可能是由于特定意图与特定实体的存在之间的强相关性。 例如，几乎所有属于 play_game 意图的语句都有一个名为game_name 的实体。 同样，实体 game_name 仅与意图 play_game 一起出现。</p>
<p>NLU-Benchmark上的消融实验结果如下图：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/diet/4.png" alt="图片"></p>
<p>观察上表，我们发现：</p>
<ul>
<li>当使用稀疏特征和Mask Loss时，没有任何预训练的嵌入，DIET 的性能具有竞争力。 在目标和实体上增加Mask Loss都会使性能提高绝对值约 1％。</li>
<li>具有 GloVe 嵌入的 DIET 也具有同等的竞争力，并且在与稀疏特征和Mask Loss结合使用时，在意图和实体上都将得到进一步增强。</li>
<li>有趣的是，使用上下文 BERT 嵌入作为密集特征的效果要比 GloVe 差。 我们假设这是因为 BERT 主要是在各种文本上预训练的，因此在转移到对话任务之前需要微调。 由于 ConveRT 专门针对会话数据进行微调，因此使用 ConveRT 嵌入的 DIET 的性能对此支持了这种说法。 稀疏特征 和 ConveRT 嵌入的结合在意图分类上获得了最佳的 F1 得分，并且在意图分类和实体识别方面都比现有最好结果高出 3％ 左右。 与 BERT 和 ConveRT 一起用作密集特征时，增加屏蔽损失似乎会稍微影响性能。</li>
</ul>
<h4 id="ATIS-和-SNIPS"><a href="#ATIS-和-SNIPS" class="headerlink" title="ATIS 和 SNIPS"></a>ATIS 和 SNIPS</h4><p>为了解 DIET 超参数的可移植性，我们采用在 NLU-Benchmark 数据集上性能最佳的 DIET 模型配置，并在 ATIS 和 SNIPS 上对其进行评估。 下表中列出 ATIS 和 SNIPS 数据集上的意图分类准确性和命名实体识别 F1 得分。* 表示使用 BILOU 标记模式对数据进行标注。†表示未使用Mask Loss。</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/diet/5.png" alt="图片"></p>
<p>值得注意的是，DIET 仅使用稀疏特征而没有任何预训练的嵌入，即使这样其性能仅比 Joint BERT 模型低 1-2％之内。 利用 NLU-Benchmark 数据集上性能最佳模型的超参数，DIET 在 ATIS 和 SNIPS 上均获得与 Joint BERT 竞争的结果。</p>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>我们引入 DIET，一种用于意图和实体建模的灵活结构。 我们研究其在多个数据集上的性能，并表明 DIET 在具有挑战性的 NLU-Benchmark 数据集上提高了最先进水平。</p>
<p>此外，我们广泛研究使用来自各种预训练方法嵌入的有效性。 我们发现没有单一的嵌入集总是在不同的数据集上最好，这凸显了模块化结构的重要性。</p>
<p>此外，我们表明，像 GloVe 这样的分布模型中的词嵌入与大规模语言模型中的嵌入效果相当，并且事实上，在不使用任何预训练嵌入的情况下，DIET 仍可以实现类似的性能，在 NLU-Benchmark 上表现出最先进的水平。</p>
<p>最后，我们还表明，在 NLU-Benchmark 上用于 DIET 的最佳预训练嵌入集优于在 DIET 内对 BERT 进行微调的速度，并且训练速度快六倍。</p>
]]></content>
      <tags>
        <tag>NLU</tag>
      </tags>
  </entry>
  <entry>
    <title>不怕的人前面才有路</title>
    <url>/2021/03/22/%E4%B8%8D%E6%80%95%E7%9A%84%E4%BA%BA%E5%89%8D%E9%9D%A2%E6%89%8D%E6%9C%89%E8%B7%AF/</url>
    <content><![CDATA[<p>今天听到董卿姐姐说的一段话，作为最近一段时间对自己的一种鼓励。相信一切都会变好。</p>
<a id="more"></a>
<p><a href="https://uploader.shimo.im/f/KYvJD9sJeL0wNztK.mp4?fileGuid=Qg8PyXXcwXDgpgwG" target="_blank" rel="noopener">不怕的人前面才有路</a></p>
<h4 id="歌德曾经说过，"><a href="#歌德曾经说过，" class="headerlink" title="歌德曾经说过，"></a>歌德曾经说过，</h4><h4 id="人在这个世界上无论选择哪一条道路，"><a href="#人在这个世界上无论选择哪一条道路，" class="headerlink" title="人在这个世界上无论选择哪一条道路，"></a>人在这个世界上无论选择哪一条道路，</h4><h4 id="他都是荆棘和鲜花同在，"><a href="#他都是荆棘和鲜花同在，" class="headerlink" title="他都是荆棘和鲜花同在，"></a>他都是荆棘和鲜花同在，</h4><h4 id="有晴空也有冷雨。"><a href="#有晴空也有冷雨。" class="headerlink" title="有晴空也有冷雨。"></a>有晴空也有冷雨。</h4><h4 id="不过就像鲁迅先生说的，"><a href="#不过就像鲁迅先生说的，" class="headerlink" title="不过就像鲁迅先生说的，"></a>不过就像鲁迅先生说的，</h4><h4 id="前途很远，很暗，"><a href="#前途很远，很暗，" class="headerlink" title="前途很远，很暗，"></a>前途很远，很暗，</h4><h4 id="然而不要怕，"><a href="#然而不要怕，" class="headerlink" title="然而不要怕，"></a>然而不要怕，</h4><h4 id="不怕的人面前才会有路。"><a href="#不怕的人面前才会有路。" class="headerlink" title="不怕的人面前才会有路。"></a>不怕的人面前才会有路。</h4>]]></content>
  </entry>
  <entry>
    <title>机器学习之数学基础</title>
    <url>/2021/02/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<p>隔离在家，终于有时间好好看看书、学学习了。今天来复习下机器学习中的一些数学基础。</p>
<a id="more"></a>
<h1 id="排列组合"><a href="#排列组合" class="headerlink" title="排列组合"></a>排列组合</h1><h2 id="排列"><a href="#排列" class="headerlink" title="排列"></a>排列</h2><p>班里有三名同学，成绩前两名有几种可能性？咱们可以用乘法原理：选第一名有 3 种可能性，选第二名有 2 中可能性，因为第一名那个人不可能同时又是第二名了，将这两步相乘起来：$P_{n}^{m}=\frac{n !}{(n-m) !}=n(n-1)(n-2) \ldots \ldots(n-m+1)$，例如$P_{3}^{2}=3 \times 2=6$。</p>
<h2 id="组合"><a href="#组合" class="headerlink" title="组合"></a>组合</h2><p>班里有三名同学，选出两名代表参加年级会议有几种选法？$C_{n}^{m}=\frac{n !}{(n-m) ! m !}=\frac{n(n-1)(n-2) \ldots \ldots(n-m+1)}{m(m-1)(m-2) \times \ldots \ldots \times 1}$，相当于计算$P_{n}^{m}$后再除以重复的个数$m!$。</p>
<h3 id="组合公式1"><a href="#组合公式1" class="headerlink" title="组合公式1"></a><strong>组合公式1</strong></h3><p>$C_{n}^{m}=C_{n}^{n-m}$，例如$C_{4}^{3}=C_{4}^{1}$，比如，班里有 A、B、C、D 四个同学，每天要选出三个同学做值日，有几种选法？这个问题对于学过排列组合的同学自然非常简单了，就是 C4 抽 3，但是，假如问一个没学过排列组合的人，他会怎么想呢？如果想 ABC，ACD……这种就会比较难想，不如去想它的反面：选Ａ、B、C 或 D 放学直接回家，总共就四种。这就能直观的理解这个公式了。这个公式对于运算 C 10 抽 8 这样的组合数时非常有用，直接转化成 C 10 抽 2 来计算。</p>
<h3 id="组合公式2"><a href="#组合公式2" class="headerlink" title="组合公式2"></a><strong>组合公式2</strong></h3><p>$C_{n}^{m-1}+C_{n}^{m}=C_{n+1}^{m}$，例如$C_{3}^{1}+C_{3}^{2}=C_{4}^{2}$。比如，四个妹子中，想约两个妹子有几种约法。如果四个人都是普通朋友，看作是相同的 A、B、C、D，那自然有 C 4 抽 2 =6 种约法。下面我们来点刺激的：假如这四个人中有一个是你女朋友，她最特殊，你会先问她来不来：</p>
<ul>
<li>如果她来，但你还想一共约两个妹子（手动滑稽），那么就需要在其他三个妹子中再约一个，有 C3 抽 1 种方法；</li>
<li>如果她不来，那你就需要在其他三个妹子中再约两个，有 C3 抽 2 种方法。</li>
</ul>
<p>两类相加，表示的意义就是从 4 个妹子中约两个妹子的情况总数，即公式成立。这个公式对于处理两个组合数相加问题非常有用。</p>
<h3 id="组合公式3"><a href="#组合公式3" class="headerlink" title="组合公式3"></a><strong>组合公式3</strong></h3><p>$C_{n}^{0}+C_{n}^{1}+\ldots \ldots C_{n}^{n}=2^{n}$，例如$C_{2}^{0}+C_{2}^{1}+C_{2}^{2}=2^{2}$。想象一个笼子里有两只兔子，抓出来的话有几种抓法？</p>
<ul>
<li>第一种方法是我去笼子里抓，我在抓的时候就想好是抓 1 只还是抓 2 只，或是抓 0 只（即不抓）。由于先想好了这一点，就会有 C 2 抽 1 和 C 2 抽 2 这些组合数，分别表示按“抓一只”、“抓两只” 分类，每类的情况数；</li>
<li>第二种情况是我把笼子打开，让每只兔子自己选择跳出来或是不跳出来（2 种可能性），每只兔子都是独立的个体，所以可以用乘法原理，总共的情况数是 n 个 2 相乘，即 2 的 n 次方。</li>
</ul>
<p>两种方法都表示“兔子出来的情况数”，因此一样，即公式得以解释。这个公式对于处理一系列“底下相同的”组合数相加的问题非常好用，大大节省计算量。</p>
<h3 id="组合公式4"><a href="#组合公式4" class="headerlink" title="组合公式4"></a><strong>组合公式4</strong></h3><p>$C_{r}^{r}+C_{r+1}^{r}+\ldots \ldots+C_{n}^{r}=C_{n+1}^{r+1}$，例如$C_{2}^{2}+C_{3}^{2}+C_{4}^{2}=C_{5}^{3}$。比如说你要在 A、B、C、D、E 这 5 个小球中抽取 3 个小球，咱们可以按“哪个小球是第一个”分类：</p>
<ul>
<li>第一类：A 为火车头，那么还需在后面四个小球中抽取两个小球；</li>
<li>第二类：B 为火车头，那么还需在后面三个小球中抽取两个小球；</li>
<li>第三类：C 为火车头，那么还需在后面两个小球中抽取两个小球。</li>
</ul>
<p>至于 D 或 E 开头的，就不足“三节车厢”了，故不计算。我们把之前说的三类加起来，就直观地理解了这个公式。</p>
<p><strong>组合公式5</strong>: $\sum_{i=0}^{k} C_{n}^{i} C_{m}^{k-i}=C_{m+n}^{k}$，例如$C_{4}^{0} C_{3}^{3}+C_{4}^{1} C_{3}^{2}+C_{4}^{2} C_{3}^{1}+C_{4}^{3} C_{3}^{0}=C_{7}^{3}$。这个公式可以想像成可以想象成班里选几名学生，分男女选和不分男女选情况数一样。</p>
<h1 id="概率统计"><a href="#概率统计" class="headerlink" title="概率统计"></a>概率统计</h1><h2 id="随机变量"><a href="#随机变量" class="headerlink" title="随机变量"></a>随机变量</h2><p>随机变量的名字虽然叫“变量”，但实际上可以理解为一个函数。随机变量用于表示随机实验的结果。例如有一个骰子（6个面），随机变量X表示连续抛两次骰子的数字相加的结果，那么X=f(x)，其中x表示骰子，f表示求两次骰子数字和的函数。</p>
<h3 id="离散随机变量"><a href="#离散随机变量" class="headerlink" title="离散随机变量"></a>离散随机变量</h3><p>例如抛骰子一次，可能的结果取值为1,2,3,4,5,6。抛骰子两次，骰子的值的和的可能取值为2,3,4,5,6,7,8,9,10,11,12。</p>
<h3 id="连续随机变量"><a href="#连续随机变量" class="headerlink" title="连续随机变量"></a>连续随机变量</h3><p>例如高中生的身高，可能的取值在[120,240]厘米之间，体重在[50,300]斤。连续随机变量也可以是多维的，比如高中生的身高和体重是[xxx厘米, xxx斤]。</p>
<p>某一支股票未来的价格X（随机变量），其值可能在[0, 10000]之间变化。预测股票的未来价格，可以通过预测X在[0, 10000]之间的概率分布来实现。</p>
<h3 id="分布函数"><a href="#分布函数" class="headerlink" title="分布函数"></a>分布函数</h3><p>设X是一个随机变量，x是任意实数，函数$F(x)=P\{X\le x\},-\infty&lt;x&lt;+\infty$称为X的分布函数。对任意实数$x_1$、$x_2$($x_1&lt;x_2$)有$P\{x_1&lt;X\le x_2\}=P\{X\le x_2\}-P\{X \le x_1\}=F(x_2)-F(x_1)$。</p>
<h3 id="离散型随机变量的概率分布"><a href="#离散型随机变量的概率分布" class="headerlink" title="离散型随机变量的概率分布"></a>离散型随机变量的概率分布</h3><p>以抛骰子为例，扔一次的值为离散随机变量，取值为1,2,3,4,5,6，则概率分布P(X=X_1)=1/6, P(X=X_2)=1/6,…。离散型随机变量的概率分布有如下特点：</p>
<ul>
<li>$P(X) &gt;= 0$</li>
<li>$\sum_{P(X_i)}=1$</li>
</ul>
<h4 id="0-1分布"><a href="#0-1分布" class="headerlink" title="0-1分布"></a>0-1分布</h4><p>设随机变量X只可能取0与1两个值，他的分布率是$P{x=k}=p^k(1-p)^{1-k},k=0,1 (0&lt;p&lt;1)$，则称X服从以p为参数的0-1分布或两点分布。其分布律可以表示为下表：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">X</th>
<th style="text-align:left">0</th>
<th style="text-align:left">1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">$p_k$</td>
<td style="text-align:left">1-p</td>
<td style="text-align:left">p</td>
</tr>
</tbody>
</table>
</div>
<p>常见的比如新生儿的性别、检查产品质量是否合格、某车间的电力消耗是否超过符合、抛硬币等问题都可以用0-1分布描述。</p>
<h4 id="伯努利-二项分布"><a href="#伯努利-二项分布" class="headerlink" title="伯努利/二项分布"></a>伯努利/二项分布</h4><p>如果进行n次不同的实验，每次实验完全相同并且只有两种可能的结果，这样的实验结果分布情况就是二项分布。最简单的比如抛一枚硬币n次，实验结果为x次正面朝上，n-x次反面朝上，这就是一个简单的二项分布。</p>
<p>二项分布的概率公式为：$p(x)=C^x_np^xq^{n-x}(x=0,1,2,3,…,n)$，$\frac{n!}{x!(n-x)!}$，其中n代表n次实验，减少每一次实验结果为T或者F，x表示实验结果为T的次数，q是实验结果为T的概率，q=1-p表示实验结果为F的概率。二项分布的性质：$E(X)=np$、$Var(X)=np(1-p)$。</p>
<h4 id="几何分布"><a href="#几何分布" class="headerlink" title="几何分布"></a>几何分布</h4><p>在n次伯努利试验中，试验k次才得到第一次成功的机率。即：前k-1次皆失败，第k次成功的概率。其概率分布函数为：$P(X=k)=(1-p)^{k-1}p$，性质：$E(x)=\frac{1}{p}$、$Var(X)=\frac{1-p}{p^2}$。</p>
<h4 id="泊松分布"><a href="#泊松分布" class="headerlink" title="泊松分布"></a>泊松分布</h4><p>日常生活中，大量时间是有固定频率的，例如某网站平均每分钟的访问次数、一本书一页中的印刷错误数、某地区在一天内邮递遗失的信件数、某医院在一天内的急诊病人数等。它们的特点是，我们可以预估这些事件的总数，但是没法知道具体的发生事件。已知平均每小时出生3个婴儿，请问下一个小时，会出生几个？有可能一下子出生6个，也有可能一个都不出生，这是我们没法知道的。</p>
<p>泊松分布就是描述某段时间内，某个事件发生的概率，其概率表达式：$P(N(t)=k)=\frac{\lambda^ke^{-\lambda}}{k!}$，其中N表示某种函数关系，t表示时间，k表示数量，$\lambda$表示事件的频率， 例如1小时内出生3个婴儿的概率就表示为$P(N(1))=3$。</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mathbasic/1.png" alt="图片"></p>
<p><strong>应用举例</strong></p>
<p>观察事物平均发生m次的条件下，实际发生x次的概率P(x)可用$P(x)=\frac{m^{x}}{x !} \times e^{-m}$表示。例如采用0.05J/㎡紫外线照射大肠杆菌时，每个基因组（～4×10核苷酸对）平均产生3个嘧啶二体。实际上每个基因组二体的分布是服从泊松分布的，将取如下形式：</p>
<ul>
<li>$P(0)=e^{-3}=0.05$</li>
<li>$P(1)=\frac{3}{1 !} e^{-3}=0.15$</li>
<li>$P(2)=\frac{3^{2}}{2 !} e^{-3}=0.22$</li>
</ul>
<p><strong>泊松定理</strong></p>
<p>松分布的产生机制可以通过如下例子来解释。为方便记，设所观察的这段时间为[0,1),取一个很大的自然数n，把时间段[0,1)分为等长的n段：$l_{1}=\left[0, \frac{1}{n}\right], l_{2}=\left[\frac{1}{n}, \frac{2}{n}\right], \ldots, l_{i}=\left[\frac{i-1}{n}, \frac{i}{n}\right], \ldots, l_{n}=\left[\frac{n-1}{n}, 1\right]$。我们做如下两个假定：</p>
<ul>
<li>在每段$l_i$内，恰发生一个事故的概率，近似的与这段时间的长$\frac{1}{n}$成正比，可设为$\frac{\lambda}{n}$。当n很大时，$\frac{1}{n}$很小时，在$l_i$这么短暂的一段时间内，要发生两次或者更多次事故是不可能的。因此在$l_i$这段时间内不发生事故的概率为$1-\frac{\lambda}{n}$。</li>
<li>$l_{i}, \ldots, l_{n}$各段是否发生事故是独立的</li>
</ul>
<p>把在[0,1)时段内发生的事故数X视作在n个划分之后的小时段$l_{i}, \ldots, l_{n}$内有事故的时段数，则按照上述两个假定，X应服从二项分布$B\left(n, \frac{\lambda}{n}\right)$。于是，我们有：$P(X=i)=\left(\begin{array}{c}n \\i \end{array}\right)\left(\frac{\lambda}{n}\right)^{i}\left(1-\frac{\lambda}{n}\right)^{n-i}$。</p>
<p>注意到当$n \rightarrow \infty$取极限时，我们有：$\frac{\left(\begin{array}{l}n\\i\end{array}\right)}{n^{i}} \rightarrow \frac{1}{i !},\left(1-\frac{\lambda}{n}\right)^{n}\rightarrow e^{-\lambda}$，从而有：$\begin{aligned}P(X=i) &amp;=\left(\begin{array}{c}n \\i\end{array}\right)\left(\frac{\lambda}{n}\right)^{i}\left(1-\frac{\lambda}{n}\right)^{n-i} \\&amp;=\frac{e^{-\lambda} \lambda^{i}}{i !}\end{aligned}$。</p>
<p>从上述推导可以看出：泊松分布可作为二项分布的极限而得到。一般的说，若$X \sim B(n, p)$，其中n很大，p很小，因而$n p=\lambda$不太大时，X的分布接近于泊松分布$P(\lambda)$，即$C^n_kp^k(1-p)^{n-k}\approx \frac{\lambda^k e^{-\lambda}}{k!}$其中$\lambda= n p$（【注】：当n&gt;=20，p&lt;=0.05时近似效果最佳）。这个事实有时可将较难计算的二项分布转化为泊松分布去计算。</p>
<p>举个栗子，某计算机硬件公司制造某种特殊型号的微型芯片，次品率达0.1%，各芯片成为次品相互独立，求在1000只产品中至少有2只次品的概率：</p>
<p>用X记次品数，$X \sim b(1000, 0.01)$，所求概率为$P\{X\ge2\}=1-P\{X=0\}-P\{X=1\}=1-(0.999)^{1000}-C^{1000}_1(0.999)^{999}(0.001)\approx1-0.367-0.368\approx 0.264$，利用泊松定力来计算得$\lambda=1000 \times 0.001 =1$，$P\{X\ge2\}=1-P\{X=0\}-P\{X=1\}\approx 1- e^{-1} - e^{-1} \approx 0.2642$。</p>
<p><strong>性质</strong></p>
<p>阶乘特点以及泰勒公式使得一类期望的计算十分简便：</p>
<p>$E(X(X-1)(X-2))=\sum_{k=0}^{n} k(k-1)(k-2) e^{-\lambda} \frac{\lambda^{k}}{k !}=\sum_{k=3}^{n} e^{-\lambda} \frac{\lambda^{k}}{(k-3) !}=\lambda^{3} e^{-\lambda} \sum_{k=3}^{n} \frac{\lambda^{k-3}}{(k-3) !}=\lambda^{3} e^{-\lambda} e^{\lambda}=\lambda$</p>
<h3 id="连续型随机变量的概率分布"><a href="#连续型随机变量的概率分布" class="headerlink" title="连续型随机变量的概率分布"></a>连续型随机变量的概率分布</h3><h4 id="概率密度"><a href="#概率密度" class="headerlink" title="概率密度"></a>概率密度</h4><p>如果对于随机变量X的分布函数F(x)，存在非负可积函数f(x)，使对于任意实数x有$F(x)=\int^x_{-\infty}f(t)dt$，则称X为连续型随机变量，f(x)称为X的概率密度函数。概率密度f(x)有以下性质：</p>
<ul>
<li>$f(x)\ge 0$</li>
<li>$\int^{\infty}_{-\infty}f(x)dx=1$</li>
<li>对于任意实数$x_1,x_2(x_1 \le x_2)$，$P\{x_1 &lt; X \le x_2\}=F(x_2)-F(x_1)=\int^{x_2}_{x_1}f(x)dx$，也就是说概率$P\{x_1&lt;X\le x_2\}$等于区间$(x_1, x_2]$上曲线f(x)之下的曲边提醒的面积。</li>
<li>若f(x)在点x处连续，则有$F’(x)=f(x)$</li>
</ul>
<h4 id="均匀分布"><a href="#均匀分布" class="headerlink" title="均匀分布"></a>均匀分布</h4><p>均匀概率分布是指连续随机变量所有可能出现值出现的概率都相同。其概率密度函数为$f(x)=\left\{\begin{array}{ll}\frac{1}{b-a}, &amp; a&lt;x&lt;b, \\0, &amp; \text { 其他 },\end{array}\right.$，如下图所示：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mathbasic/2.png" alt="图片"></p>
<p>其分布函数为：</p>
<p>$F(x)=\left\{\begin{array}{ll}0, &amp; x&lt;a \\\frac{x-a}{b-a}, &amp; a \leqslant x&lt;b \\1, &amp; x \geqslant b\end{array}\right.$，如下图所示：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mathbasic/3.png" alt="图片"></p>
<p>【注】：对于连续随机变量的概率分布中，单个点无法求概率，只能计算积分。</p>
<h4 id="正太分布"><a href="#正太分布" class="headerlink" title="正太分布"></a>正太分布</h4><p>若随机变量X服从一个位置参数为$\mu$、尺度参数为$\sigma$的正太分布，记为：$X~N(\mu, \sigma^2)$，则其概率密度函数为$f(x)=\frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$。正太分布的数学期望值$\mu$等于位置参数，决定了分布的位置；其标准差$\sigma$等于尺度参数，决定了分布的幅度。</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mathbasic/4.png" alt="图片"></p>
<p>正太分布有一个性质，如下图：深蓝色区域是距平均值小于一个标准差之内的数值范围。在正太分布中，此范围所占比率为全部数值的68%，根据正太分布，两个标准差之内的比率合起来为95%，三个标准差之内的比率合起来为99%。在实际应用上，常考虑一组数据具有近似于正太分布的概率分布。若其假设正确，则约有68.3%数值分布在距离平均值有1个标准差之内的范围，约95.4%数值分布在距离平均值有2个标准差之内的范围，以及约99.7%数值分布在距离平均值有3个标准差之内的范围，称为“68-95-99.7”法则或“经验法则”。</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mathbasic/5.png" alt="图片"></p>
<h4 id="指数分布"><a href="#指数分布" class="headerlink" title="指数分布"></a>指数分布</h4><p>指数分布用来表达独立随机事件发生的时间间隔，比如旅客进入机场的时间间隔、打进客服中心电话的时间间隔、婴儿出生的时间间隔、网站访问的时间间隔、奶粉销售的时间间隔等。</p>
<p>指数分布的概率密度函数：</p>
<script type="math/tex; mode=display">f(x;\lambda)=\left\{ \begin{aligned} \lambda e^{-\lambda x} & , & x \ge0, \\ 0 & , & x<0.\end{aligned} \right.</script><p>其中$\lambda &gt; 0$是分布的一个参数，常被称为率参数(rate parameter)，即每单位时间发生该事件的次数。指数分布的区间是$[0,\infty]$。如果一个随机变量X呈指数分布，即可以写作：$X~Exponential(\lambda)$。</p>
<p>指数分布的累计分布函数可以写成：</p>
<script type="math/tex; mode=display">f(x;\lambda)=\left\{ \begin{aligned} 1-e^{-\lambda x} & , & x \ge0, \\ 0 & , & x<0.\end{aligned} \right.</script><p>指数分布的均值(期望)是$E[X]=\frac{1}{\lambda}$，例如你平均每个小时街道2次电话，那么你预期等待每一次电话的时间是半个小时。方差是$E[X]=\frac{1}{\lambda^2}$。</p>
<p>举一个例子，假设一个医院每小时平均有3个婴儿出生，那么接下来15分钟，会有婴儿出生的概率是多少呢？我们已知指数分布的累积分布函数为$P(X\le t)=1-P(x\ge t)=1-e^{-\lambda t}$，那么接下来15分钟，会有婴儿出生的概率为$P(X\le \frac{1}{4})=1-e^{-3\cdot \frac{1}{4}}\approx 0.53$。</p>
<p>指数分布与泊松分布的关系：</p>
<ul>
<li>无记忆性：指数函数的一个重要特征是无记忆性。这表示如果一个随机变量呈现指数分布，它的条件概率遵循：$P(T&gt;s+t|T&gt;t)=P(T&gt;s)$ for all $s,t\ge 0$。举例说明，如果X是一元件的寿命，已知元件已使用了s小时，它总共能使用至少s+t小时的条件概率，与从开始使用时算起它至少能使用t小时的概率相等。</li>
<li>与泊松过程的关系：<ul>
<li>泊松过程：泊松过程是随机过程的一种，是以事件的发生时间来定义的。 我们说一个随机过程N(t) 是一个时间齐次的一维泊松过程，如果它满足以下条件： 在两个互斥（不重叠）的区间内所发生的事件的数目是互相独立的随机变量。 在一个时间区间或空间区域内的事件数，和另一个互斥（不重叠）的时间区间或空间区域内的事件数，这两个随机变数是独立的。</li>
<li>根据泊松过程的定义，长度为t的时间段内没有随机事件出现的概率等于$\frac{e^{-\lambda t (\lambda t)^0}}{0!}=e^{-\lambda t}$，长度为t的时间端内随机时间发生一次的概率等于$\frac{e^{-\lambda t (\lambda t)^1}}{1!}=e^{-\lambda t}\lambda t$，所以第k次随机事件之后长度为t的时间段，第k+n次(n=1,2,3,…)随机事件出现的概率等于$1-e^{-\lambda t}$，这是指数分布，也表明了泊松过程的无记忆性。</li>
</ul>
</li>
</ul>
<h3 id="主要随机变量一览表"><a href="#主要随机变量一览表" class="headerlink" title="主要随机变量一览表"></a>主要随机变量一览表</h3><p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mathbasic/6.png" alt="图片"></p>
<h2 id="概率性质"><a href="#概率性质" class="headerlink" title="概率性质"></a>概率性质</h2><h3 id="基本性质"><a href="#基本性质" class="headerlink" title="基本性质"></a>基本性质</h3><ul>
<li>$P(A_1\cup A_2 \cup \cdots \cup A_n)=P(A_1)+P(A_2)+ \cdots + P(A_n)$</li>
<li>设A、B是两个事件，若$A \subset B$则有$P(B-A)=P(B)-P(A)$</li>
<li>对于任意两事件A、B有$P(A\cup B)=P(A)+P(B)-P(AB)$</li>
</ul>
<h3 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h3><p>条件概率是指事件A在另外一个事件B已经发生条件下的发生概率，表示为P(A|B)。下面看一个机器学习中的例子：</p>
<ul>
<li>概率：任意一封邮件为垃圾邮件的概率，其值可能非常低，小于5%</li>
<li>条件概率：一封邮件，发件人电邮不是你的正常联系人，并且包括如下句子：“北京25岁女孩开豪车，揭秘原来是炒股暴发户，之前一直炒股好多产品都赚不到钱，很烦恼，加逍遥老师微信才给了一条名录”，其为垃圾邮件的概率。</li>
</ul>
<p>条件概率公式：$P\left\{X=x_{i} \mid Y=y_{j}\right\}=\frac{P\left\{X=x_{i}, Y=y_{j}\right\}}{P\left\{Y=y_{j}\right\}}$</p>
<h3 id="联合概率"><a href="#联合概率" class="headerlink" title="联合概率"></a>联合概率</h3><p>联合概率指的是包含多个条件且所有条件同时成立的概率，记作P(X=A, Y=B)。例如，中国人中身高低于172cm且体重高于130斤的概率为P(X<172, Y>=130)。</p>
<h3 id="贝叶斯定理"><a href="#贝叶斯定理" class="headerlink" title="贝叶斯定理"></a>贝叶斯定理</h3><p>贝叶斯定理描述在已知一些条件下，某事件的发生概率。例如，已知某癌症与年龄有关，使用贝叶斯定理可以通过某人年龄计算出它罹患癌症的概率。</p>
<p>通常，事件A在事件B的条件下发生的概率P(A|B)，与事件B在事件A条件下发生的概率P(B|A)是不一样的。然而这两者是有确定关系的，贝叶斯定理就是这种关系的陈述。贝叶斯公式的一个用途，即通过已知的三个概率而推出第四个概率：$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$。</p>
<p>在更一般化的情况，假设$\{A_i\}$是事件集合里的部分集合，对于任意的$A_i$，贝叶斯定理可用下式表示$P(A_i|B)=\frac{P(B|A_i)P(A_i)}{\sum_jP(B|A_j)P(A_j)}$，其中分母是P(B)的全概率公式。</p>
<p>举一个实际栗子来说明以贝叶斯定理：假设现存技术不是完美的，真正吸毒者被检出阳性（吸毒）的概率为99%，不吸毒的被检出是阳性的概率为1%。假设一个小区有0.5%的真正吸毒者，如果警察对小区所有人做排查，查处一个人检测结果是阳性，那么这个人的是真实吸毒的可能性有多高？我们用D表示吸毒，+表示阳性，则有：</p>
<p>$P(D|+)=\frac{P(+|D)P(D)}{P(+)}=\frac{P(+|D)P(D)}{P(+|D)P(D)+P(+|N)P(N)}=\frac{0.99\times 0.005}{0.99\times 0.005 + 0.01 \times 0.995}=0.3322$</p>
<h3 id="期望"><a href="#期望" class="headerlink" title="期望"></a>期望</h3><p>设p(x)是一个离散概率分布函数，x的取值范围为$\{x_1, x_2, …, x_n\}$，其期望被定义为：$E(x)=\sum^n_{k=1}x_kp(x_k)$。</p>
<p>设p(x)是一个连续概率密度函数，其期望为$E(x)=\int^{+\infty}_{-\infty}xp(x)dx$。期望的满足一些性质。</p>
<p>例1：设$X \sim \pi(\lambda)$，求E(X)</p>
<p>X的分布律：$P\{X=k\}=\frac{\lambda^{k} \mathrm{e}^{-\lambda}}{k !}, \quad k=0,1,2, \cdots, \quad \lambda&gt;0$</p>
<p>则$E(X)=\sum_{k=0}^{\infty} k \frac{\lambda^{k} \mathrm{e}^{-\lambda}}{k !}=\lambda \mathrm{e}^{-\lambda} \sum_{k=1}^{\infty} \frac{\lambda^{k-1}}{(k-1) !}=\lambda \mathrm{e}^{-\lambda} \cdot \mathrm{e}^{\lambda}=\lambda$</p>
<p>例2：设$X \sim U(a, b)$，求E(X)</p>
<p>X的分布律：</p>
<p>$f(x)=\left\{\begin{array}{ll}\frac{1}{b-a}, &amp; a&lt;x&lt;b \\0, &amp; \text { 其他. }\end{array}\right.$</p>
<p>则$E(X)=\int_{-\infty}^{\infty} x f(x) \mathrm{d} x=\int_{a}^{b} \frac{x}{b-a} \mathrm{~d} x=\frac{a+b}{2}$</p>
<h4 id="线性运算规则"><a href="#线性运算规则" class="headerlink" title="线性运算规则"></a>线性运算规则</h4><p>$E(ax+by+c)=aE(x)+bE(y)+c$</p>
<h4 id="函数的期望"><a href="#函数的期望" class="headerlink" title="函数的期望"></a>函数的期望</h4><p>设f(x)为x的函数，p(x)为x的概率密度函数，则f(x)的期望为：</p>
<ul>
<li>离散变量：$E(f(x))=\sum^n_{k=1}f(x_k)P(x_k)$</li>
<li>连续变量：$E(f(x))=\int^{+\infty}_{-\infty}f(x)p(x)dx$</li>
</ul>
<p>要注意的是，函数的期望不等于期望的函数，即$E(f(x))\neq f(E(x))$</p>
<h4 id="乘积的期望"><a href="#乘积的期望" class="headerlink" title="乘积的期望"></a>乘积的期望</h4><p>一般来说，乘积的期望不等于期望的乘积，除非变量相互独立。因此如果x和y相互独立，则$E(xy)=E(x)E(y)$</p>
<h3 id="方差"><a href="#方差" class="headerlink" title="方差"></a>方差</h3><p>方差是一种特殊的期望（是x-E(x)这个函数的期望），其定义为：$Var(x)=E((x-E(x))^2)$</p>
<h4 id="展开表示"><a href="#展开表示" class="headerlink" title="展开表示"></a>展开表示</h4><p>反复利用期望的线性性质，可以算出方差的另一种表示形式：$Var(x)=E((x-E(x))^2)=E(x^2)-(E(x))^2$。由这个式子也可以知道“常数的方差为0”。</p>
<h4 id="线性组合的方差"><a href="#线性组合的方差" class="headerlink" title="线性组合的方差"></a>线性组合的方差</h4><p>方差不满足线性性质，两个变量的线性组合方差计算方法如下：$Var(ax+by)=a^2Var(x)+b^2Var(y)+2Cov(x,y)$。</p>
<p>如果两个变量相互独立，则：$Var(ax+by)=a^2Var(x)+b^2Var(y)$</p>
<p>如果x和y相互独立，则：$Var(x+y)=Var(x)+Var(y)$</p>
<h3 id="协方差"><a href="#协方差" class="headerlink" title="协方差"></a>协方差</h3><p>两个随机变量的协方差被定义为：$Cov(x,y)=E((x-E(x))(y-E(y)))$。因此方差是一种特殊的协方差，当x=y时：$Cov(x,y)=Var(x)=Var(y)$。</p>
<p>协方差表示的是两个变量的总体的误差，这与只表示一个变量误差的方差不同。如果两个变量的变化趋势一致，也就是说如果其中一个大于自身的期望值，另外一个也大于自身的期望值，那么两个变量之间的协方差就是正值。如果两个变量的变化趋势相反，即其中一个大于自身的期望值，另外一个小于自身的期望值，那么两个变量之间的协方差就是负值。协方差有如下性质：</p>
<ul>
<li>独立变量的协方差为0</li>
<li>$\operatorname{Cov}\left(\sum_{i=1}^{m} a_{i} x_{i}, \sum_{j=1}^{n} b_{j} y_{j}\right)=\sum_{i=1}^{m} \sum_{j=1}^{n} a_{i} b_{j} \operatorname{Cov}\left(x_{i}, y_{j}\right)$</li>
<li>$\operatorname{Cov}(a+b x, c+d y)=b d \operatorname{Cov}(x, y)$</li>
<li>$\operatorname{Var}\left(\sum_{k=1}^{n} a_{i} x_{i}\right)=\sum_{i=1}^{n} \sum_{j=1}^{n} a_{i} a_{j} \operatorname{Cov}\left(x_{i}, x_{j}\right)$</li>
</ul>
<h4 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h4><p>两个随机变量X、Y联合概率分布如下，求协方差？</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mathbasic/7.png" alt="图片"></p>
<p>$\operatorname{cov}(X, Y)=\sum_{i=1}^{n} p_{i}\left(x_{i}-E(X)\right)\left(y_{i}-E(Y)\right)$</p>
<p>$\begin{aligned}\operatorname{cov}(X, Y)=&amp; \sigma_{X Y}=\sum_{(x, y) \in S} f(x, y)\left(x-\mu_{X}\right)\left(y-\mu_{Y}\right) \\=&amp;\left(\frac{1}{4}\right)\left(1-\frac{3}{2}\right)(1-2)+\left(\frac{1}{4}\right)\left(1-\frac{3}{2}\right)(2-2) \\&amp;+(0)\left(1-\frac{3}{2}\right)(3-2)+(0)\left(2-\frac{3}{2}\right)(1-2) \\&amp;+\left(\frac{1}{4}\right)\left(2-\frac{3}{2}\right)(2-2)+\left(\frac{1}{4}\right)\left(2-\frac{3}{2}\right)(3-2) \\=&amp; \frac{1}{4}\end{aligned}$</p>
<h3 id="相关系数"><a href="#相关系数" class="headerlink" title="相关系数"></a>相关系数</h3><p>相关系数通过方差和协方差定义，描述两个随机变量的相关性：$Corr(x,y)=\frac{Cov(x,y)}{\sqrt{Var(x)Var(y)}}$。相关系数有以下性质：</p>
<ul>
<li>有界性：相关系数的取值范围为-1到1</li>
<li>统计意义：值越接近1，说明两个变量正相关性（线性）越强，越接近-1，说明负相关性越强，当为0时表示两个变量没有相关性。</li>
</ul>
<p>下面举一个实际的栗子。有5个县的过敏生产总值分别为10,20,30,50和80亿。这5个县的贫困率分别为11%,12%,13%,15%和18%。请计算国民生产总值和贫困率的相关系数。</p>
<ul>
<li>令x和y为包含上述数据的向量：x=(10,20,30,50,80)和y=(0.11,0.12,0.13,0.15,0.18)</li>
<li>X的期望E(x)为(10+20+30+50+80)/5=38</li>
<li>Y的期望E(Y)为(0.11+0.12+0.13+0.15+0.18)/5=0.138</li>
<li>X,Y的协方差为[(10-38)<em>(0.11-0.138)+(20-38)</em>(0.12-0.138)+(30-38)<em>(0.13-0.138)+(50-38)</em>(0.15-0.138)+(80-38)*(0.18-0.138)]/5=0.616</li>
<li>X的方差为[(10-38)^2+(20-38)^2+(30-38)^2+(50-38)^2+(80-38)^2]/5=616</li>
<li>Y的方差为[(0.11-0.138)^2+(0.12-0.138)^2+(0.13-0.138)^2+(0.15-0.138)^2+(0.18-0.138)^2]/5=0.000616</li>
<li>X,Y的相关系数0.616/[sqrt(616)*sqrt(0.000616)]=1说明国明生产总值和贫困率是非常正相关的。</li>
</ul>
<h4 id="物理解释"><a href="#物理解释" class="headerlink" title="物理解释"></a>物理解释</h4><p>我们可以把相关系数看成两个向量的夹角，以上面栗子为例：</p>
<ul>
<li>令x和y为包含上述数据的向量：x=(10,20,30,50,80)和y=(0.11,0.12,0.13,0.15,0.18)</li>
<li>X的期望E(X)为(10+20+30+50+80)/5=38</li>
<li>Y的期望E(Y)为(0.11+0.12+0.13+0.15+0.18)/5=0.138</li>
<li>X减去期望[10-38,20-38,30-38,50-38,80-38]=[-28,-18,-8,12,42]</li>
<li>Y减去期望[0.11-0.138,0.12-0.138,0.13-0.138,0.15-0.138,0.18-0.138]=[-0.028,-0.018,-0.008,0.012,0.042]</li>
<li>$cos\theta = \frac{XY}{||X||||Y||}=\frac{3.08}{55.497*0.055}=1$</li>
</ul>
<h2 id="多维随机变量"><a href="#多维随机变量" class="headerlink" title="多维随机变量"></a>多维随机变量</h2><p>设(X,Y)是二维随机变量，对于任意实数x、y，二元函数$F(x,y)=P\{(X\le x)\cap(Y\le y)\}=P\{X\le x, Y\le y\}$称为二维随机变量(X,Y)的联合分布函数，如下表所示：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mathbasic/8.png" alt="图片"></p>
<p>如果将二维随机变量(X,Y)看成是平面上随机点的坐标，那么分布函数F(x,y)在(x,y)处的函数值就是随机点(X,Y)落在下图中阴影部分的面积，且有</p>
<p>$\begin{array}{l}P\left\{x_{1}&lt;X \leqslant x_{2}, y_{1}&lt;Y \leqslant y_{2}\right\} \\\quad=F\left(x_{2}, y_{2}\right)-F\left(x_{2}, y_{1}\right)+F\left(x_{1}, y_{1}\right)-F\left(x_{1}, y_{2}\right)\end{array}$</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mathbasic/9.png" alt="图片"></p>
<p>如果是连续型随机变量，则有$F(x, y)=\int_{-\infty}^{y} \int_{-\infty}^{x} f(u, v) \mathrm{d} u \mathrm{~d} v$。</p>
<h3 id="边缘分布"><a href="#边缘分布" class="headerlink" title="边缘分布"></a>边缘分布</h3><p>二维随机变量(X,Y)作为一个整体，具有分布函数F(x,y)，而X和Y都是随机变量，各自也有分布函数，记为$F_X(x)$、$F_Y(y)$，且$F_{X}(x)=P\{X \leqslant x\}=P\{X \leqslant x, Y&lt;\infty\}=F(x, \infty)$、$F_{Y}(y)=F(\infty, y)$。对于离散型随机变量有$F_{X}(x)=F(x, \infty)=\sum_{x_{i} \leqslant x} \sum_{j=1}^{\infty} p_{i j}$，如下表：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mathbasic/10.png" alt="图片"></p>
<p>对于连续型随机变量有：$F_{X}(x)=F(x, \infty)=\int_{-\infty}^{x}\left[\int_{-\infty}^{\infty} f(x, y) \mathrm{d} y\right] \mathrm{d} x$，其概率密度函数为$f_{X}(x)=\int_{-\infty}^{\infty} f(x, y) \mathrm{d} y$、$f_{Y}(y)=\int_{-\infty}^{\infty} f(x, y) \mathrm{d} x$。</p>
<h3 id="条件分布"><a href="#条件分布" class="headerlink" title="条件分布"></a>条件分布</h3><p>设(X,Y)是二维离散型随机变量，其分布律为$P\left\{X=x_{i}, Y=y_{j}\right\}=p_{i j}, \quad i, j=1,2, \cdots$，(X,Y)关于X和关于Y的边缘分布律分别为$P\left\{X=x_{i}\right\}=p_{i} .=\sum_{j=1}^{\infty} p_{i j}, \quad i=1,2, \cdots,$、$P\left\{Y=y_{j}\right\}=p_{. j}=\sum_{i=1}^{\infty} p_{i j}, \quad j=1,2, \cdots$。</p>
<p>由条件概率公式可知，在$Y=y_i$条件下随机变量X的条件分布律为$P\left\{X=x_{i} \mid Y=y_{j}\right\}=\frac{P\left\{X=x_{i}, Y=y_{j}\right\}}{P\left\{Y=y_{j}\right\}}=\frac{p_{i j}}{p \cdot j}$。例如下表的条件概率：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mathbasic/11.png" alt="图片"></p>
<p>可以计算出：</p>
<ul>
<li>$P\{Y=0 \mid X=1\}=\frac{P\{X=1, Y=0\}}{P\{X=1\}}=\frac{0.030}{0.045}$</li>
<li>$P\{Y=1 \mid X=1\}=\frac{P\{X=1, Y=1\}}{P\{X=1\}}=\frac{0.010}{0.045}$</li>
<li>$P\{Y=2 \mid X=1\}=\frac{P\{X=1, Y=2\}}{P\{X=1\}}=\frac{0.005}{0.045}$</li>
</ul>
<p>设二维随机变量(X,Y)的概率密度为f(x,y)，关于Y的边缘概率密度为$f_Y(y)$，若对于固定的y，$f_Y(y)&gt;0$则称$\frac{f(x, y)}{f_{Y}(y)}$为在Y=y的条件下X的条件概率密度，记为：$f_{X \mid Y}(x \mid y)=\frac{f(x, y)}{f_{Y}(y)}$。</p>
<h3 id="相互独立"><a href="#相互独立" class="headerlink" title="相互独立"></a>相互独立</h3><p>如果$F(x, y)=F_{X}(x) F_{Y}(y)$则称X、Y相互独立。</p>
<h3 id="两个随机变量函数的分布"><a href="#两个随机变量函数的分布" class="headerlink" title="两个随机变量函数的分布"></a>两个随机变量函数的分布</h3><ul>
<li>Z=X+Y：$f_{X+Y}(z)=\int_{-\infty}^{\infty} f(z-y, y) \mathrm{d} y$、$f_{X+Y}(z)=\int_{-\infty}^{\infty} f(x, z-x) \mathrm{d} x$<ul>
<li>若X、Y相互独立：$f_{X+Y}(z)=\int_{-\infty}^{\infty} f_{X}(z-y) f_{Y}(y) \mathrm{d} y$、$f_{X+Y}(z)=\int_{-\infty}^{\infty} f_{X}(x) f_{Y}(z-x) \mathrm{d} x$</li>
<li>卷积公式：$f_{X} * f_{Y}=\int_{-\infty}^{\infty} f_{X}(z-y) f_{Y}(y) \mathrm{d} y=\int_{-\infty}^{\infty} f_{X}(x) f_{Y}(z-x) \mathrm{d} x$</li>
</ul>
</li>
<li>$Z=\frac{Y}{X}$：$f_{Y / X}(z)=\int_{-\infty}^{\infty}|x| f(x, x z) \mathrm{d} x$<ul>
<li>若X、Y相互独立：$f_{Y / X}(z)=\int_{-\infty}^{\infty}|x| f_{X}(x) f_{Y}(x z) \mathrm{d} x$</li>
</ul>
</li>
<li>Z=XY：$f_{X Y}(z)=\int_{-\infty}^{\infty} \frac{1}{|x|} f\left(x, \frac{z}{x}\right) \mathrm{d} x$<ul>
<li>若X、Y相互独立：$f_{X Y}(z)=\int_{-\infty}^{\infty} \frac{1}{|x|} f_{X}(x) f_{Y}\left(\frac{z}{x}\right) \mathrm{d} x$</li>
</ul>
</li>
</ul>
<h3 id="协方差矩阵"><a href="#协方差矩阵" class="headerlink" title="协方差矩阵"></a>协方差矩阵</h3><p>设X、Y是随机变量，$E\left\{[X-E(X)]^{k}[Y-E(Y)]^{\prime}\right\}, \quad k, l=1,2, \cdots$称为X和Y的k+l阶混合中心矩。若n维随机变量$\left(X_{1}, X_{2}, \cdots, X_{n}\right)$的二阶混合中心矩$c_{i j}=\operatorname{Cov}\left(X_{i}, X_{j}\right)=E\left\{\left[X_{i}-E\left(X_{i}\right)\right]\left[X_{j}-E\left(X_{j}\right)\right]\right\}, i, j=1,2, \cdots, n$存在，则下述矩阵为n维随机变量的协方差矩阵：</p>
<p>$\boldsymbol{C}=\left(\begin{array}{cccc}c_{11} &amp; c_{12} &amp; \cdots &amp; c_{1 n} \\c_{21} &amp; c_{22} &amp; \cdots &amp; c_{2 n} \\\vdots &amp; \vdots &amp; &amp; \vdots \\c_{n 1} &amp; c_{n 2} &amp; \cdots &amp; c_{m}\end{array}\right)$</p>
<p>由于$c_{ij}=c_{ji}$所以上述矩阵是一个对称矩阵。【注】：矩阵中每一行代表一个样本，每一列代表一个随机变量。</p>
<p>协方差矩阵是半正定矩阵。</p>
<h1 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h1><h2 id="向量"><a href="#向量" class="headerlink" title="向量"></a>向量</h2><p>一个向量就是一列有序排列的数，可以把向量看作空间中的点，每个元素是不同坐标轴上的坐标。</p>
<h3 id="向量的导数"><a href="#向量的导数" class="headerlink" title="向量的导数"></a>向量的导数</h3><p>令$A=\left[\begin{array}{cccc}a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1 n} \\a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2 n} \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\a_{m 1} &amp; a_{m 2} &amp; \cdots &amp; a_{m n}\end{array}\right]$，$\vec{x}=\left(\begin{array}{c}x_{1} \\x_{2} \\\vdots \\x_{n}\end{array}\right)$，$A \cdot \vec{x}=\left(\begin{array}{c}a_{11} x_{1}+a_{12}x_{2}+\cdots+a_{1 n} x_{n} \\a_{21} x_{1}+a_{22} x_{2}+\cdots+a_{2 n} x_{n} \\\vdots \\a_{m 1} x_{1}+a_{m 2} x_{2}+\cdots+a_{m n} x_{n}\end{array}\right)$，那么：</p>
<p>$\frac{\partial \vec{y}}{\partial \vec{x}}=\frac{\partial A \vec{x}}{\partial\vec{x}}=\left[\begin{array}{cccc}a_{11} &amp; a_{21} &amp; \cdots &amp; a_{m 1} \\a_{12} &amp;a_{22} &amp; \cdots &amp; a_{m 2} \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\a_{1 n} &amp; a_{2 n} &amp;\cdots &amp; a_{m n}\end{array}\right]=A^{T}$</p>
<h4 id="向量偏导数"><a href="#向量偏导数" class="headerlink" title="向量偏导数"></a>向量偏导数</h4><p>$\frac{\partial A \vec{x}}{\partial \vec{x}}=A^{T}$</p>
<p>$\frac{\partial A \vec{x}}{\partial \vec{x}^{T}}=A$</p>
<p>$\frac{\partial\left(\vec{x}^{T} A\right)}{\partial \vec{x}}=A$</p>
<h4 id="标量对向量的导数"><a href="#标量对向量的导数" class="headerlink" title="标量对向量的导数"></a>标量对向量的导数</h4><p>A为$n\times n$的矩阵，x为$n\times 1$的列向量，记$\frac{\partial y}{\partial \vec{x}}=\frac{\partial\left(\vec{x}^{T} \cdot A \cdot \vec{x}\right)}{\partial \vec{x}}=\left(A^{T}+A\right) \cdot \vec{x}$，若A为对称阵，则有$\frac{\partial y}{\partial \vec{x}}=\frac{\partial\left(\vec{x}^{T} \cdot A \cdot \vec{x}\right)}{\partial \vec{x}}=2A\vec{x}$</p>
<h2 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h2><p>矩阵是二维数组，其中的每一个元素被两个索引确定。</p>
<h3 id="矩阵乘法"><a href="#矩阵乘法" class="headerlink" title="矩阵乘法"></a>矩阵乘法</h3><p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mathbasic/12.png" alt="图片"></p>
<p>其中$c_{i j}=a_{i 1} b_{1 j}+a_{i 2} b_{2 j}+\ldots+a_{i n} b_{n j}=\sum^{n}_{k=1} a_{i k} b_{k j}$</p>
<p>矩阵乘法有如下性质：</p>
<ul>
<li>不满足交换律：$A\times B \neq B \times A$</li>
<li>满足结合律：$A\times (B\times C)=(A\times B)\times C$</li>
<li>$(A+B)^2=A^2+AB+BA+B^2$</li>
<li>$(AB)^2=(AB)(AB)\neq A^2B^2$</li>
</ul>
<h3 id="矩阵转置"><a href="#矩阵转置" class="headerlink" title="矩阵转置"></a>矩阵转置</h3><p>矩阵的转置$A^T$将A的横行写为$A^T$的纵列，把A的纵列写为$A^T$的横行，即$A^T_{ij}=A_{ji}$。形式上说，$m\times n$转置A的转置是$n\times m$矩阵。</p>
<p>转置有如下基本性质：</p>
<ul>
<li>$(A+B)^T=A^T+B^T$</li>
<li>$(A\times B)^T=B^T\times A^T$ (在公式化简中常用到)</li>
<li>$(A^T)^T=A$</li>
</ul>
<h3 id="逆矩阵"><a href="#逆矩阵" class="headerlink" title="逆矩阵"></a>逆矩阵</h3><p>给定一个n阶方阵A，若存在一n阶方阵B，使得$AB=BA=I_n$，其中$I_n$为n阶单位矩阵，则称A是可逆的，且B是A的逆矩阵，记作$A^{-1}$。</p>
<p>只有方阵才可能有逆矩阵。若方阵A的逆矩阵存在，则称A为非奇异方阵或可逆方阵。与行列式类似，逆矩阵一般用于求解联立方程组。</p>
<p>逆矩阵的运算律如下：</p>
<ul>
<li>$(AB)^{-1}=B^{-1}A^{-1}$</li>
<li>$(kA)^{-1}=A^{-1}/k$</li>
<li>$(A^{-1})^T=(A^T)^{-1}$</li>
<li>$(A^n)^{-1}=(A^{-1})^n$</li>
<li>$(A^{-1})^{-1}=A$</li>
<li>$(\lambda A)^{-1}=\frac{1}{\lambda}\times A^{-1}$</li>
<li>$(AB)^{-1}=B^{-1}A^{-1}$</li>
<li>$(A^T)^{-1}=(A^{-1})^T$</li>
<li>$det(A^{-1})=\frac{1}{det(A)}$</li>
</ul>
<p>矩阵可逆的有几个充分必要条件：</p>
<ul>
<li>$|A|\neq 0$且$A^{-1}=\frac{1}{|A|}A^*$</li>
<li>A可以经过有限次初等变换化为单位矩阵</li>
<li>A可表示为有限个初等矩阵的乘积</li>
<li>A为满秩矩阵</li>
<li>A的所有特征值都不为0</li>
</ul>
<p>可逆方阵/非奇异方阵有如下定理：</p>
<ul>
<li>一个方阵非奇异当且进党它的行列式不为零</li>
<li>一个方阵非奇异当且仅当它代表的线性变换是个自同构</li>
<li>一个矩阵半正定当且进党它的每个特征值大于或等于零</li>
<li>一个矩阵正定当且仅当它的每个特征值都大于零</li>
</ul>
<h3 id="伪逆"><a href="#伪逆" class="headerlink" title="伪逆"></a>伪逆</h3><p>对于非方阵而言，其逆矩阵没有定义。假设在下面问题中，我们想通过矩阵A的左逆B来求解线性方程：$Ax=y$，等式两边同时左乘左逆B后，得到：$x=By$，是否存在唯一的映射将A映射到B取决于问题的形式：如果矩阵A的行数大于列数，那么上述方程可能没有解；如果矩阵A的行数小于列数，那么上述方程可能有多个解。</p>
<p>矩阵A的伪逆定义为：$\boldsymbol{A}^{+}=\lim _{a \backslash_{\boldsymbol{v}} 0}\left(\boldsymbol{A}^{\top} \boldsymbol{A}+\alpha \boldsymbol{I}\right)^{-1} \boldsymbol{A}^{\top}$</p>
<p>但是计算伪逆的实际算法没有基于这个式子，而是使用这个公式：$A^{+}=V D^{+} U^{\top}$</p>
<p>其中，矩阵U,D和V是矩阵A奇异值分解后得到的矩阵。对角矩阵D的伪逆D+是其非零元素取倒之后再转置得到的。</p>
<h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h4><p>任何一种广义逆阵都可以用来判断线性方程组是否有解，若有解时列出其所有的解。若一下$n\times m$的线性系统有解存在$Ax=b$，其中向量x伪未知数，向量b伪常数，一下是所有的解$x=A^gb+[I-A^gA]w$，其中参数w为任意矩阵，而$A^g$为A的任何一个广义逆阵。解存在的条件当且仅当$A^gb$为其中一个解，也就是当且进当$AA^gb=b$。</p>
<h3 id="迹"><a href="#迹" class="headerlink" title="迹"></a>迹</h3><p>矩阵的迹（trace）表示矩阵A主对角线所有元素的和，即$\operatorname{tr}(A)=a_{11}+a_{22}+\cdots+a_{n n}$，具有如下性质：</p>
<ul>
<li>$\operatorname{tr}(A)=\operatorname{tr}\left(A^{T}\right)$</li>
<li>$\operatorname{tr}(A B)=\operatorname{tr}(B A)$</li>
<li>$\operatorname{tr}(A B C)=\operatorname{tr}(B C A)=\operatorname{tr}(C A B)$</li>
<li>若A与B相似，则$\operatorname{tr}(A)=\operatorname{tr}(B)$</li>
<li>$\operatorname{tr}(A+B)=\operatorname{tr}(A)+\operatorname{tr}(B)$</li>
</ul>
<h3 id="矩阵的导数"><a href="#矩阵的导数" class="headerlink" title="矩阵的导数"></a>矩阵的导数</h3><p>矩阵求导可参考如下两篇博客：</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/263777564" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/263777564</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/273729929" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/273729929</a></li>
</ul>
<h4 id="雅可比矩阵"><a href="#雅可比矩阵" class="headerlink" title="雅可比矩阵"></a>雅可比矩阵</h4><p>雅可比矩阵的重要性在于它体现了一个可微方程与给出点的最优线性逼近. 因此, 雅可比矩阵类似于多元函数的导数。</p>
<p>假设$F: R_{n} \rightarrow R_{m}$是一个从欧式n维空间转换到欧式m维空间的函数. 这个函数由m个实函数组成: y1(x1,…,xn), …, ym(x1,…,xn). 这些函数的偏导数(如果存在)可以组成一个m行n列的矩阵, 这就是所谓的雅可比矩阵：</p>
<p>$\left[\begin{array}{ccc}\frac{\partial y_{1}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{1}}{\partial x_{n}} \\\vdots &amp; \ddots &amp; \vdots \\\frac{\partial y_{m}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{m}}{\partial x_{n}}\end{array}\right]$</p>
<p>此矩阵表示为: $J_{F}\left(x_{1}, \ldots, x_{n}\right)$或者$\frac{\partial\left(y_{1}, \ldots, y_{m}\right)}{\partial\left(x_{1}, \ldots, x_{n}\right)}$。这个矩阵的第i行是由梯度函数的转置yi(i=1,…,m)表示的。</p>
<p>如果p是$R_n$中的一点，F在p点可微分, 那么在这一点的导数由$J_F(p)$给出(这是求该点导数最简便的方法). 在此情况下, 由$F(p)$描述的线性算子即接近点p的F的最优线性逼近，x逼近于p：$F(\mathbf{x}) \approx F(\mathbf{p})+J_{F}(\mathbf{p}) \cdot(\mathbf{x}-\mathbf{p})$</p>
<p>另外介绍一下雅可比行列式。如果m = n, 那么F是从n维空间到n维空间的函数, 且它的雅可比矩阵是一个方块矩阵. 于是我们可以取它的行列式, 称为雅可比行列式。</p>
<p>在某个给定点的雅可比行列式提供了 在接近该点时的表现的重要信息. 例如, 如果连续可微函数F在p点的雅可比行列式不是零, 那么它在该点附近具有反函数. 这称为反函数定理. 更进一步, 如果p点的雅可比行列式是正数, 则F在p点的取向不变；如果是负数, 则F的取向相反. 而从雅可比行列式的绝对值, 就可以知道函数F在p点的缩放因子；这就是为什么它出现在换元积分法中。</p>
<p>对于取向问题可以这么理解, 例如一个物体在平面上匀速运动, 如果施加一个正方向的力F即取向相同, 则加速运动, 类比于速度的导数加速度为正；如果施加一个反方向的力F，即取向相反, 则减速运动, 类比于速度的导数加速度为负。</p>
<h4 id="海森矩阵"><a href="#海森矩阵" class="headerlink" title="海森矩阵"></a>海森矩阵</h4><p>海森矩阵(Hessian matrix或Hessian)是一个自变量为向量的实值函数的二阶偏导数组成的方块矩阵, 此函数如下：$f\left(x_{1}, x_{2} \ldots, x_{n}\right)$。如果f的所有二阶导数都存在, 那么f的海森矩阵即：$H(f)_{i j}(x)=D_{i} D_{j} f(x)$。其中$x=\left(x_{1}, x_{2} \ldots, x_{n}\right)$，即$H(f)$为：</p>
<p>$\left[\begin{array}{cccc}\frac{\partial^{2} f}{\partial x_{1}^{2}} &amp; \frac{\partial^{2} f}{\partial x_{1} \partial x_{2}} &amp; \cdots &amp; \frac{\partial^{2} f}{\partial x_{1} \partial x_{n}} \\\frac{\partial^{2} f}{\partial x_{2} \partial x_{1}} &amp; \frac{\partial^{2} f}{\partial x_{2}^{2}}&amp; \cdots &amp; \frac{\partial^{2} f}{\partial x_{2} \partial x_{n}} \\\vdots &amp; \vdots &amp;\ddots &amp; \vdots \\\frac{\partial^{2} f}{\partial x_{n} \partial x_{1}} &amp; \frac{\partial^{2} f}{\partial x_{n} \partial x_{2}} &amp; \cdots &amp; \frac{\partial^{2} f}{\partial x_{n}^{2}}\end{array}\right]$。</p>
<p>海森矩阵被应用于牛顿法解决的大规模优化问题。</p>
<h3 id="行列式"><a href="#行列式" class="headerlink" title="行列式"></a>行列式</h3><p>行列式是数学中的一个函数，将$n\times n$的矩阵A映射到一个标量，记作$det(A)$或$|A|$。行列式可以看作是有向面积或体积的概念在一般的欧几里得空间中的推广。或者说，在n维欧几里得空间中，行列式描述的是一个线性变换对“体积”所造成的影响。</p>
<h4 id="几何意义"><a href="#几何意义" class="headerlink" title="几何意义"></a>几何意义</h4><p>行列式是向量形成的平行四边形的面积。在一个二维平面上，两个向量X=(a,c)和X’=(b,d)的行列式是：</p>
<p>$\operatorname{det}\left(X, X^{\prime}\right)=\left|\begin{array}{ll}a &amp; b \\c &amp; d\end{array}\right|=a d-b c$</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mathbasic/13.png" alt="图片"></p>
<h4 id="代数余子式"><a href="#代数余子式" class="headerlink" title="代数余子式"></a>代数余子式</h4><p>在n阶行列式中，将元素$a_{ij}$所在的行和列上的所有元素都划去，留下的元素构成n-1阶行列式，称为元素$a_{ij}$的余子式，记为$M_{ij}$，称$A_{y}=(-1)^{i+j}M_{ij}$。</p>
<p>例如，三阶行列式$D=\left|\begin{array}{lll}a_{11} &amp; a_{12} &amp; a_{13} \\a_{21} &amp; a_{22} &amp; a_{23} \\a_{31} &amp; a_{32} &amp; a_{33}\end{array}\right|$，$a_{13}$的余子式为$M_{13}=\left|\begin{array}{ll}a_{21} &amp; a_{22} \\a_{31} &amp; a_{32}\end{array}\right|$，代数余子式为$A_{13}=(-1)^{1+3}\left|\begin{array}{cc}a_{21} &amp; a_{22} \\a_{31} &amp; a_{32}\end{array}\right|$</p>
<h4 id="计算方法"><a href="#计算方法" class="headerlink" title="计算方法"></a>计算方法</h4><p><strong>方法一</strong>：对于n阶矩阵$A=(a_{ij})$，定义n阶行列式的$|A|$的值为$\sum(-1)^{\tau(j_1j_2j_3\cdots j_n)} a_{1 j_{1}} a_{2 j_{2}} \cdots a_{n j_{n}}$，这里$\sum(-1)^{\tau(j_1j_2j_3)}$表示对所有不同的n级排列求和，$\tau(j_1j_2j_3\cdots j_n)$表示序列$j_1j_2\cdots j_n$的逆序数。例如：</p>
<p>$\begin{array}{l}\left|\begin{array}{lll}a_{11} &amp; a_{12} &amp; a_{13} \\a_{21} &amp; a_{22} &amp; a_{23} \\a_{31} &amp; a_{32} &amp; a_{33}\end{array}\right|=a_{11} a_{22} a_{33}+a_{12} a_{23}a_{31}+a_{13} a_{21} a_{32} \-a_{11} a_{23} a_{32}-a_{12} a_{21} a_{33}-a_{13}a_{22} a_{3}\end{array}$</p>
<p><strong>方法二</strong>：</p>
<p>n阶行列式的值等于它的任意一行（列）的元素与其对应的代数余子式的乘积的和。</p>
<p>例如，设三阶行列式：</p>
<p>$\left|\begin{array}{ccc}2 &amp; 1 &amp; 2 \-4 &amp; 3 &amp; 1 \\2 &amp; 3 &amp; 5\end{array}\right|=2(-1)^{1+1}\left|\begin{array}{cc}3 &amp; 1 \\3 &amp; 5\end{array}\right|+1(-1)^{1+2}\left|\begin{array}{cc}-4 &amp; 1 \\2 &amp; 5\end{array}\right|+2(-1)^{1+3}\left|\begin{array}{cc}-4 &amp; 3 \\2 &amp; 3\end{array}\right|$</p>
<p>$\begin{aligned}D &amp;=a_{11} A_{11}+a_{12} A_{12}+a_{13} A_{13} \\&amp;=2 \times(15-3)-(-20-2)+2 \times(-12-6)=10\end{aligned}$</p>
<p><strong>方法三</strong>：设n阶矩阵A的全部特征值为$\lambda_1,\lambda_2,\cdots,\lambda_n$，则$|A|=\lambda_1\lambda_2\cdots\lambda_n$</p>
<h4 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h4><ul>
<li>$D=|\begin{array}{llll}\lambda_{1} &amp; &amp; &amp; \\&amp; \lambda_{2} &amp; &amp; \\&amp; &amp; \ddots &amp; \\&amp; &amp; &amp; \lambda_{n}\end{array} \mid=\lambda_{1} \lambda_{2} \cdots \lambda_{n}$</li>
<li>$D=\left|\begin{array}{cccc}a_{11} &amp; &amp; &amp; 0 \\a_{21} &amp; a_{22} &amp; &amp; \\\vdots &amp; \vdots &amp; \ddots &amp; \\a_{n 1} &amp; a_{n 2} &amp; \cdots &amp; a_{n n}\end{array}\right|=a_{11} a_{22} \cdots a_{n n}$</li>
<li>行列式和它的转置行列式相等</li>
<li>互换行列式的两行，行列式变号</li>
<li>如果行列式有两行（列）完全相同，则行列式等于零</li>
<li>如果行列式有两行（列）元素成比例，则行列式等于零</li>
<li>分行可加性</li>
</ul>
<h3 id="秩"><a href="#秩" class="headerlink" title="秩"></a>秩</h3><p>在$m \times n$矩阵A中，任取k行k列，不改变这$k^2$个元素在A中的次序，得到k阶行列式，称为矩阵A的k阶子式。显然$m\times n$矩阵A的k阶子式有$C^k_mC^k_n$个。</p>
<p>设在矩阵A中有一个不等于0的r阶子式D，且所有r+1阶子式（如果存在）全等于0，那么D称为矩阵A的最高阶非零子式，r称为矩阵A的秩，记作$R(A)=r$，满足如下性质：</p>
<ul>
<li>设A为$m \times n$矩阵，当R(A)=m时，称A为行满秩矩阵，当R(A)=n时，称A为列满秩矩阵</li>
<li>若A为n阶方阵，且R(A)=n，则称A为满秩矩阵</li>
<li>方阵A为满秩矩阵的充要条件为$|A|\neq 0$</li>
<li>$n\times n$的可逆矩阵，秩为n，方阵A可逆的充要条件是A为满秩矩阵，可逆矩阵又称满秩矩阵</li>
<li>矩阵的秩等于它行(列)向量组的秩</li>
</ul>
<h4 id="与线性方程组的解的关系"><a href="#与线性方程组的解的关系" class="headerlink" title="与线性方程组的解的关系"></a>与线性方程组的解的关系</h4><p>$\left\{\begin{array}{l}a_{11} x_{1}+a_{12} x_{2}+\cdots+a_{1 n} x_{n}=b_{1} \\a_{21} x_{1}+a_{22} x_{2}+\cdots+a_{2 n} x_{n}=b_{2} \\\ldots \ldots \\a_{m 1} x_{1}+a_{m 2} x_{2}+\cdots+a_{m n} x_{n}=b_{m}\end{array} \Rightarrow A \vec{x}=\vec{b}\right.$</p>
<p>对于n元线性方程组$Ax=b$：</p>
<ul>
<li>无解的充要条件是$R(A)&lt;R(A,b)$</li>
<li>有唯一解的充要条件是$R(A)=R(A,b)=n$</li>
<li>有无限多解的充要条件是$R(A)=R(A,b)&lt;n$</li>
</ul>
<h3 id="范数"><a href="#范数" class="headerlink" title="范数"></a>范数</h3><p>范数用于衡量一个向量的大小，可以看成一个函数（输入是向量，输出是值），定义如下：$||x||_p=(\sum_i|x_i|^p)^{\frac{1}{p}}$。L1范数为x向量各个元素绝对值之和，L2范数为x向量各个元素平方和的开放。</p>
<h3 id="向量的内积-点积"><a href="#向量的内积-点积" class="headerlink" title="向量的内积/点积"></a>向量的内积/点积</h3><p>点积有两种定义方式：代数方式和几何方式。通过在欧式空间中引入笛卡尔坐标系，向量之间的点积既可以由向量坐标的代数运算得出，也可以通过引入两个向量的长度和角度等几何概念来求解。</p>
<h4 id="代数定义"><a href="#代数定义" class="headerlink" title="代数定义"></a>代数定义</h4><p>两个向量$\overrightarrow{a}=[a_1,a_2,…,a_n]$和$\overrightarrow{b}=[b_1,b_2,…,b_n]$的点积为：$\vec{a} \cdot \vec{b}=\sum_{i=1}^{n} a_{i} b_{i}=a_{1} b_{1}+a_{2}b_{2}+\cdots+a_{n} b_{n}$</p>
<h4 id="几何定义"><a href="#几何定义" class="headerlink" title="几何定义"></a>几何定义</h4><p>在欧几里得空间中，点积可以直观地定义为：$\overrightarrow{a}\cdot\overrightarrow{b}=|\overrightarrow{a}||\overrightarrow{b}|cos\theta$，这里$|\overrightarrow{x}|$表示$\overrightarrow{x}$的模(长度), $\theta$表示两个向量之间的角度。</p>
<p>两个互相垂直的向量的点积总是零，若$\overrightarrow{a}$和$\overrightarrow{b}$都是单位向量(长度为1)，它们的点积就是它们的夹角的余弦。给定两个向量，夹角可以通过下式得到：$cos\theta = \frac{a\cdot b}{\overrightarrow{a}\overrightarrow{b}}=a\cdot b$</p>
<h4 id="标量投影"><a href="#标量投影" class="headerlink" title="标量投影"></a>标量投影</h4><p>欧式空间中向量A在向量B上的标量投影是指：$A_{B}=|\mathbf{A}| \cos \theta$。从点积的几何定义$\mathbf{A} \cdot \mathbf{B}=|\mathbf{A}||\mathbf{B}| \cos \theta$不难得出两个向量的点积$A\cdot B$可以理解为向量A在向量B上的投影再乘以B的长度：$\mathbf{A} \cdot \mathbf{B}=A_{B}|\mathbf{B}|=B_{A}|\mathbf{A}|$，如下图：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mathbasic/14.png" alt="图片"></p>
<h3 id="向量外积"><a href="#向量外积" class="headerlink" title="向量外积"></a>向量外积</h3><p>外积(outer product)一般指两个向量的张量积，其结果为一矩阵；与外积相对，两向量的内积结果为标量。</p>
<p>给定$m\times 1$列向量u和$1\times n$行向量v，它们的外积$\mathbf{u} \otimes\mathbf{v}$被定义为$m\times n$的矩阵A：$\mathbf{u} \otimes\mathbf{v}=\mathbf{A}=\mathbf{u v}$，使用坐标：</p>
<p>$\left[\begin{array}{l} b_{1} \ b_{2} \ b_{3} \ b_{4} \end{array}\right]\otimes\left[\begin{array}{lll} a_{1} &amp; a_{2} &amp; a_{3}\end{array}\right]=\left[\begin{array}{lll} a_{1} {b_1} &amp; a_{2} b_{1} &amp; a_{3} b_{1} \ a_{1}b_{2} &amp; a_{2} b_{2} &amp; a_{3} b_{2} \ a_{1} b_{3} &amp; a_{2} b_{3} &amp; a_{3} b_{3} \ a_{1}b_{4} &amp; a_{2} b_{4} &amp; a_{3} b_{4} \end{array}\right]$</p>
<h3 id="对称矩阵"><a href="#对称矩阵" class="headerlink" title="对称矩阵"></a>对称矩阵</h3><p>$A^T=A$，对称矩阵有如下性质：</p>
<ul>
<li>对称矩阵的不同特征值对应的特征向量是正交的</li>
<li>若A是n阶实对称矩阵，则存在正交矩阵Q使得$Q^TAQ=diag(\lambda_1,\lambda_2,\cdots,\lambda_n)$，其中$\lambda_i$为A的特征值</li>
</ul>
<h3 id="相似矩阵"><a href="#相似矩阵" class="headerlink" title="相似矩阵"></a>相似矩阵</h3><p>对于n阶矩阵A,B，如果存在n阶可逆矩阵P，使得$\boldsymbol{P}^{-1} \boldsymbol{A P}=\boldsymbol{B}$，则说矩阵A相似于矩阵B，记作$A~B$。相似矩阵具有如下性质：</p>
<ul>
<li>若$A~B$，则$R(A)=R(B)$</li>
<li>若$A~B$，则$|A|=|B|$</li>
<li>若$A~B$，则$A^T~B^T$</li>
<li>若$A~B$且A可逆，则B亦可逆，且$A^T~B^T$</li>
<li>若$A~B$，则对任意多项式$f(\lambda)$，必有$f(A)~f(B)$</li>
<li>若$A~B$，则A与B具有相同的特征多项式，从而具有完全相同的特征值</li>
<li>n阶矩阵A能与对焦矩阵相似的充要条件是A存在n个线性无关的特征向量<ul>
<li>当n阶矩阵A相似于对焦矩阵时，该对角矩阵的主对角元素恰是A的全部特征值；相似因子P的各列恰是A的n各线性无关的特征向量</li>
</ul>
</li>
</ul>
<h3 id="正交"><a href="#正交" class="headerlink" title="正交"></a>正交</h3><h4 id="向量正交"><a href="#向量正交" class="headerlink" title="向量正交"></a>向量正交</h4><p>正交是垂直这一直观概念的推广。作为一个形容词，只有在一个确定的内积空间中才有意义。若内积空间中两向量的内积为0，则称它们是正交的。如果能够定义向量间的夹角，则正交可以直观的理解为垂直。</p>
<h4 id="正交向量组"><a href="#正交向量组" class="headerlink" title="正交向量组"></a>正交向量组</h4><p>如果一组非零向量两两正交，则称这组向量为正交向量组，简称正交组。正交向量组必是线性无关组。如果一个正交向量组中每个向量都是单位向量，则称该向量组为单位正交向量组。</p>
<h4 id="正交矩阵"><a href="#正交矩阵" class="headerlink" title="正交矩阵"></a>正交矩阵</h4><p>如果n阶实矩阵A的列向量组是一个单位正交向量组，则称A为正交矩阵。正交矩阵有如下定理或性质：</p>
<ul>
<li>n阶实矩阵A为正交矩阵的充要条件是$A^TA=E$</li>
<li>若A是正交矩阵，则-A也是正交矩阵</li>
<li>若A是正交矩阵，则$A^T(A^{-1})$也是正交矩阵</li>
<li>若A,B都是n阶正交矩阵，则AB也是n阶正交矩阵</li>
<li>正交矩阵的行列式值必定为+1或-1，因为$1=det(I)=det(Q^TQ)=det(Q^T)det(Q)=(det(Q))^2\Rightarrow det(Q)=\pm1$</li>
</ul>
<h4 id="正交变换"><a href="#正交变换" class="headerlink" title="正交变换"></a>正交变换</h4><p>对于n阶实矩阵A，以n阶正交矩阵Q为相似因子进行的相似变换$Q^{-1}AQ$称为对A的正交变换。这个变换的特点是不改变向量的尺寸和向量间的夹角，那么它到底是个什么样的变换呢？看下面这张图：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mathbasic/15.png" alt="图片"></p>
<p>假设二维空间中的一个向量OA，它在标准坐标系也即e1、e2表示的坐标是中表示为(a,b)’（用’表示转置），现在把它用另一组坐标e1’、e2’表示为(a’,b’)’，存在矩阵U使得(a’,b’)’=U(a,b)’，则U即为正交矩阵。从图中可以看到，正交变换只是将变换向量用另一组正交基表示，在这个过程中并没有对向量做拉伸，也不改变向量的空间位置，加入对两个向量同时做正交变换，那么变换前后这两个向量的夹角显然不会改变。上面的例子只是正交变换的一个方面，即旋转变换，可以把e1’、e2’坐标系看做是e1、e2坐标系经过旋转某个角度得到，怎么样得到该旋转矩阵U呢？如下：</p>
<p>$\mathbf{x}=\left[\begin{array}{l}a \\b\end{array}\right]$</p>
<p>$a^{\prime}=x \cdot e 1^{\prime}=e_1^{T} x$</p>
<p>$b^{\prime}=x \cdot e 2^{\prime}=e_2^{\prime T} x$</p>
<p>a’和b’实际上是x在e1’和e2’轴上的投影大小，所以直接做内积可得，那么：</p>
<p>$\left[\begin{array}{l}a^{\prime} \\b^{\prime}\end{array}\right]=\left[\begin{array}{l}e _1^{\prime T} \\e_2^{\prime T}\end{array}\right] x$</p>
<p>从图中可以看到：</p>
<p>$\boldsymbol{e} \mathbf{1}^{\prime}=\left[\begin{array}{c}\cos \theta \\\sin \theta\end{array}\right] \quad e 2^{\prime}=\left[\begin{array}{c}-\sin \theta \\\cos\theta\end{array}\right]$</p>
<p>所以：</p>
<p>$\mathbf{U}=\left[\begin{array}{cc}\cos \theta &amp; \sin \theta \-\sin \theta &amp; \cos\theta\end{array}\right]$</p>
<p>正交阵U行（列）向量之间都是单位正交向量。上面求得的是一个旋转矩阵，它对向量做旋转变换！也许你会有疑问：刚才不是说向量空间位置不变吗？怎么现在又说它被旋转了？对的，这两个并没有冲突，说空间位置不变是绝对的，但是坐标是相对的，加入你站在e1上看OA，随着e1旋转到e1’，看OA的位置就会改变。如下图：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mathbasic/16.png" alt="图片"></p>
<p>如图，如果我选择了e1’、e2’作为新的标准坐标系，那么在新坐标系中OA（原标准坐标系的表示）就变成了OA’，这样看来就好像坐标系不动，把OA往顺时针方向旋转了“斯塔”角度，这个操作实现起来很简单：将变换后的向量坐标仍然表示在当前坐标系中。</p>
<p>旋转变换是正交变换的一个方面，这个挺有用的，比如在开发中需要实现某种旋转效果，直接可以用旋转变换实现。正交变换的另一个方面是反射变换，也即e1’的方向与图中方向相反，这个不再讨论。</p>
<p>总结：正交矩阵的行（列）向量都是两两正交的单位向量，正交矩阵对应的变换为正交变换，它有两种表现：旋转和反射。正交矩阵将标准正交基映射为标准正交基（即图中从e1、e2到e1’、e2’）</p>
<h3 id="特征值与特征向量"><a href="#特征值与特征向量" class="headerlink" title="特征值与特征向量"></a>特征值与特征向量</h3><p>对于n阶矩阵$A=(a_{ij})$，把含有字母$\lambda$的矩阵$\lambda E - A$称为A的特征矩阵，行列式$\lambda E - A$的值表达式称为A的特征多项式。特征多项式的根称为A的特征值。</p>
<p>$\left|\begin{array}{cccc}a_{11}-\lambda &amp; a_{12} &amp; \cdots &amp; a_{1 n} \\a_{21} &amp; a_{22}-\lambda &amp; \cdots &amp; a_{2 n} \\\vdots &amp; \vdots &amp; &amp; \vdots \\a_{n 1} &amp; a_{n 2} &amp; \cdots &amp; a_{n n}-\lambda\end{array}\right|=0$</p>
<p>方阵A的特征向量是指与A相乘后相当于对该向量进行缩放的非零向量v：$Av=\lambda v$。对于方阵A，求其特征值与特征向量的步骤如下：</p>
<ul>
<li>写出A的特征矩阵$\lambda E - A$，并计算其特征多项式$|\lambda E - A|$</li>
<li>求出$|\lambda E - A|=0$的全部根，即A的全部特征值，记互异的特征值为$\lambda_1,\lambda_2,\cdots,\lambda_t$</li>
<li>对每一个$\lambda_i(i=1,2,\cdots,t)$求出齐次线性方程组$(\lambda_iE-A)x=0$的全部非零解，即A对应于特征值$\lambda_i$的全部特征向量$x_1,x_2,\cdots,x_s$，于是A对应于特征值$\lambda_i$的全部特征向量可表示为$k_1x_1+k_2x_2+\cdots+k_sx_s$</li>
</ul>
<p>例如求下面矩阵的特征值和特征向量：</p>
<p>$A=\left(\begin{array}{rrr}-1 &amp; 1 &amp; 0 \-4 &amp; 3 &amp; 0 \\1 &amp; 0 &amp; 2\end{array}\right)$</p>
<p>A的特征多项式为</p>
<p>$|\boldsymbol{A}-\lambda \boldsymbol{E}|=\left|\begin{array}{ccc}-1-\lambda &amp; 1 &amp; 0 \-4 &amp; 3-\lambda &amp; 0 \\1 &amp; 0 &amp; 2-\lambda\end{array}\right|=(2-\lambda)(1-\lambda)^{2}$</p>
<p>所以A的特征值是$\lambda_{1}=2, \lambda_{2}=\lambda_{3}=1$。</p>
<p>当$\lambda_1=2$时，解方程$(A-2 E) x=0$，由</p>
<p>$A-2 E=\left(\begin{array}{ccc}-3 &amp; 1 &amp; 0 \-4 &amp; 1 &amp; 0 \\1 &amp; 0 &amp; 0\end{array}\right) \sim\left[\begin{array}{ccc}1 &amp; 0 &amp; 0 \\0 &amp; 1 &amp; 0 \\0 &amp; 0 &amp; 0\end{array}\right)$</p>
<p>得到基础解系$p_{1}=\left(\begin{array}{l}0 \\0 \\1\end{array}\right)$，所以$k p_{1}(k \neq 0)$是对应于$\lambda_1=2$的全部特征向量。</p>
<p>当$\lambda_{2}=\lambda_{3}=1$时，解方程$(A-E) x=0$，由</p>
<p>$A-E=\left(\begin{array}{rrr}-2 &amp; 1 &amp; 0 \-4 &amp; 2 &amp; 0 \\1 &amp; 0 &amp; 1\end{array}\right)-\left(\begin{array}{lll}1 &amp; 0 &amp; 1 \\0 &amp; 1 &amp; 2 \\0 &amp; 0 &amp; 0\end{array}\right)$</p>
<p>得基础解系$p_{2}=\left(\begin{array}{r}-1 \-2 \\1\end{array}\right)$，所以$k p_{2}(k \neq 0)$是对应于$\lambda_{2}=\lambda_{3}=1$的全部特征向量。</p>
<h3 id="正定矩阵"><a href="#正定矩阵" class="headerlink" title="正定矩阵"></a>正定矩阵</h3><p>对于n阶实对称矩阵，若对任意n维向量$x\neq 0$，都有$x^TAx&gt;0$，则矩阵A是正定矩阵。若$x^TAx \ge 0$，则称A为半正定矩阵。</p>
<p>判断正定矩阵的方法：A的特征值都是正数；A的n个顺序主子式的值全为正数。</p>
<p>实际上，我们可以将$y=\boldsymbol{x}^{T} A \boldsymbol{x}$视作$y=a x^{2}$的多维表达式。当我们希望$y=\boldsymbol{x}^{T} A \boldsymbol{x} \geq 0$对于任意向量x都恒成立，就要求矩阵A是一个半正定矩阵。</p>
<p>从直观上怎么解释呢？若给定任意一个正定矩阵$A \in \mathbb{R}^{n \times n}$和一个非零向量$\boldsymbol{x} \in \mathbb{R}^{n}$，则两者相乘得到的向量$\boldsymbol{y}=A \boldsymbol{x} \in \mathbb{R}^{n}$与向量x的夹角恒小于$\frac{\pi}{2}$(等价于$\boldsymbol{x}^{T} A \boldsymbol{x}&gt;0$)</p>
<h4 id="主子式"><a href="#主子式" class="headerlink" title="主子式"></a>主子式</h4><p>设A是一个$m\times n$的矩阵，I是集合$\{1,\cdots,m\}$的一个k元子集，J是集合$\{1,\cdots,n\}$的一个k元子集，那么$|A|_{I,J}$表示A的k阶子式。其中抽取的k行的行标是I中所有元素，k列的列标是J中所有元素。</p>
<ul>
<li>如果$I=J$，那么称$|A|_{I,J}$是A的主子式</li>
<li>如果$I=J=\{1,\cdots,k\}$(所取的是左起前k列和上起前k行)，那么相应的主子式被称为顺序主子式。一个$n\times n$的方块矩阵有n个顺序主子式。</li>
</ul>
<h3 id="主成分分解"><a href="#主成分分解" class="headerlink" title="主成分分解"></a>主成分分解</h3><p>主成分分析（Principal components analysis，以下简称PCA）是最重要的降维方法之一。在数据压缩消除冗余和数据噪音消除等领域都有广泛的应用。具体的，假如我们的数据集是n维的，共有m个数据$\left(x^{(1)}, x^{(2)}, \ldots, x^{(m)}\right)$，我们希望将这m个数据的维度从n维降到n’维，希望这m个n’维的数据集尽可能的代表原始数据集。我们知道数据从n维降到n’维肯定会有损失，但是我们希望损失尽可能的小。那么如何让这n’维的数据尽可能表示原来的数据呢？</p>
<p>我们先看看最简单的情况，也就是n=2，n’=1,也就是将数据从二维降维到一维。数据如下图。我们希望找到某一个维度方向，它可以代表这两个维度的数据。图中列了两个向量方向，$u_1$和$u_2$，那么哪个向量可以更好的代表原始数据集呢？从直观上也可以看出，$u_1$比$u_2$好。</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mathbasic/17.png" alt="图片"></p>
<p>为什么$u_1$比$u_2$好呢？可以有两种解释，第一种解释是样本点到这个直线的距离足够近，第二种解释是样本点在这个直线上的投影能尽可能的分开。</p>
<p>假如我们把n’从1维推广到任意维，则我们的希望降维的标准为：样本点到这个超平面的距离足够近,或者说样本点在这个超平面上的投影能尽可能的分开。基于上面的两种标准，我们可以得到PCA的两种等价推导。</p>
<h4 id="PCA推导——基于最小投影距离"><a href="#PCA推导——基于最小投影距离" class="headerlink" title="PCA推导——基于最小投影距离"></a>PCA推导——基于最小投影距离</h4><p>我们首先看第一种解释的推导，即样本点到这个超平面的距离足够近。假设m个n维数据$\left(x^{(1)}, x^{(2)}, \ldots, x^{(m)}\right)$都已经进行了中心化，即$\sum_{i=1}^{m} x^{(i)}=0$。经过投影变换后得到的新坐标系为$\left\{w_{1}, w_{2}, \ldots, w_{n}\right\}$，其中w是标准正交基，即$|w|_{2}=1, w_{i}^{T} w_{j}=0$。</p>
<p>如果我们将数据从n维降到n’维，即丢弃新坐标系中的部分坐标，则新的坐标系为$\left\{w_{1}, w_{2}, \ldots, w_{n^{\prime}}\right\}$，样本点$x^{(i)}$在n’维坐标系中的投影为：$z^{(i)}=\left(z_{1}^{(i)}, z_{2}^{(i)}, \ldots, z_{n^{\prime}}^{(i)}\right)^{T}$。其中$z_{j}^{(i)}=w_{j}^{T} x^{(i)}$是$x^{(i)}$在低维坐标系里第j维的坐标。</p>
<p>如果我们用$z^{(i)}$来恢复原始数据$x^{(i)}$，则得到的恢复数据$\bar{x}^{(i)}=\sum_{j=1}^{n^{\prime}} z_{j}^{(i)} w_{j}=W z^{(i)}$，其中，W为标准正交基组成的矩阵。</p>
<p>现在我们考虑整个样本集，我们希望所有的样本到这个超平面的距离足够近，即最小化下式：$\sum_{i=1}^{m}\left|\bar{x}^{(i)}-x^{(i)}\right|_{2}^{2}$。</p>
<p>将这个式子进行整理，可以得到：</p>
<p>$\begin{aligned}\sum_{i=1}^{m}\left|\bar{x}^{(i)}-x^{(i)}\right|_{2}^{2}&amp;=\sum_{i=1}^{m}\left|W z^{(i)}-x^{(i)}\right|_{2}^{2} \\&amp;=\sum_{i=1}^{m}\left(W z^{(i)}\right)^{T}\left(W z^{(i)}\right)-2 \sum_{i=1}^{m}\left(Wz^{(i)}\right)^{T} x^{(i)}+\sum_{i=1}^{m} x^{(i) T} x^{(i)} \\&amp;=\sum_{i=1}^{m} z^{(i) T} z^{(i)}-2 \sum_{i=1}^{m} z^{(i) T} W^{T} x^{(i)}+\sum_{i=1}^{m}x^{(i) T} x^{(i)} \\&amp;=\sum_{i=1}^{m} z^{(i) T_{z}(i)}-2 \sum_{i=1}^{m}z^{(i) T_{z}(i)}+\sum_{i=1}^{m} x^{(i) T_{x}} x^{(i)} \\&amp;=-\sum_{i=1}^{m} z^{(i) T_{z}(i)}+\sum_{i=1}^{m} x^{(i) T} x^{(i)} \\&amp;=-\operatorname{tr}\left(W^{T}\left(\sum_{i=1}^{m} x^{(i)} x^{(i) T}\right)W\right)+\sum_{i=1}^{m} x^{(i) T} x^{(i)} \\&amp;=-\operatorname{tr}\left(W^{T} XX^{T} W\right)+\sum_{i=1}^{m} x^{(i) T} x^{(i)}\end{aligned}$</p>
<p>其中第(1)步用到了\bar{x}^{(i)}=W z^{(i)}，第二步用到了平方和展开，第（3）步用到了矩阵转置公式$(A B)^{T}=B^{T} A^{T}$和$W^{T} W=I$，第（4）步用到了$z^{(i)}=W^{T} x^{(i)}$，第（5）步合并同类项，第（6）步用到了$z^{(i)}=W^{T} x^{(i)}$和矩阵的迹,第7步将代数和表达为矩阵形式。</p>
<p>注意到$\sum_{i=1}^{m} x^{(i)} x^{(i) T}$是数据集的协方差矩阵，W的每一个向量$w_j$是标准正交基。而$\sum_{i=1}^{m} x^{(i) T} x^{(i)}$是一个常量。最小化上式等价于：$\underbrace{\arg \min }_{W}-\operatorname{tr}\left(W^{T} X X^{T} W\right) \text { s.t. } W^{T} W=I$。</p>
<p>这个最小化不难，直接观察也可以发现最小值对应的W由协方差矩阵$X X^{T}$最大的n’个特征值对应的特征向量组成。当然用数学推导也很容易。利用拉格朗日函数可以得到$J(W)=-\operatorname{tr}\left(W^{T} X X^{T} W+\lambda\left(W^{T} W-I\right)\right)$。对W求导有$-X X^{T} W+\lambda W=0$，整理下即为：$X X^{T} W=\lambda W$。</p>
<p>这样可以更清楚的看出，W为$X X^{T}$的n’个特征向量组成的矩阵，而$\lambda$为$X X^{T}$的若干特征值组成的矩阵，特征值在主对角线上，其余位置为0。当我们将数据集从n维降到n’维时，需要找到最大的n’个特征值对应的特征向量。这n’个特征向量组成的矩阵W即为我们需要的矩阵。对于原始数据集，我们只需要用$z^{(i)}=W^{T} x^{(i)}$，就可以把原始数据集降维到最小投影距离的n’维数据集。</p>
<h4 id="PCA推导——基于最大投影方差"><a href="#PCA推导——基于最大投影方差" class="headerlink" title="PCA推导——基于最大投影方差"></a>PCA推导——基于最大投影方差</h4><p>对于任意一个样本$x^{(i)}$，在新的坐标系中的投影为$W^{T} x^{(i)}$，在新坐标系中的投影方差为$x^{(i) T} W W^{T} x^{(i)}$，要使所有的样本的投影方差和最大，也就是最大化$\sum_{i=1}^{m} W^{T} x^{(i)} x^{(i) T} W$的迹,即：$\underbrace{\arg \max }_{W} \operatorname{tr}\left(W^{T} X X^{T} W\right) \text { s.t. } W^{T} W=I$。利用拉格朗日函数可以得到$J(W)=\operatorname{tr}\left(W^{T} X X^{T} W+\lambda\left(W^{T} W-I\right)\right)$。对W求导有$X X^{T} W+\lambda W=0$，整理下即为：$X X^{T} W=(-\lambda) W$</p>
<h4 id="PCA算法流程"><a href="#PCA算法流程" class="headerlink" title="PCA算法流程"></a>PCA算法流程</h4><ul>
<li>输入：n维样本集$D=\left(x^{(1)}, x^{(2)}, \ldots, x^{(m)}\right)$，要降维到的维数n’</li>
<li>输出：降维后的样本集D’</li>
<li>对所有的样本进行中心化：$x^{(i)}=x^{(i)}-\frac{1}{m} \sum_{j=1}^{m} x^{(j)}$</li>
<li>计算样本的协方差矩阵$X X^{T}$</li>
<li>取出最大的n’个特征值对应的特征向量$\left(w_{1}, w_{2}, \ldots, w_{n^{\prime}}\right)$，将所有的特征向量标准化后，组成特征向量矩阵W</li>
<li>对样本集中的每一个样本$x^{(i)}$，转化为新的样本$z^{(i)}=W^{T} x^{(i)}$</li>
<li>得到输出样本集$D^{\prime}=\left(z^{(1)}, z^{(2)}, \ldots, z^{(m)}\right)$</li>
</ul>
<p>有时候，我们不指定降维后的n’的值，而是换种方式，指定一个降维到的主成分比重阈值t。这个阈值t在（0,1]之间。假如我们的n个特征值为$\lambda_{1} \geq \lambda_{2} \geq \ldots \geq \lambda_{n}$，则n’可以通过下式得到：$\frac{\sum_{i=1}^{n^{\prime}} \lambda_{i}}{\sum_{i=1}^{n} \lambda_{i}} \geq t$</p>
<h4 id="PCA实例"><a href="#PCA实例" class="headerlink" title="PCA实例"></a>PCA实例</h4><p>假设我们的数据集有10个二维数据(2.5,2.4), (0.5,0.7), (2.2,2.9), (1.9,2.2), (3.1,3.0), (2.3, 2.7), (2, 1.6), (1, 1.1), (1.5, 1.6), (1.1, 0.9)，需要用PCA降到1维特征。</p>
<p>首先我们对样本中心化，这里样本的均值为(1.81, 1.91),所有的样本减去这个均值向量后，即中心化后的数据集为(0.69, 0.49), (-1.31, -1.21), (0.39, 0.99), (0.09, 0.29), (1.29, 1.09), (0.49, 0.79), (0.19, -0.31), (-0.81, -0.81), (-0.31, -0.31), (-0.71, -1.01)。</p>
<p>现在我们开始求样本的协方差矩阵，由于我们是二维的，则协方差矩阵为：</p>
<p>$\mathbf{X X}^{\mathbf{T}}=\left(\begin{array}{ll}\operatorname{cov}\left(x_{1},x_{1}\right)&amp; \operatorname{cov}\left(x_{1}, x_{2}\right)\\\operatorname{cov}\left(x_{2}, x_{1}\right) &amp; \operatorname{cov}\left(x_{2},x_{2}\right)\end{array}\right)$</p>
<p>对于我们的数据，求出协方差矩阵为：</p>
<p>$\mathbf{X X}^{\mathbf{T}}=\left(\begin{array}{ll}0.616555556 &amp; 0.615444444 \\0.615444444 &amp; 0.716555556\end{array}\right)$</p>
<p>求出特征值为（0.0490833989， 1.28402771），对应的特征向量分别为：$(0.735178656,0.677873399)^{T}(-0.677873399,-0.735178656)^{T}$，由于最大的k=1个特征值为1.28402771，对于的k=1个特征向量为$(-0.677873399,-0.735178656)^{T}$，则我们的$\mathrm{W}=(-0.677873399,-0.735178656)^{T}$。</p>
<p>我们对所有的数据集进行投影$z^{(i)}=W^{T} x^{(i)}$，得到PCA降维后的10个一维数据集为：(-0.827970186， 1.77758033， -0.992197494， -0.274210416， -1.67580142， -0.912949103， 0.0991094375， 1.14457216, 0.438046137， 1.22382056)。</p>
<h4 id="核PCA"><a href="#核PCA" class="headerlink" title="核PCA"></a>核PCA</h4><p>在上面的PCA算法中，我们假设存在一个线性的超平面，可以让我们对数据进行投影。但是有些时候，数据不是线性的，不能直接进行PCA降维。这里就需要用到和支持向量机一样的核函数的思想，先把数据集从n维映射到线性可分的高维N&gt;n,然后再从N维降维到一个低维度n’, 这里的维度之间满足n’&lt;n&lt;N。</p>
<p>使用了核函数的主成分分析一般称之为核主成分分析(Kernelized PCA, 以下简称KPCA。假设高维空间的数据是由n维空间的数据通过映射$\phi$产生。则对于n维空间的特征分解：$\sum_{i=1}^{m} x^{(i)} x^{(i) T} W=\lambda W$，映射为：$\sum_{i=1}^{m} \phi\left(x^{(i)}\right) \phi\left(x^{(i)}\right)^{T} W=\lambda W$，通过在高维空间进行协方差矩阵的特征值分解，然后用和PCA一样的方法进行降维。一般来说，映射$\phi$不用显式的计算，而是在需要计算的时候通过核函数完成。由于KPCA需要核函数的运算，因此它的计算量要比PCA大很多。</p>
<h3 id="对角化分解"><a href="#对角化分解" class="headerlink" title="对角化分解"></a>对角化分解</h3><p>给定一个大小为$m\times m$的矩阵A，其对角化分解可以写成$A=U \Lambda U^{-1}$。其中，U的每一列都是特征向量，$\Lambda$对角线上的元素是从大到小排列的特征值，若将U记作$U=\left(\vec{u}_{1}, \vec{u}_{2}, \ldots, \vec{u}_{m}\right)$，则</p>
<p>$\begin{array}{l}A U=A\left(\vec{u}_{1}, \vec{u}_{2}, \ldots,\vec{u}_{m}\right)=\left(\lambda_{1} \vec{u}_{1}, \lambda_{2} \vec{u}_{2}, \ldots,\lambda_{m} \vec{u}_{m}\right) \\=\left(\vec{u}_{1}, \vec{u}_{2}, \ldots,\vec{u}_{m}\right)\left[\begin{array}{ccc}\lambda_{1} &amp; \cdots &amp; 0 \\\vdots &amp;\ddots &amp; \vdots \\0 &amp; \cdots &amp; \lambda_{m}\end{array}\right] \\\Rightarrow A U=U\Lambda \Rightarrow A=U \Lambda U^{-1}\end{array}$</p>
<p>更为特殊的是，当矩阵A是一个对称矩阵时，则存在一个对称对角化分解，即$A=Q \Lambda Q^{T}$，其中Q的每一列都是相互正交的特征向量，且是单位向量，$\Lambda$对角线上的元素是从大到小排列的特征值。</p>
<p>当然，将矩阵Q记作$Q=\left(\vec{q}_{1}, \vec{q}_{2}, \ldots, \vec{q}_{m}\right)$，则矩阵A也可以写成如下形式：$A=\lambda_{1} \vec{q}_{1} \vec{q}_{1}^{T}+\lambda_{2} \vec{q}_{2} \vec{q}_{2}^{T}+\ldots+\lambda_{m} \vec{q}_{m} \vec{q}_{m}^{T}$。那么如果A不是方阵，即行和列不相同时，可以使用SVD进行分解。</p>
<h4 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h4><p>矩阵$A=\left[\begin{array}{ll}2 &amp; 1 \\1 &amp; 2\end{array}\right]$，根据$|\lambda I-A|=\left|\begin{array}{cc}\lambda-2 &amp; -1 \-1 &amp; \lambda-2\end{array}\right|=0$求得特征值为$\lambda_{1}=3, \quad \lambda_{2}=1$，相应地，$\vec{q}_{1}=\left(\frac{\sqrt{2}}{2},\frac{\sqrt{2}}{2}\right)^{T}, \quad \vec{q}_{2}=\left(-\frac{\sqrt{2}}{2}, \frac{\sqrt{2}}{2}\right)^{T}$，则$A=\lambda_{1} \vec{q}_{1} \vec{q}_{1}^{T}+\lambda_{2} \vec{q}_{2}\vec{q}_{2}^{T}=\left[\begin{array}{ll}2 &amp; 1 \\1 &amp; 2\end{array}\right]$，我们就很容易地得到了矩阵A的对称对角化分解。</p>
<h3 id="奇异值分解"><a href="#奇异值分解" class="headerlink" title="奇异值分解"></a>奇异值分解</h3><p>将一个普通矩阵分解为奇异向量和奇异值，比如将矩阵A分解称三个矩阵的乘积$A=UDV^T$。</p>
<p>假设A是一个m<em>n矩阵，那么U是一个m</em>m矩阵，D是一个m<em>n矩阵，V是一个n</em>n矩阵。这些矩阵每一个都拥有特殊的结构，其中U和V都是正交矩阵，D是对角矩阵(D不一定是方阵)。对角矩阵D对角线上的元素被成为矩阵A的奇异值，矩阵U的列向量被成为左奇异向量，矩阵V的列向量被称为右奇异向量。$A^TA$的特征值为$\lambda_i$，则称$\sigma_i=\sqrt{\lambda_i}(i=1,2,\cdots,n)$为A的奇异值。</p>
<h4 id="几何意义-1"><a href="#几何意义-1" class="headerlink" title="几何意义"></a>几何意义</h4><p>如果矩阵<em>A</em>是正定矩阵，它的奇异值分解就是$\boldsymbol{A}=\boldsymbol{Q} \boldsymbol{\Lambda} \boldsymbol{Q}^{T}$。可以将矩阵<em>A</em>视为一种线性变换操作，将其行空间中的一个向量v1,变为其列空间中的向量 $\mathbf{u}_{1}=\boldsymbol{A} \mathbf{v}_{1}$ 。奇异值分解就是要在行空间中寻找一组正交基，将其通过矩阵<em>A</em>线性变换生成列空间中的一组正交基$\boldsymbol{A} \mathbf{v}_{i}=\sigma_{i} \mathbf{u}_{i}$，如下图所示：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mathbasic/18.png" alt="图片"></p>
<p>G.Strang给出了二阶方阵SVD的集合意义：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mathbasic/19.png" alt="图片"></p>
<h4 id="物理意义"><a href="#物理意义" class="headerlink" title="物理意义"></a>物理意义</h4><p>苏神在博客中阐述了SVD与自编码器、聚类的关系，可以参考：<a href="https://kexue.fm/archives/4208" target="_blank" rel="noopener">https://kexue.fm/archives/4208</a>、<a href="https://kexue.fm/archives/4216" target="_blank" rel="noopener">https://kexue.fm/archives/4216</a></p>
<h4 id="求解方法"><a href="#求解方法" class="headerlink" title="求解方法"></a>求解方法</h4><p>我们首先回顾下特征值和特征向量的定义如下：$A x=\lambda x$，其中A是一个$n \times n$的实对称矩阵，x是一个n维向量，则我们说$\lambda$是矩阵A的一个特征值，而x是矩阵A的特征值$lambda$所对应的特征向量。</p>
<p>求出特征值和特征向量有什么好处呢？ 就是我们可以将矩阵A特征分解。如果我们求出了矩阵A的n个特征值$\lambda_{1} \leq \lambda_{2} \leq \ldots \leq \lambda_{n}$，以及这n个特征值所对应的特征向量$\left\{w_{1}, w_{2}, \ldots w_{n}\right\}$，如果这n个特征向量线性无关，那么矩阵A就可以用下式的特征分解表示：$A=W \Sigma W^{-1}$。其中W是这n个特征向量所张成的$n \times n$维矩阵，而$\Sigma$为这n个特征值为主对角线的$n \times n$维矩阵。</p>
<p>一般我们会把W的这n个特征向量标准化，即满足$\left|w_{i}\right|_{2}=1$，或者说$w_{i}^{T} w_{i}=1$，此时W的n个特征向量为标准正交基，满足$W^{T} W=I$，即$W^{T}=W^{-1}$，也就是说W为酉矩阵。这样我们的特征分解表达式可以写成：$A=W \Sigma W^{T}$。注意到要进行特征分解，矩阵A必须为方阵。那么如果A不是方阵，即行和列不相同时，我们还可以对矩阵进行分解吗？答案是可以，此时我们的SVD登场了。</p>
<p>SVD也是对矩阵进行分解，但是和特征分解不同，SVD并不要求要分解的矩阵为方阵。假设我们的矩阵A是一个𝑚×𝑛的矩阵，那么我们定义矩阵A的SVD为：$A=U \Sigma V^{T}$。其中U是一个$m \times m$的矩阵，$\Sigma$是一个$m \times n$的矩阵，除了主对角线上的元素以外全为0，主对角线上的每个元素都称为奇异值，V是一个$n \times n$的矩阵。U和V都是酉矩阵，即满足$U^{T} U=I, V^{T} V=I$UTU=I,VTV=I。下图可以很形象的看出上面SVD的定义：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mathbasic/20.png" alt="图片"></p>
<p>那么我们如何求出SVD分解后的$U, \Sigma, V$这三个矩阵呢？</p>
<p>如果我们将A的转置和A做矩阵乘法，那么会得到$n \times n$的一个方阵$A^{T} A$，既然$A^{T} A$是方阵，那么我们就可以进行特征分解，得到的特征值和特征向量满足下式：$\left(A^{T} A\right) v_{i}=\lambda_{i} v_{i}$。这样我们就可以得到矩阵$A^{T} A$的n个特征值和对应的n个特征向量v了。将$A^{T} A$的所有特征向量张成一个$n \times n$的矩阵V，就是我们SVD公式里面的V矩阵了。一般我们将V中的每个特征向量叫做A的右奇异向量。如果我们将A和A的转置做矩阵乘法，那么会得到$m \times m$的一个方阵$AA^{T}$是方阵，那么我们就可以进行特征分解，得到的特征值和特征向量满足下式：$\left(A A^{T}\right) u_{i}=\lambda_{i} u_{i}$。这样我们就可以得到矩阵$AA^{T}$的m个特征值和对应的m个特征向量u了。将$AA^{T}$的所有特征向量张成一个$m \times m$的矩阵U，就是我们SVD公式里面的U矩阵了。一般我们将U中的每个特征向量叫做A的左奇异向量。</p>
<p>U和V我们都求出来了，现在就剩下奇异值矩阵$\Sigma$没有求出了。由于$\Sigma$除了对角线上是奇异值其他位置都是0，那我们只需要求出每个奇异值$\sigma$就可以了。我们注意到：$A=U \Sigma V^{T} \Rightarrow A V=U \Sigma V^{T} V \Rightarrow A V=U \Sigma \Rightarrow A v_{i}=\sigma_{i} u_{i} \Rightarrow \sigma_{i}=A v_{i} / u_{i}$。</p>
<p>这样我们可以求出我们的每个奇异值，进而求出奇异值矩阵$\Sigma$。</p>
<p>【注】：我们说$A^{T} A$的特征向量组成的就是我们SVD中的V矩阵，而$AA^{T}$的特征向量组成的就是我们SVD中的U矩阵，这有什么根据吗？这个其实很容易证明，我们以V矩阵的证明为例：$A=U \Sigma V^{T} \Rightarrow A^{T}=V \Sigma^{T} U^{T} \Rightarrow A^{T} A=V \Sigma^{T} U^{T} U \Sigma V^{T}=V \Sigma^{2} V^{T}$。上式证明使用了$U^{T} U=I, \Sigma^{T} \Sigma=\Sigma^{2}$，可以看出$A^{T} A$的特征向量组成的的确就是我们SVD中的V矩阵。类似的方法可以得到$AA^{T}$的特征向量组成的就是我们SVD中的U矩阵。</p>
<p>进一步我们还可以看出我们的特征值矩阵等于奇异值矩阵的平方，也就是说特征值和奇异值满足如下关系：$\sigma_{i}=\sqrt{\lambda_{i}}$。这样也就是说，我们可以不用$\sigma_{i}=A v_{i} / u_{i}$来计算奇异值，也可以通过求出$A^{T} A$的特征值取平方根来求奇异值。</p>
<h4 id="SVD实例"><a href="#SVD实例" class="headerlink" title="SVD实例"></a>SVD实例</h4><p>我们的矩阵A定义为：</p>
<p>$\mathbf{A}=\left(\begin{array}{ll}0 &amp; 1 \\1 &amp; 1 \\1 &amp; 0\end{array}\right)$</p>
<p>我们首先求出$A^{T} A$和$A A^{T}$：</p>
<p>$\mathbf{A}^{\mathbf{T}} \mathbf{A}=\left(\begin{array}{lll}0 &amp; 1 &amp; 1 \\1 &amp; 1 &amp; 0\end{array}\right)\left(\begin{array}{ll}0 &amp; 1 \\1 &amp; 1 \\1 &amp; 0\end{array}\right)=\left(\begin{array}{ll}2 &amp; 1 \\1 &amp; 2\end{array}\right)$</p>
<p>$\mathbf{A} \mathbf{A}^{\mathbf{T}}=\left(\begin{array}{ll}0 &amp; 1 \\1 &amp; 1 \\1 &amp; 0\end{array}\right)\left(\begin{array}{lll}0 &amp; 1 &amp; 1 \\1 &amp; 1 &amp; 0\end{array}\right)=\left(\begin{array}{lll}1 &amp; 1 &amp; 0 \\1 &amp; 2 &amp; 1 \\0 &amp; 1 &amp; 1\end{array}\right)$</p>
<p>进而求出$A^TA$的特征值和特征向量：$\lambda_{1}=3 ; v_{1}=\left(\begin{array}{c}1 / \sqrt{2} \\1/ \sqrt{2}\end{array}\right) ; \lambda_{2}=1 ; v_{2}=\left(\begin{array}{c}-1 / \sqrt{2}\\1 / \sqrt{2}\end{array}\right)$</p>
<p>接着求$AA^T$的特征值和特征向量：$\lambda_{1}=3 ; u_{1}=\left(\begin{array}{c}1 / \sqrt{6} \\2 /\sqrt{6} \\1 / \sqrt{6}\end{array}\right) ; \lambda_{2}=1 ; u_{2}=\left(\begin{array}{c}1 /\sqrt{2} \\0 \-1 / \sqrt{2}\end{array}\right) ; \lambda_{3}=0 ; u_{3}=\left(\begin{array}{c}1 / \sqrt{3} \-1 / \sqrt{3} \\1 / \sqrt{3}\end{array}\right)$</p>
<p>利用$A v_{i}=\sigma_{i} u_{i}, i=1,2$求奇异值：</p>
<p>$\left(\begin{array}{ll}0 &amp; 1 \\1 &amp; 1 \\1 &amp; 0\end{array}\right)\left(\begin{array}{l}1 /\sqrt{2}\\1 / \sqrt{2}\end{array}\right)=\sigma_{1}\left(\begin{array}{l}1 / \sqrt{6} \\2 /\sqrt{6} \\1 / \sqrt{6}\end{array}\right) \Rightarrow \sigma_{1}=\sqrt{3}$</p>
<p>$\left(\begin{array}{ll}0 &amp; 1 \\1 &amp; 1 \\1 &amp; 0\end{array}\right)\left(\begin{array}{c}-1 /\sqrt{2} \\1 / \sqrt{2}\end{array}\right)=\sigma_{2}\left(\begin{array}{c}1 / \sqrt{2} \\0\-1 / \sqrt{2}\end{array}\right) \Rightarrow \sigma_{2}=1$</p>
<p>当然，我们也可以用$\sigma_{i}=\sqrt{\lambda_{i}}$直接求出奇异值为$\sqrt{3}$和1。最终得到A的奇异值分解为：</p>
<p>$A=U \Sigma V^{T}=\left(\begin{array}{ccc}1 / \sqrt{6} &amp; 1 / \sqrt{2} &amp; 1 / \sqrt{3} \\2 /\sqrt{6} &amp; 0 &amp; -1 / \sqrt{3} \\1 / \sqrt{6} &amp; -1 / \sqrt{2} &amp; 1 /\sqrt{3}\end{array}\right)\left(\begin{array}{cc}\sqrt{3} &amp; 0 \\0 &amp; 1 \\0 &amp; 0\end{array}\right)\left(\begin{array}{cc}1 / \sqrt{2} &amp; 1 / \sqrt{2} \-1 / \sqrt{2} &amp; 1 /\sqrt{2}\end{array}\right)$</p>
<h4 id="SVD性质"><a href="#SVD性质" class="headerlink" title="SVD性质"></a>SVD性质</h4><p>对于奇异值,它跟我们特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。也就是说，我们也可以用最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵。也就是说：$A_{m \times n}=U_{m \times m} \Sigma_{m \times n} V_{n \times n}^{T} \approx U_{m \times k} \Sigma_{k \times k} V_{k \times n}^{T}$</p>
<p>其中k要比n小很多，也就是一个大的矩阵A可以用三个小的矩阵$U_{m \times k}, \Sigma_{k \times k}, V_{k \times n}^{T}$来表示。如下图所示，现在我们的矩阵A只需要灰色的部分的三个小矩阵就可以近似描述了：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mathbasic/21.png" alt="图片"></p>
<p>由于这个重要的性质，SVD可以用于PCA降维，来做数据压缩和去噪。也可以用于推荐算法，将用户和喜好对应的矩阵做特征分解，进而得到隐含的用户需求来做推荐。同时也可以用于NLP中的算法，比如潜在语义索引（LSI）。</p>
<h4 id="SVD应用"><a href="#SVD应用" class="headerlink" title="SVD应用"></a>SVD应用</h4><p><strong>SVD用于PCA</strong></p>
<p>用PCA降维，需要找到样本协方差矩阵$X^TX$的最大的d个特征向量，然后用这最大的d个特征向量张成的矩阵来做低维投影降维。可以看出，在这个过程中需要先求出协方差矩阵$X^TX$，当样本数多样本特征数也多的时候，这个计算量是很大的。</p>
<p>注意到我们的SVD也可以得到协方差矩阵$X^TX$最大的d个特征向量张成的矩阵，但是SVD有个好处，有一些SVD的实现算法可以不求先求出协方差矩阵$X^TX$，也能求出我们的右奇异矩阵V。也就是说，我们的PCA算法可以不用做特征分解，而是做SVD来完成。这个方法在样本量很大的时候很有效。实际上，scikit-learn的PCA算法的背后真正的实现就是用的SVD，而不是我们我们认为的暴力特征分解。</p>
<p>另一方面，注意到PCA仅仅使用了我们SVD的右奇异矩阵，没有使用左奇异矩阵，那么左奇异矩阵有什么用呢？假设我们的样本是$m \times n$的矩阵X，如果我们通过SVD找到了矩阵$X X^{T}$最大的d个特征向量张成的$m \times d$维矩阵U，则我们如果进行如下处理：$X_{d \times n}^{\prime}=U_{d \times m}^{T} X_{m \times n}$。</p>
<p>可以得到一个$d \times n$的矩阵X‘,这个矩阵和我们原来的𝑚×𝑛维样本矩阵X相比，行数从m减到了d，可见对行数进行了压缩。也就是说，左奇异矩阵可以用于行数的压缩。相对的，右奇异矩阵可以用于列数即特征维度的压缩，也就是我们的PCA降维。</p>
<p>sklearn的PCA使用：<a href="https://www.cnblogs.com/pinard/p/6243025.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6243025.html</a></p>
<p><strong>最小二乘法</strong></p>
<p>参考：<a href="https://zhuanlan.zhihu.com/p/38128785" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/38128785</a></p>
<p><strong>推荐系统</strong></p>
<p>参考：<a href="https://blog.csdn.net/y990041769/article/details/77833622" target="_blank" rel="noopener">https://blog.csdn.net/y990041769/article/details/77833622</a>、<a href="https://www.cnblogs.com/bjwu/p/9358777.html" target="_blank" rel="noopener">https://www.cnblogs.com/bjwu/p/9358777.html</a></p>
<p><strong>图像压缩</strong></p>
<p>参考：<a href="http://blog.chinaunix.net/uid-20761674-id-4040274.html" target="_blank" rel="noopener">http://blog.chinaunix.net/uid-20761674-id-4040274.html</a></p>
<p><strong>减噪</strong></p>
<p>参考：<a href="http://blog.chinaunix.net/uid-20761674-id-4040274.html" target="_blank" rel="noopener">http://blog.chinaunix.net/uid-20761674-id-4040274.html</a></p>
<p><strong>数据分析</strong></p>
<p>参考：<a href="http://blog.chinaunix.net/uid-20761674-id-4040274.html" target="_blank" rel="noopener">http://blog.chinaunix.net/uid-20761674-id-4040274.html</a></p>
<h3 id="白化"><a href="#白化" class="headerlink" title="白化"></a>白化</h3><p>计算观测数据x的$n\times n$的对称阵$x\cdot x^T$的特征值和特征向量，用特征值形成对角阵D，特征向量形成正交阵，则$x\cdot x^T=U^TDU$，令$\tilde{x}=U^{T} D^{-0.5} U \cdot x$，这个过程就叫做白化，也就是去掉了信号的相关性，便于后续模型更好得学习。</p>
<p>白化具有一个性质：</p>
<p>$\begin{array}{l}\tilde{x} \cdot \tilde{x}^{T}=\left(U^{T} D^{-0.5} U \cdot x\right)\left(U^{T} D^{-0.5} U \cdot x\right)^{T} \\=\left(U^{T} D^{-0.5} U \cdot x\right)\left(x^{T} U^{T} D^{-0.5} U\right) \\=U^{T} D^{-0.5} U \cdot\left(x x^{T}\right) \cdot U^{T} D^{-0.5} U \\=U^{T} D^{-0.5} U \cdot U^{T} D U \cdot U^{T} D^{-0.5} U=I\end{array}$</p>
<p>相当于白化后的矩阵的行向量和列向量是正交的。</p>
<h1 id="微积分"><a href="#微积分" class="headerlink" title="微积分"></a>微积分</h1><h2 id="导数"><a href="#导数" class="headerlink" title="导数"></a>导数</h2><p>导数的两种定义：</p>
<p>$f^{\prime}\left(x_{0}\right)=\lim _{\Delta x \rightarrow 0} \frac{f\left(x_{0}+\Delta x\right)-f\left(x_{0}\right)}{\Delta x}$</p>
<p>$f^{\prime}\left(x_{0}\right)=\lim _{x \rightarrow x_{0}} \frac{f(x)-f\left(x_{0}\right)}{x-x_{0}}$</p>
<p>其物理意义是函数在点上切线的斜率</p>
<h3 id="利用极限求导"><a href="#利用极限求导" class="headerlink" title="利用极限求导"></a>利用极限求导</h3><ul>
<li>$y=x^2$</li>
</ul>
<p>$\begin{aligned}f^{\prime}(x) &amp;=\lim _{\Delta x \rightarrow 0} \frac{f(x+\Delta x)-f(x)}{\Delta x}=\lim _{\Delta x \rightarrow 0} \frac{\left(x_{\star}+\Delta x\right)^{2}-x^{2}}{\Delta x} \\&amp;=\lim _{\Delta x \rightarrow 0} \frac{x^{2}+2 x \cdot \Delta x+(\Delta x)^{2}-x^{2}}{\Delta x}=\lim _{\Delta x \rightarrow 0}(2 x+\Delta x)=2 x\end{aligned}$</p>
<ul>
<li>$\lim _{x \rightarrow 0} \frac{\sin (x)}{x}=1$</li>
</ul>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mathbasic/22.png" alt="图片"></p>
<p>假设圆形为单位圆，半径为1，三角形ACD的面积=0.5<em>AC</em>BD=0.5<em>1</em>sin(x)=0.5sin(x)</p>
<p>圆弧CD与AC,AD围成的扇面ACD的面积=pi*(x/2pi)=0.5</p>
<p>三角形ACE的面积=0.5<em>AC</em>CE=0.5<em>1</em>tanh(x)=0.5tanh(x)</p>
<p>$\begin{array}{l}-p i / 2<x<p i / 2 \\0.5 \sin (x)<=0.5 x<=0.5 \tan (x) \\\sin (x)<x<=\tan (x) \\\sin (x)<x<=\sin (x) / \cos (x) \\1 / \sin (x)>=1 / x&gt;=\cos (x) / \sin (x) \\\sin (x) /\sin (x)&gt;=\sin (x) / x&gt;=\cos (x) \\1&gt;=\sin (x) / x&gt;=\cos (x)\end{array}$</p>
<p>当x-&gt;0，cos(x)-&gt;1</p>
<p>当x-&gt;0, 1 &gt;= sin(x)/x &gt;= 1</p>
<p>根据夹逼定理，当x-&gt;0，sin(x)/x=1</p>
<ul>
<li>$\lim _{x \rightarrow 0} \frac{1-\cos (x)}{x}=0$</li>
</ul>
<p>$\frac{1-\cos (x)}{x}=\frac{1-\cos (x)}{x} \frac{1+\cos (x)}{1+\cos (x)}=\frac{1-\cos (x)^{2}}{x(1+\cos (x)}$</p>
<p>$\lim _{x \rightarrow 0} \frac{1-\cos (x)^{2}}{x(1+\cos (x))}=\lim _{x \rightarrow 0} \frac{\sin (x)^{2}}{x(1+\cos (x))}=\lim _{x \rightarrow 0} \frac{\sin (x) \sin (x)}{x(1+\cos (x))}$</p>
<p>$=\lim _{x \rightarrow 0} \frac{\sin (x)}{x} \frac{\sin (x)}{1+\cos (x)}=\lim _{x \rightarrow 0} 1 \frac{\sin (x)}{1+\cos (x)}=1 * 0=0$</p>
<ul>
<li>y=sinx</li>
</ul>
<p>已知$\lim _{x \rightarrow 0} \frac{\sin (x)}{x}=1$，$\lim _{x \rightarrow 0} \frac{1-\cos (x)}{x}=0$</p>
<p>$f^{\prime}(x)=\lim _{\Delta x \rightarrow 0} \frac{f(x+\Delta x)-f(x)}{\Delta x}=\lim _{\Delta x \rightarrow 0} \frac{\sin (x+\Delta x)-\sin (x)}{\Delta x}=\lim _{\Delta x \rightarrow 0} \frac{\sin x \cos \Delta x+\cos x \sin \Delta x-\sin x}{\Delta x}_{k}$</p>
<p>$=\sin x \cdot \lim _{\Delta x \rightarrow 0} \frac{\cos \Delta x-1}{\Delta x}+\lim _{\Delta x \rightarrow 0} \frac{\cos x \sin \Delta x}{\Delta x}=\lim _{\Delta x \rightarrow 0} \frac{\cos x \sin \Delta x}{\Delta x}=\cos x$</p>
<h3 id="左右导数"><a href="#左右导数" class="headerlink" title="左右导数"></a>左右导数</h3><p>左导数：$f_{-}^{\prime}\left(x_{0}\right)=\lim _{\Delta x \rightarrow 0^{-}} \frac{f\left(x_{0}+\Delta x\right)-f\left(x_{0}\right)}{\Delta x}=\lim _{x \rightarrow x_{0}^{-}} \frac{f(x)-f\left(x_{0}\right)}{x-x_{0}},\left(x=x_{0}+\Delta x\right)$</p>
<p>右导数：$f^{\prime}{ }_{+}\left(x_{0}\right)=\lim _{\Delta x \rightarrow 0^{+}} \frac{f\left(x_{0}+\Delta x\right)-f\left(x_{0}\right)}{\Delta x}=\lim _{x \rightarrow x_{0}^{+}} \frac{f(x)-f\left(x_{0}\right)}{x-x_{0}}$</p>
<p>右导数存在，左导数不存在的函数例子：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mathbasic/23.png" alt="图片"></p>
<h3 id="函数的可导与连续之间的关系"><a href="#函数的可导与连续之间的关系" class="headerlink" title="函数的可导与连续之间的关系"></a>函数的可导与连续之间的关系</h3><p>可导必连续，连续未必可导，可导时左右导数相等。</p>
<h3 id="四则运算法则"><a href="#四则运算法则" class="headerlink" title="四则运算法则"></a>四则运算法则</h3><ul>
<li>$y^{\prime}=(c \cdot x)^{\prime}=c \cdot x^{\prime}$，c为任意常数</li>
<li>$y^{\prime}=[a u(x)+b v(x)]^{\prime}=a u^{\prime}(x)+b v^{\prime}(x)$，a和b是任意常数</li>
<li>$y^{\prime}=[u(x) v(x)]^{\prime}=u^{\prime}(x) v(x)+u(x) v^{\prime}(x)$</li>
<li>$y^{\prime}=\left[\frac{u(x)}{v(x)}\right]^{\prime}=\frac{u^{\prime}(x) v(x)-u(x) v^{\prime}(x)}{v^{2}(x)}$，$v(x)\neq 0$</li>
</ul>
<h3 id="基本导数与微分表"><a href="#基本导数与微分表" class="headerlink" title="基本导数与微分表"></a>基本导数与微分表</h3><ul>
<li>若$f(x)=c$，则$f’(x)=0$</li>
<li>若$f(x)=x^a$，则$f’(x)=ax^{a-1}$</li>
<li>若$f(x)=sin(x)$，则$f’(x)=cos(x)$</li>
<li>若$f(x)=cos(x)$，则$f’(x)=-sin(x)$</li>
<li>若$f(x)=a^x$，则$f’(x)=a^xln(a)$</li>
<li>若$f(x)=e^x$，则$f’(x)=e^x$</li>
<li>若$f(x)=log_a(x)$，则$f’(x)=\frac{1}{xln(a)}$</li>
<li>若$f(x)=ln(x)$，则$f’(x)=\frac{1}{x}$</li>
<li>若$f(x)=\sqrt{x}$，则$f’(x)=\frac{1}{2\sqrt{x}}$</li>
<li>若$f(x)=\frac{1}{x}$，则$f’(x)=-\frac{1}{x^2}$</li>
</ul>
<h3 id="反函数运算法则"><a href="#反函数运算法则" class="headerlink" title="反函数运算法则"></a>反函数运算法则</h3><p>设$y=f(x)$在点x的某临域内单调连续，在点x处可导且$f’(x)\neq 0$，则其反函数在点x所对应的y处可导，且有$\frac{dy}{dx}=\frac{1}{\frac{dx}{dy}}$</p>
<h3 id="复合函数运算法则"><a href="#复合函数运算法则" class="headerlink" title="复合函数运算法则"></a>复合函数运算法则</h3><p>若$\mu=m(x)$在点x可导，而$y=f(\mu)$对应点$\mu$可导，则$y=f(m(x))$在点x可导且$y’=f’(\mu)\cdot m’(x)$</p>
<h3 id="常用高阶导数公式"><a href="#常用高阶导数公式" class="headerlink" title="常用高阶导数公式"></a>常用高阶导数公式</h3><ul>
<li>$\left(a^{x}\right)^{n}=a^{x} \ln ^{n} a \quad(a&gt;0)$</li>
<li>$\left(e^{x}\right)^{(n)}=e^{x}$</li>
<li>$(\sin k x)^{(n)}=k^{n} \sin \left(k x+n \cdot \frac{\pi}{2}\right)$</li>
<li>$(\cos k x)^{(n)}=k^{n} \cos \left(k x+n \cdot \frac{\pi}{2}\right)$</li>
<li>$\left(x^{m}\right)^{(n)}=m(m-1) \cdots(m-n+1) x^{m-n}$</li>
<li>$(\ln x)^{(n)}=(-1)^{(n-1)} \frac{(n-1) !}{x^{n}}$</li>
<li>莱布尼兹公式：若$u(x)$， $v(x)$均n阶可导，则$(u v)^{(n)}=\sum_{i=0}^{n} c_{n}^{i} u^{(i)} v^{(n-i)}$，其中$u^{(0)}=u, v^{(0)}=v$</li>
</ul>
<h2 id="平面曲线的切线和法线"><a href="#平面曲线的切线和法线" class="headerlink" title="平面曲线的切线和法线"></a>平面曲线的切线和法线</h2><p>切线方程：$y-y_{0}=f^{\prime}\left(x_{0}\right)\left(x-x_{0}\right)$</p>
<p>法线方程：$y-y_{0}=-\frac{1}{f^{\prime}\left(x_{0}\right)}\left(x-x_{0}\right), f^{\prime}\left(x_{0}\right) \neq 0$</p>
<h2 id="微分中值定理"><a href="#微分中值定理" class="headerlink" title="微分中值定理"></a>微分中值定理</h2><h3 id="费马定理"><a href="#费马定理" class="headerlink" title="费马定理"></a>费马定理</h3><p>若函数$f(x)$满足条件：</p>
<ul>
<li>函数$f(x)$在$x_0$的某邻域内有定义，并且在次邻域内恒有$f(x)\le f(x_0)$或$f(x).ge f(x_0)$</li>
<li>$f(x)$在$x_0$处可导</li>
</ul>
<p>则有$f’(x_0)=0$</p>
<h3 id="罗尔定理"><a href="#罗尔定理" class="headerlink" title="罗尔定理"></a>罗尔定理</h3><p>设函数$f(x)$满足条件：</p>
<ul>
<li>在闭区间$[a,b]$上连续</li>
<li>在$(a,b)$内可导</li>
<li>$f(a)=f(b)$</li>
</ul>
<p>则在$(a,b)$内存在一个$c$，使$f’(c)=0$</p>
<h3 id="拉格朗日中值定理"><a href="#拉格朗日中值定理" class="headerlink" title="拉格朗日中值定理"></a>拉格朗日中值定理</h3><p>设函数$f(x)$满足条件：</p>
<ul>
<li>在$[a,b]$上连续</li>
<li>在$(a,b)$内可导</li>
</ul>
<p>则在$(a,b)$内一定存在一个c，使得$\frac{f(b)-f(a)}{b-a}=f’(c)$</p>
<h3 id="柯西中值定理"><a href="#柯西中值定理" class="headerlink" title="柯西中值定理"></a>柯西中值定理</h3><p>设函数$f(x)$, $g(x)$满足条件：</p>
<ul>
<li>在$[a,b]$上连续</li>
<li>在$(a,b)$内可导且$f’(x)$, $g’(x)$均存在，且$g’(x)\neq 0$</li>
</ul>
<p>则在$(a,b)$内存在一个c，使$\frac{f(b)-f(a)}{g(b)-g(a)}=\frac{f’(c)}{g’(c)}$</p>
<h3 id="洛必达法则"><a href="#洛必达法则" class="headerlink" title="洛必达法则"></a>洛必达法则</h3><p>洛必达法则可以求出特定函数趋近于某数的极限值。令c属于扩展实数，两函数$f(x),g(x)$在以$x=c$为端点的开区间可微，$lim_{x-&gt;c}\frac{f’(x)}{g’(x)}$属于扩展实数。并且$g’(x)\neq 0$。如果$lim_{x-&gt;c}f(x)=lim_{x-&gt;c}g(x)=0$或$lim_{x-&gt;c}|f(x)|=lim_{x-&gt;c}|g(x)|=\infty$其中一者成立，则称欲求的极限$lim_{x-&gt;c}\frac{f(x)}{g(x)}$为未定式。此时洛必达法则表明：$lim_{x-&gt;c}\frac{f(x)}{g(x)}=lim_{x-&gt;c}\frac{f’(x)}{g’(x)}$</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mathbasic/24.png" alt="图片"></p>
<h4 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h4><p>$\begin{aligned}\lim _{x \rightarrow 0} \frac{\sin \pi x}{\pi x} &amp;=\lim _{x \rightarrow 0} \frac{\sin x}{x} \\&amp;=\lim _{x \rightarrow 0} \frac{\cos x}{1}=\frac{1}{1}=1\end{aligned}$</p>
<p>$\begin{aligned}\lim _{x \rightarrow 0} \frac{2 \sin x-\sin 2 x}{x-\sin x} &amp;=\lim _{x\rightarrow 0} \frac{2 \cos x-2 \cos 2 x}{1-\cos x} \\&amp;=\lim _{x \rightarrow 0} \frac{-2 \sin x+4 \sin 2 x}{\sin x} \\&amp;=\lim _{x \rightarrow 0} \frac{-2 \cos x+8 \cos 2 x}{\cos x} \\&amp;=\frac{-2 \cos 0+8 \cos 0}{\cos 0} \\&amp;=6\end{aligned}$</p>
<h2 id="泰勒公式"><a href="#泰勒公式" class="headerlink" title="泰勒公式"></a>泰勒公式</h2><p>设函数$f(x)$在点$x_0$处的某邻域内有$n+1$阶导数，则对该邻域内异于$x_0$的一点x，在$x_0$与$x$之间至少存在一个$\xi$使得：$\begin{array}{l}f(x)=f\left(x_{0}\right)+f^{\prime}\left(x_{0}\right)\left(x-x_{0}\right)+\frac{1}{2 !}f^{\prime \prime}\left(x_{0}\right)\left(x-x_{0}\right)^{2}+\cdots \+\frac{f^{(n)}\left(x_{0}\right)}{n !}\left(x-x_{0}\right)^{n}+R_{n}(x)\end{array}$，其中$R_{n}(x)=\frac{f^{(n+1)}(\xi)}{(n+1) !}\left(x-x_{0}\right)^{n+1}$称为$f(x)$在点$x_0$处的n阶泰勒余项。</p>
<p>令$x_0=0$，则n阶泰勒公式$f(x)=f(0)+f^{\prime}(0) x+\frac{1}{2 !} f^{\prime \prime}(0)x^{2}+\cdots+\frac{f^{(n)}(0)}{n !} x^{n}+R_{n}(x)^{\cdots \ldots(1)}$，其中$R_{n}(x)=\frac{f^{(n+1)}(\xi)}{(n+1) !} x^{n+1}$，$\xi$在0</p>
<p>与x之间，（1）式称为麦克劳林公式。</p>
<p>泰勒公式将函数表达为更简单的形式：</p>
<ul>
<li>$e^{x}=1+x+\frac{1}{2 !} x^{2}+\cdots+\frac{1}{n !} x^{n}+\frac{x^{n+1}}{(n+1) !} e^{\xi}$</li>
<li>$\sin x=x-\frac{1}{3 !} x^{3}+\cdots+\frac{x^{n}}{n !} \sin \frac{n \pi}{2}+\frac{x^{n+1}}{(n+1) !} \sin \left(\xi+\frac{n+1}{2} \pi\right)$</li>
<li>$\cos x=1-\frac{1}{2 !} x^{2}+\cdots+\frac{x^{n}}{n !} \cos \frac{n \pi}{2}+\frac{x^{n+1}}{(n+1) !} \cos \left(\xi+\frac{n+1}{2} \pi\right)$</li>
<li>$\ln (1+x)=x-\frac{1}{2} x^{2}+\frac{1}{3} x^{3}-\cdots+(-1)^{n-1} \frac{x^{n}}{n}+\frac{(-1)^{n} x^{n+1}}{(n+1)(1+\xi)^{n+1}}$</li>
<li>$\begin{array}{l}\text (1+x)^{m}=1+m x+\frac{m(m-1)}{2 !} x^{2}+\cdots+\frac{m(m-1) \cdots(m-n+1)}{n !} x^{n} \+\frac{m(m-1) \cdots(m-n+1)}{(n+1) !} x^{n+1}(1+\xi)^{m-n-1}\end{array}$</li>
</ul>
<p>最后一项因为比较小，经常被省略。</p>
<h2 id="函数单调性"><a href="#函数单调性" class="headerlink" title="函数单调性"></a>函数单调性</h2><h3 id="th1"><a href="#th1" class="headerlink" title="th1"></a>th1</h3><p>设函数$f(x)$在(a,b)区间内可导，如果对任意$x\in (a,b)$都有$f’(x)&gt;0$(或$f’(x)&lt;0$)，则函数$f(x)$在$(a,b)$内是单调增加的（或单调减少）。</p>
<h3 id="th2"><a href="#th2" class="headerlink" title="th2"></a>th2</h3><p>设函数$f(x)$在$x_0$出可导，且在$x_0$处取极值，则$f’(x_0)=0$</p>
<h3 id="th3"><a href="#th3" class="headerlink" title="th3"></a>th3</h3><p>设函数$f(x)$在$x_0$的某一邻域内可微，且$f’(x)=0$(或$f(x)$在$x_0$处连续，但$f’(x_0)$不存在)：</p>
<ul>
<li>若当x经过$x_0$时，$f’(x)$由”+”变”-“，则$f(x_0)$为极大值</li>
<li>若当x经过$x_0$时，$f’(x)$由”-“变”+”，则$f(x_0)$为极小值</li>
<li>若$f’(x)$经过$x=x_0$的两侧不变号，则$f(x_0)$不是极值</li>
</ul>
<h3 id="th4"><a href="#th4" class="headerlink" title="th4"></a>th4</h3><p>设$f(x)$在点$x_0$处有$f’’(x)\neq 0$，且$f’(x_0)=0$，则当$f’’(x_0)<0$时，$f(x_0)$为极大值；当$f''(x_0)>0$时，$f(x_0)$为极小值</p>
<h2 id="渐近线"><a href="#渐近线" class="headerlink" title="渐近线"></a>渐近线</h2><h3 id="水平渐近线"><a href="#水平渐近线" class="headerlink" title="水平渐近线"></a>水平渐近线</h3><p>若$\lim _{x \rightarrow+\infty} f(x)=b$，或$\lim _{x \rightarrow-\infty} f(x)=b$，则$y=b$称为函数$y=f(x)$的水平渐近线。</p>
<h3 id="铅值渐近线"><a href="#铅值渐近线" class="headerlink" title="铅值渐近线"></a>铅值渐近线</h3><p>若$\lim _{x \rightarrow x_{0}^{-}} f(x)=\infty$，或$\lim _{x \rightarrow x_{0}^{+}} f(x)=\infty$，则$x=x_0$称为$y=f(x)$的铅直渐近线。</p>
<h3 id="斜渐近线"><a href="#斜渐近线" class="headerlink" title="斜渐近线"></a>斜渐近线</h3><p>若$a=\lim _{x \rightarrow \infty} \frac{f(x)}{x}$，$b=\lim _{x \rightarrow \infty}[f(x)-a x]$，则$y=ax+b$称为$y=f(x)$的斜渐近线。</p>
<h2 id="函数的凹凸性"><a href="#函数的凹凸性" class="headerlink" title="函数的凹凸性"></a>函数的凹凸性</h2><h3 id="凹凸性判别定理"><a href="#凹凸性判别定理" class="headerlink" title="凹凸性判别定理"></a>凹凸性判别定理</h3><p>若在L上$f’’(x)<0$(或$f''(x)>0$)，则f(x)在L上是凸的(或凹的)。</p>
<h3 id="拐点判别定理1"><a href="#拐点判别定理1" class="headerlink" title="拐点判别定理1"></a>拐点判别定理1</h3><p>若在$x_0$处$f’’(x)=0$(或$f’’(x)$不存在)，当x变动经过$x_0$时$f’’(x)$变号，则$(x_0, f(x_0))$为拐点。</p>
<h3 id="拐点判别定理2"><a href="#拐点判别定理2" class="headerlink" title="拐点判别定理2"></a>拐点判别定理2</h3><p>设$f(x)$在$x_0$点的某邻域内有三阶导数且$f’’(x)=0$，$f’’’(x)\neq 0$，则$(x_0, f(x_0))$为拐点。</p>
<h2 id="函数最优化"><a href="#函数最优化" class="headerlink" title="函数最优化"></a>函数最优化</h2><h3 id="最优化数学模型"><a href="#最优化数学模型" class="headerlink" title="最优化数学模型"></a>最优化数学模型</h3><p>最优化的基本数学模型如下：min f(x)  s.t. $h_i(x)=0, g_j(x) \le 0$</p>
<p>它有三个基本要素，即：</p>
<ul>
<li>设计变量：x是一个实数域范围内的n维向量，被成为决策变量或问题的解</li>
<li>目标函数：f(x)为目标函数</li>
<li>约束条件：$h_i(x)=0$称为灯饰约束，$g_i(x)\le 0$为不等式约束, $i=0,1,2$</li>
</ul>
<h3 id="凸集"><a href="#凸集" class="headerlink" title="凸集"></a>凸集</h3><p>实数域R上(或复数C上)的向量空间中，如果集合S中任两点的连线上的点都在S内，则称集合S为凸集，如下图所示：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mathbasic/25.png" alt="图片"></p>
<p>数学定义为：设集合$D \subset R^{n}$，若对于任意两点$x, y \in D$，及实数$\lambda(0 \leq \lambda \leq 1)$都有：$\lambda x+(1-\lambda) y \in D$则称集合D为凸集。</p>
<h3 id="超平面和半空间"><a href="#超平面和半空间" class="headerlink" title="超平面和半空间"></a>超平面和半空间</h3><p>二维空间的超平面就是一条线(可以是曲线)，三维空间的超平面就是一个面(可以是曲面)。其数学表达式如下：</p>
<p>超平面：$H=\left\{x \in R^{n} \mid a_{1}x_1+a_{2}x_2+\ldots+a_{n}x_n=b\right\}$</p>
<p>半空间：$H^{+}=\left\{x \in R^{n} \mid a_{1}x_1+a_{2}x_2+\ldots+a_{n}x_n \geq b\right\}$</p>
<h3 id="凸集分离定理"><a href="#凸集分离定理" class="headerlink" title="凸集分离定理"></a>凸集分离定理</h3><p>所谓两个凸集分离，直观地看是指两个凸集合没有交叉和重合的部分，因此可以用一张超平面将两者隔在两边，如下图所示：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mathbasic/26.png" alt="图片"></p>
<h3 id="凸函数"><a href="#凸函数" class="headerlink" title="凸函数"></a>凸函数</h3><p>凸函数是一个定义在某个向量空间的凸子集C(区间)上的实值函数f，如果在其定义域C上的任意两点x, y, 以及$t\in [0,1]$有：$f(tx+(1-ty))\le tf(x)+(1-t)f(y)$。也就是说，一个函数是凸的当且进党其上境图（在函数图像上方的点集）为一个凸集。</p>
<p>如果对于任意的$t\in (0,1)$有$f(tx+(1-t)y)&lt;tf(x)+(1-t)f(y)$，函数f是严格凸的。</p>
<p>若对于任意的x,y,z，其中$x&lt;z&lt;y$，都有$f(z)\le max\{f(x), f(y)\}$，则称函数f是几乎凸的。</p>
<p>如果一个函数是凸函数，则其仅有一个全局最优解，没有局部最优解。这个性质在机器学习算法优化中有很重要的应用，因为机器学习模型最后就是在求某个函数的全局最优解。一旦证明该函数（机器学习里面叫“损失函数”）是凸函数，那相当于我们一定能找到它的全局最优解了。</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mathbasic/27.png" alt="图片"></p>
<h2 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h2><p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mathbasic/28.png" alt="图片"></p>
<p>如上图所示，当需要求f(x)的最小值时(机器学习中的f(x)一般就是损失函数，而我们的目标就是希望损失函数最小化)，我们就可以先任意取一个函数的初始点$x_0$(三维情况就是$(x_0,y_0,z_0)$)，让其沿着途中红色箭头(负梯度方向)走，一次到$x_0,x_1,\cdots,x_n$(迭代n次)这样可最快达到极小值点。</p>
<p>梯度下降法基于以下观察：如果实值函数$F(x)$在点a处可微且有定义，那么函数$F(x)$在a点沿着梯度相反的方向$-\nabla F(\mathbf{a})$下降最快。因而，如果$\mathbf{b}=\mathbf{a}-\gamma \nabla F(\mathbf{a})$，对于$\lambda &gt; 0$为一个够小数值时成立，那么$F(a)\ge F(b)$。考虑到这一点，我们可以从函数F的局部极小值的初始估计$x_0$除法，并考虑如下序列$x_0,x_1,x_2,\cdots$使得$\mathbf{x}_{n+1}=\mathbf{x}_{n}-\gamma_{n} \nabla F\left(\mathbf{x}_{n}\right), n \geq 0$。因此可得到$F\left(\mathbf{x}_{0}\right) \geq F\left(\mathbf{x}_{1}\right) \geq F\left(\mathbf{x}_{2}\right) \geq\cdots$，如果顺利的话序列$(x_n)$收敛到期望的极值。注意每次迭代步长$\lambda$可以改变。（注意当快到极值时，$\lambda$可设置更小一些）</p>
<h2 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h2><p>牛顿法也是求解无约束最优化问题常用的方法，最大的优点是收敛速度快。从本质上取看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。通俗地说，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从你当前所处位选一个坡度最大的方向走一步，牛短发在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。</p>
<p>从几何上说，牛顿法就是用一个二次曲面曲拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mathbasic/29.png" alt="图片"></p>
<h3 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h3><p>将目标函数f(x)在$x_k$处进行二阶泰勒展开，可得：$f(x)=f\left(x_{k}\right)+f^{\prime}\left(x_{k}\right)\left(x-x_{k}\right)+\frac{1}{2} f^{\prime \prime}\left(x_{k}\right)\left(x-x_{k}\right)^{2}$，因为目标函数f(x)有极值的必要条件是在极值点处一阶导数为0，即$f’(x)=0$。所以对上面的展开式两边同时求导（注意x才是变量，$x_k$是常量，即$f’(x_k), f’’(x_k)$都是常量），并令$f’(x)=0$可得：</p>
<p>$f^{\prime}\left(x_{k}\right)+f^{\prime \prime}\left(x_{k}\right)\left(x-x_{k}\right)=0$，即$x=x_{k}-\frac{f^{\prime}\left(x_{k}\right)}{f^{\prime \prime}\left(x_{k}\right)}$。于是可以构造如下的迭代公式：$x_{k+1}=x_{k}-\frac{f^{\prime}\left(x_{k}\right)}{f^{\prime \prime}\left(x_{k}\right)}$。这样我们就可以利用该迭代公式依次产生序列$\left\{x_{1}, x_{2}, \ldots, x_{k}\right\}$才逐渐逼近f(x)的极小值点。</p>
<h3 id="高维情况"><a href="#高维情况" class="headerlink" title="高维情况"></a>高维情况</h3><p>上面讲的x都是实数，当x是向量时，迭代公式为：</p>
<p>$\mathbf{x}_{n+1}=\mathbf{x}_{n}-\left[H f\left(\mathbf{x}_{n}\right)\right]^{-1} \nabla f\left(\mathbf{x}_{n}\right), n \geq 0$</p>
<p>$\nabla f=\left[\begin{array}{c}\frac{\partial f}{\partial x_{1}} \\\frac{\partial f}{\partial x_{2}} \\\vdots \\\frac{\partial f}{\partial x_{N}}\end{array}\right]$</p>
<p>$H(f)=\left[\begin{array}{cccc}\frac{\partial^{2} f}{\partial x_{1}^{2}} &amp; \frac{\partial^{2} f}{\partial x_{1} \partial x_{2}} &amp; \cdots &amp; \frac{\partial^{2} f}{\partial x_{1} \partial x_{n}} \\\frac{\partial^{2} f}{\partial x_{2} \partial x_{1}} &amp; \frac{\partial^{2} f}{\partial x_{2}^{2}}&amp; \cdots &amp; \frac{\partial^{2} f}{\partial x_{2} \partial x_{n}} \\\vdots &amp; \vdots &amp;\ddots &amp; \vdots \\\frac{\partial^{2} f}{\partial x_{n} \partial x_{1}} &amp; \frac{\partial^{2} f}{\partial x_{n} \partial x_{2}} &amp; \cdots &amp; \frac{\partial^{2} f}{\partial x_{n}^{2}}\end{array}\right]$</p>
<p>【注】：基本的牛顿迭代公式是用一阶导数除以二阶导数，高阶牛顿迭代公式仍然是一阶导除以二阶导（把海森矩阵的逆运算看作是除法，海森矩阵为二阶导）</p>
<h3 id="阻尼牛顿法"><a href="#阻尼牛顿法" class="headerlink" title="阻尼牛顿法"></a>阻尼牛顿法</h3><p>牛顿法的迭代公式中没有步长因子，是定步长迭代。对于非二次型目标函数，有时候会出现$f(x_{k+1})&gt;f(x_k)$的情况，这表明，原始牛顿法不能保证函数值稳定的下降。在严重的情况下甚至会造成序列发散而导致计算失败。</p>
<p>为消除这一弊病，人们又提出阻尼牛顿法。阻尼牛顿法每次迭代的方向仍然是$x_k$，但每次迭代会沿此方向做一维搜索，寻求最优的步长因子$\lambda_k$，即$\lambda_k=minf(x_k+\lambda d_k)$。其具体计算过程如下：</p>
<ul>
<li>给定初值$x_0$和精度阈值$\varepsilon$，并令$k=0$</li>
<li>计算$x_k$和$H_k$</li>
<li>若$\left|g_{k}\right|&lt;\varepsilon$则停止迭代；否则确定搜索方向：$d_{k}=-H_{k}^{-1} \cdot g_{k}$</li>
<li>计算新的迭代点：$x_{k+1}=x_{k}+d_{k}$</li>
<li>令$k=k+1$，转至2</li>
</ul>
<h2 id="拟牛顿法"><a href="#拟牛顿法" class="headerlink" title="拟牛顿法"></a>拟牛顿法</h2><p>由于牛顿法每一步都要求解目标函数的Hessen矩阵的逆矩阵(矩阵求逆的复杂度是$O(n^3)$)，计算量比较大(求矩阵的逆运算量比较大)，因此提出一种改进方法，即通过正定矩阵近似代替Hessen矩阵的逆矩阵，简化这一计算过程，改进后的方法称为拟牛顿法。其计算过程如下：</p>
<ul>
<li>先将目标函数在$x_{k+1}$处展开，得到：$f(x)=f\left(x_{k+1}\right)+f^{\prime}\left(x_{k+1}\right)\left(x-x_{k+1}\right)+\frac{1}{2} f^{\prime \prime}\left(x_{k+1}\right)\left(x-x_{k+1}\right)^{2}$</li>
<li>两边同时取梯度，得：$f^{\prime}(x)=f^{\prime}\left(x_{k+1}\right)+f^{\prime \prime}\left(x_{k+1}\right)\left(x_{k}-x_{k+1}\right)$</li>
<li>取上式中的$x=x_k$得：$f^{\prime}\left(x_{k}\right)=f^{\prime}\left(x_{k+1}\right)+f^{\prime \prime}\left(x_{k+1}\right)\left(x-x_{k+1}\right)$，如果用$g_k$表示$f’(x_k)$，则$g_{k+1}-g_{k}=H_{k+1} \cdot\left(x_{k+1}-x_{k}\right)$，可推导出$H_{k}^{-1} \cdot\left(g_{k+1}-g_{k}\right)=x_{k+1}-x_{k}$。这个式子就称为“拟牛顿条件”，由它来对Hessen矩阵做约束。</li>
</ul>
<h2 id="线性规划"><a href="#线性规划" class="headerlink" title="线性规划"></a>线性规划</h2><p>参考：<a href="https://zhuanlan.zhihu.com/p/31644892" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31644892</a></p>
]]></content>
      <tags>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP注意力机制总结</title>
    <url>/2021/02/21/NLP%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p>在NLP中我们经常使用注意力机制处理复杂的问题，那么注意力机制是怎么产生的，都有哪些变种，是如何应用在模型中的呢？本篇我们来对NLP中的注意力机制进行一些总结。</p>
<a id="more"></a>
<h1 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h1><p>神经网络有很强的能力。但是对于复杂任务，需要大量的输入信息和复杂的计算流程。计算机的计算能力是神经网络的一个瓶颈。为了减少计算复杂度，常见的方法有局部连接、权值共享、汇聚操作，但仍然需要：尽量少增加模型复杂度（参数），来提高模型的表达能力。</p>
<p>简单文本分类可以使用单向量表达文本只需要一些关键信息即可，所以一个向量足以表达一篇文章，可以用来分类。对阅读理解来说，文章比较长时，一个RNN很难反应出文章的所有语义信息。对于阅读理解任务来说，编码时并不知道会遇到什么问题。这些问题可能会涉及到文章的所有信息点，如果丢失任意一个信息就可能导致无法正确回答问题。</p>
<p>神经网络中可以存储的信息称为网络容`。 存储的多，参数也就越多，网络也就越复杂。 LSTM就是一个存储和计算单元。输入的信息太多(信息过载问题)，但不能同时处理这些信息。只能选择重要的信息进行计算，同时用额外空间进行信息存储。这里有两种方法：</p>
<ul>
<li>信息选择：聚焦式自上而下地选择重要信息，过滤掉无关的信息，也就是注意力机制。</li>
<li>外部记忆： 优化神经网络的记忆结构，使用额外的外部记忆，来提高网络的存储信息的容量，即记忆力机制。</li>
</ul>
<p>比如，一篇文章，一个问题。答案只与几个句子相关。所以只需把相关的片段挑选出来交给后续的神经网络来处理，而不需要把所有的文章内容都给到神经网络。</p>
<p>注意力机制Attention Mechanism是解决信息过载的一种资源分配方案，把计算资源分配给更重要的任务。就像人脑可以有意或无意地从大量的输入信息中，选择小部分有用信息来重点处理，并忽略其它信息。一般可以分为聚焦式注意力和显著性注意力两种：</p>
<ul>
<li>聚焦式注意力：自上而下有意识的注意力。有预定目的、依赖任务、主动有意识的聚焦于某一对象的注意力。</li>
<li>显著性注意力：自下而上无意识的注意力。由外界刺激驱动的注意力，无需主动干预，也和任务无关。如Max Pooling和Gating。</li>
</ul>
<h1 id="普通注意力机制"><a href="#普通注意力机制" class="headerlink" title="普通注意力机制"></a>普通注意力机制</h1><h2 id="加性注意力"><a href="#加性注意力" class="headerlink" title="加性注意力"></a>加性注意力</h2><p>加性注意力来自于论文《Neural Machine Translation By Jointly Learning to Align and Translate》[1]，又称为Bahdanau Attention，在论文中应用于机器翻译任务。在介绍Bahdanau Attention前，我们先来看一下LSTM以及普通的Encoder-Decoder的计算过程。</p>
<h3 id="LSTM计算过程"><a href="#LSTM计算过程" class="headerlink" title="LSTM计算过程"></a>LSTM计算过程</h3><p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/NLPAttention/1.png" alt="图片"></p>
<p>传统的DNN的隐节点可以表示为$h_{t}=\sigma\left(x_{t} \times w_{x t}+b\right)$，而普通RNN的隐节点可以表示为$h_{t}=\sigma\left(x_{t} \times w_{x t}+h_{t-1} \times w_{h t}+b\right)$，因此RNN的隐节点$h_{t-1}$有两个作用：</p>
<ul>
<li>计算在该时刻的预测值：$\hat{y}_{t}=\sigma\left(h_{t} * w+b\right)$</li>
<li>计算下个时间片的隐节点状态：$h_t$</li>
</ul>
<p>LSTM在计算h_t时增加门控机制：</p>
<ul>
<li>$f_{t}=\sigma\left(W_{f} \cdot\left[h_{t-1}, x_{t}\right]+b_{f}\right)$</li>
<li>$i_{t}=\sigma\left(W_{i} \cdot\left[h_{t-1}, x_{t}\right]+b_{i}\right)$</li>
<li>$\tilde{C}_{t}=\tanh \left(W_{C} \cdot\left[h_{t-1}, x_{t}\right]+b_{C}\right)$</li>
<li>$h_{t}=o_{t} * \tanh \left(C_{t}\right)$</li>
</ul>
<h3 id="GRU计算过程"><a href="#GRU计算过程" class="headerlink" title="GRU计算过程"></a>GRU计算过程</h3><p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/NLPAttention/2.png" alt="图片"></p>
<ul>
<li>$z_{t}=\sigma\left(W_{z} \cdot\left[h_{t-1}, x_{t}\right]\right)$</li>
<li>$r_{t}=\sigma\left(W_{r} \cdot\left[h_{t-1}, x_{t}\right]\right)$</li>
<li>$\tilde{h}_{t}=\tanh \left(W \cdot\left[r_{t} * h_{t-1}, x_{t}\right]\right)$</li>
<li>$h_{t}=\left(1-z_{t}\right) <em> h_{t-1}+z_{t} </em> \tilde{h}_{t}$</li>
</ul>
<h3 id="Encoder-Decoder计算过程"><a href="#Encoder-Decoder计算过程" class="headerlink" title="Encoder-Decoder计算过程"></a>Encoder-Decoder计算过程</h3><p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/NLPAttention/3.png" alt="图片"></p>
<h3 id="Bahdanau-Attention"><a href="#Bahdanau-Attention" class="headerlink" title="Bahdanau Attention"></a>Bahdanau Attention</h3><p>加性模型的表达式：$s\left(\mathbf{x}_{i}, \mathbf{q}\right)=v^{T} \tanh \left(\mathbf{W} \mathbf{x}_{\mathbf{i}}+\mathbf{U q}\right)$，加性Attention并行不大容易实现（或者实现起来占用显存多），所以一般只用来将变长向量序列编码为固定长度的向量（取代简单的Pooling），而很少用来做序列到序列的编码。</p>
<h3 id="Encoder-Decoder-with-Bahdanau-Attention"><a href="#Encoder-Decoder-with-Bahdanau-Attention" class="headerlink" title="Encoder-Decoder with Bahdanau Attention"></a>Encoder-Decoder with Bahdanau Attention</h3><p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/NLPAttention/4.png" alt="图片"></p>
<ul>
<li>Encoder<ul>
<li>$h_{i}=\tanh \left(W\left[h_{i-1}, x_{i}\right]\right)$</li>
<li>$o_{i}=\operatorname{softmax}\left(V h_{i}\right)$</li>
</ul>
</li>
<li>Decoder<ul>
<li>生成该时刻的语义向量<ul>
<li>$e_{t i}=v_{a}^{\top} \tanh \left(W_{a}\left[s_{i-1}, h_{i}\right]\right)$：Encoder中第 i 时刻 Encoder隐层状态 $h_i$ 对Decoder中 $t$ 时刻隐层状态 $s_t$ 的影响程度</li>
<li>$\alpha_{t i}=\frac{\exp \left(e_{t i}\right)}{\sum_{k=1}^{T} \exp \left(e_{t k}\right)}$：对$e_{ti}$的softmax归一化</li>
<li>$c_{t}=\sum_{i=1}^{T} \alpha_{t i} h_{i}$</li>
</ul>
</li>
<li>传递隐层信息并预测<ul>
<li>$s_{t}=\tanh \left(W\left[s_{t-1}, y_{t-1}, c_{t}\right]\right)$</li>
<li>$o_{t}=\operatorname{softmax}\left(V s_{t}\right)$</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>相关代码可参考：<a href="https://github.com/majing2019/keras_bahdanau" target="_blank" rel="noopener">https://github.com/majing2019/keras_bahdanau</a></p>
<h2 id="双线性注意力"><a href="#双线性注意力" class="headerlink" title="双线性注意力"></a>双线性注意力</h2><p>双线性注意力来自于论文《Effective Approaches to Attention-based Neural Machine Translation》，又叫做Luong Attention。与Bahdanau Attention不同之处在于Decoder部分，其计算表达式为$h_{t}^{T} W_{a} \bar{h}_{s}$，使用双线性注意力的机器翻译模型计算过程如下：</p>
<ul>
<li>Encoder：和Bahdanau Attention一样</li>
<li>Decoder<ul>
<li>生成该时刻的语义向量<ul>
<li>$s_{t}=\tanh \left(W\left[s_{t-1}, y_{t-1}\right]\right)$</li>
<li>$e_{t i}=s_{t}^{\top} W_{a} h_{i}$</li>
<li>$\alpha_{t i}=\frac{\exp \left(e_{t i}\right)}{\sum_{k=1}^{T} \exp \left(e_{t k}\right)}$</li>
<li>$c_{t}=\sum_{i=1}^{T} \alpha_{t i} h_{i}$</li>
</ul>
</li>
<li>传递隐层信息并预测<ul>
<li>$\tilde{s}_{t}=\tanh \left(W_{c}\left[s_{t}, c_{t}\right]\right)$</li>
<li>$o_{t}=\operatorname{softmax}\left(V \tilde{s}_{t}\right)$</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>代码参考：<a href="https://github.com/asmekal/keras-monotonic-attention/blob/master/attention_decoder.py" target="_blank" rel="noopener">https://github.com/asmekal/keras-monotonic-attention/blob/master/attention_decoder.py</a></p>
<h2 id="点击注意力"><a href="#点击注意力" class="headerlink" title="点击注意力"></a>点击注意力</h2><p>点击注意力来自于论文《Attention Is All You Need》，其表达式为$s\left(\mathbf{x}_{i}, \mathbf{q}\right)=\mathbf{x}_{i}^{T} \mathbf{q}$，也有进行缩放的点击注意力，表达式为$s\left(\mathbf{x}_{i}, \mathbf{q}\right)=\frac{\mathbf{x}_{i}^{\mathrm{T}} \mathbf{q}}{\sqrt{d}}$，点击注意力易于并行实现。下面我们介绍一下Attention的通用表示，顺便理解点击注意力模型。</p>
<p>Attention机制的实质其实就是一个寻址（addressing）的过程，如下图所示：给定一个和任务相关的查询Query向量 q，通过计算与Key的注意力分布并附加在Value上，从而计算Attention Value，这个过程实际上是Attention机制缓解神经网络模型复杂度的体现：不需要将所有的N个输入信息都输入到神经网络进行计算，只需要从X中选择一些和任务相关的信息输入给神经网络。</p>
<p><img src="https://uploader.shimo.im/f/71K2jjeGWanav7iY.png!thumbnail?fileGuid=RWVcK8gvvcyk39p9" alt="图片"></p>
<p>点击注意力的计算也可以分为3步，一是信息输入；二是计算注意力分布α；三是根据注意力分布α 来计算输入信息的加权平均：</p>
<ul>
<li>信息输入：用X = [x1, · · · , xN ]表示N 个输入信息</li>
<li>注意力分布计算：令Key=Value=X，则可以给出注意力分布$\alpha_{i}=\operatorname{softmax}\left(s\left(k e y_{i}, q\right)\right)=\operatorname{softmax}\left(s\left(X_{i}, q\right)\right)$<ul>
<li>我们将$\alpha_{i}$称之为注意力分布，$s\left(X_{i}, q\right)$为注意力打分机制，有几种打分机制<ul>
<li>加性模型：$s\left(\mathbf{x}_{i}, \mathbf{q}\right)=\mathbf{v}^{\mathrm{T}} \tanh \left(W \mathbf{x}_{i}+U \mathbf{q}\right)$</li>
<li>点击模型：$s\left(\mathbf{x}_{i}, \mathbf{q}\right)=\mathbf{x}_{i}^{\mathrm{T}} \mathbf{q}$</li>
<li>缩放点击模型：$s\left(\mathbf{x}_{i}, \mathbf{q}\right)=\frac{\mathbf{x}_{i}^{\mathrm{T}} \mathbf{q}}{\sqrt{d}}$</li>
<li>双线性模型：$s\left(\mathbf{x}_{i}, \mathbf{q}\right)=\mathbf{x}_{i}^{\mathrm{T}} W \mathbf{q}$</li>
</ul>
</li>
</ul>
</li>
<li>信息加权平均：注意力分布$\alpha_{i}$可以解释为在上下文查询q时，第i个信息受关注的程度，采用一种“软性”的信息选择机制对输入信息X进行编码为：$\operatorname{att}(q, X)=\sum_{i=1}^{N} \alpha_{i} X_{i}$<ul>
<li>这种编码方式为软性注意力机制（soft Attention），软性注意力机制有两种：普通模式（Key=Value=X）和键值对模式（Key!=Value），如下图中的左图和右图</li>
</ul>
</li>
</ul>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/NLPAttention/5.png" alt="图片"></p>
<h1 id="注意力机制变体"><a href="#注意力机制变体" class="headerlink" title="注意力机制变体"></a>注意力机制变体</h1><p>Attention上的变种主要有3种：</p>
<ul>
<li>硬性注意力：之前提到的注意力是软性注意力，其选择的信息是所有输入信息在注意力分布下的期望。还有一种注意力是只关注到某一个位置上的信息，叫做硬性注意力（hard attention）。硬性注意力有两种实现方式：<ul>
<li>一种是选取最高概率的输入信息；</li>
<li>另一种硬性注意力可以通过在注意力分布式上随机采样的方式实现。</li>
<li>硬性注意力模型的缺点：<ul>
<li>硬性注意力的一个缺点是基于最大采样或随机采样的方式来选择信息。因此最终的损失函数与注意力分布之间的函数关系不可导，因此无法使用在反向传播算法进行训练。为了使用反向传播算法，一般使用软性注意力来代替硬性注意力。硬性注意力需要通过强化学习来进行训练。</li>
</ul>
</li>
</ul>
</li>
<li>键值对注意力：即上面说的Key！=Value</li>
<li>多头注意力：多头注意力（multi-head attention）是利用多个查询Q = [q1, · · · , qM]，来平行地计算从输入信息中选取多个信息。每个注意力关注输入信息的不同部分，然后再进行拼接：$\operatorname{att}((K, V), Q)=\operatorname{att}\left((K, V), \mathbf{q}_{1}\right) \oplus \cdots \oplus \operatorname{att}\left((K, V), \mathbf{q}_{M}\right)$</li>
<li>自注意力：self-Attention中的Q是对自身（self）输入的变换，而在传统的Attention中，Q来自于外部，具体代码可参考：<a href="https://cloud.tencent.com/developer/article/1451523" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1451523</a></li>
</ul>
<p>下面会依据一些论文罗列一些注意力的变体。</p>
<h2 id="自注意力"><a href="#自注意力" class="headerlink" title="自注意力"></a>自注意力</h2><p>传统的Attention是基于source端和target端的隐变量（hidden state）计算Attention的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但Self Attention不同，它分别在source端和target端进行，仅与source input或者target input自身相关的Self Attention，捕捉source端或target端自身的词与词之间的依赖关系；然后再把source端的得到的self Attention加入到target端得到的Attention中，捕捉source端和target端词与词之间的依赖关系。因此，self Attention Attention比传统的Attention mechanism效果要好，主要原因之一是，传统的Attention机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，self Attention可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系。</p>
<h3 id="Sentence-Embedding"><a href="#Sentence-Embedding" class="headerlink" title="Sentence Embedding"></a>Sentence Embedding</h3><p>在论文《A Structured Self-attentive Sentence Embedding》中使用了self-attention。 现有的处理文本的常规流程第一步就是Word embedding，也有一些 embedding 的方法是考虑了 phrase 和 sentences 的。这些方法大致可以分为两种： universal sentence（general 的句子）和 certain task（特定的任务）。常规的做法是利用 RNN 最后一个隐层的状态，或者 RNN hidden states 的 max or average pooling 或者 convolved n-grams，也有一些工作考虑到解析和依赖树（parse and dependence trees）。</p>
<p>对于一些工作，人们开始考虑通过引入额外的信息，用 attention 的思路，以辅助 sentence embedding。但是对于某些任务，如情感分类，并不能直接使用这种方法，因为并没有此类额外的信息。此时，最常用的做法就是 max pooling or averaging 所有的 RNN 时间步骤的隐层状态，或者只提取最后一个时刻的状态作为最终的 embedding。</p>
<p>而本文提出一种 self-attention 的机制来替换掉通常使用的 max pooling or averaging step。不同于前人的方法，本文所提出的 self-attention mechanism 允许提取句子的不同方便的信息，来构成多个向量的表示。在我们的句子映射模型中，是在 LSTM 的顶端执行的。这确保了 attention 模型可以应用于没有额外信息输入的任务当中，并且减少了 lstm 的一些长期记忆负担。另外一个好处是，可视化提取embedding 变的非常简单和直观，模型结构如下：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/NLPAttention/6.png" alt="图片"></p>
<p>模型的计算过程如下：</p>
<ul>
<li>给定一个句子，我们首先将其进行 Word embedding，得到：S = (w1, w2, … , wn)，然后讲这些 vector 拼成一个 2-D 的矩阵，维度为[n, d]</li>
<li>用双向 lstm 来建模，得到其两个方向的隐层状态，然后，此时我们可以得到维度为[n, 2*u]，记为H</li>
<li>为了将变长的句子，编码为固定长度的 embedding。论文通过选择 n 个 LSTM hidden states 的线性组合，来达到这一目标。计算这样的线性组合，需要利用 self-attention 机制，该机制将 lstm 的所有隐层状态 H 作为输入，并且输出为一个向量权重$\mathbf{a}=\operatorname{softmax}\left(\mathbf{w}_{\mathbf{s} \mathbf{2}} \tanh \left(W_{s 1} H^{T}\right)\right)$，其中W_{s1}维度是[d_a, 2*u]，w_{s2}是大小为d_a的向量</li>
<li>将 lstm 的隐层状态 H 和 attention weight a 进行加权，即可得到 attend 之后的向量 m</li>
<li>为了表示句子的总体的语义，需要多个 m 来聚焦于不同的部分。所以，需要用到multiple hops of attention，即想从句子中提取出 r 个不同的部分。论文将w_{s2}拓展为[r, d_a]的矩阵，记为W_{s2}，使得向量a变为矩阵A，即$A=\operatorname{softmax}\left(W_{s 2} \tanh \left(W_{s 1} H^{T}\right)\right)$，此时softmax是沿着输入的 第二个维度执行的，我们也可以把它看成是没有bias的2层的MLP。映射向量 m 然后就变成了[r, 2*u]的矩阵，我们通过将 annotation A 和 lstm 的隐层状态 H 进行相乘，得到的结果矩阵就是句子的映射M=AH</li>
</ul>
<p>代码参考：<a href="https://github.com/chaitjo/structured-self-attention" target="_blank" rel="noopener">https://github.com/chaitjo/structured-self-attention</a></p>
<h2 id="键值注意力"><a href="#键值注意力" class="headerlink" title="键值注意力"></a>键值注意力</h2><p>Transformer中使用呢了键值注意力，这里我们介绍一些其他论文中出现的key-value attention。</p>
<h3 id="Neural-Language-Modeling"><a href="#Neural-Language-Modeling" class="headerlink" title="Neural Language Modeling"></a>Neural Language Modeling</h3><p>在论文《Frustratingly Short Attention Spans in Neural Language Modeling》中使用了键值注意力进行语言模型建模。作者首先将注意力机制拆分为三部分：</p>
<ul>
<li>keys: 对比当前时刻的状态和过去时刻的状态</li>
<li>values: 比较结果用于建模上下文</li>
<li>prediction: 综合当前时刻和上下文的信息来进行预测</li>
</ul>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/NLPAttention/7.png" alt="图片"></p>
<p>对比上图中的 (a) 和 (b)，可以看到，作者将解码器状态 $h_i$ 拆分为 $k_i$ 和$v_i$，并用 $k_i$ 比较每一个时刻的解码器状态，用得到的 $v_i$ 建模上下文和预测下一个词。这就是作者提出的第一个改进模型，key-value attention model。</p>
<p>显然，这个模型中，$v_i$ 仍然一人分饰两角。所以作者进一步提出了第二个改进模型，key-value-predict attention model：将解码器状态拆分为三个向量，其中新引入的 $p_i$ 用于做预测。这个模型示意图请参考图 (c)。</p>
<p>在实验中，作者提出的两个改进模型在大部分数字指标下的表现，都超过了传统注意力机制模型，并且符合预期地：key-value-predict attention &gt; key-value attention &gt; traditional attention。然而，从可视化的结果来看，这些模型的差异却并没有那么大：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/NLPAttention/8.png" alt="图片"></p>
<p>作者发现，这些基于注意力机制的模型的注意力基本都局限在最近的5个状态的窗口里，也就是题目中所谓的 short span。这也直接导致了，当作者用基于 N-gram 的 RNN 代替基于注意力机制的 RNN 时，也取得了非常惊人的表现。作者也通过改变注意力窗口大小，再次印证了，注意力机制的局限性。当窗口增大到一定程度，也就是注意力范围扩大到一定程度后，实验结果并不会继续提高了。换句话说，注意力机制对于捕捉 long-term 范围内的信息的帮助很小。</p>
<h2 id="相对位置自注意力"><a href="#相对位置自注意力" class="headerlink" title="相对位置自注意力"></a>相对位置自注意力</h2><p>不同于卷积网络或循环网络，Transformer结构中并不明确地建模（输入字符的）相对或绝对信息。相反，其需要向输入中添加绝对位置表示。在论文《Self-Attention with Relative Position Representations》中提出了一种可供选择的方法来扩展self-attention机制，使其能够高效地考虑到序列元素之间的相对位置或距离。</p>
<p>对于一个输入序列 “I BELIEVE THAT I CAN DO IT”，如果不添加位置信息，那么transformer模型是无法感知序列中的两个 “I” 的先后关系。 常见的做法是输入序列的词嵌入（Word Embedding）上加上位置编码（Position Encodings），这些位置编码可以是随时间变化的函数或者是可训练的参数（例如$P E_{(\text {pos }, 2 i)}=\sin \left(\text { pos } / 10000^{2 i / d_{\text {model }}}\right)$、$P E_{(p o s, 2 i+1)}=\cos \left(p o s / 10000^{2 i / d_{\text {model }}}\right)$）。</p>
<p>Transformer模型中注意力计算形式为$\text { attn }=\text { Attention }\left(X W^{Q}, X W^{K}, X W^{V}\right)$，其中表$X=X_{E}+X_{P}$示输入序列的<em>WordEmbedding</em>和<em>PositionEncodings</em>之和，在<em>Attention</em>函数中主要利用位置编码的地方在于计算<em>AttnScore</em>，而计算<em>AttnScore</em>的核心公式为：$\text { scores }=\left(X W^{Q}\right)\left(X W^{K}\right)^{T}=X W^{Q}\left(W^{K}\right)^{T} X^{T}$，经过变$W^{Q}\left(W^{K}\right)^{T}$换之后，位置编码信息会有所缺失。论文从结构上对<em>AttentionMechanism</em>进行改变，提出相对感知的注意力机制(<em>Relation</em>−<em>aware Self</em>−<em>Attention</em>）。</p>
<p>我们以one-head的self-attention为例进行介绍。Self-attention的输入为$x=\left(x_{1}, \ldots, x_{n}\right), x_{i} \in \mathbb{R}^{d_{x}}$，输出为，$z=\left(z_{1}, \ldots, z_{n}\right), z_{i} \in \mathbb{R}^{d_{z}}$每个z_i的计算公式为$z_{i}=\sum_{j=1}^{n} \alpha_{i j}\left(x_{j} W^{V}\right)$，参数的$\alpha_{i j}$计算公式为$\alpha_{i j}=\frac{\exp e_{i j}}{\sum_{k=1}^{n} \exp e_{i k}}$，而$e_{i j}$的计算公式为$e_{i j}=\frac{\left(x_{i} W^{Q}\right)\left(x_{j} W^{K}\right)^{T}}{\sqrt{d_{z}}}$，其中$W^{Q}, W^{K}, W^{V} \in \mathbb{R}^{d_{x} \times d_{z}}$为参数矩阵，用图表示如下：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/NLPAttention/9.png" alt="图片"></p>
<p>为了在计算attention-score时加入位置信息，作者将输入序列之间的关系视为带标记的全连接有向图。输入序列中两个元素x_i和x_j之间的边表示为$\alpha_{i j}^{V}, \alpha_{i j}^{K} \in \mathbb{R}^{d_{a}}$，并将其加入以下两个式子中：$z_{i}=\sum_{j=1}^{n} \alpha_{i j}\left(x_{j} W^{V}+\alpha_{i j}^{V}\right)$、$e_{i j}=\frac{x_{i} W^{Q}\left(x_{j} W^{K}+\alpha_{i j}^{K}\right)^{T}}{\sqrt{d_{z}}}$。参数$\alpha_{i j}^{V}, \alpha_{i j}^{K} \in \mathbb{R}^{d_{a}}$在每个<em>head</em>和一个<em>sequence</em>之间共享且$d_{a}=d_{z}$。序列中两个位置i、j之间相对位置信息参数表示为$\alpha_{i j}^{V}=w_{j-i}^{V}, \alpha_{i j}^{K}=w_{j-i}^{K}$，序列元素之间的相对位置关系图如下所示：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/NLPAttention/10.png" alt="图片"></p>
<p>论文假设序列中两个元素之间的相对位置超过一定距离k之后就不再有用。因而对于每个元素x_i，只考虑其2k+1个相对位置的元素，公式上表示如下：</p>
<p>$\begin{aligned}<br>a_{i j}^{K} &amp;=w_{\mathrm{clip}(j-i, k)}^{K} \\<br>a_{i j}^{V} &amp;=w_{\operatorname{clip}(j-i, k)}^{V} \\<br>\operatorname{clip}(x, k) &amp;=\max (-k, \min (k, x))<br>\end{aligned}$</p>
<p>其中$w^{K}=\left(w_{-k}^{K}, \ldots, w_{k}^{K}\right), w^{V}=\left(w_{-k}^{V}, \ldots, w_{k}^{V}\right)$并且$w_{i}^{K}, w_{i}^{V} \in \mathbb{R}^{d_{a}}$，对于上式中的clip函数，能够将相对距离限制在[-k, k]内(符号表示方向)。用分段函数表示clip函数更为直观，如下：</p>
<p>$\operatorname{clip}(x, k)=\left\{\begin{array}{ll}<br>-k &amp; , x \leq-k \\<br>x &amp; ,-k&lt;x&lt;k \\<br>k &amp; , x \geq k<br>\end{array}\right.$</p>
<p>代码参考：<a href="https://github.com/tensorflow/tensor2tensor" target="_blank" rel="noopener">https://github.com/tensorflow/tensor2tensor</a></p>
<h2 id="层次注意力"><a href="#层次注意力" class="headerlink" title="层次注意力"></a>层次注意力</h2><p>层级“注意力”网络的网络结构如下图所示，网络可以被看作为两部分，第一部分为词“注意”部分，另一部分为句“注意”部分。整个网络通过将一个句子分割为几部分，对于每部分，都使用双向RNN结合“注意力”机制将小句子映射为一个向量，然后对于映射得到的一组序列向量，我们再通过一层双向RNN结合“注意力”机制实现对文本的分类。</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/NLPAttention/11.png" alt="图片"></p>
<p>论文针对的是任务是文档分类任务，即认为每个要分类的文档都可以分为多个句子。因此层级“注意力”模型的第一部分是来处理每一个分句。对于第一个双向RNN输入是每句话的每个单词w_{it}，其计算公式如下：</p>
<ul>
<li>$x_{i t}=W_{e} w_{i t}, t \in[1, T]$</li>
<li>$\vec{h}_{i t}=\overrightarrow{\operatorname{GRU}}\left(x_{i t}\right), t \in[1, T]$</li>
<li>$\overleftarrow{h}_{i t}=\overleftarrow{\operatorname{GRU}}\left(x_{i t}\right), t \in[T, 1]$</li>
<li>$h_{i t}=\left[\vec{h}_{i t}, \stackrel{\leftarrow}{h}_{i t}\right]$</li>
</ul>
<p>其中$w_{it}$表示单词，$W_e$是词向量矩阵，GRU的计算过程如下：</p>
<ul>
<li>$r_{t}=\sigma\left(W_{r} x_{t}+U_{r} h_{t-1}+b_{r}\right)$，$r_t$是遗忘门，决定留下多少历史信息，如果$r_t$为0表示不保留历史信息</li>
<li>$\tilde{h}_{t}=\tanh \left(W_{h} x_{t}+r_{t} \odot\left(U_{h} h_{t-1}\right)+b_{h}\right)$</li>
<li>$z_{t}=\sigma\left(W_{z} x_{t}+U_{z} h_{t-1}+b_{z}\right)$，$z_t$控制历史信息和当前信息的保留比例</li>
<li>$h_{t}=\left(1-z_{t}\right) \odot h_{t-1}+z_{t} \odot \tilde{h}_{t}$</li>
</ul>
<p>对于一句话中的单词，并不是每一个单词对分类任务都是有用的，比如在做文本的情绪分类时，可能我们就会比较关注“很好”、“伤感”这些词。为了能使循环神经网络也能自动将“注意力”放在这些词汇上，论文设计了基于单词的注意力模型，其计算公式如下：</p>
<ul>
<li>$u_{i t}=\tanh \left(W_{w} h_{i t}+b_{w}\right)$</li>
<li>$\alpha_{i t}=\frac{\exp \left(u_{i t}^{\top} u_{w}\right)}{\sum_{t} \exp \left(u_{i t}^{\top} u_{w}\right)}$</li>
<li>$s_{i}=\sum_{t} \alpha_{i t} h_{i t}$</li>
</ul>
<p>通过一个线性层对双向RNN的输出进行变换，然后通过softmax公式计算出每个单词的重要性，最后通过对双向RNN的输出进行加权平均得到每个句子的表示。</p>
<p>句层面的“注意力”模型和词层面的“注意力”模型有异曲同工之妙。其计算公式如下所示：</p>
<ul>
<li>$\vec{h}_{i}=\overrightarrow{\operatorname{GRU}}\left(s_{i}\right), i \in[1, L]$</li>
<li>$\overleftarrow{h}_{i}=\overleftarrow{\operatorname{GRU}}\left(s_{i}\right), t \in[L, 1]$</li>
<li>$h_{i}=\left[\vec{h}_{i}, \overleftarrow{h}_{i}\right]$</li>
<li>$u_{i}=\tanh \left(W_{s} h_{i}+b_{s}\right)$</li>
<li>$\alpha_{i}=\frac{\exp \left(u_{i}^{\top} u_{s}\right)}{\sum_{i} \exp \left(u_{i}^{\top} u_{s}\right)}$</li>
<li>$v=\sum_{i} \alpha_{i} h_{i}$</li>
</ul>
<p>最后就是使用最常用的softmax分类器对整个文本进行分类了：$p=\operatorname{softmax}\left(W_{c} v+b_{c}\right)$，损失函数为：$L=-\sum_{d} \log p_{d j}$。</p>
<h2 id="稀疏注意力"><a href="#稀疏注意力" class="headerlink" title="稀疏注意力"></a>稀疏注意力</h2><p>从理论上来讲，Self Attention的计算时间和显存占用量都是$O(n^2)$级别的（n是序列长度），这就意味着如果序列长度变成原来的2倍，显存占用量就是原来的4倍，计算时间也是原来的4倍。当然，假设并行核心数足够多的情况下，计算时间未必会增加到原来的4倍，但是显存的4倍却是实实在在的，无可避免，这也是微调Bert的时候时不时就来个OOM的原因了。</p>
<p>我们说Self Attention是$O(n^2)$的，那是因为它要对序列中的任意两个向量都要计算相关度，得到一个$n^2$大小的相关度矩阵：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/NLPAttention/12.png" alt="图片"></p>
<p>在上图中，左边显示了注意力矩阵，右变显示了关联性，这表明每个元素都跟序列内所有元素有关联。所以，如果要节省显存，加快计算速度，那么一个基本的思路就是减少关联性的计算，也就是认为每个元素只跟序列内的一部分元素相关，这就是稀疏Attention的基本原理。</p>
<h3 id="Atrous-Self-Attention"><a href="#Atrous-Self-Attention" class="headerlink" title="Atrous Self Attention"></a>Atrous Self Attention</h3><p>Atrous Self Attention的中文可以称之为“膨胀自注意力”、“空洞自注意力”、“带孔自注意力”等。Atrous Self Attention就是启发于“膨胀卷积（Atrous Convolution）”，如下右图所示，它对相关性进行了约束，强行要求每个元素只跟它相对距离为k,2k,3k,…的元素关联，其中k&gt;1是预先设定的超参数。从下左的注意力矩阵看，就是强行要求相对距离不是k的倍数的注意力为0（白色代表0）：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/NLPAttention/13.png" alt="图片"></p>
<p>由于现在计算注意力是“跳着”来了，所以实际上每个元素只跟大约n/k个元素算相关性，这样一来理想情况下运行效率和显存占用都变成了$O(n^2/k)$，也就是说能直接降低到原来的1/k。</p>
<h3 id="Local-Self-Attention"><a href="#Local-Self-Attention" class="headerlink" title="Local Self Attention"></a>Local Self Attention</h3><p>Local Self Attention中文可称之为“局部自注意力”。其实自注意力机制在CV领域统称为“Non Local”，而显然Local Self Attention则要放弃全局关联，重新引入局部关联。具体来说也很简单，就是约束每个元素只与前后k个元素以及自身有关联，如下图所示：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/NLPAttention/14.png" alt="图片"></p>
<p>从注意力矩阵来看，就是相对距离超过k的注意力都直接设为0。其实Local Self Attention就跟普通卷积很像了，都是保留了一个2k+1大小的窗口，然后在窗口内进行一些运算，不同的是普通卷积是把窗口展平然后接一个全连接层得到输出，而现在是窗口内通过注意力来加权平均得到输出。对于Local Self Attention来说，每个元素只跟2k+1个元素算相关性，这样一来理想情况下运行效率和显存占用都变成了O((2k+1)n)∼O(kn)了，也就是说随着n而线性增长，这是一个很理想的性质——当然也直接牺牲了长程关联性。</p>
<h3 id="Sparse-Self-Attention"><a href="#Sparse-Self-Attention" class="headerlink" title="Sparse Self Attention"></a>Sparse Self Attention</h3><p>论文《Generating Long Sequences with Sparse Transformers》直接将两个Atrous Self Attention和Local Self Attention合并为一个，如下图：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/NLPAttention/15.png" alt="图片"></p>
<p>从注意力矩阵上看就很容易理解了，就是除了相对距离不超过k的、相对距离为k,2k,3k,…的注意力都设为0，这样一来Attention就具有“局部紧密相关和远程稀疏相关”的特性，这对很多任务来说可能是一个不错的先验，因为真正需要密集的长程关联的任务事实上是很少的。</p>
<h2 id="线性注意力"><a href="#线性注意力" class="headerlink" title="线性注意力"></a>线性注意力</h2><p>传统的Transformer Attention形式为：$\text { Attention }(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V})=\operatorname{softmax}\left(\boldsymbol{Q K}^{\top}\right) \boldsymbol{V}$，其中，$\boldsymbol{Q} \in \mathbb{R}^{n \times d_{k}}, \boldsymbol{K} \in \mathbb{R}^{m \times d_{k}}, \boldsymbol{V} \in \mathbb{R}^{m \times d_{v}}$。在Self Attention场景下，为了介绍上的方便统一设，$\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V} \in \mathbb{R}^{n \times d}$，一般场景下都有n&gt;d甚至n&gt;&gt;d。</p>
<p>制约Attention性能的关键因素，其实是定义里边的Softmax：QK^T这一步我们得到一个n^2的矩阵，就是这一步决定了Attention的复杂度是$O(n^2)$，如果没有Softmax，那么就是三个矩阵连乘$QK^TV$，而矩阵乘法是满足结合率的，所以我们可以先算$K^TV$，得到一个$d^2$的矩阵，然后再用Q左乘它，由于d&lt;&lt;n，所以这样算大致的复杂度只是O(n)。也就是说，去掉Softmax的Attention的复杂度可以降到最理想的线性级别O(n)！</p>
<p>问题是，直接去掉Softmax还能算是Attention吗？它还能有标准的Attention的效果吗？为了回答这个问题，我们先将Scaled-Dot Attention的定义等价地改写为：</p>
<p>$\text { Attention }(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V})_{i}=\frac{\sum_{j=1}^{n} e^{\boldsymbol{q}_{i}^{\top} \boldsymbol{k}_{j} \boldsymbol{v}_{j}}}{\sum_{j=1}^{n} e^{\boldsymbol{q}_{i}^{\top} \boldsymbol{k}_{j}}}$</p>
<p>所以，Scaled-Dot Attention其实就是以$e^{\boldsymbol{q}_{i}^{\top} \boldsymbol{k}_{j}}$为权重对$v_j$做加权平均。所以我们可以提出一个Attention的一般化定义：</p>
<p>$\text { Attention }(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V})_{i}=\frac{\sum_{j=1}^{n} \operatorname{sim}\left(\boldsymbol{q}_{i}, \boldsymbol{k}_{j}\right) \boldsymbol{v}_{j}}{\sum_{j=1}^{n} \operatorname{sim}\left(\boldsymbol{q}_{i}, \boldsymbol{k}_{j}\right)}$</p>
<p>也就是把$e^{\boldsymbol{q}_{i}^{\top} \boldsymbol{k}_{j}}$换成的$\boldsymbol{q}_{i}, \boldsymbol{k}_{j}$一般函数$\operatorname{sim}\left(\boldsymbol{q}_{i}, \boldsymbol{k}_{j}\right)$，为了保留Attention相似的分布特性，我们要求$\operatorname{sim}\left(\boldsymbol{q}_{i}, \boldsymbol{k}_{j}\right) \geq 0$恒成立。也就是说，我们如果要定义新式的Attention，那么要保留式上式的形式，并且满足$\operatorname{sim}\left(\boldsymbol{q}_{i}, \boldsymbol{k}_{j}\right) \geq 0$。</p>
<p>如果直接去掉Softmax，那么就是$\operatorname{sim}\left(\boldsymbol{q}_{i}, \boldsymbol{k}_{j}\right)=\boldsymbol{q}_{i}^{\top} \boldsymbol{k}_{j}$，问题是内积无法保证非负性，所以这还不是一个合理的选择。</p>
<p>论文《Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention》的想法是：如果q_i、k_j的每个元素都是非负的，那么内积自然也就是非负的。为了完成这点，可以给q_i、k_j各自加值域非负的激活函数$\phi, \varphi$，即$\operatorname{sim}\left(\boldsymbol{q}_{i}, \boldsymbol{k}_{j}\right)=\phi\left(\boldsymbol{q}_{i}\right)^{\top} \varphi\left(\boldsymbol{k}_{j}\right)$。论文选择的是$\phi(x)=\varphi(x)=\operatorname{elu}(x)+1$。</p>
<p>论文《Efficient Attention: Attention with Linear Complexities》则给出了一个更有意思的选择：它留意到在$QK^T$中，$\boldsymbol{Q}, \boldsymbol{K}, \in \mathbb{R}^{n \times d}$，如果“Q在d那一维是归一化的、并且K在n那一维是归一化的”，那么QK^T就是自动满足归一化了，所以它给出的选择是：$\text { Attention }(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V})=\operatorname{softmax}_{2}(\boldsymbol{Q}) \operatorname{softmax}_{1}(\boldsymbol{K})^{\top} \boldsymbol{V}$。其中softmax1、softmax2分别指在第一个（n）、第二个维度（d）进行Softmax运算。也就是说，这时候我们是各自给Q,K加Softmax，而不是QK^T算完之后才加Softmax。其实可以证明这个形式也是的$\operatorname{sim}\left(\boldsymbol{q}_{i}, \boldsymbol{k}_{j}\right)=\phi\left(\boldsymbol{q}_{i}\right)^{\top} \varphi\left(\boldsymbol{k}_{j}\right)$一个特例，此时对应于$\phi\left(\boldsymbol{q}_{i}\right)=\operatorname{softmax}\left(\boldsymbol{q}_{i}\right), \varphi\left(\boldsymbol{k}_{j}\right)=e^{k j}$。</p>
<p>类似的论文还有《Rethinking Attention with Performers》、《Linformer: Self-Attention with Linear Complexity》等，其中第二篇论文还是借鉴了CV领域的《Expectation-Maximization Attention Networks for Semantic Segmentation》。</p>
<h2 id="基于距离自注意力"><a href="#基于距离自注意力" class="headerlink" title="基于距离自注意力"></a>基于距离自注意力</h2><p>论文《Distance-based Self-Attention Network for Natural Language Inference》上做了微创新, 使用 distance mask, 对相对位置进行了建模, 相距越远的单词, mask matrix 中对应位置将是一个越大的负值, 从而一定程度上抑制了远距离单词间的依赖, 换言之, 强调了单词对邻近单词的依赖, 从而更好地分配 attention。</p>
<p>文章将提出的模型应用于自然语言推理 NLI, 沿用了传统框架 (如下)， 创新点体现在 sentence encoder上：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/NLPAttention/16.png" alt="图片"></p>
<p>sentence encoder 基于 self-attention 对句子进行编码 (如下), 可以看到中间那一部分像极了 Transformer 的 encoder. 不同点在于, mutl-head attention 带上了 mask, 后一层的 add 变成了 gate：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/NLPAttention/17.png" alt="图片"></p>
<p>可以看到, 模型从 forward 和 backward 两个方向分别进行了学习, 因此, 即使使用了 distance mask, 也没有抛弃 DiSAN 中提出的 directional mask. Masked Attention 的计算如下：</p>
<p>$\begin{array}{l}<br>\operatorname{Masked}(Q, K, V) \\<br>\quad=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}+M_{d i r}+\alpha M_{d i s}\right) V<br>\end{array}$</p>
<p>Distance mask 中每个元素代表句中两个单词间绝对距离的负. 由于$e^{-inf}=0$，因此距离越远, 负值越大, 单词间的依赖程度越低：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/NLPAttention/18.png" alt="图片"></p>
<p>Distance mask 强化单词对邻近单词的依赖, 作用类似于 CNN 的 filter提取局部特征. 不同点在于, 前者是对整个句子的 mask, 而后者仅仅局部像素的 mask。</p>
<p>Masked multi-head attention 之后是一个 Fusion gate, 控制 attention 输出和 word embedding 的比例：$\operatorname{Gate}(S, H)=F \odot S^{F}+(1-F) \odot H^{F}$</p>
<p>其中(S 是 word embedding 的矩阵, H 是 attention 的矩阵，$F=\operatorname{sigmoid}\left(S^{F}+H^{F}+b^{F}\right)$)</p>
<p>最后使用 MaxPooling 或 Multi-dimensional attention 或两者一起, 将拼接结果矩阵压缩为向量。</p>
<p>实验证明：</p>
<ul>
<li>相对位置很重要, 使用 distance mask 的实验组比不使用 distance mask 的实验组对句子长度具有更强的鲁棒性;</li>
<li>distance mask 强化了单词对邻近单词的依赖, 但真正具有强依赖关系的单词, 在远距离的情况下也能保持依赖. 换言之, 在保证局部依赖的同时, 又不失全局依赖.</li>
<li>Fusion gate 具有调节输出的作用, 关键词将更多地从 attention 输出, 非关键词更多地走 shortcut connection, 保持 word embedding.</li>
<li>Multi-dimensional attention 与 max pooling 的行为很相似, 都更关注关键词.</li>
</ul>
<h2 id="全局和局部注意力"><a href="#全局和局部注意力" class="headerlink" title="全局和局部注意力"></a>全局和局部注意力</h2><p>在论文《Effective Approaches to Attention-based Neural Machine Translation》中，对NMT任务使用的attention机制提出两种结构，global attention将attention作用于全部输入序列，local attention每个时间步将attention作用于输入序列的不同子集。前者被称为soft attention，后者是hard attention和soft attention的结合。</p>
<h3 id="Global-Attention"><a href="#Global-Attention" class="headerlink" title="Global Attention"></a>Global Attention</h3><p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/NLPAttention/19.png" alt="图片"></p>
<p>Global Attention考虑Encoder顶层所有的隐藏状态，此时，对齐向量a_t的长度与输入的长度相同，也就是：$a_{t}(s)=\operatorname{align}\left(h_{t}, \bar{h}_{s}\right)=\frac{\exp \left(\operatorname{score}\left(h_{t}, \bar{h}_{\mathrm{s}}\right)\right)}{\sum_{s^{\prime}} \exp \left(\operatorname{score}\left(h_{t}, \bar{h}_{s}\right)\right)}$。</p>
<p>score其实就是计算当前隐层输出与源隐层输出的相似度比对，作者给出了score的三种content-based function：</p>
<p>$\operatorname{score}\left(\boldsymbol{h}_{t}, \overline{\boldsymbol{h}}_{s}\right)=\left\{\begin{array}{ll}<br>\boldsymbol{h}_{t}^{\top} \overline{\boldsymbol{h}}_{s} &amp; \text { dot } \\<br>\boldsymbol{h}_{t}^{\top} \boldsymbol{W}_{\boldsymbol{a}} \overline{\boldsymbol{h}}_{s} &amp; \text { general } \\<br>\boldsymbol{v}_{a}^{\top} \tanh \left(\boldsymbol{W}_{\boldsymbol{a}}\left[\boldsymbol{h}_{t} ; \overline{\boldsymbol{h}}_{s}\right]\right) &amp; \text { concat }<br>\end{array}\right.$</p>
<h3 id="Local-Attention"><a href="#Local-Attention" class="headerlink" title="Local Attention"></a>Local Attention</h3><p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/NLPAttention/20.png" alt="图片"></p>
<p>Local Attention是这篇论文的重点部分，首先，Global Attention是有缺陷的，当输入的句子特别长的时候（比如输入是篇章级别的文档），Global Attention的计算量将会变得很大，因为我们要求源句子中所有的词都参与每一时刻的计算，所以作者才提出了Local Attention，即只注意源句子的一个小子集，而不是所有单词，这可以看作是一种软对齐和硬对齐的折中办法，不像软对齐那样注意所有的输入而导致计算量过大，也不像硬对齐那样只选择一个输入而导致过程不可微，需要加入复杂的技巧（variance reduction、reinforcement learning）来训练模型。因此Local Attention既可微，能训练，同时计算量小。</p>
<p>因此，该机制的重点就在于如何寻找与预测词对应的输入隐状态，首先，模型需先生成一个对齐中心 $p_t$（aligned position），在预测输出词的每一时刻，上下文向量 $c_t$ 由窗口 $[p_t - D, p_t + D]$ 中的词导出，这里的 D 是窗口半径，是一个超参数。这样，对齐向量 $a_t$ 是一个定长的向量，长度便为 2D+1，接下来的问题便是，如何确定 $p_t$的值。作者给出了两种确定注意力中心的方法：</p>
<ul>
<li>Monotonic alignment：很简单，作者假设输入和输出在很大程度是一一对应的，直接设 $p_t=t$ 就行了，也就是假设输入和输出是单调对齐（Monotonic）的。有了窗口之后，后续步骤就和Global Attention相同了。</li>
<li>Predictive alignment：不像上面那样假设输入输入单调对齐，而是预测对齐中心$p_{t}=S \cdot \operatorname{sigmoid}\left(\boldsymbol{v}_{p}^{\top} \tanh \left(\boldsymbol{W}_{\boldsymbol{p}} \boldsymbol{h}_{t}\right)\right)$，其中$W_p$、$v_p$为参数，S为源句子的长度，这样就有$p_{t} \in[0, S]$。另外，这里还有一个trick，作者对对齐向量的计算做了修改，作者在$p_t$周围引入了一个服从$N\left(p_{t}, D / 2\right)$的高斯分布来对齐权重，从直觉上考虑，距离目标位置越近的词理当起到更到的作用，因此对齐向量为$\boldsymbol{a}_{t}(s)=\operatorname{align}\left(\boldsymbol{h}_{t}, \overline{\boldsymbol{h}}_{s}\right) \exp \left(-\frac{\left(s-p_{t}\right)^{2}}{2 \sigma^{2}}\right)$，其中$\sigma$也是超参数，作者凭经验选择$\sigma=\frac{D}{2}$。同时也可以看到，在没有引入高斯分布之前，位置$p_t$并没有直接与网络相连，虽然计算$p_t$的过程可以微分，但是作为窗口中心这个操作是不可微的，因此也需要某种额外的方式（用$p_t$来导出$a_t$）将其与网络关联起来，使得参数可以通过backprop训练。<ul>
<li>可以想下，没有Gassian的引入，是学习不了计算$p_t$中的$W_p$和$v_p$的，因为它不是直接通过某个函数和网络联系起来，而是计算出$p_t$的值，然后把它当做窗口的中心。虽然计算$p_t$的操作可以微分，但是当做窗口中心这个操作是没办法微分的，所以没办法BP。在Hard Attention中，直接取score最大的作为Attention，同理，这个取max的操作无法微分，所以Hard Attention需要使用其他technique来进行优化BP。</li>
</ul>
</li>
</ul>
<h2 id="基于记忆的注意力"><a href="#基于记忆的注意力" class="headerlink" title="基于记忆的注意力"></a>基于记忆的注意力</h2><p>论文《An Introductory Survey on Attention Mechanisms in NLP Problems》对Attention进行了总结，其中包含了Memory-based Attention。</p>
<p>给定存储在 memory 中的键值对$\left\{\left(\mathrm{k}_{\mathrm{i}}, \mathrm{v}_{\mathrm{i}}\right)\right\}$列表和一个查询向量q：</p>
<ul>
<li>$e_{i}=a\left(q, k_{i}\right)$(address memory)</li>
<li>$\alpha_{\mathrm{i}}=\frac{\exp \left(\mathrm{e}_{\mathrm{i}}\right)}{\sum_{\mathrm{i}} \exp \left(\mathrm{e}_{\mathrm{i}}\right)}$(normalize)</li>
<li>$c=\sum_{i} \alpha_{i} v_{i}$(read contents)</li>
</ul>
<p>事实上，在很多文献中，“memory”只是输入序列的同义词。注意，如果$k_i$和$v_i$是相同的，则基于记忆的注意力和最基本的注意力是一样的。</p>
<p>但是，由于我们结合了其他功能来实现可重用性和提高灵活性，基于记忆的注意力机制会变得更加强大。</p>
<h3 id="Reusability"><a href="#Reusability" class="headerlink" title="Reusability"></a><strong>Reusability</strong></h3><p>在一些问答任务中，一个基本的困难是答案与问题间接相关，因此无法通过基本的注意力技巧轻松解决。然而，如果我们可以通过迭代内存更新（也称为多跳 multi-hop）来逐步引导注意力到答案的正确位置来模拟时间推理过程，就可以实现这一点。直观地说，在每次迭代中，查询都会更新为新内容，而更新后的查询则用于检索相关内容：$q^{(t+1)}=q^{(t)}+c^{(t)}$</p>
<p>更复杂的更新方法包括在多个时间步长的查询和内容之间构建一个循环网络，或根据内容和位置信息进行输出。结果表明，当给出复杂的时间推理任务时，基于记忆的注意模型可以在几次跳跃后成功地找到答案，如下图：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/NLPAttention/21.png" alt="图片"></p>
<h3 id="Flexibility"><a href="#Flexibility" class="headerlink" title="Flexibility"></a><strong>Flexibility</strong></h3><p>由于键和值被清楚地表示，我们可以自由地将先验知识纳入设计单独的键和值嵌入中，以使它们分别更好地捕获相关信息。具体来说，可以手动设计 key embeddings 来匹配question，而 value embeddings 来匹配响应(response)。在 key-value 记忆网络中，提出了一种窗口级表示形式，以便将 key 构造为围绕 entity tokens 为中心的窗口，而 value 则是那些对应的 entities，旨在实现更有效和准确的匹配。如在 Figure 3 中，“apple” 和 “bedroom” 是 value embeddings，而在它们周围的 tokens 是 key embeddings。</p>
<p>更复杂的结构包括动态记忆网络，整体的结构细分为四个部分：question module、input module、episodic memory module 和 answer module。这种模块化的设计可以实现领域知识的逐段注入、模块之间的有效通信以及对传统问题回答之外的更广泛任务的泛化。</p>
<p>一种相似的结构被提出用于处理文本和视觉问答任务，其中视觉输入被送入一个深卷积网络中，高层特征被提取并处理成一个注意力网络的输入序列。</p>
<p>如果我们进一步将 memory 和 query 表示扩展到问答任务之外的领域，基于记忆的注意力技术也被用于方面和观点挖掘，query表示为 aspect prototype；在推荐系统中，用户称为 memory component，items 成为 queries；在主题模型中，从深层网络中提取的潜在主题表示构成了 memory 等。</p>
<h2 id="基于图上注意力"><a href="#基于图上注意力" class="headerlink" title="基于图上注意力"></a>基于图上注意力</h2><p>论文可参考《Graph Attention Networks》，在以后的图神经网络的博客中会具体介绍。</p>
<h2 id="基于强化学习的注意力"><a href="#基于强化学习的注意力" class="headerlink" title="基于强化学习的注意力"></a>基于强化学习的注意力</h2><p>论文《Reinforced Self-Attention Network: a Hybrid of Hard and Soft Attention for Sequence Modeling》中利用强化学习将soft-attention和hard-attention进行了结合。Soft-attention具有参数少、训练快、可微分的优点，但是会将较小但非零的概率分配给琐碎的元素，这降低了少数真正重要元素的注意力，对于较长的输入序列效果不好。Hard-attention的优点是能处理较长的输入序列，缺点是序列采样耗时较大、不可微分。</p>
<p>论文的motivation是将soft attention和hard attention结合起来，使其保留二者的优点，同时丢弃二者的缺点。具体地说，hard attention用于编码关于上下文依赖的丰富的结构信息，并将长序列修剪成短得多的序列，以便soft attention处理。相反，soft attention被用来提供一个稳定的环境和强烈的award来帮助训练hard attention处理之后的序列。该方法既能提高soft attention的预测质量，又能提高hard attention的可训练性，同时提高了对上下文依赖关系建模的能力。模型的整体框架如下图：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/NLPAttention/22.png" alt="图片"></p>
<h3 id="RSS"><a href="#RSS" class="headerlink" title="RSS"></a>RSS</h3><p>Hard attention的目标是从输入序列中选择关键的words，这些关键的words能够提供足够的信息来完成下游任务，这样就可以排除掉许多boring words，从而减少模型的训练时间。</p>
<p>给定一个输入序列$\boldsymbol{x}=\left[x_{1}, \ldots, x_{n}\right]$，RSS生成一个等长的向量$\boldsymbol{z}=\left[z_{1}, \ldots, z_{n}\right]$，其中意$z_{i}=1$味着x_i会被选择，而则$z_{i}=0$意味着$x_i$会被忽略掉。在RSS中，$z_i$是通过attention机制计算的结果作为其采样的概率。RSS的目标是学习到以下的分布：$p\left(\boldsymbol{z} \mid \boldsymbol{x} ; \theta_{r}\right)=\prod_{i=1}^{n} p\left(z_{i} \mid \boldsymbol{x} ; \theta_{r}\right)$，其中$p\left(z_{i} \mid \boldsymbol{x} ; \theta_{r}\right)=g\left(f\left(\boldsymbol{x} ; \theta_{f}\right)_{i} ; \theta_{g}\right)$</p>
<p>其中，$f\left(\cdot ; \theta_{f}\right)$表示一个上下文融合层（context fusion layer），如Bi-LSTM，Bi-GRU等，为$x_i$生成一个上下文敏感的representation。$g\left(\cdot ; \theta_{g}\right)$将$f\left(\cdot ; \theta_{f}\right)$映射到$x_i$被选中的概率。注意到$z_i$的计算方式不依赖于$z_{i-1}$，因此这个步骤可以并行完成。为了进一步提高了效率。文章通过下面这个式子来计算$f\left(\cdot ; \theta_{f}\right)$：$f\left(\boldsymbol{x} ; \theta_{f}\right)_{i}=\left[x_{i} ; \text { mead_pooling }(\boldsymbol{x}) ; x_{i} \odot \text { mead_pooling }(\boldsymbol{x})\right]$</p>
<p>而$g\left(f\left(x ; \theta_{f}\right)_{i} ; \theta_{g}\right)$的计算方式则与论文《DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding》相似：</p>
<p>$g\left(f\left(x ; \theta_{f}\right)_{i} ; \theta_{g}\right)=\operatorname{sigmoid}\left(w^{T} \sigma\left(W^{(R)} f\left(x ; \theta_{f}\right)_{i}+b^{(R)}\right)+b\right)$</p>
<h3 id="ReSA"><a href="#ReSA" class="headerlink" title="ReSA"></a>ReSA</h3><p>在ReSA中，两个参数独立的RSS分别对输入序列的进行采样，采样结果分别称为head tokens和dependent tokens：</p>
<p>$\hat{z}^{h}=\left[\hat{z}_{1}^{h}, \ldots, \hat{z}_{n}^{h}\right] \sim \operatorname{RSS}\left(x ; \theta_{r h}\right)$</p>
<p>$\hat{z}^{d}=\left[\hat{z}_{1}^{d}, \ldots, \hat{z}_{n}^{d}\right] \sim \operatorname{RSS}\left(x ; \theta_{r d}\right)$</p>
<p>然后使用$\hat{z}^{h} 、 \hat{z}^{d}$生成一个mask：</p>
<p>$M_{i j}^{r s s}=\left\{\begin{array}{ll}<br>0, &amp; \hat{z}_{i}^{d}=\hat{z}_{j}^{h}=1 \&amp; i \neq j \\<br>-\infty, &amp; \text { otherwise }<br>\end{array}\right.$</p>
<p>把$M^{r s s}$放到<em>Masked Self-Attention</em>中：</p>
<p>$f^{r s s}\left(x_{i}, x_{j}\right)=c \cdot \tanh \left(\left[W^{(1)} x_{i}+W^{(2)} x_{j}+b^{(1)}\right] / c\right)+M_{i j}^{r s s}$</p>
<p>$f^{r s s}\left(x_{i}, x_{j}\right)$即score function，然后使用softmax函数计算概率：</p>
<p>$P^{j}=\operatorname{softmax}\left(\left[f^{r s s}\left(x_{i}, x_{j}\right)\right]_{i=1}^{n}\right), \text { for } j=1, \ldots, n$</p>
<p>$x_j$的上下文注意力特性通过以下方式计算：</p>
<p>$s_{j}=\sum_{i=1}^{n} P_{i}^{j} \odot x_{i}, \text { for } j=1, \ldots, n$</p>
<p>最后，使用与DiSAN相同的融合层给出最终的输出：</p>
<p>$F=\operatorname{sigmoid}\left(W^{(f)}[\boldsymbol{x} ; s]+b^{(f)}\right)$</p>
<p>$\boldsymbol{u}=F \odot \boldsymbol{x}+(1-F) \odot \boldsymbol{s}$</p>
<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p>使用Policy Gradient方法，让soft先训练，经过冷启动以后再启动hard的强化学习。</p>
<h2 id="多源注意力机制"><a href="#多源注意力机制" class="headerlink" title="多源注意力机制"></a>多源注意力机制</h2><p>论文《Multi-Source Neural Translation》使用英语，德语，法语三种语言建立了一种多源机器翻译模型。Martin Kay曾在他关于多语言翻译的文章中提到过，如果一篇文章被翻译成了另一种语言，那么就更加倾向于被翻译成其他语言。这样的观点给在机器翻译任务中给人以启发，将原本的单一源语言替换为多种源语言，应该可以取得更好的效果。如英语中的“bank”一词原本可以翻译为河岸或是银行，如果源语言中有德语词汇“Flussufer”（河岸）作为帮助，则自然可以精确得到法语中“Berge”（河岸）这样的翻译结果。基于这样的思想，作者在原有的seq2seq+attention模型的基础上做了修改，引入更多源语句，建立一种多源的翻译模型。</p>
<p>经典的seq2seq翻译模型如下图，在本文中作者采用的四层LSTM作为encoder和decoder，hidden state的维度为1000：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/NLPAttention/23.png" alt="图片"></p>
<p>改造后的翻译模型结构如下图，每一个源语言都有自己的一个encoder，并且两个encoder的结构一致。改造之后的关键问题在于如何融合两个encoder的hidden state(h1, h2)和cell state(c1, c2)，再将融合好之后的状态送入decoder中解码输出；以及在attention model如何改造能在两个encoder中学习到权重。</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/NLPAttention/24.png" alt="图片"></p>
<h3 id="状态融合方法"><a href="#状态融合方法" class="headerlink" title="状态融合方法"></a>状态融合方法</h3><h4 id="基本融合法"><a href="#基本融合法" class="headerlink" title="基本融合法"></a>基本融合法</h4><p>该方法比较简单粗暴，将两个encoder最后的hidden state直接相连，再经过一个线性变换和激活函数，变换公式为：$h=\tanh \left(W_{c}\left[h_{1} ; h_{2}\right]\right)$，其中矩阵W_c的维度是2000*1000，细胞状态就是简单的相加：$c=c_{1}+c_{2}$。作者在论文中提到也尝试将两个细胞状态做跟隐藏层一样的拼接变换，但是在训练时可能因为细胞的值太大而无法收敛。</p>
<h4 id="Child-Sum-Method"><a href="#Child-Sum-Method" class="headerlink" title="Child-Sum Method"></a>Child-Sum Method</h4><p>第二种融合方法出自Child-SumTree-LSTMs ，采用一个LSTM的变体将两个encoder的状态融合到一起。所有标准的LSTM输入、输出和新的单元值都是经过计算的。然后每个编码器的细胞状态都有自己的遗忘门。最终的细胞状态和隐藏状态在LSTM中被计算出来。计算公式为：</p>
<ul>
<li>$i=\operatorname{sigmoid}\left(W_{1}^{i} h_{1}+W_{2}^{i} h_{2}\right)$代表输入门，是1000维的向量</li>
<li>$f=\operatorname{sigmoid}\left(W_{i}^{f} h_{i}\right)$代表遗忘门，是1000维的向量</li>
<li>$o=\operatorname{sigmoid}\left(W_{1}^{o} h_{1}+W_{2}^{o} h_{2}\right)$代表输出门，是1000维的向量</li>
<li>$u=\tanh \left(W_{1}^{u} h_{1}+W_{2}^{u} h_{2}\right)$是1000维的向量</li>
<li>$c=i_{f} \odot u_{f}+f_{1} \odot c_{1}+f_{2} \odot c_{2}$</li>
<li>$h=o_{f} \odot \tanh \left(c_{f}\right)</li>
</ul>
<p>上述公式中新增了8个矩阵，每个矩阵的尺寸都是1000*1000。</p>
<h3 id="多源注意力机制-1"><a href="#多源注意力机制-1" class="headerlink" title="多源注意力机制"></a>多源注意力机制</h3><p>本文中作为对比试验使用的单源注意力模型采用的是local-p attention model。decoder的隐藏状态可以回顾encoder的所有隐藏状态，从而学得一个更好的隐藏状态。local-p attention model采用如下方式操作：$p_{t}=S \cdot \operatorname{sigmoid}\left(v_{p}^{T} \tanh \left(W_{p} h_{t}\right)\right)$，S是源语句的长度。pt计算完之后用一个尺寸为(2D+1)的窗口在以pt（D＝10）为中心的源编码器的顶层中查看。对每一个窗口内的隐层状态，都计算一个0-1之间的对齐分数（权重）at(s)。计算方法如下：</p>
<ul>
<li>$a_{t}(s)=\operatorname{align}\left(h_{t}, h_{s}\right) \exp \left(\frac{-\left(s-p_{t}\right)^{2}}{2 \sigma^{2}}\right)$，s是隐藏状态的源索引</li>
<li>$\operatorname{align}\left(h_{t}, h_{s}\right)=\frac{\exp \left(\operatorname{score}\left(h_{t}, h_{s}\right)\right)}{\sum_{s^{\prime}} \exp \left(\operatorname{score}\left(h_{t}, h_{s^{\prime}}\right)\right)}$</li>
<li>$\operatorname{score}\left(h_{t}, h_{s}\right)=h_{t}^{T} W_{a} h_{s}$</li>
</ul>
<p>当所有对齐参数都被计算出来之后，ct是通过求所有源隐藏状态乘以它们的对齐权重的加权总和来创建的。最终送入softmax层的隐层状态为$\tilde{h}_{t}=\tanh \left(W_{c}\left[h_{t} ; c_{t}\right]\right)$。</p>
<p>论文中修改权重模型，同时查看两个源的encoder。为每一个encoder创建一个上下文向量ct1 和ct2（在单源模型中只有ct），即：$\tilde{h_{t}}=\tanh \left(W_{c}\left[h_{t} ; c_{t}^{1} ; c_{t}^{2}\right]\right)$。因此在修改之后的注意力机制模型中有两个pt变量。还有两套不同的对齐参数。还有两个ct值表示为上述的c1t和c2t。</p>
<p>在对话方向上，也有一篇论文《Two are Better than One: An Ensemble of Retrieval- and Generation-Based Dialog Systems》借鉴了该思想，具体思想可参考知乎文章：<a href="https://zhuanlan.zhihu.com/p/31277046" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31277046</a>。</p>
<h2 id="注意力之上的注意力"><a href="#注意力之上的注意力" class="headerlink" title="注意力之上的注意力"></a>注意力之上的注意力</h2><p>论文《Attention-over-Attention Neural Networks for Reading Comprehension》使用一种新颖的模式解决了完形填空式阅读理解任务。其具体过程如下：</p>
<ul>
<li>获取document hdoc和query hquery的文本嵌入<ul>
<li>把document D和query Q中的每个单词都转换为一个one-hot表示</li>
<li>用一个共享嵌入矩阵We把它们转换为连续表示$e(x)=W_{e} \cdot x, \text { where } x \in \mathcal{D}, \mathcal{Q}$</li>
<li>使用两个双向rnn来获取query和document的文本表示，其中每一个词的表达都是通过前向和后向隐藏层的拼接，使用GRU来作为使用的循环单元<ul>
<li>$\overrightarrow{h_{s}(x)}=\overrightarrow{G R U}(e(x))$</li>
<li>$\overleftarrow{h_{s}(x)}=\overleftarrow{G R U}(e(x))$</li>
<li>$h_{s}(x)=\left[\overrightarrow{h_{s}(x)} ; \overleftarrow{h_{s}(x)}\right]$</li>
<li>$h_{d o c} \in \mathbb{R}^{|\mathcal{D}| * 2 d}$</li>
<li>$h_{q u e r y} \in \mathbb{R}|\mathcal{Q}| * 2 d$</li>
</ul>
</li>
</ul>
</li>
<li>计算pair-wise匹配矩阵：$M(i, j)=h_{d o c}(i)^{T} \cdot h_{q u e r y}(j)$表示document的第i个词和query的第j个词的匹配分数，依据这个分数可以构造矩阵$M \in \mathbb{R}^{|\mathcal{D}| * \mid \mathcal{Q}}$</li>
<li>计算query-to-document attention：应用一个column-wise的softmax函数来得到了每一列的概率分布，每一列都是考虑一个query词的document的attention分布。我们把时间t（query）处的document attention分布称作$\alpha(t) \in \mathbb{R}^{|\mathcal{D}|}$，其中$\alpha(t)=\operatorname{softmax}(M(1, t), \ldots, M(|\mathcal{D}|, t))$，$\alpha=[\alpha(1), \alpha(2), \ldots, \alpha(|\mathcal{Q}|)]$</li>
<li>计算document-to-query attention：用一个row-wise softmax函数来对pair-wise匹配矩阵M来得到query级别的attention，对于一个在时间t的document词，我们计算query中每个词对他的重要性分布，来表明query中哪个词对这个document词更加重要。把时间t（document）处的query attention分布称作$\beta(t) \in \mathbb{R}^{|\mathcal{Q}|}$，其中$\beta(t)=\operatorname{softmax}(M(t, 1), \ldots, M(t,|\mathcal{Q}|))$</li>
<li>计算document和query的交互信息<ul>
<li>平均所有的β（t）来得到一个平均的query上的attention β：$\beta=\frac{1}{n} \sum_{t=1}^{|\mathcal{D}|} \beta(t)$</li>
<li>计算α和β之间的点积：$s=\alpha^{T} \beta$，$s \in \mathbb{R}^{|\mathcal{D}|}$这个操作就是看着query的词汇t的时候，计算所有document单体attention的加权和。这样，每一个query词汇的贡献度就能直接学习到。</li>
</ul>
</li>
<li>预测结果：用sum attention mechanism 方法来聚合结果。注意最后的输出应该被reflected 到词汇空间V，而不是document级别的attention |D|，这会在性能上引起很大差异：$P(w \mid \mathcal{D}, \mathcal{Q})=\sum_{i \in I(w, \mathcal{D})} s_{i}, w \in V$<ul>
<li>其中I（w,D）表示词汇w出现在document中的位置的集合。</li>
<li>作为训练目标，我们最大化正确答案的log函数：$\mathcal{L}=\sum_{i} \log (p(x)), x \in \mathcal{A}$</li>
</ul>
</li>
</ul>
<p>整体计算过程如下图所示：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/NLPAttention/25.png" alt="图片"></p>
<h2 id="多跳注意力机制"><a href="#多跳注意力机制" class="headerlink" title="多跳注意力机制"></a>多跳注意力机制</h2><p>论文《Convolutional Sequence to Sequence Learning》使用encoder-decoder + attention模块的大框架来处理机器翻译的任务：encoder 和 decoder采用了相同的卷积结构，其中的非线性部分采用的是门控结构 gated linear units（GLM）；attention 部分采用的是多跳注意 multi-hop attention，也即在 decoder 的每一个卷积层都会进行 attention 操作，并将结果输入到下一层。其具体过程如下：</p>
<ul>
<li>Position Embeddings<ul>
<li>词向量：$w=\left(w_{1}, \ldots, w_{n}\right)$</li>
<li>位置向量：$p=\left(p_{1}, \ldots, p_{n}\right)$</li>
<li>最终表示向量：$e=\left(w_{1}+p_{1}, \ldots, w_{n}+p_{n}\right)$</li>
</ul>
</li>
<li>Convolutional Block Structure<ul>
<li>encoder 和 decoder 都是由 l 层卷积层构成，encoder输出为 $z^l $，decoder输出为 $h^l$，由于卷积网络是层级结构，通过层级叠加能够得到远距离的两个词之间的关系信息</li>
<li>卷积计算：卷积核的大小为$W \in \mathbb{R}^{2 d \times k d}$，其中d为词向量长度，k为卷积窗口大小，每次卷积生成两列d维向量$Y=[A B] \in \mathbb{R}^{2 d}$</li>
<li>非线性计算：非线性部分采用的是门控结构 gated linear units（GLM），即$v([A, B])=A \otimes \delta(B)$，其中$\delta(B)$是门控函数，控制着网络中的信息流，即哪些能够传递到下一个神经元中。</li>
<li>残差连接：$h_{i}^{l}=v\left(W^{l}\left[h_{i-k / 2}^{l-1}, \ldots, h_{i+k / 2}^{l-1}\right]+b^{l}\right)+h_{i}^{l-1}$</li>
<li>Decoder输出：$p\left(y_{i+1} \mid y_{1}, \ldots, y_{i}, x\right)=\operatorname{softmax}\left(W_{o} h_{i}^{L}+b_{o}\right)$</li>
</ul>
</li>
<li>Multi-step Attention<ul>
<li>$d_{i}^{l}=W_{d}^{l} h_{i}^{l}+b_{d}^{l}+g_{i}$</li>
<li>$a_{i j}^{l}=\frac{\exp \left(d_{i}^{l} \cdot z_{j}^{u}\right)}{\sum_{t=1}^{m} \exp \left(d_{i}^{l} \cdot z_{t}^{u}\right)}$为权重信息，采用了向量点积的方式再进行softmax操作，这里向量点积可以通过矩阵计算，实现并行计算</li>
<li>$c_{i}^{l}=\sum_{j=1}^{m} a_{i j}^{l}\left(z_{j}^{u}+e_{j}\right)$</li>
<li>$h_i = c_i + h_i$每一个卷积层都会进行 attention 的操作，得到的结果输入到下一层卷积层，这就是多跳注意机制multi-hop attention。这样做的好处是使得模型在得到下一个主意时，能够考虑到之前的已经注意过的词。</li>
</ul>
</li>
</ul>
<p>模型的整体结构如下图：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/NLPAttention/26.png" alt="图片"></p>
<h2 id="多维度注意力"><a href="#多维度注意力" class="headerlink" title="多维度注意力"></a>多维度注意力</h2><h3 id="DiSAN"><a href="#DiSAN" class="headerlink" title="DiSAN"></a>DiSAN</h3><p>论文《DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding》创新性地提出了 2 种 attention mechanism: multi-dimensional attention 和 directional attention。 前者用矩阵代替向量来表示 attention weights, 将加权求和细化到状态向量的每一维; 后者通过 Mask 矩阵将方向编码进 attention, 解决了时序丢失问题. 基于两种 attention提出了 Directional Self-Attention Network, DiSAN 模型, 在不依赖 RNN/CNN 的情况下, 在多项任务 NLP 任务上取得了 SOTA 的成绩.</p>
<h4 id="Multi-Dimensional-Attention"><a href="#Multi-Dimensional-Attention" class="headerlink" title="Multi-Dimensional Attention"></a>Multi-Dimensional Attention</h4><p>普通 attention 的 alignment score 是一个标量即$f\left(x_{i}, q\right)=w^{T} \sigma\left(W^{(1)} x_{i}+W^{(2)} q\right)$，之后、$a=\left[f\left(x_{i}, q\right)\right]_{i=1}^{n}$、$p(z \mid \boldsymbol{x}, q)=\operatorname{softmax}(a)$得到的 attention weight p_i也是一个标量, 最后计算输出状态$s=\sum_{i=1}^{n} p(z=i \mid \boldsymbol{x}, q) x_{i}=\mathbb{E}_{i \sim p(z \mid \boldsymbol{x}, q)}\left(x_{i}\right)$，$s \in \mathbb{R}^{d_{e}}$是句子表示。</p>
<p>Multi-dimensional attention 的依据是x_k的每一位数字都是一个特征 (比如 king 的词向量, 某一位数字表征性别, 某一位数字表征尊贵程度), 可以拥有自己的 attention weight, 文中称为 feature-wise attention. 具体做法就是将上面 f 函数中的权值向量替换为权值矩阵：</p>
<p>$f\left(x_{i}, q\right)=W^{T} \sigma\left(W^{(1)} x_{i}+W^{(2)} q+b^{(1)}\right)+b$</p>
<p>如此得到的$z_i$将是与$x_i$具有相同 embedding dimension 的向量, 该向量的每一位数字表征$x_i$与 q 相应位置上的对齐程度。 后续的计算过程与上总体无异, 只是 z 成了矩阵, softmax 应用于 z 的第二维，如下图(b)：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/NLPAttention/27.png" alt="图片"></p>
<p>Multi-dimensional attention 对于同一个 token embedding 的不同 feature 计算不同的 attention weight, 充分利用了上下文与 token 不同特征之间的对齐程度. 还是以 king 为例, 当上下文强调性别属性时, 性别特征将得到更大的 attention weight, 而尊贵程度不那么重要, 可能就只分配很小的 attention weight。 对于一词多义的情况, multi-dimensional                 attention 还具有一定的消除歧义作用。</p>
<p>在 multi-dimensional attention 的基础上, 文章得到了 2 个 self-attention 变种, 两者的差别在于 query:</p>
<ul>
<li><p>token2token: 计算序列中两个位置的依赖程度 (对于 self-attention, 依赖的说法比对齐更合适), 此时只需将外部 query q 替换为序列中另一个位置的状态即可(由于需要计算位置两两之间的对齐程度, token2token 的输出是一个矩阵)：$f\left(x_{i}, x_{j}\right)=W^{T} \sigma\left(W^{(1)} x_{i}+W^{(2)} x_{j}+b^{(1)}\right)+b$</p>
</li>
<li><p>source2token: 计算序列某位置状态与整个序列的对齐程度, 文章简单地将 query 剔除了(source2token 的 输出是向量)：$f\left(x_{i}\right)=W^{T} \sigma\left(W^{(1)} x_{i}+b^{(1)}\right)+b$</p>
</li>
</ul>
<h4 id="Directional-Self-Attention"><a href="#Directional-Self-Attention" class="headerlink" title="Directional Self-Attention"></a>Directional Self-Attention</h4><p>Directional self-attention (DiSA) 由一个全连阶层 FC, 一个 masked multi-dimensional token2token self-attention block, 一个 fusion gate 组成：</p>
<ul>
<li>FC 将输入序列转换成隐藏状态的序列</li>
<li>multi-dimensional token2token self-attention 学习各隐藏状态之间的关系, 并使用 Mask 将方向编码进结果<ul>
<li>通过 mask 矩阵, 实现了非对称依赖, 即h_j对h_i有依赖, 反之却没有， 从而实现了方向性</li>
<li>Mask 是一个非零即负无穷的方阵, 因为负无穷的指数为0，从而能抑制依赖关系</li>
<li>上式可修改为$\begin{array}{l}<br>f\left(h_{i}, h_{j}\right)= \\<br>c \cdot \tanh \left(\left[W^{(1)} h_{i}+W^{(2)} h_{j}+b^{(1)}\right] / c\right)+M_{i j} \mathbf{1}<br>\end{array}$</li>
<li>文中使用了 forward mask 与 backward mask， 前者的作用是使得只存在后面状态对前面状态的依赖;，后者则恰好相反：</li>
</ul>
</li>
</ul>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/NLPAttention/28.png" alt="图片"></p>
<ul>
<li>fusion gate 是一个门控单元, 控制 DiSA 中 FC 的输出与 token2token 输出的比例<ul>
<li>Fusion gate 是 dimension-wise 的, 即全面控制输出结果向量的每一位数字</li>
<li>$F=\operatorname{sigmoid}\left(W^{(f 1)} s+W^{(f 2)} \boldsymbol{h}+b^{(f)}\right)$</li>
<li>$\boldsymbol{u}=F \odot \boldsymbol{h}+(1-F) \odot \boldsymbol{s}$</li>
</ul>
</li>
</ul>
<p>最终, DiSAN 由两个 DiSA blocks 和一个 multi-dimensional source2token self-attention layer 组成. 通过 DiSA 分别对前向与反向 上下文依赖 context dependency 进行建模, 为每一个 token 生成一个上下文感知 context-aware 的表示; 然后用 source2token 计算整个序列的向量表示：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/NLPAttention/29.png" alt="图片"></p>
<h2 id="基于拷贝机制的注意力"><a href="#基于拷贝机制的注意力" class="headerlink" title="基于拷贝机制的注意力"></a>基于拷贝机制的注意力</h2><h3 id="PtrNet"><a href="#PtrNet" class="headerlink" title="PtrNet"></a>PtrNet</h3><p>论文《Pointer Networks》提出了一种新的指针网络，它主要解决的是Seq2Seq Decoder端输出的词汇表大小不可变的问题。换句话说，传统的Seq2Seq无法解决输出序列的词汇表会随着输入序列长度的改变而改变的那些问题，而某些问题的输出可能会严重依赖于输入。</p>
<p>我们已经知道，Seq2Seq的出现很好地解开了以往的RNN模型要求输入与输出长度相等的约束，而其后的Attention机制（content-based）又很好地解决了长输入序列表示不充分的问题。尽管如此，这些模型仍旧要求输出的词汇表需要事先指定大小，因为在softmax层中，词汇表的长度会直接影响到模型的训练和运行速度，因此人们往往会丢弃生词，保留高频词。之前也有人采用过一些简单的trick解决生词问题，但都不如Copy机制那么有效。下图是PtrNet结构：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/NLPAttention/30.png" alt="图片"></p>
<p>不难想象，Seq2Seq需要词汇表固定的原因是在预测输出时，模型采用了Softmax distribution来计算生成词汇表中每一个词的似然概率，而对于这里的几何问题来说，输出集合是与输入相关联的，为了解决这个问题，作者将Attention机制改为了如下形式：</p>
<ul>
<li>$u_{j}^{i}=v^{T} \tanh \left(W_{1} e_{j}+W_{2} d_{i}\right) \quad j \in(1, \ldots, n)$</li>
<li>$p\left(C_{i} \mid C_{1}, \ldots, C_{i-1}, \mathcal{P}\right)=\operatorname{softmax}\left(u^{i}\right)$</li>
</ul>
<p>第一个式子和Attention一样，即计算当前输出与对应输入之间的相关性分数，然后对其进行softmax归一化得到权重，在这里，PrtNets直接将权重最大的输入（也就是所谓指针指向的输入）作为输出，也就是直接copy注意力得分最高的输入作为输出。实际上，传统的带有注意力机制的Seq2Seq模型输出的是针对输出词汇表的一个概率分布，而Pointer Networks输出的则是针对输入文本序列的概率分布。虽然简单，但十分有用，由于输出元素来自输入元素的特点，PtrNets特别适合用来直接复制输入序列中的某些元素给输出序列。这对于那些输出为输入的position的任务非常有效。</p>
<p>论文《Get To The Point: Summarization with Pointer-Generator Networks》融合了seq2seq模型和pointer network的pointer-generator network以及覆盖率机制(coverage mechanism)应用于摘要生成。模型主要框架如下：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/NLPAttention/31.png" alt="图片"></p>
<p>我们知道，对于普通的seq2seq模型的attention计算公式为：</p>
<ul>
<li>$e_{i}^{t}=v^{T} \tanh \left(W_{h} h_{i}+W_{s} s_{t}+b_{\mathrm{attn}}\right)$，$h_i$是输出的编码隐层状态，$s_t$是解码的隐层状态</li>
<li>$a^{t}=\operatorname{softmax}\left(e^{t}\right)$</li>
<li>$h_{t}^{*}=\sum_{i} a_{i}^{t} h_{i}$对encoder输出的隐层做加权平均，获得原文的动态表示，称为语境向量</li>
<li>$P_{\text {vocab }}=\operatorname{softmax}\left(V^{\prime}\left(V\left[s_{t}, h_{t}^{*}\right]+b\right)+b^{\prime}\right)$依靠decoder输出的隐层和语境向量，共同决定当前步预测在词表上的概率分布</li>
<li>$P(w)=P_{\text {vocab }}(w)$</li>
<li>$\operatorname{loss}_{t}=-\log P\left(w_{t}^{*}\right)$</li>
</ul>
<p>Pointer-generator network是seq2seq模型和pointer network的混合模型，一方面通过seq2seq模型保持抽象生成的能力，另一方面通过pointer network直接从原文中取词，提高摘要的准确度和缓解OOV问题。在预测的每一步，通过动态计算一个生成概率$p_{gen}$，把二者软性地结合起来：$p_{\text {gen }}=\sigma\left(w_{h^{<em>}}^{T} h_{t}^{</em>}+w_{s}^{T} s_{t}+w_{x}^{T} x_{t}+b_{\mathrm{ptr}}\right)$</p>
<p>文章对pointer network的处理比较巧妙，直接把seq2seq模型计算的attention分布作为pointer network的输出，通过参数复用，大大降低了模型的复杂度，最终的预测为：$P(w)=p_{\text {gen }} P_{\text {vocab }}(w)+\left(1-p_{\text {gen }}\right) \sum_{i: w_{i}=w} a_{i}^{t}$</p>
<p>论文额外还增加了一个coverage向量$c^{t}=\sum_{t^{\prime}=0}^{t-1} a^{t^{\prime}}$计算过去所有预测步计算的attention分布的累加和，记录着模型已经关注过原文的哪些词,并且让这个coverage向量影响当前步的attention计算：$e_{i}^{t}=v^{T} \tanh \left(W_{h} h_{i}+W_{s} s_{t}+w_{c} c_{i}^{t}+b_{\mathrm{attn}}\right)$。这样做的目的在于，在模型进行当前步attention计算的时候，告诉它之前它已经关注过的词，希望避免出现连续attention到某几个词上的情形。同时，coverage模型还添加一个额外的coverage loss，来对重复的attention作惩罚：$\text { covloss }_{t}=\sum_{i} \min \left(a_{i}^{t}, c_{i}^{t}\right)$。值得注意的是这个loss只会对重复的attention产生惩罚，并不会强制要求模型关注原文中的每一个词。最终，模型的整体损失函数为：$\operatorname{loss}_{t}=-\log P\left(w_{t}^{*}\right)+\lambda \sum_{i} \min \left(a_{i}^{t}, c_{i}^{t}\right)$。不过文章在实验部分提到，如何移除了covloss，单纯依靠coverage向量去影响attention的计算并不能缓解重复问题，模型还是会重复地attention到某些词上。而加上covloss的模型训练上也比较trick，需要先用主函数训练好一个收敛的模型，然后再把covloss加上，做个finetune，不然的话效果还是不好。</p>
<h3 id="CopyNet"><a href="#CopyNet" class="headerlink" title="CopyNet"></a>CopyNet</h3><p>论文《Incorporating Copying Mechanism in Sequence-to-Sequence Learning》提出了CopyNet，在 Seq2Seq + Attention 的基础上，引入了拷贝机制，模型结构如下图：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/NLPAttention/32.png" alt="图片"></p>
<p>一般的encoder－decoder结构和基于attention的encoder－decoder结构都严重依赖于input的表征，很难处理output中需要copy input中一些实体名字的问题。本文提出的COPYNET就可以兼顾表征和copy。还有个问题就是，copy这个操作是一个0-1操作，意思就是选择copy时是1，不选择copy是0，这种硬选择很难优化，本文还是用end－end的网络进行了训练。模型的具体计算过程如下：</p>
<ul>
<li>encoder：bi-directional RNN 得到source sentence的定长hidden states。每个h_t 对应一个source word $x_t$，source sentence的表征就变成了 $\{h_1, …, h_{T_s}\}$ (文中用符号M表示)</li>
<li>decoder：把M输入到RNN中，预测target sequence。<ul>
<li>Prediction：从decoder hidden state s_t到预测输出y_t的过程，本来是一个线性映射就可以，现在要用generate-mode和copy-mode的混合概率预测：$p\left(y_{t} \mid s_{t}, y_{t-1}, c_{t}, M\right)=p\left(y_{t}, g \mid s_{t}, y_{t-1}, c_{t}, M\right)+p\left(y_{t}, c \mid s_{t}, y_{t-1}, c_{t}, M\right)$<ul>
<li>Generate-Mode：对输出词表$\mathcal{V}=\left\{v_{1}, \ldots v_{N}\right\}$中的单词$v_i$（one-hot表示），未登录词UNK，输入词$\mathcal{X}$有：<ul>
<li>$\psi_{g}\left(y_{t}=v_{i}\right)=\mathbf{v}_{i}^{\top} \mathbf{W}_{o} \mathbf{s}_{t}, \quad v_{i} \in \mathcal{V} \cup \mathrm{UNK}$，其中$\mathbf{W}_{o} \in \mathbb{R}^{(N+1) \times d_{s}}$</li>
<li>$p\left(y_{t}, \mathrm{~g} \mid \cdot\right)=\left\{\begin{array}{cc}<br>\frac{1}{Z} e^{\psi_{g}\left(y_{t}\right)}, &amp; y_{t} \in \mathcal{V} \\<br>0, &amp; y_{t} \in \mathcal{X} \cap \bar{V} \\<br>\frac{1}{Z} e^{\psi_{g}(\mathrm{UNK})} &amp; y_{t} \notin \mathcal{V} \cup \mathcal{X}<br>\end{array}\right.$</li>
<li>$Z=\sum_{v \in \mathcal{V} \cup\{U N K\}} e^{\psi_{g}(v)}+\sum_{x \in X} e^{\psi_{c}(x)}$</li>
</ul>
</li>
<li>Copy-Mode：论文模型不仅基于内容（the content），也基于句子的信息位置（location information），因此当计算得分函数时，用$\{h_1, …, h_{T_S}\}$来替代源句子$\{x_1, …, x_{T_S}\}$，因此对输入中出现的单词$\mathcal{X}$有：<ul>
<li>$\psi_{c}\left(y_{t}=x_{j}\right)=\sigma\left(\mathbf{h}_{j}^{\top} \mathbf{W}_{c}\right) \mathbf{s}_{t}, \quad x_{j} \in \mathcal{X}$，其中$\mathbf{W}_{c} \in \mathbb{R}^{d_{h} \times d_{s}}$，$d_h$、$d_s$是$h_j$、$s_t$的维度</li>
<li>$p\left(y_{t}, \mathrm{c} \mid \cdot\right)=\left\{\begin{array}{cc}<br>\frac{1}{Z} \sum_{j: x_{j}=y_{t}} e^{\psi_{c}\left(x_{j}\right)}, &amp; y_{t} \in \mathcal{X} \\<br>0 &amp; \text { otherwise }<br>\end{array}\right.$</li>
</ul>
</li>
</ul>
</li>
<li>State Update：对于seq2seq中的Attention，发生在更新$s_t$上，即$\mathbf{s}_{t}=f\left(y_{t-1}, \mathbf{s}_{t-1}, \mathbf{c}_{t}\right)$其中$c_t$是context vector，也就是attention模块，其计算公式如下：$\mathbf{c}_{t}=\sum_{\tau=1}^{T_{S}} \alpha_{t \tau} \mathbf{h}_{\tau}, \quad \alpha_{t \tau}=\frac{e^{\eta\left(\mathbf{s}_{t-1}, \mathbf{h}_{\tau}\right)}}{\sum_{\tau^{\prime}} e^{\eta\left(\mathbf{s}_{t-1}, \mathbf{h}_{\tau^{\prime}}\right)}}$，CopyNet的$y_{t-1}$在这里有些不同，这里被表示成了$\left[\mathbf{e}\left(y_{t-1}\right) ; \zeta\left(y_{t-1}\right)\right]^{\top}$，前一项是word embedding，多一项的后一项叫做selective read，是为了能连续拷贝较长的短语：<ul>
<li>$\zeta\left(y_{t-1}\right)=\sum_{\tau=1}^{T_{S}} \rho_{t \tau} \mathbf{h}_{\tau}$表示M中和$y_t$对应的hidden states加权</li>
<li>$\rho_{t \tau}=\left\{\begin{array}{cc}<br>\frac{1}{K} p\left(x_{\tau}, \mathbf{c} \mid \mathbf{s}_{t-1}, \mathbf{M}\right), &amp; x_{\tau}=y_{t-1} \\<br>0 &amp; \text { otherwise }<br>\end{array}\right.$，K类似softmax的分母，因为$y_{t-1}$可能会出现在source sequence中的多个位置，实际中总$\rho_{t \tau}$是只关心其中一个</li>
</ul>
</li>
<li>Reading M：除了添加注意力机制外，还添加了选择机制。相当于content-based addressing 和location-based addressing的混合。<ul>
<li>Location-based Addressing：利用{$h_i$}中的位置信息，X每向右移动一步，产生的信息流动为$\zeta\left(y_{t-1}\right) \stackrel{\text { update }}{\longrightarrow} s_{t} \stackrel{\text { predict }}{\longrightarrow} y_{t} \stackrel{\text { sel.read }}{\longrightarrow} \zeta\left(y_{t}\right)$</li>
<li>Handing Out-of-Vocabulary Words：因为对于OOV的词，是没有embedding来表示其语义信息的。直接copy的话可以利用很多在source中的OOV词。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="特定任务注意力"><a href="#特定任务注意力" class="headerlink" title="特定任务注意力"></a>特定任务注意力</h2><p>论文《Multilingual Neural Machine Translation with Task-Specific Attention》使用特定任务注意力机制进行多语言机器翻译，通过在训练语料中增加语言方向标志，在训练和解码中动态使用相应语言标志的注意力参数。</p>
<h1 id="注意力机制应用"><a href="#注意力机制应用" class="headerlink" title="注意力机制应用"></a>注意力机制应用</h1><p>注意力机制可以应用于机器翻译、关系抽取、自然语言推理、对话生成、文本匹配、机器阅读理解（《Text Understanding with the Attention Sum Reader Network 》）、自动摘要（《A Deep Reinforced Model for Abstractive Summarization》）等中。下面以一个自然语言推理的应用为例进行介绍。</p>
<h2 id="自然语言推理"><a href="#自然语言推理" class="headerlink" title="自然语言推理"></a>自然语言推理</h2><p>在论文《A Decomposable Attention Model for Natural Language Inference》中也使用了self-attention。传统的自然语言推理大都采用RNN或者CNN来进行句子或者字词特征的提取，而这些网络大都参数较多，并且RNN无法实现并行计算导致计算缓慢。作者这里采用attention的方法，将两个句子的关系问题转化为两个句子对应字词的关系上。由于只采用了几个不同的全连接网络以及attention，所以模型参数较少且可以进行并行计算。模型结构如下：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/NLPAttention/33.png" alt="图片"></p>
<p>计算过程如下：</p>
<ul>
<li>每个训练数据由三个部分组成$\left\{a^{(n)}, b^{(n)}, y^{(n)}\right\}_{n=1}^{N}$，模型的输入为$a=\left(a_{1}, \ldots, a_{l_{a}}\right)$、分$b=\left(b_{1}, \ldots, b_{l_{b}}\right)$别代表前提和假说，$y^{n}=\left(y_{1}^{(n)}, \ldots, y_{C}^{(n)}\right)$表示a和b之间的关系标签，C为输出类别的个数，因此y是个C维的0,1向量。训练目标就是根据输入的a和b正确预测出他们的关系标签y。</li>
<li>Input Representation：原始版本的就是采用预训练好的词向量作为输入的表示，但是由于整个模型只采用了attention机制，所以就没有任何的文本位置信息在里面。所以论文又加了一个intra-sentence attention来得到位置信息：<ul>
<li>$f_{i j}:=F_{\text {intra }}\left(a_{i}\right)^{T} F_{\text {intra }}\left(a_{j}\right)$：$F_{intra}$是一个前馈神经网络</li>
<li>$a_{i}^{\prime}:=\sum_{j=1}^{\ell_{a}} \frac{\exp \left(f_{i j}+d_{i-j}\right)}{\sum_{k=1}^{\ell_{a}} \exp \left(f_{i k}+d_{i-k}\right)} a_{j}$：$d_{i-j}$表示当前词i与句子中的其他词j之间的距离偏差，距离大于10的词共享一个距离偏差（distance-sensitive bias）</li>
<li>每一个时刻的输入就变为原始输入跟self-attention后的值的拼接所得到的向量：$\bar{a}_{i}:=\left[a_{i}, a_{i}^{\prime}\right], \bar{b}_{i}:=\left[b_{i}, b_{i}^{\prime}\right]$</li>
<li>可能加入transformer中的position embedding会有同样的效果</li>
</ul>
</li>
<li>Attend<ul>
<li>a和b中的每个词计算它们之间的attention weights $e_{i j}:=F^{\prime}\left(\bar{a}_{i}, \bar{b}_{j}\right):=F\left(\bar{a}_{i}\right)^{T} F\left(\bar{b}_{j}\right)$<ul>
<li>$a_i$和$b_j$分别代表的是句子a和b中的每个词</li>
<li>F是一个激活函数为RELU的前馈神经网络</li>
<li>假设句子a和句子b的长度分别为 $l_a$ 和 $l_b$ ，如果采用F(a, b)这种计算方式，那个使用F这个方法的次数就是$l_{a} \times l_{b}$，而论文在这里采用$F(a)^TF(b)$的方式，就让计算量变为了$l_{a} \oplus l_{b}$</li>
</ul>
</li>
<li>用这个attention weights分别对a和b进行归一化以及加权<ul>
<li>$\beta_{i}:=\sum_{j=1}^{\ell_{b}} \frac{\exp \left(e_{i j}\right)}{\sum_{k=1}^{\ell_{b}} \exp \left(e_{i k}\right)} \bar{b}_{j}$</li>
<li>$\alpha_{j}:=\sum_{i=1}^{\ell_{a}} \frac{\exp \left(e_{i j}\right)}{\sum_{k=1}^{\ell_{a}} \exp \left(e_{k j}\right)} \bar{a}_{i}$</li>
<li>$\beta_{i}$其实就是所有b中的词对于句子a中i位置的加权求和，权重就是之前计算的attention matrix</li>
</ul>
</li>
</ul>
</li>
<li>Compare：该模块的功能主要是对加权后的一个句子与另一个原始句子进行比较<ul>
<li>$\mathbf{v}_{1, i}:=G\left(\left[\bar{a}_{i}, \beta_{i}\right]\right) \quad \forall i \in\left[1, \ldots, \ell_{a}\right]$</li>
<li>$\mathbf{v}_{2, j}:=G\left(\left[\bar{b}_{j}, \alpha_{j}\right]\right) \quad \forall j \in\left[1, \ldots, \ell_{b}\right]$</li>
<li>这里[.,.]表示向量拼接，G()是一个前馈神经网络</li>
</ul>
</li>
<li>Aggregate<ul>
<li>先对两个句子集合求和：$\mathbf{v}_{1}=\sum_{i=1}^{\ell_{a}} \mathbf{v}_{1, i}$、$\mathbf{v}_{2}=\sum_{j=1}^{\ell_{b}} \mathbf{v}_{2, j}$</li>
<li>将求和的结果输入前馈神经网络做最后的分类：$\hat{\mathbf{y}}=H\left(\left[\mathbf{v}_{1}, \mathbf{v}_{2}\right]\right)$</li>
<li>用多分类的交叉熵作为损失函数：$L\left(\theta_{F}, \theta_{G}, \theta_{H}\right)=\frac{1}{N} \sum_{n=1}^{N} \sum_{c=1}^{C} y_{c}^{(n)} \log \frac{\exp \left(\hat{y}_{c}\right)}{\sum_{c^{\prime}=1}^{C} \exp \left(\hat{y}_{c^{\prime}}\right)}$</li>
</ul>
</li>
</ul>
<p>代码参考：<a href="https://github.com/martbert/decomp_attn_keras" target="_blank" rel="noopener">https://github.com/martbert/decomp_attn_keras</a></p>
]]></content>
      <tags>
        <tag>注意力机制</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：《A Contextual-Bandit Approach to Personalized News Article Recommendation》</title>
    <url>/2020/11/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E3%80%8AA-Contextual-Bandit-Approach-to-Personalized-News-Article-Recommendation%E3%80%8B/</url>
    <content><![CDATA[<p>这篇文章属于推荐领域采用强化学习方法的文章。与其他推荐算法相比，基于强化学习的推荐算法更多地关注了explore/exploit问题，即探索/利用问题。也就是强调推荐算法不应该仅仅基于历史数据中用户的偏好进行推荐，而应该给用户更多的新鲜事物，引导和发掘用户的爱好。而强化学习恰恰是基于系统采取的动作所接收到的环境的反馈，来不断训练系统的。因此，在强化学习的过程中适度的选择一些探索性的动作，并观察反馈进一步调整后续的动过选择，天然地具有解决该问题的潜质。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://zhuanlan.zhihu.com/p/34940176" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/34940176</a><br><a href="https://zhuanlan.zhihu.com/p/129336798" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/129336798</a><br><a href="https://www.yuque.com/el1s/paper-reading/vai69h" target="_blank" rel="noopener">https://www.yuque.com/el1s/paper-reading/vai69h</a><br><a href="https://mp.weixin.qq.com/s/boUy1h_SuiMohLBu1Tatkg" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/boUy1h_SuiMohLBu1Tatkg</a><br><a href="https://zhuanlan.zhihu.com/p/162804829" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/162804829</a></p>
</blockquote>
<p>论文中把文章推荐看作是多臂老虎机问题，一种有效的做法是根据用户和文章的上下文信息，选择文章推荐给用户，同时基于用户对文章的点击行为动态调整策略，追求点击次数最大化。实验证明加入上下文信息的Bandit算法比传统的Bandit算法CTR提升在12.5%以上，如果是基于稀疏的数据，效果会变得更好。</p>
<p>论文分析了已有的Bandit算法，包括UCB、E-Greedy、Thompson Smapling，然后提出了LinUCB算法，LinUCB分为两种：</p>
<ul>
<li>简单的线性不相交模型 disjoint LinUCB</li>
<li>混合相交的线性模型 hybrid LinUCB</li>
</ul>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>人生中有很多选择问题，当每天中午吃饭的时候，需要选择吃饭的餐馆，那么就面临一个选择，是选择熟悉的好吃的餐馆呢，还是冒风险选择一个没有尝试过的餐馆呢。同样的，推荐系统处处也面临着这样的选择，是推荐一个已经熟悉的点击率很高的物品呢，还是选择一个新的物品呢。这些都可以泛化成一个经典问题，多臂老虎机问题，也是一个研究很广的问题。</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/bandit/DZTIKX87kwnUnXlY.png%21thumbnail%3FfileGuid%3DRR3Vv6gjtQwp9kcW.png" alt="图片"></p>
<p>下面介绍一些常用的bandit算法。</p>
<h2 id="Epsilon-Greedy-算法"><a href="#Epsilon-Greedy-算法" class="headerlink" title="Epsilon-Greedy 算法"></a>Epsilon-Greedy 算法</h2><ul>
<li>选一个 (0,1) 之间较小的数作为 epsilon</li>
<li>每次以概率 epsilon 做一件事：所有臂中随机选一个</li>
<li>每次以概率 1-epsilon 选择截止到当前，平均收益最大的那个臂。</li>
</ul>
<p>算法存在的缺点：</p>
<ul>
<li>算法中需要人工调的参数比较多，比如epsilon控制explore比例，还有对于一个物品探索到那个界限K才会让利用起来比较置信</li>
<li>没有利用历史的信息，完全有可能探索我们已知的不好的行动</li>
</ul>
<h2 id="汤普森采样"><a href="#汤普森采样" class="headerlink" title="汤普森采样"></a>汤普森采样</h2><ul>
<li>假设每个臂是否产生收益，其背后有一个概率分布，产生收益的概率为 p。</li>
<li>我们不断地试验，去估计出一个置信度较高的 “概率 p 的概率分布” 就能近似解决这个问题了。</li>
<li>怎么能估计 “概率 p 的概率分布” 呢？ 答案是假设概率 p 的概率分布符合 beta(wins, lose)分布，它有两个参数: wins, lose。</li>
<li>每个臂都维护一个 beta 分布的参数。每次试验后，选中一个臂，摇一下，有收益则该臂的 wins 增加 1，否则该臂的 lose 增加 1。</li>
<li>每次选择臂的方式是：用每个臂现有的 beta 分布产生一个随机数 b，选择所有臂产生的随机数中最大的那个臂去摇。</li>
</ul>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/bandit/YTaQcWZ7tRyeCOeM.png%21thumbnail%3FfileGuid%3DRR3Vv6gjtQwp9kcW.png" alt="图片"></p>
<ul>
<li>注：<ul>
<li>beta分布中的两个参数可以根据先验知识去设置，比如物料平均点击率3%， 那么我可以设置beta分布参数α=3，β=97，这样更贴近真实情况，可能更有效</li>
<li>为什么可以这样做的理解：<ul>
<li>伯努利分布（二项分布）:$P\{X=k\}=\left(\begin{array}{c}<br>n \\<br>k<br>\end{array}\right) p^{k}(1-p)^{n-k}$</li>
<li>Beta分布：$f(x ; \alpha, \beta)=\frac{x^{\alpha-1}(1-x)^{\beta-1}}{\int_{0}^{1} u^{\alpha-1}(1-u)^{\beta-1} d u}=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha) \Gamma(\beta)} x^{\alpha-1}(1-x)^{\beta-1}=\frac{1}{B(\alpha, \beta)} x^{\alpha-1}(1-x)^{\beta-1}$</li>
<li>贝叶斯更新：$P(\theta \mid x)=\frac{P(x \mid \theta) P(\theta)}{P(x)}$，先验分布 * 最大似然 = 后验分布。<ul>
<li>对随机变量X，假设服从参数为theta1的分布Xi~P1(theta1)，且theta1~P2(theta2)。</li>
<li>当对X抽样得到似然概率后，可以用似然和theta1的先验分布计算出theta1的后验分布</li>
</ul>
</li>
<li>如果P1和P2属于同一种分布，则称P2是P1的共轭先验分布</li>
<li>Beta分布是伯努利分布的共轭先验</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="UCB算法"><a href="#UCB算法" class="headerlink" title="UCB算法"></a>UCB算法</h2><ul>
<li><p>初始化：先对每一个臂都试一遍</p>
</li>
<li><p>按照如下公式计算每个臂的分数，然后选择分数最大的臂作为选择：$U C B_{j}=\bar{x}_{j}(t)+\sqrt{\frac{2 l n t}{T_{j, t}}}$</p>
</li>
<li><p>观察选择结果，更新t和Tjt。</p>
<ul>
<li>加号前面是这个臂到目前的收益均值，后面的叫做 bonus，本质上是均值的标准差，t 是目前的试验次数，Tj,t是这个臂被试次数。</li>
</ul>
</li>
<li><p>注：</p>
<ul>
<li><p>公式前面一项是收益均值，可以理解为点击率</p>
</li>
<li><p>核心是第二项，我们知道当x取值大于1时ln函数的作用是平滑，如果实验次数很多，那么后一项就会趋近很小，从而收敛到前一项收益均值，如果是一个新物料或者explore次数很少，那么后一项很大甚至大于1从而优先进行explore，支持探索到这个臂到达置信的次数。</p>
</li>
<li><p>UCB其实是一个算法框架，即每次选择动作的时候，采用乐观态度去衡量每个动作的预期收益。即预期收益有一个置信区间，以置信区间的上限作为动作的预期收益进行比较选择。所谓置信区间，其本质就是偏离预期收益(历史上的每次选择该动作的收益的均值)某个范围（例如，95%）的偏差。而这个偏差会随着选择该动作次数的增加，而逐渐收缩，即置信区间的上限会减小。也就是该动作被选择次数很多后，其预期收益会趋于稳定在一个范围（减小了噪音等因素引起的偏差）。可以被视为，该动作没有潜力了。如果该动作稳定在一个高收益，则仍会被选择，而如果该动作稳定在一个低收益，则又没有潜力，则不会再被选择。因此，UCB这种以置信区间上限作为选择动作依据的方式，自然而然地对探索的方向进行调整，使其偏向于高收益或有潜力的动作。</p>
</li>
<li><p>公式推导：</p>
<ul>
<li><p>观测到行动a的奖励的均值Qt(a)，假设奖励的真实值变化范围在[Qt(a)-delta, Q(t)+Ut(a)]之间，我们乐观的认为行动a的奖励是Qt(a)+Ut(a)，每一次都按这个值来选择行动a，其中Ut(a)是关于Nt(a)的函数，Nt(a)越大，Ut(a)越小：$N_{t}(a)=\sum_{i=1}^{t} 1\left[a_{i}=a\right]$。Ut(a)总是采取最贪心的行动：$a_{t}^{U C B}=\operatorname{argmax}_{a \in \mathcal{A}} \hat{Q}_{t}(a)+\hat{U}_{t}(a)$</p>
</li>
<li><p>Hoeffding’s Inequality：不对数据分布进行任何假设，设X1, …, Xt为独立同分布的随机变量，他们的值在[0,1]之间，样本平均值为$\overline{X_{t}}=\frac{1}{t} \sum_{i}^{t} \mathrm{X}_{i}$，对于$\delta&gt;0$以下不等式总是成立：$P\left\{\left|E[X]-\overline{X_{t}}\right| \leq \delta\right\} \geq 1-2 e^{-2 t \delta^{2}}$</p>
<ul>
<li>当$\delta$的取值为$\sqrt{\frac{2 \ln N_{t}(a)}{t}}$时，$2 e^{-2 t \delta^{2}}=\frac{2}{N_{t}(a)^{4}}$即</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>$\overline{X_{t}}-\sqrt{\frac{2 \ln N_{t}(a)}{t}} \leq E[X] \leq \overline{X_{t}}+\sqrt{\frac{2 \ln N_{t}(a)}{t}}$是以$1-\frac{2}{N_{t}(a)^{4}}$的概率成立的：</p>
<pre><code>  * 当$N_{t}(a)=2$时，成立的概率为0.88
</code></pre><ul>
<li><p>当$N_{t}(a)=3$时，成立的概率为0.98</p>
<pre><code>* 当$N_{t}(a)=4$时，成立的概率为0.99
</code></pre><ul>
<li>根据Hoeffding’s Inequality：$a_{t}^{U C B}=\arg \max _{a \in \mathcal{A}} \hat{Q}_{t}(a)+\sqrt{\frac{2 \log t}{N_{t}(a)}}$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="本文贡献"><a href="#本文贡献" class="headerlink" title="本文贡献"></a>本文贡献</h1><h2 id="LinUCB-with-Disjoint-Linear-Model"><a href="#LinUCB-with-Disjoint-Linear-Model" class="headerlink" title="LinUCB with Disjoint Linear Model"></a>LinUCB with Disjoint Linear Model</h2><p>本文正是在UCB框架下进行的研究工作，即在衡量每个动作预期收益时，引入了个性化因素——上下文环境，使每个动作的预期收益与上下文环境特征有相关性。相比于普通UCB，本文提出了一种LinUCB的方法。LinUCB改变的是预期收益（均值）这一部分。论文给出了下面的公式：</p>
<ul>
<li>(1): $R_{\mathrm{A}}(T) \quad \stackrel{\text { def }}{=} \quad \mathbf{E}\left[\sum_{t=1}^{T} r_{t, a_{t}^{*}}\right]-\mathbf{E}\left[\sum_{t=1}^{T} r_{t, a_{t}}\right]$</li>
<li>(2): $\mathbf{E}\left[r_{t, a} \mid \mathbf{x}_{t, a}\right]=\mathbf{x}_{t, a}^{\top} \boldsymbol{\theta}_{a}^{*}$</li>
</ul>
<p>通常UCB在预期收益部分采用的是（1）式左边的E值，通常是历史数据统计出来的收益均值。而本文LinUCB采用（2）式的线性回归模型对这部分进行了拟合，使得（1）式的差最小化。即（1）式左边的E值为Label，右边的E值为线性回归的预测值，RA(T)可看作是Loss。（2）式中的X为与用户和物品（arm）相关的特征值向量，里面可能包括了用户的年龄、性别以及新闻的推荐位置，类型等。而$\theta_{a}^{*}$则是待训练的参数。</p>
<p>使用岭回归作为loss，通过岭回归公式，可以给出theta的计算公式：$\hat{\boldsymbol{\theta}}_{a}=\left(\mathbf{D}_{a}^{\top} \mathbf{D}_{a}+\mathbf{I}_{d}\right)^{-1} \mathbf{D}_{a}^{\top} \mathbf{c}_{a}$其中Da为m*d的矩阵，包含m个训练数据$x_{t, a}$，$c_a$为m维的向量，每一个值为m个训练数据对应的行动产生的相应奖励值。</p>
<p>这种回归拟合粗看似乎跟强化学习没有什么关系，因为已经知道了历史数据统计出来的预期收益，直接就能训练出参数$\theta_{a}^{*}$，那么选择下一个动作（推荐哪个新闻），直接就比较预测值就可以了。然而，在采取动作以后，还是要收集反馈（用户点击情况），并纳入历史数据中，以更新预期收益的值，并用于下一次动作的选择。这是强化学习与普通的监督学习最大的区别，即根据环境的反馈进行学习，而没有固定的样本数据和对应的标签值。而（1）式中左边的E作为Label，这个Label其实就是环境的反馈。</p>
<p>在将线性回归与MAB结合后，没有解决探索/利用问题，即此时选择下一个动作完全是根据历史最优，选择了历史看来收益最好的动作。因此，论文采用了UCB的架构，利用下面的公式（另一篇论文Exploring compact reinforcement-learning representations with linear regression，这里将其作为一个定理使用）：$\left|\mathbf{x}_{t, a}^{\top} \hat{\boldsymbol{\theta}}_{a}-\mathbf{E}\left[r_{t, a} \mid \mathbf{x}_{t, a}\right]\right| \leq \alpha \sqrt{\mathbf{x}_{t, a}^{\top}\left(\mathbf{D}_{a}^{\top} \mathbf{D}_{a}+\mathbf{I}_{d}\right)^{-1} \mathbf{x}_{t, a}}$构造了UCB的置信区间上限。在此基础上，给出了下面的动作选择依据公式：$a_{t} \stackrel{\text { def }}{=} \arg \max _{a \in \mathcal{A}_{t}}\left(\mathbf{x}_{t, a}^{\top} \hat{\boldsymbol{\theta}}_{a}+\alpha \sqrt{\mathbf{x}_{t, a}^{\top} \mathbf{A}_{a}^{-1} \mathbf{x}_{t, a}}\right)$，其中$\alpha=1+\sqrt{\ln (2 / \delta) / 2}$，$\mathbf{A}_{a} \stackrel{\text { def }}{=} \mathbf{D}_{a}^{\top} \mathbf{D}_{a}+\mathbf{I}_{d}$也即是，选择括号内部分得分最大的那个arm进行执行。括号内左边就是回归拟合的预期收益，右边是置信区间的上限。</p>
<p>在这个基本思想下，算法的伪代码可以写成：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/bandit/3.png" alt="图片"></p>
<h2 id="LinUCB-with-Hybrid-Linear-Model"><a href="#LinUCB-with-Hybrid-Linear-Model" class="headerlink" title="LinUCB with Hybrid Linear Model"></a>LinUCB with Hybrid Linear Model</h2><p>前面讲的是LinUCB的一种实现方式，对于每个臂维护theta参数。当我们希望解决共享类context的需求时，即一个用户可能有偏好，只喜欢体育，那么这个参数就应该被所有待推荐给他的文章池（arm池）所共享。即在t时刻为该用户推荐文章时，所有待选文章（arm）都要用到这个context。因此，作者引入了下面的公式：$\mathbf{E}\left[r_{t, a} \mid \mathbf{x}_{t, a}\right]=\mathbf{z}_{t, a}^{\top} \boldsymbol{\beta}^{<em>}+\mathbf{x}_{t, a}^{\top} \boldsymbol{\theta}_{a}^{</em>}$。其中z为共享类context（当前用户和文章的共享类特征的组合向量），而$\beta$为待学习的权重，可以看到它没有下标，即所有arm（所有待选文章）都只学习且采用这一个权重，从而实现共享类context的信息交互。与之相对的是权重$\theta_{a}$，每个arm都会学到一个，各arm之间不会共享context的信息。</p>
<p>在这个思想下，算法的伪代码可以写成：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/bandit/4.png" alt="图片"></p>
<h2 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a>评估方法</h2><p>由于是探测问题，如果不上线，就没办法探到算法希望的内容。有点像鸡生蛋、蛋生鸡的问题。也可以看作强化学习里的“off-policy evaluation problem”问题。一个方式是根据离线数据构造一个模拟器，但是这种模拟器会引入bias。论文提出了一个没有bias的简单实现：</p>
<ul>
<li>假设有若干未知的分布D服从独立同分布$\left(x_{1}, \ldots, x_{K}, r_{1}, \ldots, r_{K}\right)$。每个分布包含了所有arm的可观测特征向量、隐含的奖励</li>
<li>假设要处理现实世界中，根据日志策略的交互结果生成的一系列事件：<ul>
<li>每个事件包含了上下文特征向量$x_{1}, \ldots, x_{K}$，一个选择的arm a，以及观察到的奖励ra。</li>
<li>简单起见，我们把实验的事件流看作是无限的，但是实验所需的事件是有上限的。</li>
<li>目标是利用这份数据来评估一个bandit算法$\pi$。形式上$\pi$是从历史t-1个事件$h_{t-1}$以及当前的上下文特征$x_{1}, \ldots, x_{K}$到当前时刻t选择arm at的映射。</li>
</ul>
</li>
<li>论文提出的评估策略如下：<img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/bandit/5.png" alt="图片"></li>
<li>把策略$\pi$和一系列事件T作为输入。然后逐一考察日志事件。如果，给定当前历史$h_{t-1}$，如果策略$\pi$和日志策略选择同一个arm，那么保留这个事件，也就是把它加到历史中，更新总体奖励Rt；否则，如果策略$\pi$和日志策略选择的arm不一样，则整个事件被忽略，也不更新状态。</li>
<li>日志策略选择arm的策略是uniformly at random，所以每个事件被算法保留的概率是$\frac{1}{K}$并且各自是独立的。</li>
<li>也就是说，被D选择并保留的事件有相同的分布。</li>
</ul>
<h1 id="实验细节"><a href="#实验细节" class="headerlink" title="实验细节"></a>实验细节</h1><p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/bandit/6.png" alt="图片"></p>
<h2 id="数据收集"><a href="#数据收集" class="headerlink" title="数据收集"></a>数据收集</h2><ul>
<li>随机抽取用户</li>
<li>曝光的文章是从内容池中随机选取的</li>
<li>为了避免曝光位置bias，我们只考察F1位置的文章</li>
<li>每个用户交互事件包含三部分内容<ul>
<li>曝光的文章</li>
<li>用户、文章信息</li>
<li>是否点击</li>
</ul>
</li>
<li>选了5月1日470万事件作为训练集，5月3日～9日的3600万个事件作为验证集</li>
</ul>
<h2 id="特征构造"><a href="#特征构造" class="headerlink" title="特征构造"></a>特征构造</h2><ul>
<li>用“支持度”来选择特征，支持度指的是包含这个特征的用户占比。为了减少噪声，选择支持度大于0.1的特征。</li>
<li>用户特征的向量超过1000维。包括（且只包括）：<ul>
<li>人口属性学：性别（2种）、年龄（离散化为10段）</li>
<li>位置信息：美国200个大城市</li>
<li>行为类型： 1000种行为标签</li>
</ul>
</li>
<li>类似的，文章通常用100种类目特征向量来表示：<ul>
<li>URL种类：10种类型</li>
<li>编辑类型：人工标注的数十种文章类型</li>
</ul>
</li>
<li>用论文Personalized recommendation on dynamic content using predictive bilinear models中的编码方式，将特征向量统一长度。同时，我们会为每个特征向量增加一个常数特征1。这样，每个文章和user被表示成83维和1193维的向量</li>
<li>为了降维和获取非线性特征，我们用08年9月份的数据进行联合分析，然后利用论文A case study of behavior-driven conjoint analysis on Yahoo!中的降维方法。<ul>
<li>把用户特征投射到文章分类上，然后根据用户兴趣聚类。<ul>
<li>首先训练一个CTR预估模型：$\varphi_{u}^{\top} W \varphi_{a}$算出的是用户u对文章a的点击率。$\varphi_{u}, \varphi_{a}$分别表示用户和文章的特征向量。W是LR学到的权重矩阵</li>
<li>原始的用户特征可以用$\psi_{u}=\varphi^{\top} W$，$\psi_{u}$中的第i维表示用户u对文章i的兴趣度。然后用K-means把用户在$\psi_{u}$空间中聚成5类。最终的用户特征是一个6维向量，前五个维是对应5个聚类的相应（用高斯核计算，然后归一化，相加为1）、第六维是常数1。</li>
</ul>
</li>
<li>每个文章a有个单独的6维向量$x_{t, a}$，类似用户特征计算方法。因为文章特征是不重合的，他们是属于disjoint模型特征。</li>
<li>对于每个文章a，我们把它的6维向量和用户的6维向量做外积，得到6*6=36维特征，表示成$z_{t, a} \in R^{36}$，代表用户和文章的交叉信息，属于hybrid模型特征。</li>
</ul>
</li>
</ul>
<h2 id="算法比较"><a href="#算法比较" class="headerlink" title="算法比较"></a>算法比较</h2><p>本文对比了如下算法的性能差异：</p>
<ul>
<li>random：从内容池中随机选取一个，每个内容选中的概率相同。</li>
<li>Epsilon-Greedy</li>
<li>UCB</li>
<li>disjoint LinUCB</li>
<li>Hybrid LinUCB</li>
</ul>
<h2 id="学习桶和部署桶"><a href="#学习桶和部署桶" class="headerlink" title="学习桶和部署桶"></a>学习桶和部署桶</h2><p>在线上执行时，虽然线性拟合和统计等步骤计算量不大，但是用户和待选文章的数量增大后，仍然是比较大的代价。因此，论文给出了学习桶和部署桶的概念，即利用学习桶更新参数，而在部署桶中利用学习桶得到的参数进行推荐（动作的选择）。</p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>处于保密原因，使用相对的CTR，也就是算法的CTR除以随机策略的。假设随机策略的CTR等于1。论文中直接用CTR代替“相对CTR”。</p>
<p>由于部署集的数据量大于训练集，所以，部署集的CTR更重要一些。但是学习集中的高CTR意味着更快的学习率（或者等价的，更小的regret）。所以论文把训练集和部署集中的CTR都列出来了。</p>
<h3 id="训练集数据"><a href="#训练集数据" class="headerlink" title="训练集数据"></a>训练集数据</h3><p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/bandit/7.png" alt="图片"></p>
<ul>
<li>每个对比的算法（除random和无所不知算法外）都有一个参数：如Epsilon-Greedy中的$\epsilon$和UCB算法中的$\alpha$</li>
<li>学习集中的CTR都是呈倒U型。当参数较小时，探索不充分，算法不能发现CTR较好的文章，导致点击量较低。同理，当参数很大时，探索过度，导致浪费了流量在CTR较低的文章上。论文为每种算法选择最合适的参数，在下一节的测试数据上跑一遍。</li>
<li>相比于没有上下文特征的Epsilon-Greedy算法，UCB算法，热启动算法Epsilon-Greedy(warm)和UCB(warm)在最好的参数时，可以击败无所不知算法。但是热启动算法的稳定性不如在线算法，因为热启动算法都是离线用的完全随机数据训练的。Epsilon-Greedy算法在$\epsilon$接近1的时候性能比较稳定。热启动可以帮助ucb（warm）为用户选择更多吸引力的文章，但是不能保证线上的效果。因为ucb（warm）依赖于探测的置信区间，但是不太容易纠正热启动的初始偏置。因此，论文没有在验证集上跑热启动算法。</li>
<li>在部署集上，Epsilon-Greedy和UCB算法的效果差不多，但是在训练集上的效果差一些，跟上下文无关的算法表现一致。</li>
<li>为了验证数据稀疏性对模型的影响，论文减少了训练集的样本数量，30%、20%、10%、5%、1%等，部署集样本量不变。</li>
</ul>
<h3 id="验证集数据"><a href="#验证集数据" class="headerlink" title="验证集数据"></a>验证集数据</h3><p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/bandit/8.png" alt="图片"></p>
<h4 id="特征对结果的影响"><a href="#特征对结果的影响" class="headerlink" title="特征对结果的影响"></a>特征对结果的影响</h4><p>从上表中明显看出，无论是 Epsilon-Greedy （seg/disjoint/hybrid）还是UCB（ucb（seg）、linucb（disjoint/hybrid））都能使CTR提升10%左右。</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/bandit/9.png" alt="图片"></p>
<p>为了更好的可视化特征的效果，上图描述的是对比base CTR是如何提升的。base CTR是描述的文章是否被随机用户感兴趣。提升的比例越高，说明算法把文章推给了潜在感兴趣的用户。</p>
<ul>
<li>(a）说明没有用上下文特征的算法也能提升CTR。其他三张图更明显的表明了个性化的效果。极端的例子，3（c）中最高的那个点代表的文章，CTR从1.31到3.03，提升了132%</li>
</ul>
<h4 id="样本大小对结果的影响"><a href="#样本大小对结果的影响" class="headerlink" title="样本大小对结果的影响"></a>样本大小对结果的影响</h4><p>论文减少了训练集的样本数量，30%、20%、10%、5%、1%等，来模拟一个较大的内容池，但是流量固定的情况。</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/bandit/10.png" alt="图片"></p>
<p>上图中描述了不同数据量对CTR的影响：</p>
<ul>
<li>在所有数量级中，特征仍然是有作用的。</li>
<li>在部署集上，ucb比Epsilon-Greedy算法的效果普遍要好，在1%流量时更明显。</li>
<li>相比与 ucb (seg) 和 linucb (disjoint)，linucb（hybrid）在数据量小时优势更明显。原因是混合模型的特征是共享的，CTR的信息可以从一个文章传递到另一个文章，当内容池越大的时候，这个优势越明显。相反，在独立模型中，没法传递这种信息。</li>
<li>图4（a）描述了迁移学习在稀疏数据中的作用。对比ucb（seg）和linucb（disjoint）：从图4（a）中看出，这两个算法的效果差不多。我们认为这不是巧合。是用户聚类的作用。</li>
<li>图5描述的是用户最近临（非常数的5个维度）的聚类的直方图。可以看出，大部分用户非常接近某一个聚类，比如85%的用户大于0.5，40%的用户大于0.8。</li>
</ul>
]]></content>
      <tags>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：《Ad Click Prediction: a View from the Trenches》</title>
    <url>/2020/08/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E3%80%8AAd-Click-Prediction-a-View-from-the-Trenches%E3%80%8B/</url>
    <content><![CDATA[<p>现在做在线学习和CTR常常会用到逻辑回归，而传统的批量算法无法有效地处理超大规模的数据集和在线数据流，google在2010年～2013年从理论研究到实际工程化实现的FTRL算法，在处理诸如逻辑回归之类的带非光滑正则化项（例如L1范数，做模型复杂度控制和稀疏化）的凸优化问题上性能非常出色。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://zhuanlan.zhihu.com/p/32903540" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32903540</a><br><a href="https://zhuanlan.zhihu.com/p/61724627" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/61724627</a></p>
</blockquote>
<p>Online Learning的学习算法主要有两类：在线凸优化和在线Bayesian。在线凸优化方法有很多，像FOBOS算法、RDA、FTRL等；在线Bayesian 方面有比如AdPredictor 算法、基于内容的在线矩阵分解算法等。本文是Google在2013年KDD上发表的论文，描述了FTRL-Proximal算法的工程化的实践。国内外公司纷纷进行了各自的实验，比如亚马逊，Yahoo，阿里，百度等也应用了该算法在其搜索领域、推荐领域等的产品中，且都取得了明显提升。另外在各大CTR竞赛，Kaggle比赛上时常能看到它的身影。</p>
<h1 id="先验知识"><a href="#先验知识" class="headerlink" title="先验知识"></a>先验知识</h1><h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><blockquote>
<p>参考：<br><a href="https://zhuanlan.zhihu.com/p/44591359" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/44591359</a></p>
</blockquote>
<h3 id="模型表达式"><a href="#模型表达式" class="headerlink" title="模型表达式"></a>模型表达式</h3><p>$y=\sigma(f(\boldsymbol{x}))=\sigma\left(\boldsymbol{w}^{T} \boldsymbol{x}\right)=\frac{1}{1+e^{-\boldsymbol{w}^{T} \boldsymbol{x}}}$</p>
<p>假设我们已经训练好了一组权值$w^T$ 。只要把我们需要预测的 x 代入到上面的方程，输出的y值就是这个标签为A的概率，我们就能够判断输入数据是属于哪个类别。</p>
<h3 id="模型损失函数"><a href="#模型损失函数" class="headerlink" title="模型损失函数"></a>模型损失函数</h3><p>我们把单个样本看做一个事件，那么这个事件发生的概率就是：$P(y \mid \boldsymbol{x})=\left\{\begin{array}{r}<br>p, y=1 \\<br>1-p, y=0<br>\end{array}\right.$</p>
<p>这个函数不方便计算，它等价于：$P\left(y_{i} \mid \boldsymbol{x}_{i}\right)=p^{y_{i}}(1-p)^{1-y_{i}}$。如果我们采集到了一组数据一共N个，即$\left\{\left(\boldsymbol{x}_{1}, y_{1}\right),\left(\boldsymbol{x}_{2}, y_{2}\right),\left(\boldsymbol{x}_{3}, y_{3}\right) \ldots\left(\boldsymbol{x}_{N}, y_{N}\right)\right\}$，这个合事件发生的总概率为：$\begin{aligned}<br>P_{\text {总 }} &amp;=P\left(y_{1} \mid \boldsymbol{x}_{1}\right) P\left(y_{2} \mid \boldsymbol{x}_{2}\right) P\left(y_{3} \mid \boldsymbol{x}_{3}\right) \ldots P\left(y_{N} \mid \boldsymbol{x}_{N}\right) \\<br>&amp;=\prod_{n=1}^{N} p^{y_{n}}(1-p)^{1-y_{n}}<br>\end{aligned}$</p>
<p>我们通过两边取对数来把连乘变成连加的形式：</p>
<p>$\begin{aligned}<br>F(\boldsymbol{w})=\ln \left(P_{\text {总 }}\right) &amp;=\ln \left(\prod_{n=1}^{N} p^{y_{n}}(1-p)^{1-y_{n}}\right) \\<br>&amp;=\sum_{n=1}^{N} \ln \left(p^{y_{n}}(1-p)^{1-y_{n}}\right) \\<br>&amp;=\sum_{n=1}^{N}\left(y_{n} \ln (p)+\left(1-y_{n}\right) \ln (1-p)\right)<br>\end{aligned}$</p>
<p>其中$p=\frac{1}{1+e^{-\boldsymbol{w}^{T} \boldsymbol{x}}}$。这个函数又叫做它的损失函数。损失函数可以理解成衡量我们当前的模型的输出结果，跟实际的输出结果之间的差距的一种函数。这里的损失函数的值等于事件发生的总概率，我们希望它越大越好。但是跟损失的含义有点儿违背，因此也可以在前面取个负号。</p>
<h3 id="最大似然估计MLE"><a href="#最大似然估计MLE" class="headerlink" title="最大似然估计MLE"></a>最大似然估计MLE</h3><p>我们知道损失函数 F(w) 是正比于总概率 P_总 的，而 F(w) 又只有一个变量 w 。也就是说，通过改变 w 的值，就能得到不同的总概率值 P_总 。那么当我们选取的某个 $w^<em>$ 刚好使得总概率 P_总 取得最大值的时候。我们就认为这个 $w^</em>$ 就是我们要求得的 w 的值，这就是最大似然估计的思想，即：$\boldsymbol{w}^{*}=\arg \max _{w} F(\boldsymbol{w})=-\arg \min _{w} F(\boldsymbol{w})$</p>
<p>下面推导一下F(w)的梯度：</p>
<ul>
<li><p>计算p的导数：</p>
<p>$\begin{aligned}<br>p^{\prime}=f^{\prime}(\boldsymbol{w}) &amp;=\left(\frac{1}{1+e^{-\boldsymbol{w}^{T} \boldsymbol{x}}}\right)^{\prime} \\<br>&amp;=-\frac{1}{\left(1+e^{-\boldsymbol{w}^{T} \boldsymbol{x}}\right)^{2}} \cdot\left(1+e^{-\boldsymbol{w}^{T} \boldsymbol{x}}\right)^{\prime} \\<br>&amp;=-\frac{1}{\left(1+e^{-\boldsymbol{w}^{T} \boldsymbol{x}}\right)^{2}} \cdot e^{-\boldsymbol{w}^{T} \boldsymbol{x}} \cdot\left(-\boldsymbol{w}^{T} \boldsymbol{x}\right)^{\prime} \\<br>&amp;=-\frac{1}{\left(1+e^{-\boldsymbol{w}^{T} \boldsymbol{x}}\right)^{2}} \cdot e^{-\boldsymbol{w}^{T} \boldsymbol{x}} \cdot(-\boldsymbol{x}) \\<br>&amp;=\frac{e^{-\boldsymbol{w}^{T} \boldsymbol{x}}}{\left(1+e^{-\boldsymbol{w}^{T} \boldsymbol{x}}\right)^{2}} \cdot \boldsymbol{x} \\<br>&amp;=\frac{1}{1+e^{-\boldsymbol{w}^{T} \boldsymbol{x}}} \cdot \frac{e^{-\boldsymbol{w}^{T} \boldsymbol{x}}}{1+e^{-\boldsymbol{w}^{T} \boldsymbol{x}}} \cdot \boldsymbol{x} \\<br>&amp;=p(1-p) \boldsymbol{x}<br>\end{aligned}$</p>
</li>
<li><p>计算F(w)的导数：</p>
</li>
</ul>
<p>$\begin{aligned}<br>\nabla F(\boldsymbol{w}) &amp;=\nabla\left(\sum_{n=1}^{N}\left(y_{n} \ln (p)+\left(1-y_{n}\right) \ln (1-p)\right)\right) \\<br>&amp;=\sum\left(y_{n} \ln ^{\prime}(p)+\left(1-y_{n}\right) l n^{\prime}(1-p)\right) \\<br>&amp;=\sum\left(\left(y_{n} \frac{1}{p} p^{\prime}\right)+\left(1-y_{n}\right) \frac{1}{1-p}(1-p)^{\prime}\right) \\<br>&amp;=\sum_{N}\left(y_{n}(1-p) \boldsymbol{x}_{n}-\left(1-y_{n}\right) p \boldsymbol{x}_{n}\right) \\<br>&amp;=\sum_{n=1}^{N}\left(y_{n}-p\right) \boldsymbol{x}_{n}<br>\end{aligned}$</p>
<p>$\nabla F(\boldsymbol{w})=\sum_{n=1}^{N}\left(y_{n}-\frac{1}{1+e^{-\boldsymbol{w}^{T} \boldsymbol{x}_{n}}}\right) \boldsymbol{x}_{n}$</p>
<h3 id="参数求解"><a href="#参数求解" class="headerlink" title="参数求解"></a>参数求解</h3><h4 id="梯度下降法（GD）"><a href="#梯度下降法（GD）" class="headerlink" title="梯度下降法（GD）"></a>梯度下降法（GD）</h4><p>随便初始化一个$w_0$，然后给定一个步长$\eta$，通过不断地修改$\boldsymbol{w}_{t+1}&lt;-\boldsymbol{w}_{t}$，从而最后靠近到达取得最大值的点，即不断进行下面的迭代过程，直到达到指定次数，或者梯度等于0为止：$\boldsymbol{w}_{t+1}=\boldsymbol{w}_{t}+\eta \nabla F \quad(\boldsymbol{w})$</p>
<h4 id="随机梯度下降法（SGD）"><a href="#随机梯度下降法（SGD）" class="headerlink" title="随机梯度下降法（SGD）"></a>随机梯度下降法（SGD）</h4><p>如果我们能够在每次更新过程中，加入一点点噪声扰动，可能会更加快速地逼近最优值。在SGD中，我们不直接使用$\nabla F(w)$，而是采用另一个输出为随机变量的替代函数$G(w)$：$\boldsymbol{w}_{t+1}=\boldsymbol{w}_{t}+\eta G(\boldsymbol{w})$。</p>
<p>当然，这个替代函数G(w)需要满足它的期望值等于$\nabla F(\boldsymbol{w})$，相当于这个函数围绕着$\nabla F(\boldsymbol{w})$的输出值随机波动。</p>
<p>具体地，在SGD中，我们每次只要均匀地、随机选取其中一个样本(x_i, y_i)，即把它的值乘以N，就相当于获得了梯度的无偏估计值，即$E(G(\boldsymbol{w}))=\nabla F(\boldsymbol{w})$，因此SGD的更新公式为：$\boldsymbol{w}_{t+1}=\boldsymbol{w}_{t}+\eta N\left(y_{n}-\frac{1}{1+e^{-\boldsymbol{w}^{T} \boldsymbol{x}_{n}}}\right) \boldsymbol{x}_{n}$</p>
<p>这样我们前面的求和就没有了，同时$\eta N$都是常数，N的值刚好可以并入$\eta$当中,因此SGD的迭代更新公式为：$\boldsymbol{w}_{t+1}=\boldsymbol{w}_{t}+\eta\left(y_{n}-\frac{1}{1+e^{-\boldsymbol{w}^{T} \boldsymbol{x}_{n}}}\right) \boldsymbol{x}_{n}$</p>
<p>其中$(x_i, y_i)$是对所有样本随机抽样的一个结果。</p>
<h2 id="L1、L2正则化的直观解释"><a href="#L1、L2正则化的直观解释" class="headerlink" title="L1、L2正则化的直观解释"></a>L1、L2正则化的直观解释</h2><blockquote>
<p>参考：<br><a href="https://blog.csdn.net/red_stone1/article/details/80755144" target="_blank" rel="noopener">https://blog.csdn.net/red_stone1/article/details/80755144</a><br><a href="https://www.zhihu.com/question/37096933" target="_blank" rel="noopener">https://www.zhihu.com/question/37096933</a><br><a href="https://blog.csdn.net/xpy870663266/article/details/104573194" target="_blank" rel="noopener">https://blog.csdn.net/xpy870663266/article/details/104573194</a><br><a href="https://www.cnblogs.com/heguanyou/p/7688344.html" target="_blank" rel="noopener">https://www.cnblogs.com/heguanyou/p/7688344.html</a></p>
</blockquote>
<p>稀疏解指的是参数矩阵的大部分参数值为0，这样得到的模型相对较小。稀疏的特征会大大减少predict时的内存和复杂度。先说几个结论：</p>
<ul>
<li>L1正则化会产生更稀疏的解，因此基于L1正则化的学习方法相当于嵌入式的特征选择方法.</li>
<li>L2正则化计算更加方便，只需要计算向量内积，L1范数的计算效率特别是遇到非稀疏向量时非常低</li>
<li>L1正则化相当于给权重设置拉普拉斯先验，L2正则化相当于给权重设置高斯先验</li>
<li>L1倾向于学得稀疏的权重矩阵，L2倾向于学得更小更分散的权重</li>
</ul>
<h3 id="L2-正则化直观解释"><a href="#L2-正则化直观解释" class="headerlink" title="L2 正则化直观解释"></a>L2 正则化直观解释</h3><p>L2 正则化公式：$L=E_{i n}+\lambda \sum_{j} w_{j}^{2}$，这个式子是怎么得来的呢？我们知道，正则化的目的是限制参数过多或者过大，避免模型更加复杂。例如，使用多项式模型，如果使用 10 阶多项式，模型可能过于复杂，容易发生过拟合。所以，为了防止过拟合，我们可以将其高阶部分的权重 w 限制为 0，这样，就相当于从高阶的形式转换为低阶。</p>
<p>为了达到这一目的，最直观的方法就是限制 w 的个数，但是这类条件属于 NP-hard 问题，求解非常困难。所以，一般的做法是寻找更宽松的限定条件：$\sum_{j} w_{j}^{2} \leq C$。因此，我们的目标转换为：最小化训练样本误差 Ein，但是要遵循 w 平方和小于 C 的条件，用一张图表示如下：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/adclick/1.png" alt="图片"></p>
<p>蓝色椭圆区域是最小化 Ein 区域，红色圆圈是 w 的限定条件区域。在没有限定条件的情况下，一般使用梯度下降算法，在蓝色椭圆区域内会一直沿着 w 梯度的反方向前进，直到找到全局最优值 wlin。例如空间中有一点 w（图中紫色点），此时 w 会沿着 -∇Ein 的方向移动，如图中蓝色箭头所示。但是，由于存在限定条件，w 不能离开红色圆形区域，最多只能位于圆上边缘位置，沿着切线方向。w 的方向如图中红色箭头所示。</p>
<p>那么w 最终会在什么位置取得最优解呢？w 是沿着圆的切线方向运动，如上图绿色箭头所示。运动方向与 w 的方向（红色箭头方向）垂直。运动过程中，根据向量知识，只要 -∇Ein 与运行方向有夹角，不垂直，则表明 -∇Ein 仍会在 w 切线方向上产生分量，那么 w 就会继续运动，寻找下一步最优解。只有当 -∇Ein 与 w 的切线方向垂直时，-∇Ein在 w 的切线方向才没有分量，这时候 w 才会停止更新，到达最接近 wlin 的位置，且同时满足限定条件。</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/adclick/2.png" alt="图片"></p>
<p>-∇Ein 与 w 的切线方向垂直，即 -∇Ein 与 w 的方向平行。如上图所示，蓝色箭头和红色箭头互相平行。这样，根据平行关系得到：$\nabla E_{i n}+\lambda w=0$，这样，我们就把优化目标和限定条件整合在一个式子中了。也就是说只要在优化 Ein 的过程中满足上式，就能实现正则化目标。</p>
<p>根据最优化算法的思想：梯度为 0 的时候，函数取得最优值。已知 ∇Ein 是 Ein 的梯度，观察上式，λw 是否也能看成是某个表达式的梯度呢？当然可以！λw 可以看成是 1/2λw*w 的梯度：$\frac{\partial}{\partial w}\left(\frac{1}{2} \lambda w^{2}\right)=\lambda w$。这样，我们根据平行关系求得的公式，构造一个新的损失函数：$E_{a u g}=E_{i n}+\frac{\lambda}{2} w^{2}$</p>
<h3 id="L1-正则化直观解释"><a href="#L1-正则化直观解释" class="headerlink" title="L1 正则化直观解释"></a>L1 正则化直观解释</h3><p>L1 正则化公式也很简单，直接在原来的损失函数基础上加上权重参数的绝对值：$L=E_{i n}+\lambda \sum_{j}\left|w_{j}\right|$，我仍然用一张图来说明如何在 L1 正则化下，对 Ein 进行最小化的优化：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/adclick/3.png" alt="图片"></p>
<h3 id="L1-与-L2-解的稀疏性"><a href="#L1-与-L2-解的稀疏性" class="headerlink" title="L1 与 L2 解的稀疏性"></a>L1 与 L2 解的稀疏性</h3><h4 id="直观视角"><a href="#直观视角" class="headerlink" title="直观视角"></a>直观视角</h4><p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/adclick/4.png" alt="图片"></p>
<p>以二维情况讨论，上图左边是 L2 正则化，右边是 L1 正则化。从另一个方面来看，满足正则化条件，实际上是求解蓝色区域与黄色区域的交点，即同时满足限定条件和 Ein 最小化。对于 L2 来说，限定区域是圆，这样，得到的解 w1 或 w2 为 0 的概率很小，很大概率是非零的。</p>
<p>对于 L1 来说，限定区域是正方形，方形与蓝色区域相交的交点是顶点的概率很大，这从视觉和常识上来看是很容易理解的。也就是说，方形的凸点会更接近 Ein 最优解对应的 w 位置，而凸点处必有 w1 或 w2 为 0。这样，得到的解 w1 或 w2 为零的概率就很大了。所以，L1 正则化的解具有稀疏性。</p>
<p>扩展到高维，同样的道理，L2 的限定区域是平滑的，与中心点等距；而 L1 的限定区域是包含凸点的，尖锐的。这些凸点更接近 Ein 的最优解位置，而在这些凸点上，很多 wj 为 0。</p>
<h4 id="例子视角"><a href="#例子视角" class="headerlink" title="例子视角"></a>例子视角</h4><p>上面说的稍微有点抽象，我们以下图中一个具体的Loss函数为例：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/adclick/5.png" alt="图片"></p>
<p>则最优的 x 在绿点处，x 非零。现在施加 L2 regularization，新的损失函数$\left(L+C x^{2}\right)$如图中蓝线所示：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/adclick/6.png" alt="图片"></p>
<p>最优的 x 在黄点处，x 的绝对值减小了，但依然非零。而如果施加 L1 regularization，则新的损失函数$(L+C|x|)$如图中粉线所示：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/adclick/7.png" alt="图片"></p>
<p>最优的 x 就变成了 0。这里利用的就是绝对值函数的尖峰。两种 regularization 能不能把最优的 x 变成 0，取决于原先的费用函数在 0 点处的导数。如果本来导数不为 0，那么施加 L2 regularization 后导数依然不为 0，最优的 x 也不会变成 0。而施加 L1 regularization 时，只要保证施加L1后x=0处左右两边导数异号，且施加L1后x=0处左右两边导数分别是f’(0)-C和f’(0)+C，即C&gt;|f’(0)|，那么就会形成极小值点。</p>
<h4 id="概率视角"><a href="#概率视角" class="headerlink" title="概率视角"></a>概率视角</h4><p>为f(w)加入正则化项，相当于为f(w)的参数加入先验，即要求w满足某以分布。L1正则化项相当于为w加入拉普拉斯先验，L2正则化项相当于加入高斯分布的先验，两种分布的概率函数如下图：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/adclick/8.png" alt="图片"></p>
<p>很明显可以观察出，在紫色区域中，P_G(w) &lt; P_L(w)，说明Gauss分布中，值大的w更少。即L2与L1相比，值大的w更少，因此L2比L1更平滑。</p>
<p>在红色区域中，P_L(w) &lt; P_G(w)，结合图来看，Gauss分布中值很小的w和值为0的w概率很近。而Laplace分布中，值很小的w小于值为0的w。这说明laplace要w更多为0，而Gauss分布要w很小但不一定为0。因此L1比L2更稀疏。</p>
<h4 id="梯度视角"><a href="#梯度视角" class="headerlink" title="梯度视角"></a>梯度视角</h4><p>在L2正则下对参数求导为：</p>
<p>$\begin{aligned}<br>C=&amp; C_{0}+\frac{\lambda}{2 n} \sum_{w} w^{2} \\<br>\frac{\partial C}{\partial w} &amp;=\frac{\partial C_{0}}{\partial w}+\frac{\lambda}{n} w \\<br>\frac{\partial C}{\partial b} &amp;=\frac{\partial C_{0}}{\partial b} \\<br>w &amp; \rightarrow w-\eta \frac{\partial C_{0}}{\partial w}-\frac{\eta \lambda}{n} \\<br>&amp;=\left(1-\frac{\eta \lambda}{n}\right) w-\eta \frac{\partial C_{0}}{\partial w} .<br>\end{aligned}$</p>
<p>由上式可以看出加入L2正则项后梯度更新的变化，即在每步执行通常的梯度更新之前先缩放权重向量。在L1正则下对参数求导为：</p>
<ul>
<li>$C=C_{0}+\frac{\lambda}{n} \sum_{w}|w|$</li>
<li>$\frac{\partial C}{\partial w}=\frac{\partial C_{0}}{\partial w}+\frac{\lambda}{n} \operatorname{sgn}(w)$</li>
<li>$w \rightarrow w^{\prime}=w-\frac{\eta \lambda}{n} \operatorname{sgn}(w)-\eta \frac{\partial C_{0}}{\partial w}$</li>
</ul>
<p>可以看到L1正则化的效果与L2正则化的效果很不一样，不再是线性地缩放每个$w_i$，而是减去了一项与权重w i同号的常数，因此当$w_i$ ​&gt;0时，更新权重使得其减小，当$w_i&lt; 0$时，更新权重使得其增大。从这点来说也可以看出L1正则化更有可能使得权重为0，而L2正则化虽也使得权重减小，但缩放操作使得其仍保持同号。</p>
<p>可以想象用梯度下降时，当w小于1时，L2正则项的惩罚效果越来越小，L1正则项惩罚效果依然很大，L1可以惩罚到0，而L2很难。</p>
<h3 id="正则化参数-λ"><a href="#正则化参数-λ" class="headerlink" title="正则化参数 λ"></a>正则化参数 λ</h3><p>正则化是结构风险最小化的一种策略实现，能够有效降低过拟合。损失函数实际上包含了两个方面：一个是训练样本误差。一个是正则化项。其中，参数 λ 起到了权衡的作用。</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/adclick/9.png" alt="图片"></p>
<p>以 L2 为例，若 λ 很小，对应上文中的 C 值就很大。这时候，圆形区域很大，能够让 w 更接近 Ein 最优解的位置。若 λ 近似为 0，相当于圆形区域覆盖了最优解位置，这时候，正则化失效，容易造成过拟合。相反，若 λ 很大，对应上文中的 C 值就很小。这时候，圆形区域很小，w 离 Ein 最优解的位置较远。w 被限制在一个很小的区域内变化，w 普遍较小且接近 0，起到了正则化的效果。但是，λ 过大容易造成欠拟合。欠拟合和过拟合是两种对立的状态。</p>
<p>【注】：有关Laplace先验与L1正则化和Gauss先验与L2正则化可参考<a href="https://www.cnblogs.com/heguanyou/p/7688344.html" target="_blank" rel="noopener">https://www.cnblogs.com/heguanyou/p/7688344.html</a></p>
<h1 id="在线学习和稀疏性简介"><a href="#在线学习和稀疏性简介" class="headerlink" title="在线学习和稀疏性简介"></a>在线学习和稀疏性简介</h1><p>稀疏性在机器学习中是很看重的事情，尤其我们做工程应用，稀疏的特征会大大减少predict时的内存和复杂度。这一点其实很容易理解，说白了，即便加入L1范数，因为是浮点运算，训练出的w向量也很难出现绝对的零。到这里，大家可能会想说，那还不容易，当计算出的w对应维度的值很小时，我们就强制置为零不就稀疏了么。对的，其实不少人就是这么做的，后面的Truncated Gradient和FOBOS都是类似思想的应用。GD求解的模型虽然精度相对较高，但具有训练太费时、不易得到稀疏解和对不可微点迭代效果欠佳等缺点；SGD则存在模型解的精度低、收敛速度慢和很难得到稀疏解的缺点。虽然学术界提出了很多加快收敛或者提高模型精度的方法（如添加momentum项、添加nesterov项、Adagrad算法、Adadelta算法、GSA算法等），但这些方法在提高模型解的稀疏性方面效果有限，而FTRL在这方面则更加有效。</p>
<h2 id="Online-Gradient-Descent-OGD"><a href="#Online-Gradient-Descent-OGD" class="headerlink" title="Online Gradient Descent(OGD)"></a>Online Gradient Descent(OGD)</h2><p>在线梯度下降法类似于随机梯度下降SGD，一次只处理一个样本，但不同的是不要求序列的样本满足独立同分布的性质，其更新公式为：$w_{t+1}=w_{t}-\eta_{t} \mathbf{g}_{t}$，其中$\eta_{t}=\frac{1}{\sqrt{t}}$是全局非增的。OGD方法在许多问题上均有不错的表现，但是获得的解不够稀疏。在OGD目标函数中添加L1正则化项，可以带来一定的稀疏性，使得模型的参数接近零。但由于参数的更新是两个浮点数进行相加，很难有机会产生精确零解。</p>
<h2 id="Truncated-Gradient"><a href="#Truncated-Gradient" class="headerlink" title="Truncated Gradient"></a>Truncated Gradient</h2><p>Truncated Gradient具体算法如下：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/adclick/10.png" alt="图片"></p>
<p>从上面的公式可以看出，$\lambda, \theta$决定了 W 的稀疏程度，如果两者都很大，那么稀疏性就会越强。特别的，当$\lambda=\theta$时，此时只需要控制一个参数就可以控制稀疏性。</p>
<h2 id="FOBOS"><a href="#FOBOS" class="headerlink" title="FOBOS"></a>FOBOS</h2><p>在该算法中，权重的更新分成两个步骤：</p>
<ul>
<li>$W^{(t+0.5)}=W^{(t)}-\eta^{(t)} G^{(t)}$</li>
<li>$W^{(t+1)}=\operatorname{argmin}_{W}\left\{\frac{1}{2}\left|W-W^{(t+0.5)}\right|_{2}^{2}+\eta^{(t+0.5)} \Psi(W)\right\}$</li>
</ul>
<p>第一个步骤实际上是一个标准的梯度下降（SGD），第二个步骤是对第一个步骤的结果进行局部调整。写成一个公式那就是：$W^{(t+1)}=\operatorname{argmin}_{W}\left\{\frac{1}{2}\left|W-W^{(t)}+\eta^{(t)} G^{(t)}\right|_{2}^{2}+\eta^{(t+0.5)} \Psi(W)\right\}$</p>
<p>假设$F(W)=\frac{1}{2}\left|W-W^{(t)}+\eta^{(t)} G^{(t)}\right|_{2}^{2}+\eta^{(t+0.5)} \Psi(W)$，如果$W^{t+1}$存在一个最优解，那么可以推断0向量一定属于F(W)的次梯度集合：$0 \in \partial F(W)=W-W^{(t)}+\eta^{(t)} G^{(t)}+\eta^{(t+0.5)} \partial \Psi(W)$。因为$W^{(t+1)}=\operatorname{argmin}_{W} F(W)$，那么可以得到权重更新的另外一种形式：$W^{(t+1)}=W^{(t)}-\eta^{(t)} G^{(t)}-\eta^{(t+0.5)} \partial \Psi\left(W^{(t+1)}\right)$</p>
<p>从上面的公式可以看出，更新后的$W^{t+1}$不仅和$W^{(t)}$有关，还和自己$\Psi\left(W^{(t+1)}\right)$有关。这也许就是”前向后向切分”这个名称的由来。</p>
<h3 id="L1-FOBOS"><a href="#L1-FOBOS" class="headerlink" title="L1-FOBOS"></a>L1-FOBOS</h3><p>FOBOS的L1正则化形式为：</p>
<p>$\begin{array}{l}<br>W^{(t+1)}=\operatorname{argmin}_{W}\left\{\frac{1}{2}\left|W-W^{(t+0.5)}\right|_{2}^{2}+\eta^{(t+0.5)} \Psi(W)\right\} \\<br>=\operatorname{argmin}_{W} \sum_{i=1}^{N}\left(\frac{1}{2}\left(w_{i}-v_{i}\right)^{2}+\tilde{\lambda}\left|w_{i}\right|\right)<br>\end{array}$</p>
<p>其中$\Psi(W)$是L1范数，中间向量是$W^{(t+0.5)}=\left(v_{1}, \ldots, v_{N}\right) \in \mathbb{R}^{N}$，并且参数$\tilde{\lambda}=\eta^{(t+0.5)} \lambda$。可以对特征权重W的每一个维度进行单独求解：$w_{i}^{(t+1)}=\operatorname{argmin}_{w_{i}}\left(\frac{1}{2}\left(w_{i}-v_{i}\right)^{2}+\tilde{\lambda}\left|w_{i}\right|\right) \text { for all } 1 \leq i \leq N$</p>
<p>通过数学计算可证明，在L1正则化下，FOBOS 的特征权重的各个维度的更新公式是：</p>
<p>$\begin{array}{l}<br>w_{i}^{(t+1)}=\operatorname{sgn}\left(v_{i}\right) \max \left(0,\left|v_{i}\right|-\tilde{\lambda}\right) \\<br>=\operatorname{sgn}\left(w_{i}^{(t)}-\eta^{(t)} g_{i}^{(t)}\right) \max \left\{0,\left|w_{i}^{(t)}-\eta^{(t)} g_{i}^{(t)}\right|-\eta^{(t+0.5)} \lambda\right\} \text { for all } 1 \leq i \leq N<br>\end{array}$</p>
<p>其中$g_{i}^{(t)}$是梯度$G^{(t)}$)在第i个维度的分量。</p>
<p>从公式上看，如果$\left|w_{i}^{(t)}-\eta^{(t)} g_{i}^{(t)}\right| \leq \eta^{(t+0.5)} \lambda$，则有$w_{i}^{(t+1)}=0$。意思就是如果这次训练产生梯度的变化不足以令权重值发生足够大的变化时，就认为在这次训练中该维度不够重要，应该强制其权重是0。如果$\left|w_{i}^{(t)}-\eta^{(t)} g_{i}^{(t)}\right| \geq \eta^{(t+0.5)} \lambda$，则有$w_{i}^{(t+1)}=\left(w_{i}^{(t)}-\eta^{(t)} g_{i}^{(t)}\right)-\eta^{(t+0.5)} \lambda \cdot \operatorname{sgn}\left(w_{i}^{(t)}-\eta^{(t)} g_{i}^{(t)}\right)$。</p>
<p>那么L1-FOBOS和Truncated Gradient有什么关系呢？令$\theta=+\infty, k=1, \lambda_{T G}^{(t)}=\eta^{(t+0.5)} \lambda$，可以得到L1-FOBOS和Truncated Gradient完全一致，换句话说L1-FOBOS是Truncated Gradient在一些特定条件下的形式。</p>
<h2 id="Regularized-Dual-Averaging-RDA"><a href="#Regularized-Dual-Averaging-RDA" class="headerlink" title="Regularized Dual Averaging (RDA)"></a>Regularized Dual Averaging (RDA)</h2><p>RDA又叫做正则对偶平均算法，特征权重的更新策略是：$W^{(t+1)}=\operatorname{argmin}_{W}\left\{\frac{1}{t} \sum_{r=1}^{t} G^{(r)} \cdot W+\Psi(W)+\frac{\beta^{(t)}}{t} h(W)\right\}$</p>
<p>其中$G^{(t)} \cdot W$指的是向量$G^{(t)}$和W的内积，$\frac{1}{t} \sum_{r=1}^{t} G^{(r)} \cdot W$包括了之前所有梯度的平均值，$\Psi(W)$是正则项，h(W)是一个严格凸函数，$\left\{\beta^{(t)} \mid t \geq 1\right\}$是一个非负递增序列。</p>
<h3 id="L1-RDA"><a href="#L1-RDA" class="headerlink" title="L1-RDA"></a>L1-RDA</h3><p>令$\Psi(W)=|W|_{1}$，$h(W)=\frac{1}{2}|W|_{2}^{2}$（满足凸函数要求），$\beta^{(t)}=\gamma \sqrt{t} \text { with } \gamma&gt;0$（满足非负递增序列要求），则RDA的L1正则化形式为：</p>
<p>$W^{(t+1)}=\operatorname{argmin}_{W}\left\{\frac{1}{t} \sum_{r=1}^{t} G^{(r)} \cdot W+\lambda|W|_{1}+\frac{\gamma}{2 \sqrt{t}}|W|_{2}^{2}\right\}$</p>
<p>此时可以采取同样的策略，把各个维度拆分成N个独立的问题来处理，即：</p>
<p>$w_{i+1}^{(t+1)}=\operatorname{argmin}_{w_{i}}\left\{\bar{g}_{i}^{(t)}+\lambda\left|w_{i}\right|+\frac{\gamma}{2 \sqrt{t}} w_{i}^{2}\right\}$</p>
<p>其中$\lambda&gt;0, \gamma&gt;0, \bar{g}_{i}^{(t)}=\frac{1}{t} \sum_{r=1}^{t} g_{i}^{(r)}$。</p>
<p>通过数学推导可以证明：</p>
<ul>
<li>$w_{i}^{(t+1)}=0, \text { if }\left|\bar{g}_{i}^{(t)}\right|&lt;\lambda$</li>
<li>$w_{i}^{(t+1)}=-\frac{\sqrt{t}}{\gamma}\left(\bar{g}_{i}^{(t)}-\lambda \cdot \operatorname{sgn}\left(\bar{g}_{i}^{(t)}\right)\right), \text { otherwise }$</li>
</ul>
<p>意思就是：当某个维度的累加平均值$\left|\bar{g}_{i}^{(t)}\right|$小于$\lambda$时，该维度的权重被设置成零，此时就可以产生特征权重的稀疏性。</p>
<p>RDA有如下特点：</p>
<ul>
<li>非梯度下降类方法，属于更加通用的一个primal-dual algorithmic schema的一个应用</li>
<li>克服了SGD类方法所欠缺的exploiting problem structure，especially for problems with explicit regularization。</li>
<li>能够更好地在精度和稀疏性之间做trade-off</li>
</ul>
<h3 id="L1-RDA和L1-FOBOS"><a href="#L1-RDA和L1-FOBOS" class="headerlink" title="L1-RDA和L1-FOBOS"></a>L1-RDA和L1-FOBOS</h3><p>L1-RDA和L1-FOBOS有什么区别和联系吗？从截断方式来看，在 RDA 的算法中，只要梯度的累加平均值小于参数$\lambda$就直接进行截断，说明 RDA 更容易产生稀疏性；同时，RDA 中截断的条件是考虑梯度的累加平均值，可以避免因为某些维度训练不足而导致截断的问题，这一点与 TG，FOBOS 不一样。通过调节参数$\lambda$可以在精度和稀疏性上进行权衡。</p>
<p>L1-RDA和L1-FOBOS的形式上的统一。在 L1-FOBOS中，假设$\eta^{(t+0.5)}=\eta^{(t)}=\Theta\left(t^{-0.5}\right)$，可以得到$W^{(t+1)}=\operatorname{argmin}_{W}\left\{\frac{1}{2}\left|W-W^{(t)}+\eta^{(t)} G^{(t)}\right|_{2}^{2}+\eta^{(t)} \lambda|W|_{1}\right\}$。通过计算可以把L1-FOBOS写成：$W^{(t+1)}=\operatorname{argmin}_{W}\left\{G^{(t)} \cdot W+\lambda|W|_{1}+\frac{1}{2 \eta^{(t)}}\left|W-W^{(t)}\right|_{2}^{2}\right\}$。同理L1-RDA可以写成$W^{(t+1)}=\operatorname{argmin}_{W}\left\{G^{(1: t)} \cdot W+t \lambda|W|_{1}+\frac{1}{2 \eta^{(t)}}|W-0|_{2}^{2}\right\}$（其中$G^{(1: t)}=\sum_{s=1}^{t} G^{(s)}$）。</p>
<p>如果令$\sigma^{(s)}=\frac{1}{\eta^{(s)}}-\frac{1}{\eta^{(s-1)}}$、$\sigma^{(1: t)}=\frac{1}{\eta^{(t)}}$、$\sigma^{(1: t)}=\sum_{s=1}^{t} \sigma^{(s)}$，则L1-FOBOS可以写成$W^{(t+1)}=\operatorname{argmin}_{W}\left\{G^{(t)} \cdot W+\lambda|W|_{1}+\frac{1}{2} \sigma^{(1: t)}\left|W-W^{(t)}\right|_{2}^{2}\right\}$，L1-RDA可以写成$W^{(t+1)}=\operatorname{argmin}_{W}\left\{G^{(1: t)} \cdot W+t \lambda|W|_{1}+\frac{1}{2} \sigma^{(1: t)}|W-0|_{2}^{2}\right\}$。</p>
<p>可以看出：</p>
<ul>
<li>L1-FOBOS考虑了梯度和当前模的L1 正则项，L1-RDA考虑的是累积梯度。</li>
<li>L1-FOBOS的第三项限制W不能够距离已经迭代过的$W^{(t)}$值太远，L1-RDA的第三项限制的是W不能够距离0太远。</li>
</ul>
<h2 id="Follow-The-Proximally-Regularized-Leader-FTRL-Proximal"><a href="#Follow-The-Proximally-Regularized-Leader-FTRL-Proximal" class="headerlink" title="Follow The (Proximally) Regularized Leader (FTRL-Proximal)"></a>Follow The (Proximally) Regularized Leader (FTRL-Proximal)</h2><blockquote>
<p>参考：<br><a href="https://zhuanlan.zhihu.com/p/25315553" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/25315553</a><br><a href="https://zhuanlan.zhihu.com/p/32903540" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32903540</a></p>
</blockquote>
<p>理论及实验均证明，L1-FOBOS这类基于梯度下降的算法有比较高的精度，但L1-RDA却能在损失一定精度的情况下产生更好的稀疏性。把这二者的优点结合成一个算法，这就是FTRL算法的来源,如下图：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/adclick/11.png" alt="图片"></p>
<p>FTRL 综合考虑了 FOBOS 和 RDA 对于梯度和正则项的优势和不足，其权重更新公式是：$W^{(t+1)}=\operatorname{argmin}_{W}\left\{G^{(1: t)} \cdot W+\lambda_{1}|W|_{1}+\frac{\lambda_{2}}{2}|W|_{2}^{2}+\frac{1}{2} \sum_{s=1}^{t} \sigma^{(s)}\left|W-W^{(s)}\right|_{2}^{2}\right\}$</p>
<p>上面的公式出现了L2范数，不过这一项的引入不会影响 FTRL 的稀疏性，只是使得求解结果更加“平滑”。通过数学计算并且放弃常数项可以得到上面的优化问题相当于求使得下面式子的最小的参数 W：$\left(G^{(1: t)}-\sum_{s=1}^{t} \sigma^{(s)} W^{(s)}\right) \cdot W+\lambda_{1}|W|_{1}+\frac{1}{2}\left(\lambda_{2}+\sum_{s=1}^{t} \sigma^{(s)}\right)|W|_{2}^{2}$</p>
<p>假设$Z^{(t)}=G^{(1: t)}-\sum_{s=1}^{l} \sigma^{(s)} W^{(s)}=Z^{(t-1)}+G^{(t)}-\sigma^{(t)} W^{(t)}$，则上式等价于$W^{(t+1)}=\operatorname{argmin}_{W}\left\{Z^{(t)} \cdot W+\lambda_{1}|W|_{1}+\frac{1}{2}\left(\lambda_{2}+\sum_{s=1}^{t} \sigma^{(s)}\right)|W|_{2}^{2}\right\}$，写成分量的形式就是：$\operatorname{argmin}_{w_{i}}\left\{z_{i}^{(t)} w_{i}+\lambda_{1}\left|w_{i}\right|+\frac{1}{2}\left(\lambda_{2}+\sum_{s=1}^{t} \sigma^{(s)}\right) w_{i}^{2}\right\}$</p>
<p>通过数学推导可以证明：</p>
<ul>
<li>$w_{i}^{(t+1)}=0, \text { if }\left|z_{i}^{(t)}\right|&lt;\lambda_{1}$</li>
<li>$w_{i}^{(t+1)}=-\left(\lambda_{2}+\sum_{s=1}^{t} \sigma^{(s)}\right)^{-1} \cdot\left(z_{i}^{(t)}-\lambda_{1} \cdot \operatorname{sgn}\left(z_{i}^{(t)}\right)\right), \text { otherwise }$</li>
</ul>
<p>上式也可以说明，引入 L2 正则化并没有对 FTRL 的稀疏性产生影响。</p>
<h3 id="Per-Coordinate-Learning-Rates"><a href="#Per-Coordinate-Learning-Rates" class="headerlink" title="Per-Coordinate Learning Rates"></a>Per-Coordinate Learning Rates</h3><p>在 SGD 的算法里面使用的是一个全局的学习率$\eta^{(t)}=O\left(t^{-0.5}\right)$，意味着学习率是一个正数并且逐渐递减，对每一个维度都是一样的。而在 FTRL 算法里面，每个维度的学习率是不一样的。如果特征 A 比特征 B变化快，那么在维度 A 上面的学习率应该比维度 B 上面的学习率下降得更快。在 FTRL 中，维度i的学习率的定义如下：$\eta_{i}^{(t)}=\alpha /\left(\beta+\sqrt{\sum_{s=1}^{t}\left(g_{i}^{(s)}\right)^{2}}\right)$</p>
<p>按照之前的定义$\sigma^{(1: t)}=1 / \eta^{(t)}$，所以$\sum_{s=1}^{t} \sigma^{(s)}=\eta_{i}^{(t)}=\alpha /\left(\beta+\sqrt{\sum_{s=1}^{t}\left(g_{i}^{(s)}\right)^{2}}\right)$</p>
<p>因此，FTRL的整体算法如下：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/adclick/12.png" alt="图片"></p>
<p>所谓的per-coordinate，意思就是FTRL是对w每一维分开训练更新的，每一维使用的是不同的学习速率。与w所有特征维度使用统一的学习速率相比，这种方法考虑了训练样本本身在不同特征上分布的不均匀性，如果包含w某一个维度特征的训练样本很少，每一个样本都很珍贵，那么该特征维度对应的训练速率可以独自保持比较大的值，每来一个包含该特征的样本，就可以在该样本的梯度上前进一大步，而不需要与其他特征维度的前进步调强行保持一致。</p>
<h3 id="Logistic-Regression的FTRL形式"><a href="#Logistic-Regression的FTRL形式" class="headerlink" title="Logistic Regression的FTRL形式"></a>Logistic Regression的FTRL形式</h3><p>Logistic Regression 的 FTRL算法如下：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/adclick/13.png" alt="图片"></p>
<h3 id="不同算法的统一描述"><a href="#不同算法的统一描述" class="headerlink" title="不同算法的统一描述"></a>不同算法的统一描述</h3><p>不同的方法在统一的描述形式下，区别点仅在第二项和第三项的处理方式：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/adclick/14.png" alt="图片"></p>
<ul>
<li>第一项：梯度或累积梯度；</li>
<li>第二项：L1正则化项的处理；</li>
<li>第三项：这个累积加和限定了新的迭代结果x不要离已迭代过的解太远（也即FTRL-Proximal中proximal的含义），或者离0太远（central），这一项其实也是low regret的需求。</li>
</ul>
<h1 id="节省内存的其他方法"><a href="#节省内存的其他方法" class="headerlink" title="节省内存的其他方法"></a>节省内存的其他方法</h1><blockquote>
<p>参考：<br><a href="https://zhuanlan.zhihu.com/p/100922128" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/100922128</a></p>
</blockquote>
<p>下面介绍一些google在FTRL工程实践上的一些tricks。</p>
<h2 id="Feature-Inclusion"><a href="#Feature-Inclusion" class="headerlink" title="Feature Inclusion"></a>Feature Inclusion</h2><p>海量的特征中(数亿)往往包含一些特征，它们的可能在海量的样例中只有一个或极少个非零值，这样的特征对于模型基本无效，所以在数据预处理时要尽量去掉这些无效特征。但是在线学习中，要重新读取和重写海量数据是非常耗费资源的，而且如果真的在预处理阶段就删除了这些特征，那么在学习的整个过程中就不再用到这些特征。</p>
<p>针对这个问题，有以下常见思路：1）使用L1正则项来实现训练中的稀疏性，这种方法不用刻意统计特征出现的频率，将频率低的删除，因为随着训练的进行，信息量较少的特征就会被自动删除，但是经过实验发现这个方法相比FTRL-Proximal会降低模型预测的准确率。2）制造hash冲突，但是作用也不大。</p>
<p>本文探索了一种新思路：probabilistic feature inclusion，意思就是当新特征首次出现的时候就概率性的包含在模型中，这相当于在第一次读入数据的时候就进行了预处理，不必再重新读写。这种读入数据时预处理的方式可以在线设置，本文测试了两种方式，下图为两种方式的效果比较，显然BLOOM的方法在RAM和AucLos之间有更好的平衡：</p>
<ul>
<li>Poisson Inclusion：假设特征是随机到达的，也就是说应该符合Poisson分布。以p的概率随机加入模型并进行训练。</li>
<li>Bloom Filter Inclusion：使用布隆过滤器来进行计数，当特征出现次数超过n时才加入模型。但布隆过滤器并非精确的，而是false positive的，因为产生冲突时，可能会加入还没有满n次的特征来进行训练。实际中我们并不要求完全精确的筛选。在实验中，Bloom Filter表现出更高的RAM节省及更少的AucLoss，更推荐。</li>
</ul>
<h2 id="Encoding-Values-with-Fewer-Bits"><a href="#Encoding-Values-with-Fewer-Bits" class="headerlink" title="Encoding Values with Fewer Bits"></a>Encoding Values with Fewer Bits</h2><blockquote>
<p>参考：<br>定点数与浮点数：<br><a href="https://blog.csdn.net/dong_zhihong/article/details/8255690" target="_blank" rel="noopener">https://blog.csdn.net/dong_zhihong/article/details/8255690</a><br><a href="https://blog.csdn.net/niaolianjiulin/article/details/82764511" target="_blank" rel="noopener">https://blog.csdn.net/niaolianjiulin/article/details/82764511</a><br><a href="https://zhuanlan.zhihu.com/p/63897066" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/63897066</a></p>
</blockquote>
<p>我们平常使用的都是32或64位浮点数编码，但是在实际场景中，系数值普遍落在(-2,+2)之间，普通的编码方式就显得非常奢侈了。这里提出了q2.13编码方式：</p>
<p>1 bit表达正负号，2 bits表达整数位，13 bits表达小数点右边的值，总计是16 bits。用更少的bit来表达就意味着信息损失。但是算法中要运用到对每一小步的累计值，不做处理就会对模型产生影响。具体解决策略为：</p>
<p>加入一个随机的regret项:R，R符合标准高斯分布，值介于0和1之间。对系数进行round，保证其整体离散误差为0：$w_{i, \text { rounded }}=2^{-13}\left\lfloor 2^{13} w_{i}+R\right\rfloor$。实验表明，使用了q2.13相较于64位浮点编码可以节省75%的参数存储RAM。</p>
<h2 id="Training-Many-Similar-Models"><a href="#Training-Many-Similar-Models" class="headerlink" title="Training Many Similar Models"></a>Training Many Similar Models</h2><p>我们在做特征筛选或者设置优化时，基于已有的训练数据，控制变量，生成多个不同的变体来进行对比实验是一个不错的方法。但这样做比较耗费资源。之前有一个比较省资源的方法是基于一个先验模型，使用其他模型来预测残差。但是这样不能进行特征移除和特征替换的效果。这里Google提出了一个方法，由于每个模型变体的相似度是很高的，一部分特征是变体之间共享的，一部分特征是变体独有的。如果采用一张hashtable来存储所有变体的模型参数，就可以摊销各个模型都进行独立存储key的内存消耗。而且也同时会降低看网络带宽、CPU、磁盘的消耗。</p>
<h2 id="A-Single-Value-Structure"><a href="#A-Single-Value-Structure" class="headerlink" title="A Single Value Structure"></a>A Single Value Structure</h2><p>这个是指不同模型变体之间共享一个特征参数，而不是每一个模型变体存储单独的一个。用一个位字段来标识这个特征被哪些变体使用。</p>
<p>更新的方式如下：每一个模型变体去进行prediction并计算loss，然后对于每一个特征给出一个迭代后的参数值。对所有的更新后的参数值进行平均并进行更新存储。</p>
<h2 id="Computing-Learning-Rates-with-Counts"><a href="#Computing-Learning-Rates-with-Counts" class="headerlink" title="Computing Learning Rates with Counts"></a>Computing Learning Rates with Counts</h2><p>前面提到学习率的计算per-coordinate learning rates，要对梯度的平方要进行累计。这里给出了一个梯度平方累计近似解决方案：</p>
<p>$\begin{aligned}<br>\sum g_{t, i}^{2} &amp;=\sum_{\text {positive events }}\left(1-p_{t}\right)^{2}+\sum_{\text {negative events }} p_{t}^{2} \\<br>&amp; \approx P\left(1-\frac{P}{N+P}\right)^{2}+N\left(\frac{P}{N+P}\right)^{2} \\<br>&amp;=\frac{P N}{N+P}<br>\end{aligned}$</p>
<h2 id="Subsampling-Training-Data"><a href="#Subsampling-Training-Data" class="headerlink" title="Subsampling Training Data"></a>Subsampling Training Data</h2><p>实际CTR问题中，点击率远远低于50%，一般在1%左右，那就意味着负样本数量远超正样本数量。做数据重采样不仅能平衡正负样本比例，还可以在不影响整体准确度的情况下降低训练集大小。具体的筛选规则描述如下：</p>
<ul>
<li>query对应至少一个ad点击的全采样；</li>
<li>query不对应任何ad点击的按r比例采样；</li>
</ul>
<p>然后对样本进行权重修正：</p>
<p>$\omega_{t}=\left\{\begin{array}{ll}<br>1 &amp; \text { event } t \text { is in a clicked query } \\<br>\frac{1}{r} &amp; \text { event } t \text { is in a query with no clicks. }<br>\end{array}\right.$</p>
]]></content>
      <tags>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>YouTube双塔DNN</title>
    <url>/2020/06/15/YouTube%E5%8F%8C%E5%A1%94DNN/</url>
    <content><![CDATA[<p>这篇文章是谷歌的Youtube团队在推荐系统上DNN方面的尝试，发表在16年9月的RecSys会议。YouTube用户数量超19亿日，日观看时长达1.8亿小时，算是世界范围内视频领域的最大的网站。这篇论文从信息检索领域的经典的两阶段——召回、排序——来阐述如何用深度模型做candidate generate 和 rank。文中不但详细介绍了Youtube推荐算法和架构细节，还给了不少实践经验和方法论。</p>
<a id="more"></a>
<h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h1><p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/youtubednn/1.png" alt="图片"></p>
<p>在推荐系统领域，特别是YouTube的所在视频推荐领域，主要面临三个挑战：</p>
<ul>
<li>规模大：用户和视频的数量都很大，只能适应小规模数据集的算法就不考虑了。</li>
<li>更新快：<ul>
<li>youtube视频更新频率很高，每秒有小时级别的视频上传，需要在新发布视频和已有存量视频间进行balance。</li>
<li>用户实时行为切换很快，模型需要很好的追踪用户的实时行为。</li>
</ul>
</li>
<li>噪音：<ul>
<li>用户的历史行为往往是稀疏的并且是不完整的，并且没有一个明确的ground truth的满意度signal，我们面对的都是noisy implicit feedback signals。</li>
<li>视频本身很多数据都是非结构化的。</li>
</ul>
</li>
</ul>
<h1 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h1><p>整个推荐系统分为candidate generation (采用的判别模型，因此又称为Matching) 和Ranking两个阶段。</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/youtubednn/2.png" alt="图片"></p>
<ul>
<li>Matching阶段通过i2i/u2i/u2u/user profile/hot&amp;new等方式“粗糙”的召回候选商品</li>
</ul>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/youtubednn/3.png" alt="图片"></p>
<pre><code>* i2i：计算item-item相似度，用于相似推荐、相关推荐、关联推荐；
* u2i：基于矩阵分解、协同过滤的结果，直接给u推荐i；
* u2u2i：基于用户的协同过滤，先找相似用户，再推荐相似用户喜欢的item；
* u2tag2i：基于标签的泛化推荐，先统计用户偏好的tag向量，然后匹配所有的Item，这个tag一般是item的标签、分类、关键词等tag；
    * 标签召回可以借助用户画像:
        * 固定属性标签：{&#39;年龄&#39;: &#39;13&#39;, &#39;年级&#39;: &#39;初二&#39;, &#39;性别&#39;: 男, &#39;设备&#39;: &#39;iphone 7&#39;, &#39;地域&#39;: &#39;北京&#39;}
        * 行为标签：{&#39;王者荣耀&#39;: 0.9, &#39;深度学习&#39;: 0.6, &#39;天线宝宝&#39;: 0.1}
* 热门召回
* 新文章召回
* Matching阶段视频的数量是百级别了
</code></pre><ul>
<li>Ranking阶段对Matching后的视频采用更精细的特征计算user-item之间的排序分，作为最终输出推荐结果的依据。</li>
</ul>
<p>之所以把推荐系统划分成Matching和Ranking两个阶段，主要是从性能方面考虑的。Matching阶段面临的是百万级视频，单个视频的性能开销必须很小；而Ranking阶段的算法则非常消耗资源，不可能对所有视频都算一遍，实际上即便资源充足也完全没有必要，因为往往来说通不过Matching粗选的视频，大部分在Ranking阶段排名也很低。</p>
<p>在这个架构中，我们使用precision、recall、ranking loss作为离线的metrics来指导系统的迭代，但是最终会通过A/B test来最终决定算法质量的好坏。在线上环境中，会使用点击率、观看时长、转化率等指标来对推荐系统进行评价。需要注意的是，A/B test的指标结果跟离线实验的结果不一定相关。</p>
<h1 id="Match模型召回"><a href="#Match模型召回" class="headerlink" title="Match模型召回"></a>Match模型召回</h1><h2 id="前传"><a href="#前传" class="headerlink" title="前传"></a>前传</h2><p>在DNN之前，推荐/广告系统都经历过什么模型？</p>
<h3 id="LR"><a href="#LR" class="headerlink" title="LR"></a>LR</h3><p>LR模型是CTR预估领域早期最成功的模型，具有简单、易于并行化实现、可解释性强等优点，但是LR模型中的特征是默认相互独立的，遇到具有交叉可能性的特征需进行大量的人工特征工程进行交叉(连续特征的离散化、特征交叉)，不能处理目标和特征之间的非线性关系。大多工业推荐排序系统采取LR这种“线性模型+人工特征组合引入非线性”的模式。但是，LR模型最大的缺陷就是人工特征工程，耗时费力。</p>
<p>【注】：</p>
<ul>
<li>特征组合是指通过将两个或多个输入特征相乘来对特征空间中的非线性规律进行编码的合成特征。“cross”（组合）这一术语来自cross product（向量积）。以下图为例，我们通过将x_1和x_2组合来创建一个名为 x_3的特征组合：$x_3=x_1x_2$，则我们需要求解的线性公式变为$y=b+w_1x_1+w2x_2+w_3x_3$。</li>
</ul>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/youtubednn/4.png" alt="图片"></p>
<ul>
<li>连续特征的离散化就是把原始的连续值划分多个区间，如等频划分或等间距划分。特征离散化相当于把线性函数变成了分段线性函数，引入了非线性结构。离散化还可以使得数据中的噪音有更好的鲁棒性(异常值落在同一区间)；还可以使得模型更加稳定，特征值的微小变化(在同一个区间)不会引起模型预测值变化。</li>
<li>其实看到这儿你可能想到了带核函数的SVM：</li>
</ul>
<p>$\begin{array}{l}<br>\min _{\alpha} \frac{1}{2} \sum_{i=1}^{l} \sum_{j=1}^{l} \alpha_{i} \alpha_{j} y_{i} y_{j} K\left(\mathrm{x}_{i}^{\mathrm{T}} \mathrm{x}_{j}\right)-\sum_{k=1}^{l} \alpha_{k} \\<br>0 \leq \alpha_{i} \leq C \\<br>\sum_{j=1}^{l} \alpha_{j} y_{j}=0<br>\end{array}$</p>
<p>SVM看上去解决了特征组合的问题，但是要知道推荐算法使用的特征是非常稀疏的，如果在训练数据里两个特征并未同时在训练实例里见到过，也就是说xi和xj一起出现的次数是0，那么SVM是无法学会这个特征组合的权重的。这就会导致模型的泛化性变差。</p>
<h3 id="GBDT-LR"><a href="#GBDT-LR" class="headerlink" title="GBDT+LR"></a>GBDT+LR</h3><p>后来，Facebook 2014年的论文《Practical Lessons from Predicting Clicks on Ads at Facebook》中介绍了通过GBDT解决LR的特征组合问题，用公式表示是$f(x)=\operatorname{logistics}\left(g b d t_{-} \text {tree}_{1}(X)+g b d t_{-} \operatorname{tree}_{2}(X)+\ldots\right)$。由于GDBT树的分裂算法，它会对特征进行重要性排序，因此具有一定特征组合的能力。GBDT学习到的高阶非线性特征组合，对应树的一条路径。通常将一些连续值特征、值空间不大的categorical特征都丢给GBDT模型；空间很大的ID特征留在LR模型中训练，既能做高阶特征组合又可以利用线性模型易于处理大规模稀疏数据的优势。</p>
<p>不过GBDT+LR也有它的缺点，比如要调的参数太多；而且只能使用batch的方式训练（而且整体需要串行训练，只能局部并行提高速度），比FTRL的方式慢很多；对于高维的稀疏特征，GBDT容易过拟合。</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/youtubednn/5.png" alt="图片"></p>
<h3 id="MF"><a href="#MF" class="headerlink" title="MF"></a>MF</h3><p>MF（Matrix Factorization，矩阵分解）模型是个在推荐系统领域里资格很深的老前辈协同过滤模型了。核心思想是通过两个低维小矩阵（一个代表用户embedding矩阵，一个代表物品embedding矩阵）的乘积计算，来模拟真实用户点击或评分产生的大的协同信息稀疏矩阵，本质上是编码了用户和物品协同信息的降维模型。当训练完成，每个用户和物品得到对应的低维embedding表达后，如果要预测某个 User_i对 Item_j的评分的时候，只要它们做个内积计算 &lt;$User_i$, $Item_j$&gt;，这个得分就是预测得分。<img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/youtubednn/6.png" alt="图片"></p>
<p>MF有什么缺点？基础矩阵分解只用了userId和itemId两个维度的信息，所有学到的知识都蕴含在user向量和item向量中。可解释性差是一个方面，另一个不足的方面在于学习过程中很难融合更多有用的特征，比如用户的人口统计学信息，商品的基础特征信息，如类目、品牌等，用户的长期偏好信息等等。因而，基础矩阵分解的泛化能力受到一定的限制。</p>
<h3 id="FM"><a href="#FM" class="headerlink" title="FM"></a>FM</h3><p>再后来就有了Factorization Machine。因子分解机（FM）可以看做是基础矩阵分解的推广，它就能够很好地融入更多维度的特征，从而学到的模型泛化能力更强。简单的说FM（Factorization Machine）是一类自动特征组合算法。相比于传统的SVM、LR等模型，FM可以自动的进行特征的组合运算用以替代一部分人工特征工程的工作，并且计算量可控，对于海量输入特征仍然可以尝试使用这个方法。另一方面，对于稀疏特征，SVM等模型相对难以取得特别好的效果，而FM应用分解的方法可以很好的处理稀疏的输入。相比较于其他的分解方法（SVD、PITF、FPMC等等）FM是一个更加通用化的特征组合与分解手段，可以应用在多个场景下。</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/youtubednn/7.png" alt="图片"></p>
<p>比如上图中的数据，我们可以建立一个FM模型：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/youtubednn/8.png" alt="图片"></p>
<p>通过SGD优化目标函数，最终会获得每一个特征的embedding表示。对于某个用户，我们可以把属于这个用户子集合的特征，查询离线训练好的FM模型对应的特征embedding向量，然后将n个用户子集合的特征embedding向量累加，形成用户兴趣向量U(如A)。类似的，我们也可以把每个物品，其对应的物品子集合的特征，查询离线训练好的FM模型对应的特征embedding向量，然后将m个物品子集合的特征embedding向量累加，形成物品向量I。</p>
<p>当用户登陆或者刷新页面时，可以根据用户ID从redis中取出其对应的兴趣向量embedding，然后和Faiss中存储的物料embedding做内积计算，按照得分由高到低返回得分Top K的物料作为召回结果。提交给第二阶段的排序模型进行进一步的排序。</p>
<h2 id="问题建模"><a href="#问题建模" class="headerlink" title="问题建模"></a>问题建模</h2><p>文章将推荐问题建模成一个“超大规模多分类”问题。即在时刻 t ，为用户U（上下文信息 C ）在视频库 V 中精准的预测出视频 i 的类别（每个具体的视频视为一个类别，i 即为一个类别），用数学公式表达如下：$P\left(w_{t}=i \mid U, C\right)=\frac{e^{v_{i} u}}{\sum_{j \in V} e^{v_{j} u}}$。</p>
<p>很显然上式为一个softmax多分类器的形式。向量$u \in R^{N}$是<user, context>信息的高维“embedding”，而向量$v_{j} \in R^{N}$则是视频 j 的embedding向量。所以DNN的目标就是在用户信息和上下文信息为输入条件下学习用户的embedding向量 u 。用公式表达DNN就是在拟合函数 $u = f_{DNN}(user_{info}, context_{info})$。而这种超大规模分类问题上，至少要有几百万个类别，实际训练采用的是Negative Sample，类似于word2vec的Skip-Gram方法。</p>
<p>【注】：对于超大规模的softmax，论文采用的方法是负采样（negative sampling）并用importance weighting的方法对采样进行calibration。文中同样介绍了一种替代方法，hierarchical softmax，但并没有取得更好的效果。</p>
<h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/youtubednn/9.png" alt="图片"></p>
<p>整个模型架构是包含三个隐层的DNN结构。输入是用户浏览历史、搜索历史、人口统计学信息和其余上下文信息concat成的输入向量；输出分线上和离线训练两个部分。离线训练阶段输出层为softmax层，输出2.1公式表达的概率。而线上则直接利用user向量查询相关商品，最重要问题是在性能。我们利用类似局部敏感哈希的算法为用户提供最相关的N个视频。</p>
<p>可以看出，模型架构很像word2vec的Skip-Gram。每个视频都会被embedding到固定维度的向量中。把推荐问题定义为“超大规模多分类”问题的数学公式和word2vec的Skip-Gram方法的公式基本相同，所不同的是user_vec是通过DNN学习到的，而引入DNN的好处则是任意的连续特征和离散特征可以很容易添加到模型当中。同样的，推荐系统常用的矩阵分解方法虽然也能得到user_vec和item_vec，但同样是不能嵌入更多feature。</p>
<p>【注】：论文中提到 “our approach can be viewed as a nonlinear generalization of factorization techniques.”</p>
<h2 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h2><h3 id="主要特征"><a href="#主要特征" class="headerlink" title="主要特征"></a>主要特征</h3><ul>
<li>历史观看视频序列：用户的观看视频历史则是通过变长的视频序列表达，最终通过加权平均（可根据重要性和时间进行加权）得到固定维度的watch vector作为DNN的输入。</li>
<li>历史搜索query：把历史搜索的query分词后的token的embedding向量进行加权平均，能够反映用户的整体搜索历史状态</li>
<li>人口统计学信息：性别、年龄、地域等其他上下文信息：设备、登录状态等</li>
</ul>
<h3 id="“Example-Age”-（视频上传时间）特征"><a href="#“Example-Age”-（视频上传时间）特征" class="headerlink" title="“Example Age” （视频上传时间）特征"></a>“Example Age” （视频上传时间）特征</h3><p>视频网络的时效性是很重要的，每秒YouTube上都有大量新视频被上传，而对用户来讲，哪怕牺牲相关性代价，用户还是更倾向于更新的视频。当然我们不会单纯的因为一个视频新就直接推荐给用户。</p>
<p>视频网站视频的分布是高度非静态（non-stationary）的，但我们的推荐系统产生的视频集合在视频的分布，基本上反映的是训练所取时间段的平均的观看喜好的视频。因此我们我们把样本的 “age” 作为一个feature加入模型训练中。从下图可以很清楚的看出，加入“example age” feature后和经验分布更为match。</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/youtubednn/10.png" alt="图片"></p>
<p>【注】：问题，在serving阶段这个值怎么设置？</p>
<h2 id="标签及上下文选择"><a href="#标签及上下文选择" class="headerlink" title="标签及上下文选择"></a>标签及上下文选择</h2><p>在有监督学习问题中，最重要的选择是label了，因为label决定了你做什么，决定了你的上限，而feature和model都是在逼近label。比如说我们假设对电影的评分反应了推荐的效果，那么就会将评分作为预测的目标label。</p>
<p>YouTube在标签及上下文的选择上有一些建议：</p>
<ul>
<li>使用更广的数据源：不仅仅使用推荐场景的数据进行训练，其他场景比如搜索等的数据也要用到，这样也能为推荐场景提供一些explore。</li>
<li>为每个用户生成固定数量训练样本：我们在实际中发现如果为每个用户固定样本数量上限，平等的对待每个用户，避免loss被少数活跃用户所主导，能明显提升线上效果。</li>
<li>抛弃序列信息：我们在实现时尝试的是去掉序列信息，对过去观看视频/历史搜索query的embedding向量进行加权平均。这点其实违反直觉。<ul>
<li>比如一个用户刚刚搜索”taylor swift”，那么我们可能会想当然地把”taylor swift”搜索结果页的视频作为用户home page的推荐项。但是把用户的最后一个搜索词作为特征训练出来的模型，在a/b test中表现很差。</li>
<li>论文原话：Somewhat counter-intuitively, great care must be taken to withhold information from the classifier in order to prevent the model from exploiting the structure of the site and overfitting the surrogate problem.</li>
</ul>
</li>
<li>不对称的共同浏览（asymmetric co-watch）问题：所谓asymmetric co-watch指的是用户在浏览视频时候，往往都是序列式的，开始看一些比较流行的，逐渐找到细分的视频。下图所示图(a)是held-out方式，利用上下文信息预估中间的一个视频；图(b)是predicting next watch的方式，则是利用上文信息，预估下一次浏览的视频。我们发现图(b)的方式在线上A/B test中表现更佳。而实际上，传统的协同过滤类的算法，都是隐含的采用图(a)的held-out方式，忽略了不对称的浏览模式。<ul>
<li>Many collaborative filtering systems implicitly choose the labels and context by holding out a random item and predicting it from other items in the user’s history</li>
<li>图(a)的方式其实泄漏了future information</li>
</ul>
</li>
</ul>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/youtubednn/11.png" alt="图片"></p>
<h2 id="不同网络深度和特征的实验"><a href="#不同网络深度和特征的实验" class="headerlink" title="不同网络深度和特征的实验"></a>不同网络深度和特征的实验</h2><p>简单介绍下我们的网络构建过程，采用的经典的“tower”模式搭建网络，基本同2.2所示的网络架构，所有的视频和search token都embedded到256维的向量中，开始input层直接全连接到256维的softmax层，依次增加网络深度（+512—&gt;+1024—&gt;+2048—&gt; …）。</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/youtubednn/12.png" alt="图片"></p>
<p>上图反映了不同网络深度（横坐标）下不同特征组合情况下的MAP。可以很明显看出，增加了观看历史之外的特征很明显的提升了预测得准确率；从网络深度看，随着网络深度加大，预测准确率在提升，但继续增加第四层网络已经收益不大了。</p>
<h1 id="Rank模型精排"><a href="#Rank模型精排" class="headerlink" title="Rank模型精排"></a>Rank模型精排</h1><p>Ranking阶段的最重要任务就是精准的预估用户对视频的喜好程度。不同于Matching阶段面临的是百万级的候选视频集，Ranking阶段面对的只是百级别的商品集，因此我们可以使用更多更精细的feature来刻画视频（item）以及用户与视频（user-item）的关系。比如用户可能很喜欢某个视频，但如果list页的用的“缩略图”选择不当，用户也许不会点击，等等。</p>
<p>此外，Matching阶段的来源往往很多，没法直接比较。Ranking阶段另一个关键的作用是能够把不同来源的数据进行有效的ensemble。</p>
<p>在目标的设定方面，单纯CTR指标是有迷惑性的，有些靠关键词吸引用户高点击的视频未必能够被播放。因此设定的目标基本与期望的观看时长相关，具体的目标调整则根据线上的A/B进行调整。s</p>
<h2 id="模型架构-1"><a href="#模型架构-1" class="headerlink" title="模型架构"></a>模型架构</h2><p>Ranking阶段的模型和Matching基本相似，不同的是training最后一层是一个weighted LR层，而serving阶段激励函数用的是e^x。</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/youtubednn/13.png" alt="图片"></p>
<h2 id="特征选择-1"><a href="#特征选择-1" class="headerlink" title="特征选择"></a>特征选择</h2><p>尽管深度学习在图像、语音和NLP等场景都能实现end-to-end的训练，没有了人工特征工程工作。然而在搜索和推荐场景，我们的很难吧原始数据直接作为FNN的输入，特征工程仍然很重要。而特征工程中最难的是如何建模用户时序行为，并且关联这些行为和要rank的item。</p>
<p>我们发现最重要的Signal是描述用户与商品本身或相似商品之间交互的Signal，这与Facebook在14年提出LR+GBDT模型的paper中得到的结论是一致的。比如我们要度量用户对视频的喜欢，可以考虑用户与视频所在频道间的关系：</p>
<ul>
<li>数量特征：浏览该频道的次数</li>
<li>时间特征：比如最近一次浏览该频道距离现在的时间</li>
</ul>
<p>这两个连续特征的最大好处是具备非常强的泛化能力。另外除了这两个偏正向的特征，用户对于视频所在频道的一些PV但不点击的行为，即负反馈Signal同样非常重要。</p>
<p>另外，我们还发现，把Matching阶段的信息传播到Ranking阶段同样能很好的提升效果，比如推荐来源和所在来源的分数。</p>
<p>NN更适合处理连续特征，因此稀疏的特别是高基数空间的离散特征需要embedding到稠密的向量中。每个维度（比如query/user_id）都有独立的embedding空间。实际并非为所有的id进行embedding，比如视频id，只需要按照点击排序，选择top N视频进行embedding，其余置为0向量。而对于像“过去点击的视频”这种multivalent特征，与Matching阶段的处理相同，进行加权平均即可。</p>
<p>另外一个值得注意的是，同维度不同feature采用的相同ID的embedding是共享的（比如“过去浏览的视频id” “seed视频id”），这样可以大大加速训练，但显然输入层仍要分别填充。</p>
<p>论文中还提到了对特征进行归一化的问题。除了Tree-Based的模型（比如GBDT/RF），机器学习的大多算法大都对输入特征的尺度和分布都是非常敏感。因此建议将特征归一化到[0,1)区间以便于模型收敛。除此之外，我们还把归一化后的x的根号$\sqrt{x}$和平方作$\sqrt{x^2}$为网络输入，以期能使网络能够更容易得到特征的次线性（sub-linear）和（super-linear）超线性函数。</p>
<h2 id="建模期望观看时长"><a href="#建模期望观看时长" class="headerlink" title="建模期望观看时长"></a>建模期望观看时长</h2><p>我们的目标是预测期望观看时长。有点击的为正样本，有PV无点击的为负样本，正样本需要根据观看时长进行加权。因此，我们训练阶段网络最后一层用的是 weighted logistic regression作为输出层，并在serving阶段使用e^{Wx+b}来预测用户观看时常。</p>
<p>问题是，为什么要这样做呢？下面来解释一下。</p>
<p>我们知道sigmoid函数的数学形式：$h_{\theta}(x)=\frac{1}{1+e^{-\theta^{T} x}}$，我们经常用它将值域映射到0~1之间，来表示一件事情发生/不发生的概率。下面我们从一个新的角度去理解sigmoid。</p>
<p>首先我们需要定义一个新的变量——Odds，中文可以叫发生比或者机会比：$O d d s=\frac{p}{1-p}$，假设一件事情发生的概率是p，那么Odds就是一件事情发生和不发生的比值。</p>
<p>如果对Odds取自然对数，再让ln(Odds)等于一个线性回归函数，那么就得到了下面的等式：$\operatorname{logit}(p)=\ln \left(\frac{p}{1-p}\right)=\theta_{0}+\theta_{1} x_{1}+\theta_{2} x_{2}$。其中ln(p/(1-p))就是logit函数。上面的式子就是逻辑回归的由来。我们再做进一步运算，就可以转变成我们熟悉的逻辑回归的形式：$\ln \left(\frac{p}{1-p}\right)=\theta^{T} x \Rightarrow \frac{p}{1-p}=e^{\theta^{T} x} \Rightarrow p=\frac{1}{1+e^{-\theta^{T} x}} \Rightarrow p=\operatorname{sigmoid}\left(\theta^{T} x\right)$</p>
<p>【注】：由上式我们看出，LR模型假设数据服从伯努利分布</p>
<p>如果在对上式进行小小的转换，就会发现一个神奇的结论：$\ln (O d d s)=\theta^{T} x \Rightarrow O d d s=e^{\theta^{T} x}=\text {YouTubeServingFunction }$</p>
<p>原来Youtube的Serving函数e^{Wx+b}计算的不是别的，正是Odds！不过这个Odds跟用户观看时长有什么关系呢？</p>
<p>这就要提到YouTube采用的独特的训练方式Weighted LR，这里的Weight，对于正样本i来说就是观看时长Ti，对于负样本来说，则指定了单位权重1。Weighted LR的特点是，正样本权重w的加入会让正样本发生的几率变成原来的w倍，也就是说样本i的Odds变成了下面的式子：$O d d s(i)=\frac{w_{i} p}{1-w_{i} p}$。由于在视频推荐场景中，用户打开一个视频的概率p往往是一个很小的值，因此上式可以继续简化：$O d d s(i)=\frac{w_{i} p}{1-w_{i} p} \approx w_{i} p=T_{i} p=E\left(T_{i}\right)$。</p>
<p>而且由于YouTube采用了用户观看时长Ti作为权重，因此式子进一步等于Ti<em>p，这里真相就大白了，由于p就是用户打开视频的概率，Ti是观看时长，因此Ti</em>p就是用户观看某视频的期望时长！</p>
<p>因此，YouTube采用$e^{Wx+b}$这一指数形式预测的就是曝光这个视频时，用户观看这个视频的时长的期望！利用该指标排序后再进行推荐，是完全符合YouTube的推荐场景和以观看时长为优化目标的设定的。</p>
<p>【注】：</p>
<ul>
<li>为什么不直接预测观看视频的期望时长呢？一个明显的好处就是，分类的问题一般都比回归问题易于求解，且预测准确率更高。</li>
<li>训练Weighted LR一般来说有两种办法：<ul>
<li>将正样本按照weight做重复sampling，然后输入模型进行训练；</li>
<li>在训练的梯度下降过程中，通过改变梯度的weight来得到Weighted LR。</li>
</ul>
</li>
</ul>
<h2 id="不同隐层的实验"><a href="#不同隐层的实验" class="headerlink" title="不同隐层的实验"></a>不同隐层的实验</h2><p>下表是在不同NN网络结构下的实验结果。如果用户对模型预估高分的反而没有观看，我们认为是预测错误的观看时长。weighted, per-user loss就是预测错误观看时长占总观看时长的比例。</p>
<p>我们对网络结构中隐层的宽度和深度方面都做了测试，从下图结果看增加隐层网络宽度和深度都能提升模型效果。而对于1024—&gt;512—&gt;256这个网络，测试的不包含归一化后根号和方式的版本，loss增加了0.2%。而如果把weighted LR替换成LR，效果下降达到4.1%。</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/youtubednn/14.png" alt="图片"></p>
]]></content>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title>因式分解机</title>
    <url>/2020/06/07/%E5%9B%A0%E5%BC%8F%E5%88%86%E8%A7%A3%E6%9C%BA/</url>
    <content><![CDATA[<p>最近在做推荐系统，Factorization Machine是比较经典的特征组合算法。论文原文在：<a href="https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf" target="_blank" rel="noopener">https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf</a>。写这篇博客其实借鉴了很多其他人的介绍，记录博客也只是为了自己总结学习使用，在文后参考文献中可以具体看看大佬们的讲解。</p>
<a id="more"></a>
<p>简单的说FM（Factorization Machine）是一类自动特征组合算法。相比于传统的SVM、LR等模型，FM可以自动的进行特征的组合运算用以替代一部分人工特征工程的工作，并且计算量可控，对于海量输入特征仍然可以尝试使用这个方法。另一方面，对于稀疏特征，SVM等模型相对难以取得特别好的效果，而FM应用分解的方法可以很好的处理稀疏的输入。</p>
<p>相比较于其他的分解方法（SVD、PITF、FPMC等等）FM是一个更加通用化的特征组合与分解手段，其可以应用在多个场景下。总的来说FM有如下几个优势：</p>
<ul>
<li>适用于极端稀疏的输入</li>
<li>线性的计算复杂度支持海量数据训练</li>
<li>适用范围广</li>
</ul>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在认识因式分解机前，我们需要了解下LR、GBDT和SVD，他们都是算法上的老前辈。实际上因式分解机是LR和SVD的交集。</p>
<p>LR模型是CTR预估领域早期最成功的模型，大多工业推荐排序系统采取LR这种“线性模型+人工特征组合引入非线性”的模式。但是，LR模型最大的缺陷就是人工特征工程，耗时费力费人力资源，那么能否将特征组合的能力体现在模型层面呢？</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/fm/1.png" alt="图片"></p>
<p>后来，Facebook 2014年的文章介绍了通过GBDT解决LR的特征组合问题，这篇文章可以说红极一时，随后Kaggle竞赛也有实践此思路。为什么GBDT+LR可以做特征组合呢？GBDT是由多棵回归树组成的树林，后一棵树利用前面树林的结果与真实结果的残差做为拟合目标。每棵树生成的过程是一棵标准的回归树生成过程，因此每个节点的分裂是一个自然的特征选择的过程，而多层节点的结构自然进行了有效的特征组合，也就非常高效的解决了过去非常棘手的特征选择和特征组合的问题。</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/fm/2.png" alt="图片"></p>
<p>不过GBDT+LR也有它的缺点，比如要调的参数太多，而且只能使用batch的方式训练，比FTRL的方式慢很多。那么我们能不能在LR上直接增加两两特征组合(如下)呢？</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/fm/3.png" alt="图片"></p>
<p>其实上面的模型，跟带核函数的SVM有点像：</p>
<ul>
<li>核函数：$K(x, z)=&lt;\phi(x), \phi(z)&gt;=(<x, z>+1)^{2}$</li>
<li>$\begin{aligned}<br>\phi(x)=&amp;\left(1, \sqrt{2} x_{1}, \cdots, \sqrt{2} x_{n}, x_{1}^{2}, \cdots, x_{n}^{2}\right.\\<br>&amp;\left.\sqrt{2} x_{1} x_{2}, \cdots, \sqrt{2} x_{1} x_{n}, \sqrt{2} x_{2} x_{3}, \cdots, \sqrt{2} x_{n-1} x_{n}\right)<br>\end{aligned}$</li>
<li>输出：$\begin{aligned}<br>\hat{y}(x) &amp;=\sum_{i=1}^{n} w_{i} \phi\left(x_{i}\right)+w_{0} \\<br>&amp;=w_{0}+\sqrt{2} \sum_{i=1}^{n} w_{i} x_{i}+\sum_{i=1}^{n} w_{i i} x_{i}^{2}+\sqrt{2} \sum_{i=1}^{n-1} \sum_{j=i+1}^{n} w_{i j} x_{i} x_{i}<br>\end{aligned}$</li>
</ul>
<p>SVM看上去解决了特征组合的问题，但是要知道推荐算法使用的特征是非常稀疏的，如果在训练数据里两个特征并未同时在训练实例里见到过，也就是说xi和xj一起出现的次数是0，那么SVM是无法学会这个特征组合的权重的。这就会导致模型的泛化性变差。</p>
<p>换个思路，我们还可以使用Matrix Factorization。MF（Matrix Factorization，矩阵分解）模型是个在推荐系统领域里资格很深的老前辈协同过滤模型了。核心思想是通过两个低维小矩阵（一个代表用户embedding矩阵，一个代表物品embedding矩阵）的乘积计算，来模拟真实用户点击或评分产生的大的协同信息稀疏矩阵，本质上是编码了用户和物品协同信息的降维模型。当训练完成，每个用户和物品得到对应的低维embedding表达后，如果要预测某个 $User_i$对 $Item_j$的评分的时候，只要它们做个内积计算 &lt;$User_i$, $Item_j$&gt;，这个得分就是预测得分。<img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/fm/4.png" alt="图片"></p>
<p>在求解SVD时，我们可以经过一些转换把它变成一个经典机器学习问题。比如在SVD++中，用户u对物品i的评分就可以表示为：$\hat{r}_{u i}=\mu+b_{u}+b_{i}+q_{i}^{T}\left(p_{u}+\frac{1}{\sqrt{|N(u)|}} \sum_{j \in N(u)} x_{j}\right)$，其中x_j是物品向量，N(u)是和用户u有过隐式交互的物品集合。我们可以使用SGD来求解其中的参数。不过SVD++类的模型仅在非常受限的输入数据上工作，通用性不强。</p>
<h1 id="FM基本原理"><a href="#FM基本原理" class="headerlink" title="FM基本原理"></a>FM基本原理</h1><p>为了很好地解决上面这些问题，FM闪亮登场。</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/fm/5.png" alt="图片"></p>
<p>FM模型也直接引入任意两个特征的二阶特征组合，和SVM模型最大的不同，在于特征组合权重的计算方法。FM对于每个特征，学习一个大小为k的一维向量，于是，两个特征 $x_i$ 和$ x_j$ 的特征组合的权重值，通过特征对应的向量 $v_i$ 和 $v_j$ 的内积 $<v_i, v_j>$ 来表示。这本质上是在对特征进行embedding化表征，和目前非常常见的各种实体embedding本质思想是一脉相承的。</p>
<p>那么为什么说FM的这种特征embedding模式，在大规模稀疏特征应用环境下比较好用？为什么说它的泛化能力强呢？即使在训练数据里两个特征并未同时在训练实例里见到过，意味着 $x_i$和$x_j $一起出现的次数为0，如果换做SVM的模式，是无法学会这个特征组合的权重的。但是因为FM是学习单个特征的embedding，并不依赖某个特定的特征组合是否出现过，所以只要特征 $x_i$ 和其它任意特征组合出现过，那么就可以学习自己对应的embedding向量。于是，尽管 $x_i$和$x_j$ 这个特征组合没有看到过，但是在预测的时候，如果看到这个新的特征组合，因为 $x_i 和 x_j 都能学会自己对应的embedding，所以可以通过内积算出这个新特征组合的权重。这是为何说FM模型泛化能力强的根本原因。</p>
<p>其实本质上，这也是目前很多花样的embedding的最核心特点，就是从0/1这种二值硬核匹配，切换为向量软匹配，使得原先匹配不上的，现在能在一定程度上算密切程度了，具备很好的泛化性能。</p>
<h2 id="FM和MF的关系"><a href="#FM和MF的关系" class="headerlink" title="FM和MF的关系"></a>FM和MF的关系</h2><p>本质上，MF模型是FM模型的特例，MF可以被认为是只有User ID 和Item ID这两个特征Fields的FM模型，MF将这两类特征通过矩阵分解，来达到将这两类特征embedding化表达的目的。而FM则可以看作是MF模型的进一步拓展，除了User ID和Item ID这两类特征外，很多其它类型的特征，都可以进一步融入FM模型里，它将所有这些特征转化为embedding低维向量表达，并计算任意两个特征embedding的内积，就是特征组合的权重，如果FM只使用User ID 和Item ID，你套到FM公式里，看看它的预测过程和MF的预测过程是一样的。</p>
<h2 id="FM计算公式改进"><a href="#FM计算公式改进" class="headerlink" title="FM计算公式改进"></a>FM计算公式改进</h2><p>从FM的原始数学公式看，因为在进行二阶（2-order）特征组合的时候，假设有n个不同的特征，那么二阶特征组合意味着任意两个特征都要进行交叉组合，所以可以直接推论得出：FM的时间复杂度是n的平方。我们对它进行一些改进，以提升计算效率。改进总共分为4步：<img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/fm/6.png" alt="图片"></p>
<p>我们一步一步来看：</p>
<ul>
<li>step-1</li>
</ul>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/fm/7.png" alt="图片"></p>
<ul>
<li>step-2</li>
</ul>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/fm/8.png" alt="图片"></p>
<ul>
<li>step-3</li>
</ul>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/fm/9.png" alt="图片"></p>
<p>第3步转换不是很直接，我们把它再分成两步：</p>
<p>** step3-1</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/fm/10.png" alt="图片"></p>
<p>** step3-2</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/fm/11.png" alt="图片"></p>
<ul>
<li>step-4</li>
</ul>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/fm/12.png" alt="图片"></p>
<p>通过上述四步的公式改写，可以看出在实现FM模型时，时间复杂度就降低到了$O(k*n)$了，而虽说看上去n还有点大，但是其实真实的推荐数据的特征值是极为稀疏的，就是说大量xi其实取值是0，意味着真正需要计算的特征数n是远远小于总特征数目n的，无疑这会进一步极大加快FM的运算效率。【注】：上式中，n是特征总数；$v_{i,f}$是指第i个特征的embedding的第f个元素，k是特征向量的维度，我们最后求解的参数是v_i，也就是第i个特征的k维embedding；x_i是第i个特征值。</p>
<p>经过改写后，我们发现FM的第一个平方项等价于将FM的所有特征项的embedding的第i个元素进行累加，之后求内积。</p>
<p>从上面的描述可以知道FM可以在线性的时间内进行预测。因此模型的参数可以通过梯度下降的方法（例如随机梯度下降）来学习，FM模型的梯度如下：$\frac{\partial}{\partial \theta} \hat{y}(x)=\left\{\begin{array}{ll}<br>1, &amp; \text {if } \theta \text { is } w_{0} \\<br>x_{i}, &amp; \text {if } \theta \text { is } w_{i} \\<br>x_{i} \sum_{j=1}^{n} v_{j, f} x_{j}-v_{i, f} x_{i}^{2}, &amp; \text { if } \theta \text { is } v_{i, f}<br>\end{array}\right.$</p>
<p>由于$\sum_{j=1}^{n} v_{j, f} x_{j}$只与 f 有关，与 i 是独立的，可以提前计算出来，并且每次梯度更新可以在常数时间复杂度内完成，因此FM参数训练的复杂度也是 $O(kn)$。综上可知，FM可以在线性时间训练和预测，是一种非常高效的模型。FM的参数学习可以使用SGD或FTRL。</p>
<h1 id="FM应用"><a href="#FM应用" class="headerlink" title="FM应用"></a>FM应用</h1><h2 id="如何用FM做召回"><a href="#如何用FM做召回" class="headerlink" title="如何用FM做召回"></a>如何用FM做召回</h2><h3 id="主要步骤"><a href="#主要步骤" class="headerlink" title="主要步骤"></a>主要步骤</h3><p>首先我们离线训练好一个FM模型，我们把每个特征(也就是每一列)对应训练好的embedding向量保存起来。我们其实可以把特征划分为三个子集合: User、Item、Context：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/fm/13.png" alt="图片"></p>
<p>如果将推荐系统做个很高层级的抽象的话，可以表达成学习如下形式的映射函数：$y=F(\text {User, Item, Context})$，有了这个函数，当以后新碰到一个Item，我们把用户特征，物品特征以及用户碰到这个物品时的上下文特征输入F函数，F函数会告诉我们用户是否对这个物品感兴趣。</p>
<h3 id="具体实例"><a href="#具体实例" class="headerlink" title="具体实例"></a>具体实例</h3><p>假设一个电影评分系统，根据用户和电影的特征，预测用户对电影的评分。系统记录了用户(U)在特定时间(t)对电影(i)的评分(r)，评分为1，2，3，4，5。给定一个例子：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/fm/14.png" alt="图片"></p>
<p>把它转化成Feature Vector如下图：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/fm/15.png" alt="图片"></p>
<p>每行表示目标 y^{(i)}与其对应的特征向量 x^{(i)}，蓝色区域表示了用户变量，红色区域表示了电影变量，黄色区域表示了其他隐含的变量，进行了归一化，绿色区域表示一个月内的投票时间，棕色区域表示了用户上一个评分的电影，最右边的区域是评分。</p>
<p>通过训练FM，对于某个用户，我们可以把属于这个用户子集合的特征，查询离线训练好的FM模型对应的特征embedding向量，然后将n个用户子集合的特征embedding向量累加，形成用户兴趣向量U(如A)。类似的，我们也可以把每个物品，其对应的物品子集合的特征，查询离线训练好的FM模型对应的特征embedding向量，然后将m个物品子集合的特征embedding向量累加，形成物品向量I(如Movie TI + Other Movies rated TI + Last Movie rated TI)。</p>
<p>用户兴趣向量U可以离线算好，然后更新线上的对应内容；物品兴趣向量I可以类似离线计算或者近在线计算，问题都不大。</p>
<p>当用户登陆或者刷新页面时，可以根据用户ID从redis中取出其对应的兴趣向量embedding，然后和Faiss中存储的物料embedding做内积计算，按照得分由高到低返回得分Top K的物料作为召回结果。提交给第二阶段的排序模型进行进一步的排序。</p>
<p>不过在这里我们要想一想，这种累加用户embedding特征向量以及累加物品embedding特征向量，之后做向量内积。这种算法符合FM模型的原则吗？和常规的FM模型是否等价？我们来分析一下。这种做法其实是在做用户特征集合U和物品特征集合I之间两两特征组合，是符合FM的特征组合原则的，考虑下列公式是否等价就可以明白了：</p>
<ul>
<li>$\left\langle\sum_{i} U_{i}, \sum_{j} I_{j}\right\rangle$</li>
<li>$\sum_{i} \sum_{j}\left\langle U_{i}, I_{j}\right\rangle$</li>
</ul>
<p>当然，跟完全版本的FM比，我们没有考虑U和I特征集合内部任意两个特征的组合。也可以这么思考问题：在上文我们说过，FM为了提升计算效率，对公式进行了改写，改写后的高效计算公式的第一个平方项其实等价于：把所有特征embedding向量逐位累加成一个求和向量V，然后自己和自己做个内积操作$<V,V>$。这样等价于根据FM的原则计算了任意两个特征的二阶特征组合了。而上面描述的方法，和标准的FM的做法其实是一样的，区别无非是将特征集合划分为两个子集合U和I，分别代表用户相关特征及物品相关特征。而上述做法其实等价于在用户特征和物品特征之间做两两特征组合，只是少了U内部之间特征，及I内部特征之间的特征组合而已。一般而言，其实我们不需要做U内部特征之间以及I内部特征之间的特征组合，对最终效果影响很小。于是，沿着这个思考路径，我们也可以推导出上述做法基本和FM标准计算过程是等价的。</p>
<h3 id="增加上下文信息"><a href="#增加上下文信息" class="headerlink" title="增加上下文信息"></a>增加上下文信息</h3><p>之所以把上下文特征单独拎出来，是因为它有自己的特点，有些上下文特征是近乎实时变化的，比如刷新微博的时间，再比如对于美团嘀嘀这种对地理位置特别敏感的应用，用户所处的地点可能随时也在变化，而这种变化在召回阶段就需要体现出来。所以，上下文特征是不太可能像用户特征离线算好存起来直接使用的，而是用户在每一次刷新可能都需要重新捕获当前的特征值。动态性强是它的特点。我们可以通过下面的步骤将上下文信息融入召回模型中。</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/fm/16.png" alt="图片"></p>
<p>首先，由于上下文特征的动态性，所以给定用户UID后，可以在线查询某个上下文特征对应的embedding向量，然后所有上下文向量求和得到综合的上下文向量C。</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/fm/17.png" alt="图片"></p>
<p>然后，将在线算好的上下文向量C和这个用户的事先算好存起来的用户兴趣向量U进行内积计算Score=<U,C>。这个数值代表用户特征和上下文特征的二阶特征组合得分，算好备用。至于为何这个得分能够代表FM中的两者（U和C）的特征组合，其实道理和上面讲的U和I做特征组合道理是一样的。</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/fm/18.png" alt="图片"></p>
<p>再然后，将U和C向量累加求和，利用（U+C）去Faiss通过内积方式取出Top K物品，这个过程和极简版是一样的，无非查询向量由U换成了（U+C）。通过这种方式取出的物品同时考虑到了用户和物品的特征组合<U,I>，以及上下文和物品的特征组合<C,I>。</p>
<p>假设返回的Top K物品都带有内积的得分Score1，再考虑上一步<U,C>的得分Score，将两者相加对物品重排序（<U,C>因为跟物品无关，所以其实不影响物品排序，但是会影响最终得分，FM最外边的Sigmoid输出可能会因为加入这个得分而发生变化），就得到了最终结果，而这个最终结果考虑了U/I/C两两之间的特征组合。</p>
<h2 id="FM与多路召回"><a href="#FM与多路召回" class="headerlink" title="FM与多路召回"></a>FM与多路召回</h2><p>目前工业界的推荐系统，在召回阶段，一般都采取多路召回策略，常用的召回策略，基本都会包含，比如兴趣标签，兴趣Topic，兴趣实体，协同过滤，热门，相同地域等，多者几十路召回，少者也有7／8路召回。</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/fm/19.png" alt="图片"></p>
<p>对于每一路召回，会拉回K条相关物料，这个K值是个超参，需要通过线上AB测试来确定合理的取值范围。如果召回路数太多，对应的超参就多，这些超参组合空间很大。另外，如果是多路召回，这个超参往往不太可能是用户个性化的，而是对于所有用户，每一路拉回的数量都是固定的，但是不同用户也许对于每一路内容感兴趣程度是不一样的，更感兴趣的那一路就应该多召回一些，所以如果能把这些超参改为个性化配置是很好的，但是多路召回策略下不是很好做。</p>
<p>在召回阶段，能否用一个统一的模型把多路召回改造成利用单个模型，单路召回的模式？FM就可以解决这个问题。我们先来说一些多路召回需要解决哪些问题：</p>
<ul>
<li>不同路的item召回得分不可比</li>
<li>每一路召回的item个数k是个超参，需要不断调整</li>
<li>做recall和ranking的是两拨人，如果新增了一路召回和做ranking的人不知道，导致新增一路的召回特征没能加到ranking特征中，那么即便召回来了，也排不到前面去</li>
</ul>
<p>那么用FM就能解决上面这些问题吗？答案是可以的。比如如果有的用户喜欢看热门的内容，那么热门内容在召回阶段返回的比例就高，这样的参数完全用FM进行个性化。在比如新增一路召回时，就是新增了一路特征，对ranking来说也好加进去。</p>
<h1 id="FM的引申——FFM"><a href="#FM的引申——FFM" class="headerlink" title="FM的引申——FFM"></a>FM的引申——FFM</h1><p>FFM（Field-aware Factorization Machine）最初的概念来自Yu-Chin Juan与其比赛队员，是他们借鉴了来自Michael Jahrer的论文中的field概念提出了FM的升级版模型。通过引入field的概念，FFM把相同性质的特征归于同一个field。以广告分类为例，“Day=26/11/15”、“Day=1/7/14”、“Day=19/2/15”这三个特征都是代表日期的，可以放到同一个field中。同理，商品的末级品类编码生成了550个特征，这550个特征都是说明商品所属的品类，因此它们也可以放到同一个field中。简单来说，同一个categorical特征经过One-Hot编码生成的数值特征都可以放到同一个field，包括用户性别、职业、品类偏好等。在FFM中，每一维特征x_i，针对其它特征的每一种field f_j，都会学习一个隐向量 v_{i,f_j}。因此，隐向量不仅与特征相关，也与field相关。也就是说，“Day=26/11/15”这个特征与“Country”特征和“Ad_type”特征进行关联的时候使用不同的隐向量，这与“Country”和“Ad_type”的内在差异相符，也是FFM中“field-aware”的由来。</p>
<p>假设样本的n个特征属于f个field，那么FFM的二次项有nf个隐向量。而在FM模型中，每一维特征的隐向量只有一个。FM可以看作FFM的特例，是把所有特征都归属到一个field时的FFM模型。根据FFM的field敏感特性，可以导出其模型方程：$y(\mathbf{x})=w_{0}+\sum_{i=1}^{n} w_{i} x_{i}+\sum_{i=1}^{n} \sum_{j=i+1}^{n}\left\langle\mathbf{v}_{i, f_{j}}, \mathbf{v}_{j, f_{i}}\right\rangle x_{i} x_{j}$</p>
<p>其中，f_j 是第j个特征所属的field。如果隐向量的长度为k，那么FFM的二次参数有nfk 个，远多于FM模型的nk 个。此外，由于隐向量与field相关，FFM二次项并不能够化简，其预测复杂度是O(kn^2)。</p>
<p>有关于FFM的具体实现细节，美团的这篇博客写得很好，可以参考：<a href="https://tech.meituan.com/2016/03/03/deep-understanding-of-ffm-principles-and-practices.html" target="_blank" rel="noopener">https://tech.meituan.com/2016/03/03/deep-understanding-of-ffm-principles-and-practices.html</a>。</p>
<h1 id="代码及参考文献"><a href="#代码及参考文献" class="headerlink" title="代码及参考文献"></a>代码及参考文献</h1><p>可阅读并参考的代码：</p>
<ul>
<li><a href="https://github.com/hibayesian/spark-fm/blob/master/src/main/scala/org/apache/spark/ml/fm/FactorizationMachines.scala" target="_blank" rel="noopener">https://github.com/hibayesian/spark-fm/blob/master/src/main/scala/org/apache/spark/ml/fm/FactorizationMachines.scala</a></li>
<li><a href="https://github.com/shenweichen/DeepMatch/blob/master/deepmatch/models/fm.py" target="_blank" rel="noopener">https://github.com/shenweichen/DeepMatch/blob/master/deepmatch/models/fm.py</a></li>
</ul>
<p>参考文献：</p>
<ul>
<li>[<a href="https://zhuanlan.zhihu.com/p/58160982](" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/58160982](</a></li>
</ul>
]]></content>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title>CRF学习——基础学习</title>
    <url>/2020/05/17/CRF%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>CRF是NLP中很常用且经典的模型，今天就复习一下CRF模型。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://arxiv.org/abs/1011.4088" target="_blank" rel="noopener">https://arxiv.org/abs/1011.4088</a><br><a href="http://cseweb.ucsd.edu/~elkan/250Bwinter2012/loglinearCRFs.pdf" target="_blank" rel="noopener">http://cseweb.ucsd.edu/~elkan/250Bwinter2012/loglinearCRFs.pdf</a></p>
</blockquote>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><h2 id="生成模型-VS-判别模型"><a href="#生成模型-VS-判别模型" class="headerlink" title="生成模型 VS 判别模型"></a>生成模型 VS 判别模型</h2><p>下面这张图展示了生成模型和判别模型的联系和区别：这两种模型都是用极大似然估计来估计概率，只是两个估计的概率公式不同，生成模型是$p(x, y)$，判别模型是$p(y|x)$。为什么生成模型能生成数据呢，因为$p(x, y) = p(y) * p(x|y)$，我们可以学习到$p(x|y)$，那么当我们采样y时，就可以根据它生成x了。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/1.png" alt="图片"></p>
<p>生成模型可以用于生成，也可以用于判别。但判别模型只能用于判别。生成模型需要学习到更多有关数据的细节。比如有很多猫和狗的图片，对于生成模型，它就会学到很多猫和狗的长什么样的细节（因为学不到的话它也生成不出来）；但对于判别模型，主要学习到猫和狗的区别，如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/2.png" alt="图片"></p>
<p>生成模型的目标函数是P(x, y)，而判别函数是P(y|x)。转化$P(x,y)=P(y)P(x|y)$，也就是生成模型学习到的一个是$P(x|y)$，比如对于y=猫来说，能学习到一个x的分布（即猫的特征分布，有时这个分布用高斯分布去模拟），如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/3.png" alt="图片"></p>
<p>那怎么生成呢？事实上我们可以从$P(x|y=猫)$这个分布中去采样一个x（比如对于图片来说，x就是像素矩阵），而这个x就是一个猫的图片了。下面再以预测男、女身高为例，做一个生成模型，过程如下图（当需要生成时，直接从正太分布x~$N(\mu, \sigma^2$)中采样即可）：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/4.png" alt="图片"></p>
<p>那么在判别问题上，哪个模型效果更好呢？从生成视角看，生成式建模的是$P(x,y)=P(y)<em>P(x|y)$，从判别视角看，生成式建模的是$P(x,y)=P(x)</em>P(y|x)$。而判别式建模的就是$P(y|x)$。也就是两类模型都去做了判别$P(y|x)$，而生成模型同时学习了$P(x)$（可以理解成一个prior）。从直观上感觉，只做一件事情的判别模型会更好。因为生成模型要同时兼顾$P(x)$和$P(y|x)$，它要找到这两者之间的一个平衡点。因此，我们可以得出以下结论：</p>
<ul>
<li>当数据量大时，使用判别模型会优于生成模型</li>
<li>当数据量小时，生成模型有可能比判别模型要好（因为生成模型有prior，即p(x)，有时为了防止过拟合，会在模型中加入正则项，其实这个正则项就是一个prior）</li>
</ul>
<p>【注】：上面提到的p(x, y)、p(x|y)其实是所有概率的乘积，即<img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/5.png" alt="图片"></p>
<h2 id="Directed-Model-VS-Undirected-Model"><a href="#Directed-Model-VS-Undirected-Model" class="headerlink" title="Directed Model VS Undirected Model"></a>Directed Model VS Undirected Model</h2><p>有明确依赖关系的叫有向图。比如做金融风控，如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/6.png" alt="图片"></p>
<p>没有依赖关系的叫无向图。比如在做Image Segmentation时，需要对每个点进行分类。如果把每个点看成时独立的话，缺少了像素点之间的联系。因此可以把它看成一个graphic model。因为点之间很难有方向的关系，因此使用无向图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/7.png" alt="图片"></p>
<h2 id="Joint-Probability"><a href="#Joint-Probability" class="headerlink" title="Joint Probability"></a>Joint Probability</h2><p>使用联合概率来建模图模型（后续可以用这个联合概率进行分类）。有向图的联合概率很好写：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/8.png" alt="图片"></p>
<p>在生成模型HMM中，其概率公式如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/9.png" alt="图片"></p>
<p>无向图有两种方法建模：</p>
<ul>
<li>Maximal Clique方法：先分成团，每个团用score function计算一个契合度。而这个契合度和团里的元素的联合概率是正相关的，因此这个score function也可以用联合概率表示，如下图：</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/10.png" alt="图片"></p>
<ul>
<li>Pair-wise</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/11.png" alt="图片"></p>
<p>【注】：</p>
<ul>
<li>因为score function得出的是一个相对值，不同feature、不同算法得出的结果不一样。但最后我们是想得到P(1,2,3,4,5)这个联合概率的绝对值的（后续做判别时会用到），因此我们除以了一个归一化项z。这个z要考虑所有可能的情况，在实际情况中我们可以使用维特比来简化计算量。</li>
<li>z是一个global normalization，使用这种全局的归一化有一个好处，就是图中的结点是有一个偏好的，但是z是一个全局的量，可以修正结点的偏好，使得模型考虑得更加全面。</li>
<li>在有向图中使用的是local normalization，比如P(4|2,5)只需要关注P(2,5)，因此有向图是从局部角度考虑的</li>
<li>z的计算量比较大，如果变量是离散型的，则可以使用维特比算法；如果变量是连续型的，则只能使用一些近似算法（partition funciton，比如MCMC）</li>
</ul>
<p>接下来有一个问题，这个score function究竟是怎么计算出来的呢？</p>
<p>首先，我们假设score function计算的是变量之间的compatability。所以，变量之间关系越紧密，则score function结果越大。例如，假设a、b、c是三位学生，他们在一起做项目。则score function可以表示三位学生合作的结果。他们关系越好，合作越紧密，则得分越高。紧接着，我们要考虑这三位学生的一些特征。我们要选择一些特征，能刻画出三位学生是否能合作好。比如三个学生是否来自同一个班级(f1)，以及他们之前是否合作过(f2)，以及他们之间的能力是否互补(f3)等等。我们可以把这些特征做一个线性组合来得到score（linear model）。当然也可以使用非线性的方式构造score function，这个时候就是log-linear crf。下图概括了这个过程：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/12.png" alt="图片"></p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/13.png" alt="图片"></p>
<p>特征工程一般分两种，一种是人工特征，另一种是使用深度学习特征（比如BiLSTM-CRF，卷积核等）。</p>
<h2 id="RoadMap"><a href="#RoadMap" class="headerlink" title="RoadMap"></a>RoadMap</h2><p>我们通过对特征函数进行不同假设，可以得到不同模型：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/14.png" alt="图片"></p>
<p>由上图可以看到，假设特征函数是线性函数时，可以产生Log-Linear Model（顾名思义，就是对score funciton先取linear，再取log）。Log-Linear Model的具体模型常见的有逻辑回归和Linear-Chain CRF。那么从另一个角度，有向图模型HMM对于NLP问题有一些缺陷，我们把它从生成模型改造成一个判别模型MEMM，但MEMM有Label Bias的问题，因此我们又把MEMM改成一个无向图模型，就得到了Linear Chain CRF。对于Linear Chain CRF，我们关心的是它的参数估计和Inference算法。因为它也属于Log-Linear Model，所以我们要先学习的是Log-Linear Model的参数估计和Inference。</p>
<p>当然，如果我们假设特征函数是非线性的，会得到Non-linear CRF。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/15.png" alt="图片"></p>
<p>上面这张图是更简明一些的roadmap。对于生成模型，最简单的是朴素贝叶斯，当数据是一条链时，就是HMM，当数据是一个图的时候，就是贝叶斯网络。对于判别模型，最简单的是逻辑回归，当是一条链是就是CRF，当是一个图时就是更general的CRF。</p>
<h1 id="Log-Linear-Model"><a href="#Log-Linear-Model" class="headerlink" title="Log-Linear Model"></a>Log-Linear Model</h1><p>首先我们来看这样一个无向图模型和它的联合概率（用all cliques方法定义联合概率）：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/16.png" alt="图片"></p>
<p>那么接下来的问题就是，如何去定义这个score function呢？我们用一个指数函数计算score：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/17.png" alt="图片"></p>
<p>上图式子计算了一个clique的分数，其中$F_j(y_c)$是特征函数。比如$y_1$-$y_2$-$y_3$这个无向图中，我们有2个特征，分别就是$F_1(y_1, y_2, y_3)$、$F_2(y_1, y_2, y_3)$。也就是说，每一个clique里可以抽取出J个特征（每个特征都是一个数）。$w_j*F_j(y_c)$就是一个线性函数，整个模型的参数$\theta={w}$。</p>
<p>那么F特征是什么呢？F主要评估了变量之间的契合度。那为什么这个模型叫Log-Linear呢？看下图就知道了：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/18.png" alt="图片"></p>
<p>下面用文本的一个例子来说明计算这个联合概率的过程：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/19.png" alt="图片"></p>
<h2 id="Multinomial-Logistic-Regression"><a href="#Multinomial-Logistic-Regression" class="headerlink" title="Multinomial Logistic Regression"></a>Multinomial Logistic Regression</h2><p>我们可以将逻辑回归看成是x、y的一个无向图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/20.png" alt="图片"></p>
<p>那么逻辑回归的条件概率是什么呢？（这里我们把每个标签作为一个clique）</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/21.png" alt="图片"></p>
<p>其中特征函数f_j定义如下，可以看出来我们并没有做特征工程，而是把输入x_j直接看成是特征：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/22.png" alt="图片"></p>
<p>那么一共有多少特征呢？如果标签y有3个(即y={1,2,3})，x的维度为d，则特征总个数为3d，即J=3d。比如，我们分别令y = 1, 2, 3，则$f_j$的具体值如下：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/23.png" alt="图片"></p>
<p>接着，我们把$f_j$的值带入到$P(y|x;\theta)$中：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/24.png" alt="图片"></p>
<p>可以看出来，参数w的维度是3d。我们把上面式子在形式上进行一些简化：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/25.png" alt="图片"></p>
<p>可以看出来这就是一个多元逻辑回归模型（可以用在多分类问题上）。</p>
<h1 id="HMM存在的一些问题"><a href="#HMM存在的一些问题" class="headerlink" title="HMM存在的一些问题"></a>HMM存在的一些问题</h1><h3 id="MEMM"><a href="#MEMM" class="headerlink" title="MEMM"></a>MEMM</h3><p>假设我们用HMM做词性标注，构建如下模型，会发现有什么问题？</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/26.png" alt="图片"></p>
<p>很显然，对于词性标注而言，词性$z_1$不仅仅只依赖于当前词$x_3$，还可能跟其他词有关。那么怎么改进这个模型呢？为了让$z_1$跟所有词相关，我们把它改造成下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/27.png" alt="图片"></p>
<p>但上图这个模型又有什么问题呢？我们先写一下上图的联合概率：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/28.png" alt="图片"></p>
<p>可以发现上式中第二项，也就是发射概率，是很难表示的。如果说联合概率很难表示，能不能把它改造成一个判别模型呢，用条件概率去表示呢？下图就是改造后的判别模型：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/29.png" alt="图片"></p>
<p>我们可以看出来上图中$P(z_i|z_{i-1}, x)$是很好表示的（比如用一个线性函数+softmax），那么这个模型就是MEMM模型。下面这张图是MEMM的另一种表示：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/30.png" alt="图片"></p>
<p>但是MEMM有什么问题呢？</p>
<h3 id="Label-Bias-Problem"><a href="#Label-Bias-Problem" class="headerlink" title="Label Bias Problem"></a>Label Bias Problem</h3><p>首先，我们来看一看什么叫Label Bias。我们先来看下图，哪一条路径的概率最大呢？</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/31.png" alt="图片"></p>
<p>很明显，分支比较多的状态的概率概率较小（相对而言不对它们不是很公平），分支较少的路径概率更大。举个例子，做词性分析时，动词的后面可能的词性较少，名词后面的词性就比较多。Label bias的根本原因是local normalization。那么怎么解决这个问题呢？</p>
<p>我们可以把概率换成是score来表示，有了分数，我们就可以用global normalization了。这也就是无向图和有向图的主要区别，如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/32.png" alt="图片"></p>
<p>我们可以将(z1, z2)、(z2, z3)、(z3, z4)看成是clique，计算(z1, z2, x)等的feature function (f1, f2, f3, …f10)，再进行加权，就有了score。</p>
<h3 id="从Log-linear-Model到Log-linear-CRF"><a href="#从Log-linear-Model到Log-linear-CRF" class="headerlink" title="从Log-linear Model到Log-linear CRF"></a>从Log-linear Model到Log-linear CRF</h3><p>我们直接从Log-linear的公式进行推导，其实就是把特征函数$f_j(x,y)$表示出来。在无向图中我们把这个特征写成是$F_j(x,y)$（这里的x、y可以看成是一个时间的序列，而不是一个单独的变量）。由于y是一个状态集合，因而(x,y)的特征函数由(y1, y2, x)、(y2, y3, x)、(y3, y4, x)…组成，因此$F_j(x,y)=sum(f_j(y_i, y_{i-1}, x))$。如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/33.png" alt="图片"></p>
<p>$f_j$可以通过人工，或者神经网络的方式得到，接下来就是求解$w_j$。我们的目标是最大化似然函数：<img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/34.png" alt="图片">，因此选择好一个优化方法，就可以求解$w_j$了（比如将loss作为分子对w求偏导）。</p>
<p>【注】：</p>
<ul>
<li>上式中T是团的个数，在实践序列中，就是timestep-1</li>
<li>上式中exp有两个sum，其中第一个sum是对每个clique的结果求和（因为我们要把所有clique的结果乘起来），第二个sum是对每个clique内部所有特征的加和。？好像写反了</li>
<li>$f_j(y_i, y_{i-1}, x)$中的$f_j$在神经网络中常使用softmax</li>
<li>$f_j(y_i, y_{i-1}, x)$中的的x可以理解为x1, x2,…，即特征是根据$y_i, y_{i-1}$和所有的x计算出来的，即$f_j(y_i, y_{i-1}, x_1, x_2, …)$</li>
</ul>
<h1 id="Log-linear-CRF"><a href="#Log-linear-CRF" class="headerlink" title="Log-linear CRF"></a>Log-linear CRF</h1><p>我们的目标是求解：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/35.png" alt="图片"></p>
<p>看上去跟逻辑回归差不多，只不过这里的x、y是时间序列。</p>
<h2 id="Inference-Probelm"><a href="#Inference-Probelm" class="headerlink" title="Inference Probelm"></a>Inference Probelm</h2><p>首先我们来看一下，在x、w已知的情况下，我们怎么求解使得P(y|x;w)最大的y。这看上去和逻辑回归没有区别哈，逻辑回归里y={0,1}，这里的y是一个序列。</p>
<p>我们先把式子按照定义展开：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/36.png" alt="图片"></p>
<p>看到上面的形式，我们应首先想到维特比算法，一想到维特比算法，就要想到路径，路径的sum值是最大的，如下图（n是时间长度，m是状态个数）：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/37.png" alt="图片"></p>
<p>我们定义的子问题是：$u(k, v)$表示从$t=1$到$t=k$，且t时刻的状态为v的最好的路径的分数。我们最终想求的是$max(u(n, 1)$、$u(n, 2)$… $u(n, m))$为最终答案，如下图：<img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/38.png" alt="图片"></p>
<p>那么我们怎么把$u(k, v)$表示成一个子问题呢？从t=1到t=k一定会经过$u(k-1, 1)$或$u(k-1, 2)$…或$u(k-1, m)$，如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/39.png" alt="图片"></p>
<p>我们用式子表达出来是：<img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/40.png" alt="图片"></p>
<p>看上去，我们把这个二维的矩阵里的score值逐步填充进去就可以了。当想求解最佳状态的路径时，只需要设置好pointer，反向就能从矩阵中找到最佳路径了。整个算法的复杂度是O(m<em>n)</em>O(m)。</p>
<p>【注】：如果score函数是$g_i(y_{i-2}, y_{i-1}, y_{i})$，那么算法复杂度就可能变为$O(m^2<em>n)</em>O(m)$</p>
<h2 id="Parameter-Estimation"><a href="#Parameter-Estimation" class="headerlink" title="Parameter Estimation"></a>Parameter Estimation</h2><h3 id="Log-Linear的w求解"><a href="#Log-Linear的w求解" class="headerlink" title="Log-Linear的w求解"></a>Log-Linear的w求解</h3><p>接下来我们来看，在x、y已知的情况下，我们怎么求解w？求导数就好了。我们先来看一下Log-Linear的w是怎么求解的：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/41.png" alt="图片"></p>
<p>上式中最后一项可以看成是$F_j(x, y’)$的期望：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/42.png" alt="图片"></p>
<p>可以看到第一项$F_j(x, y)$是x、y的特征工程，后一项是关于y’的期望。</p>
<h3 id="Log-Linear-CRF的w求解"><a href="#Log-Linear-CRF的w求解" class="headerlink" title="Log-Linear CRF的w求解"></a>Log-Linear CRF的w求解</h3><ul>
<li>求解partition function z<ul>
<li>Forward算法：在CRF中，partition function z(x, w)中的x变成了一个序列，我们来看一下这个z怎么计算。我们依然可以使用动态规划算法求解$sum(g_i(y_{i-1}, y_i))$。具体如下图所示：<img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/43.png" alt="图片"></li>
</ul>
</li>
</ul>
<p>上图中$\alpha(k+1, v)$表示$y_{k+1}=v$的情况下，所有状态的可能性。为了求解$\alpha(k+1, v)$，我们把$y_k$作为一个中介，而$y_k$有1~u种可能。举个栗子，假如有4个状态，3个输出，那么我们最后想求$\alpha(4, 1)+\alpha(4,2)+\alpha(4,3)$的和作为z(x, w)​，且$\alpha(4,1)=\alpha(3,1) + \alpha(3,2) + \alpha(3,3)$，如下图：<img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/44.png" alt="图片"></p>
<ul>
<li><p>Backward算法：跟Forward相似，只不过是$y_k=u$时考虑${y_k{k+1}, y_n}$的所有可能性，这时候$y_{k+1}$成为了中介。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/45.png" alt="图片"><img src="https://uploader.shimo.im/f/wJoM9tAvdSPjIp06.png!thumbnail" alt="图片"></p>
</li>
</ul>
<ul>
<li><p>求解$P(y_k=u|x;w)$</p>
<ul>
<li>$z(x, w)$是考虑所有的可能性，因此可以认为$z(x, w)=p(y_k=1|x;w) + p(y_k=2|x, w) + … + p(y_k=m|x;w)$</li>
<li>根据前向算法和后向算法，可以算出$z(x, w)$</li>
<li>$p(y_k=u|x;w)$表示$y_k=u$的情况下，前向和后向的所有可能情况，如下图：</li>
</ul>
</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/46.png" alt="图片"></p>
<p>为了求出w，我们需要对$p(y|x;w)$求导，</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/crf/47.png" alt="图片"></p>
]]></content>
      <tags>
        <tag>CRF</tag>
      </tags>
  </entry>
  <entry>
    <title>词对词翻译的那些事儿</title>
    <url>/2020/05/17/%E8%AF%8D%E5%AF%B9%E8%AF%8D%E7%BF%BB%E8%AF%91%E7%9A%84%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF/</url>
    <content><![CDATA[<p>当我们有大量平行语料，通过什么方法能快速构建出一个词典呢？今天讲的就是基于这个需求所调研的一些方法总结。基本方法有3种：基于统计、基于词对齐、基于词向量。由于基于词对齐比较好理解，就是使用IBM Model进行词对齐后，将translation table拿出来就可以作为一个词表，因此本文不对此进行介绍。本文重点说明另外两种方法：基于统计和基于词向量方法。</p>
<a id="more"></a>
<h1 id="基于统计的方法"><a href="#基于统计的方法" class="headerlink" title="基于统计的方法"></a>基于统计的方法</h1><blockquote>
<p>参考论文：word2word: A Collection of Bilingual Lexicons for 3,564 Language Pairs<br><a href="https://zhuanlan.zhihu.com/p/45730674" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/45730674</a><br><a href="https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf" target="_blank" rel="noopener">https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf</a><br><a href="https://cloud.tencent.com/developer/article/1436217" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1436217</a></p>
</blockquote>
<p>这篇论文的思想很简单，就是从translation pair共现的角度在平行语料库中挖掘出符合条件的pair。因此，算法首先找到跟每一个source word共现过的target word，其次使用下面三种方法对target word与source word的相关度进行重新排序，最终取前n名作为source word的释义：</p>
<ul>
<li>共现次数：$p(y | x)=\frac{p(x, y)}{p(x)} \approx \frac{c(x, y)}{c(x)} \propto c(x, y)$</li>
<li><p>互信息：$\begin{aligned}<br>\operatorname{PMl}(x, y) &amp;=\log \frac{p(x, y)}{p(x) p(y)} \\<br>&amp; \approx \log \frac{c(x, y)}{c(x) c(y)} \propto \log c(x, y)-\log c(y)<br>\end{aligned}$</p>
</li>
<li><p>Controlled Predictive Effects（CPE）：$\begin{aligned}<br>\operatorname{CPE}(y | x) &amp;=p(y | x)-\sum_{x^{\prime} \in \mathcal{X}} p\left(y | x^{\prime}\right) p\left(x^{\prime} | x\right) \\<br>&amp;=\sum_{x^{\prime} \in \mathcal{X}} \operatorname{CPE}_{y | x}\left(x^{\prime}\right) p\left(x^{\prime} | x\right)<br>\end{aligned}$</p>
</li>
</ul>
<h2 id="互信息"><a href="#互信息" class="headerlink" title="互信息"></a>互信息</h2><p>PMI是NLP中的一个重要指标，在本文中它可以用来阻止一些stop word有较高的得分。互信息是信息论里一种有用的信息度量，它可以看成是一个随机变量中包含的关于另一个随机变量的信息量，或者说是一个随机变量由于已知另一个随机变量而减少的不肯定性。</p>
<p>设两个随机变量$(X,Y)$的联合分布为$p(x, y)$，边缘分布分别为$p(x),p(y)$    ，互信息$I(X;Y)$是联合分布$p(x,y)$与边缘分布$p(x)p(y)$的相对熵，即$I(X ; Y)=\sum_{x \in X} \sum_{y \in Y} p(x, y) \log \frac{p(x, y)}{p(x) p(y)}$</p>
<p>上面的$I(X;Y)$其实是可以推导出来的。根据熵的连锁规则，有$H(X, Y)=H(X)+H(Y | X)=H(Y)+H(X | Y)$。因此，$H(X)-H(X | Y)=H(Y)-H(Y | X)$</p>
<p>这个差叫做X和Y的互信息，记作I(X;Y)。按照熵的定义展开可以得到：$\begin{aligned}<br>I(X ; Y) &amp;=H(X)-H(X | Y) \\<br>&amp;=H(X)+H(Y)-H(X, Y) \\<br>&amp;=\sum_{X} p(x) \log \frac{1}{p(x)}+\sum_{y} p(y) \log \frac{1}{p(y)}-\sum_{x, y} p(x, y) \log \frac{1}{p(x, y)} \\<br>&amp;=\sum_{x, y} p(x, y) \log \frac{p(x, y)}{p(x) p(y)}<br>\end{aligned}$</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/word2word/1.png" alt="图片"></p>
<p>直观上，互信息度量 X 和 Y 共享的信息：它度量知道这两个变量其中一个，对另一个不确定度减少的程度。例如，如果 X 和 Y 相互独立，则知道 X 不对 Y 提供任何信息，反之亦然，所以它们的互信息为零。在另一个极端，如果 X 是 Y 的一个确定性函数，且 Y 也是 X 的一个确定性函数，那么传递的所有信息被 X 和 Y 共享：知道 X 决定 Y 的值，反之亦然。因此，在此情形互信息与 Y（或 X）单独包含的不确定度相同，称作 Y（或 X）的熵。而且，这个互信息与 X 的熵和 Y 的熵相同。</p>
<p>互信息是 X 和 Y 联合分布相对于假定 X 和 Y 独立情况下的联合分布之间的内在依赖性。于是互信息以下面方式度量依赖性：I(X; Y) = 0 当且仅当 X 和 Y 为独立随机变量。从一个方向很容易看出：当 X 和 Y 独立时，$p(x,y) = p(x) p(y)$，因此：$\log \left(\frac{p(x, y)}{p(x) p(y)}\right)=\log 1=0$</p>
<p>可见，互信息是非负的（$I(X;Y) ≥ 0$），且是对称的（即 $I(X;Y) = I(Y;X)$）。</p>
<p>其实从PMI角度，可以将skip-gram word2vec模型看成是对一个shifted PMI matrix的矩阵分解的结果，同时PMI和TF-IDF的原理也十分接近。下面我们介绍一篇论文，来从PMI角度理解word2vec，这篇论文为Neural Word Embedding as Implicit Matrix Factorization，其原文在<a href="https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf" target="_blank" rel="noopener">https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf</a>中。</p>
<p>这篇文章认为，word2vec本质上是对一个word-context相关度的PMI矩阵进行分解。我们可以把word-context看成一个row为word，column为context的矩阵。首先我们定义一些变量：</p>
<ul>
<li>word: $w \in V_{W}$ </li>
<li>context: $c \in V_{C}$<ul>
<li>$w_i$的context为其窗口长度为L的附近的词：$w_{i-L}, \ldots, w_{i-1}, w_{i+1}, \ldots, w_{i+L}$</li>
</ul>
</li>
<li>word vocabulary: $V_w$</li>
<li>context vocabulary: $V_c$</li>
<li>word和context的pair集合：$D$</li>
<li>(w, c)在D中出现的次数：$c(w,c)$</li>
<li>w在D中出现的次数：$c(w)=\sum_{c^{\prime} \in V_{C}} c\left(w, c^{\prime}\right)$</li>
<li>c在D中出现的次数：$c(c)=\sum_{w^{\prime} \in V_{W}} c\left(w^{\prime}, c\right)$</li>
<li>每个词w可以表示成向量：$\vec{w} \in \mathbb{R}^{d}$，有时用$\left|V_{W}\right| \times d$的矩阵W表示，其中$W_i$表示第$i$个word的表示</li>
<li>每个context c可以表示成向量：$\vec{c} \in \mathbb{R}^{d}$，有时用$\left|V_{C}\right| \times d$ 的矩阵C表示，这里的d表示embedding的维度，其中$C_i$表示第$i$个context的表示</li>
<li>(w, c)属于D的概率：$P(D=1 | w, c)$</li>
</ul>
<p>其次，我们要建立目标函数。假设有一个(w, c)对，我们如何知道它是否属于D呢？我们使用sigmoid建模这个概率：$P(D=1 | w, c)=\sigma(\vec{w} \cdot \vec{c})=\frac{1}{1+e^{-\vec{w} \cdot \vec{c}}}$，其中的$\vec{w}$和$\vec{c}$就是我们要学习的参数。</p>
<p>所谓的负采样，就是对于在$D$中的$(w,c)$要最大化$P(D=1|w,c)$，而对于不再$D$中的$(w,c)$要最大化$P(D=0|w,c)$。因此目标函数可以写成：$\log \sigma(\vec{w} \cdot \vec{c})+k \cdot \mathbb{E}_{c_{N} \sim P_{D}}\left[\log \sigma\left(-\vec{w} \cdot \vec{c}_{N}\right)\right]$，其中$k$是负采样的个数，$c_N$是使用均匀分布$P_{D}(c)=\frac{c(c)}{|D|}$随机采样的context。</p>
<p>通常我们会使用SGD求解，全局的目标函数定义为：$\ell=\sum_{w \in V_{W}} \sum_{c \in V_{C}} c(w, c)\left(\log \sigma(\vec{w} \cdot \vec{c})+k \cdot \mathbb{E}_{c_{N} \sim P_{D}}\left[\log \sigma\left(-\vec{w} \cdot \vec{c}_{N}\right)\right]\right)$。这样的建模会使得出现在相同context的word有相近的embedding。</p>
<p>那么这个问题是如何跟矩阵分解联系上的呢？在论文中有详细的推导过程，这里我们直接写出结论。当满足下述条件时，函数$l$达到最值：$w \cdot c=\log \left(\frac{c(w, c) \cdot|D|}{c(w) \cdot c(c)}\right)-\log (k)$。其中$D$是训练文本，$c$是词$w$的上下文，$c(w, c)$ 是$(w,c)$ 在$D$中出现的次数， $c(w)$ 和$c(c)$ 以此类推。$\log \left(\frac{c(w, c) \cdot|D|}{c(w) \cdot c(c)}\right)$ 就是$(w,c)$ 的pointwise mutual information（简称PMI），即$M_{i j}^{S G N S}=w_{i} \cdot c_{j}=P M I\left(w_{i}, c_{j}\right)-\log (k)$</p>
<p>由此，我们找到了矩阵$M$ ，而这个$M$就是我们刚才说的shifted PMI。其他一些词向量表示，也可以由相似的推导过程得出。比如对于noise-contrastive estimation，计算的是(shifted) log-conditional-probability matrix：$M_{i j}^{\mathrm{NCE}}=\vec{w}_{i} \cdot \vec{c}_{j}=\log \left(\frac{c(w, c)}{c(c)}\right)-\log k=\log P(w | c)-\log k$</p>
<p>【题外话】：早期的word2vec也有基于词-词共现矩阵的方法，使用LSA将从corpus中统计的word-word共现矩阵进行分解。共现矩阵里的值有时用PMI替代：$P M I\left(w_{i}, w_{j}\right)=\log \frac{p\left(w_{i}, w_{j}\right)}{p\left(w_{i}\right) p\left(w_{j}\right)}=\log \frac{c\left(w_{i}, w_{j}\right) \cdot|\mathcal{C}|}{c\left(w_{i}\right) \cdot c\left(w_{j}\right)}$。使用SVD对矩阵分解得到：$\mathbf{P}=\mathbf{U} \Psi \mathbf{V}^{\top}$，那么当我们想得到K维的，就可以计算：$\mathbf{X}=\mathbf{U}_{k} \mathbf{\Psi}_{k}$。</p>
<p>另外，<strong>关于句子、词语、句子-词语之间相似性的计算，推荐参考这篇文章</strong>：<a href="https://cloud.tencent.com/developer/article/1436217" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1436217</a></p>
<h2 id="CPE"><a href="#CPE" class="headerlink" title="CPE"></a>CPE</h2><p>如下图所示，对目标端词pomme，我们如何区分出the、apple、juice哪个对于pomme更重要呢？这时就有了CPE排序。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/word2word/2.png" alt="图片"></p>
<p>通过引入修正量$p(y|x’)$、$p(x|x’)$来建模CPE：$\begin{aligned}<br>\operatorname{CPE}(y | x) &amp;=p(y | x)-\sum_{x^{\prime} \in \mathcal{X}} p\left(y | x^{\prime}\right) p\left(x^{\prime} | x\right) \\<br>&amp;=\sum_{x^{\prime} \in \mathcal{X}} \operatorname{CPE}_{y | x}\left(x^{\prime}\right) p\left(x^{\prime} | x\right)<br>\end{aligned}$</p>
<p>其中的x’表示confounder words，对应于上图x=apple，则x’=the 或 juice。上式中CPE项可进一步写成$\mathrm{CPE}_{y | x}\left(x^{\prime}\right)=p\left(y | x, x^{\prime}\right)-p\left(y | x^{\prime}\right)$。CPE可以表达出对于x-&gt;y这个词翻译来说，当我们看到一个comfounder word x’时，会对x-&gt;y的概率产生多大的影响。如果这个值为0，当我们观察到x’时，发现x和y不相关了。</p>
<p>对于一个词x，我们需要计算所有其CPE_{y|x}(x’)的边缘概率，这在实际中计算量非常大。因此在实际计算中，我们只对top 5000的x进行修正。</p>
<h2 id="源码解读"><a href="#源码解读" class="headerlink" title="源码解读"></a>源码解读</h2><h3 id="get-trans-co"><a href="#get-trans-co" class="headerlink" title="get_trans_co"></a>get_trans_co</h3><p>get_trans_co用于获取x-&gt;y共现最多的pair，传入参数x2ys已计算好x-&gt;y的共现次数。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def get_trans_co(x2ys, n_trans):</span><br><span class="line">    x2ys_co &#x3D; dict()</span><br><span class="line">    for x, ys in x2ys.items():</span><br><span class="line">        ys &#x3D; [y for y, cnt in sorted(</span><br><span class="line">            ys.items(),</span><br><span class="line">            key&#x3D;operator.itemgetter(1),</span><br><span class="line">            reverse&#x3D;True)[:n_trans]]</span><br><span class="line">        x2ys_co[x] &#x3D; ys</span><br><span class="line">    return x2ys_co</span><br></pre></td></tr></table></figure>
<h3 id="get-trans-pmi"><a href="#get-trans-pmi" class="headerlink" title="get_trans_pmi"></a>get_trans_pmi</h3><p>get_trans_pmi用于对x的所有translation y进行pmi排序。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def get_trans_pmi(x2ys, x2cnt, y2cnt, Nxy, Nx, Ny, width, n_trans):</span><br><span class="line">    x2ys_pmi &#x3D; dict()</span><br><span class="line">    pmi_diff &#x3D; -np.log2(Nxy) + np.log2(Nx) + np.log2(Ny)</span><br><span class="line">    for x, ys in tqdm(x2ys.items()):</span><br><span class="line">        l_scores &#x3D; []</span><br><span class="line">        for y, cnt in sorted(ys.items(), key&#x3D;operator.itemgetter(1),</span><br><span class="line">                             reverse&#x3D;True)[:width]:</span><br><span class="line">            pmi &#x3D; np.log2(cnt) - np.log2(x2cnt[x]) - np.log2(y2cnt[y])</span><br><span class="line">            pmi +&#x3D; pmi_diff</span><br><span class="line">            l_scores.append((y, pmi))</span><br><span class="line">        trans &#x3D; sorted(</span><br><span class="line">            l_scores,</span><br><span class="line">            key&#x3D;operator.itemgetter(1, 0),</span><br><span class="line">            reverse&#x3D;True)[:n_trans]</span><br><span class="line">        trans &#x3D; [each[0] for each in trans]</span><br><span class="line">        x2ys_pmi[x] &#x3D; trans</span><br><span class="line"></span><br><span class="line">    return x2ys_pmi</span><br></pre></td></tr></table></figure>
<h3 id="rerank"><a href="#rerank" class="headerlink" title="rerank"></a>rerank</h3><p>rerank用于对x的所有translation y进行cpe排序。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def rerank(x2ys, x2cnt, x2xs, width, n_trans):</span><br><span class="line">    x2ys_cpe &#x3D; dict()</span><br><span class="line">    for x, ys in tqdm(x2ys.items()):</span><br><span class="line">        cntx &#x3D; x2cnt[x]</span><br><span class="line">        y_scores &#x3D; []</span><br><span class="line">        for y, cnty in sorted(</span><br><span class="line">                ys.items(),</span><br><span class="line">                key&#x3D;operator.itemgetter(1),</span><br><span class="line">                reverse&#x3D;True)[:width]:</span><br><span class="line">            ts &#x3D; cnty &#x2F; float(cntx)  # translation score: initial value</span><br><span class="line">            if x in x2xs:</span><br><span class="line">                for x2, cntx2 in x2xs[x].items():  # Collocates</span><br><span class="line">                    p_x_x2 &#x3D; cntx2 &#x2F; float(cntx)</span><br><span class="line">                    p_x2_y2 &#x3D; 0</span><br><span class="line">                    if x2 in x2ys:</span><br><span class="line">                        p_x2_y2 &#x3D; x2ys[x2].get(y, 0) &#x2F; float(x2cnt[x2])</span><br><span class="line">                    ts -&#x3D; (p_x_x2 * p_x2_y2)</span><br><span class="line">            y_scores.append((y, ts))</span><br><span class="line">        _ys_ &#x3D; sorted(</span><br><span class="line">            y_scores,</span><br><span class="line">            key&#x3D;lambda y_score: y_score[1],</span><br><span class="line">            reverse&#x3D;True)[:n_trans]</span><br><span class="line">        _ys_ &#x3D; [each[0] for each in _ys_]</span><br><span class="line">        </span><br><span class="line">        x2ys_cpe[x] &#x3D; _ys_</span><br><span class="line">        return x2ys_cpe</span><br></pre></td></tr></table></figure>
<h3 id="rerank-mp"><a href="#rerank-mp" class="headerlink" title="rerank_mp"></a>rerank_mp</h3><p>rerank_mp使用多进程方式进行CPE重排序。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def rerank_mp(x2ys, x2cnt, x2xs, width, n_trans, num_workers):</span><br><span class="line">    from multiprocessing import Pool</span><br><span class="line">    shared_inputs &#x3D; x2ys, x2cnt, x2xs, width, n_trans</span><br><span class="line">    print(f&quot;Entering multiprocessing with &#123;num_workers&#125; workers...&quot;</span><br><span class="line">          f&quot; (#words&#x3D;&#123;len(x2ys)&#125;)&quot;)</span><br><span class="line">    with Pool(num_workers) as p:</span><br><span class="line">        x2ys_cpe &#x3D; dict(p.starmap(</span><br><span class="line">            _rerank_mp,</span><br><span class="line">            zip(x2ys.items(), it.repeat(shared_inputs)),</span><br><span class="line">        ))</span><br><span class="line">    return x2ys_cpe</span><br></pre></td></tr></table></figure>
<h1 id="基于词向量的方法"><a href="#基于词向量的方法" class="headerlink" title="基于词向量的方法"></a>基于词向量的方法</h1><blockquote>
<p>参考论文：<br>（1）A Survey Of Cross-lingual Word Embedding Models<br><a href="https://arxiv.org/abs/1706.04902" target="_blank" rel="noopener">https://arxiv.org/abs/1706.04902</a><br><a href="http://ir.hit.edu.cn/~xiachongfeng/slides/x-lingual-v1.0.pdf" target="_blank" rel="noopener">http://ir.hit.edu.cn/~xiachongfeng/slides/x-lingual-v1.0.pdf</a><br><a href="https://zhuanlan.zhihu.com/p/69366459" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/69366459</a><br>（2）Word Translation Without Parallel Data<br><a href="https://arxiv.org/abs/1710.04087" target="_blank" rel="noopener">https://arxiv.org/abs/1710.04087</a><br>（3）Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond<br><a href="https://arxiv.org/abs/1812.10464" target="_blank" rel="noopener">https://arxiv.org/abs/1812.10464</a><br><a href="https://github.com/yannvgn/laserembeddings" target="_blank" rel="noopener">https://github.com/yannvgn/laserembeddings</a><br>（4）Loss in Translation: Learning Bilingual Word Mapping with a Retrieval Criterion<br><a href="https://arxiv.org/pdf/1804.07745.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1804.07745.pdf</a><br><a href="https://fasttext.cc/docs/en/aligned-vectors.html" target="_blank" rel="noopener">https://fasttext.cc/docs/en/aligned-vectors.html</a></p>
</blockquote>
<h2 id="跨语言词向量综述"><a href="#跨语言词向量综述" class="headerlink" title="跨语言词向量综述"></a>跨语言词向量综述</h2><p>A Survey Of Cross-lingual Word Embedding Models这篇论文中对跨语言的词向量的相关研究进行了总结，并且发现很多模型其实本质上是一个模型，只是使用了不同的目标函数和数据。为了方便理解，我们先将后续用到的符号进行说明：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/word2word/3.png" alt="图片"></p>
<p>我们使用$\operatorname{sen} t_{1}^{s}, \ldots, \operatorname{sen} t_{n}^{s}$表示源语言的句子序列$\mathbf{y}_{1}^{s}, \ldots, \mathbf{y}_{n}^{s}$，$\operatorname{sen} t_{1}^{t}, \ldots, \operatorname{sen} t_{n}^{t}$表示与之对齐的目标端语言的句子序列$\mathbf{y}_{1}^{t}, \ldots, \mathbf{y}_{n}^{t}$。同样的，用$d o c_{1}^{s}, \ldots, d o c_{n}^{s}$表示源语言的文档$\mathbf{z}_{1}^{s}, \ldots, \mathbf{z}_{n}^{s}$。用$d o c_{1}^{t}, \ldots, d o c_{n}^{t}$表示目标语言的文档$\mathbf{z}_{1}^{t}, \ldots, \mathbf{z}_{n}^{t}$。</p>
<p>基本上所有模型的目标函数都可以写成：$J=\mathcal{L}^{1}+\ldots+\mathcal{L}^{\ell}+\Omega$，其中$\mathcal{L}^{\ell}$表示第l个语种的monolingual loss，$\Omega$是一个正则项。</p>
<h3 id="词向量综述"><a href="#词向量综述" class="headerlink" title="词向量综述"></a>词向量综述</h3><p>在正式介绍跨语言词向量模型前，我们简单介绍一下词向量的发展史。从刚才介绍的用LSA矩阵分解的方法开始，后续产生了基于Max-margin loss来最大化correct word sequence和incorrect word sequence的hinge loss：$\begin{aligned}<br>&amp;\mathcal{L}_{\mathrm{MML}}=\sum_{k=C+1}^{|\mathcal{C}|-C} \sum_{w^{\prime} \in V} \max \left(0,1-f\left(\left[\mathbf{x}_{w_{k-C}}, \ldots, \mathbf{x}_{w_{i}}, \ldots, \mathbf{x}_{w_{k+C}}\right]\right)\right.\\<br>&amp;\left.+f\left(\left[\mathbf{x}_{w_{k-C}}, \ldots, \mathbf{x}_{w^{\prime}}, \ldots, \mathbf{x}_{w_{k+C}}\right]\right)\right)<br>\end{aligned}$</p>
<p>接着又有了比较出名的Skip-gram with negative sampling方法：$\mathcal{L}_{\mathrm{SGNS}}=-\frac{1}{|\mathcal{C}|-C} \sum_{k=C+1}^{|\mathcal{C}|-C} \sum_{-C \leq j \leq C, j \neq 0} \log P\left(w_{k+j} | w_{k}\right)$，其中P使用softmax计算：$P\left(w_{k+j} | w_{k}\right)=\frac{\exp \left(\tilde{\mathbf{x}}_{w_{k+j}}^{\top} \mathbf{x}_{w_{k}}\right)}{\sum_{i=1}^{|V|} \exp \left(\tilde{\mathbf{x}}_{w_{i}} \top_{\mathbf{x}_{w_{k}}}\right)}$，为了减少计算量使用了negative-sample的方法：$P\left(w_{k+j} | w_{k}\right)=\log \sigma\left(\tilde{\mathbf{x}}_{w_{k+j}}^{\top} \mathbf{x}_{w_{k}}\right)+\sum_{i=1}^{N} \mathbb{E}_{w_{i} \sim P_{n}} \log \sigma\left(-\tilde{\mathbf{x}}_{w_{i}}^{\top} \mathbf{x}_{w_{k}}\right)$。正如在本篇上文中介绍，这种负采样方法，通过数学推导可以发现，本质上和矩阵分解的作用是一样的。</p>
<p>后来又有了Continuous bag-of-words（CBOW）模型：$\mathcal{L}_{\mathrm{CBOW}}=-\frac{1}{|\mathcal{C}|-C} \sum_{k=C+1}^{|\mathcal{C}|-C} \log P\left(w_{k} | w_{k-C}, \ldots, w_{k-1}, w_{k+1}, \ldots, w_{k+C}\right)$，其中$P\left(w_{k} | w_{k-C}, \ldots, w_{k+C}\right)=\frac{\exp \left(\tilde{\mathbf{x}}_{w_{k}}^{\top} \overline{\mathbf{x}}_{w_{k}}\right)}{\sum_{i=1}^{|V|} \exp \left(\tilde{\mathbf{x}}_{w_{i}} \top \overline{\mathbf{x}}_{w_{k}}\right)}$，$\overline{\mathbf{x}}_{w_{k}}$是$w_{k-C}, \dots, w_{k+C}$词向量的平均值。</p>
<p>再后来就有了我们常用的Glove词向量模型：$\mathcal{L}_{\mathrm{GloVe}}=\sum_{i, j=1}^{|V|} f\left(\mathbf{C}_{i j}\right)\left(\mathbf{x}_{w_{i}}^{\top} \tilde{\mathbf{x}}_{w_{j}}+b_{i}+\tilde{b}_{j}-\log \mathbf{C}_{i j}\right)^{2}$，其中$C_{ij}$计算$w_i$和$w_j$在给定窗口大小下共现的次数。如果我们令$b_{i}=\log c\left(w_{i}\right)$、$\tilde{b}_{j}=\log c\left(w_{j}\right)$，则会发现Glove模型实际上也是对一个PMI矩阵（进行$log|C|$ shift后）进行分解。</p>
<h3 id="Typology"><a href="#Typology" class="headerlink" title="Typology"></a>Typology</h3><p>训练跨语言词向量的方法，跟使用什么类型的语料有关，如下图，你可以使用平行词典、可比词典、平行语料或者可比语料等等：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/word2word/4.png" alt="图片"></p>
<p>下面这张图展示了整个跨语言向量的路线图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/word2word/5.png" alt="图片"><img src="https://uploader.shimo.im/f/DJJgrUv5aQXvcj9A.png!thumbnail" alt="图片"></p>
<h3 id="Word-Level-Alignment-Models"><a href="#Word-Level-Alignment-Models" class="headerlink" title="Word-Level Alignment Models"></a>Word-Level Alignment Models</h3><p>首先我们来看基于词对齐的方法。基于词对齐可以使用两种语料：平行语料、可比语料。基于平行语料的方法大致分3种：</p>
<ul>
<li>Mapping-based：通过学习平行语料或者平行词典的映射关系，现训练一个源语言的embedding，再用学习到的映射矩阵映射到目标语言词向量空间中</li>
<li>Pseudo-multi-lingual corpora-based：用跨语言的伪语料来捕捉不同语言单词间的相互作用，这个伪语料是人工构造的。例如根据翻译，可以定义英语 house 和法语 maison 是等价 的，根据词性标注，可以定义英语 car 和法语 maison 都是名词是等价的。因此这里的对齐方式不一 定是翻译，可以根据具体的任务来定义，然后利用这种对齐关系来构造双语伪语料。首先将源语言 和目标语言数据混合打乱。对于统一语料库中一句话的每一个词语，如果存在于对齐关系中，以一 定概率来替换为另一种语言的词语。通过该方法可以构建得到真实的双语语料库。例如根据翻译关 系，原始句子 build the house 经过构建可以得到 build the maison，就是将 house 替换为了 maison。 利用构建好的全部语料来使用 CBOW 算法学习词向量，由于替换以后的词语有相似的上下文，因 此会得到相似的表示。对于那些没有对齐关系的词语，例如“我吃苹果”和“I eat apple”，吃和 eat 没有对齐关系，但如果我和 I、苹果和 apple 有对齐关系，根据构造出来的语料“I 吃 apple”也可 以完成吃和 eat 的隐式对齐。这种方法对齐词语有相似表示。</li>
<li>Joint-method：利用平行语料来最小化monolingual losses + cross-lingual regularization term</li>
</ul>
<p>基于可比语料的方法大致分2种：</p>
<ul>
<li>Language grounding models：把图片作为anchor，通过图片特征来获得language similarity的特征</li>
<li>Comparable feature models：利用POS信息建立两个language的桥梁</li>
</ul>
<p>基于Mapping的方法有以下4个要素：</p>
<ul>
<li>mapping method：负责将monolingual embedding spaces转换到cross-lingual embedding space。Mapping method有以下几种方法：<ul>
<li>Regression methods<ul>
<li>学习source到target的转移矩阵，并最大化source embedding和target embedding的相似度：$\Omega_{\mathrm{MSE}}=\sum_{i=1}^{n}\left|\mathbf{W} \mathbf{x}_{i}^{s}-\mathbf{x}_{i}^{t}\right|^{2}$，其中$x^s_i$是source embedding，经过W转换后，希望最小化它和其真正的翻译$x^t_i$的embedding之间的距离。论文认为这个目标函数也可以写成这种形式：$J=\underbrace{\mathcal{L}_{\mathrm{SGNS}}\left(\mathbf{X}^{s}\right)+\mathcal{L}_{\mathrm{SGNS}}\left(\mathbf{X}^{t}\right)}_{1}+\underbrace{\Omega_{\mathrm{MSE}}\left(\underline{\mathbf{X}}^{s}, \underline{\mathbf{X}}^{t}, \mathbf{W}\right)}_{2}$。Regression method的想法来源于一个观察，就是source word之间的相关性，和他们的所对应的target word之间的相关性相似，如下图：</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/word2word/6.png" alt="图片"></p>
<ul>
<li>Orthogonal methods<ul>
<li>同Regression method，但要求W是正交的，即$\mathbf{W}^{\top} \mathbf{W}=\mathbf{I}$</li>
</ul>
</li>
<li>Canonical methods<ul>
<li>将source和target共同映射到另一个空间，并最大化两个embedding的相似度。我们定义映射后的两个单词的相关性为：$\rho\left(\mathbf{W}^{s \rightarrow} \mathbf{x}_{i}^{s}, \mathbf{W}^{t \rightarrow} \mathbf{x}_{i}^{t}\right)=\frac{\operatorname{cov}\left(\mathbf{W}^{s \rightarrow} \mathbf{x}_{i}^{s}, \mathbf{W}^{t \rightarrow} \mathbf{x}_{i}^{t}\right)}{\sqrt{\operatorname{var}\left(\mathbf{W}^{s \rightarrow \mathbf{x}_{i}^{s}}\right) \operatorname{var}\left(\mathbf{W}^{t \rightarrow \mathbf{x}_{i}^{t}}\right)}}$，则canonical method的目标是最大化所有的相关性：$\Omega_{\mathrm{CCA}}=-\sum_{i=1}^{n} \rho\left(\mathbf{W}^{s \rightarrow} \mathbf{x}_{i}^{s}, \mathbf{W}^{t \rightarrow} \mathbf{x}_{i}^{t}\right)$。论文认为这个目标函数等价于：$J=\underbrace{\mathcal{L}_{\mathrm{LSA}}\left(\mathbf{X}^{s}\right)+\mathcal{L}_{\mathrm{LSA}}\left(\mathbf{X}^{t}\right)}_{1}+\underbrace{\Omega_{\mathrm{CCA}}\left(\underline{\mathbf{X}}^{s}, \underline{\mathbf{X}}^{t}, \mathbf{W}^{s \rightarrow}, \mathbf{W}^{t \rightarrow}\right)}_{2}$</li>
</ul>
</li>
<li>Margin methods<ul>
<li>该方法将Loss函数改成了margin-based rank loss来减轻hubness的问题：$\Omega_{\mathrm{MML}}=\sum_{i=1}^{n} \sum_{j \neq i}^{k} \max \left\{0, \gamma-\cos \left(\mathbf{W} \mathbf{x}_{i}^{s}, \mathbf{x}_{i}^{t}\right)+\cos \left(\mathbf{W} \mathbf{x}_{i}^{s}, \mathbf{x}_{j}^{t}\right)\right\}$，并认为此时目标函数就是：$J=\underbrace{\mathcal{L}_{\mathrm{CBOW}}\left(\mathbf{X}^{s}\right)+\mathcal{L}_{\mathrm{CBOW}}\left(\mathbf{X}^{t}\right)}_{1}+\underbrace{\Omega_{\mathrm{MML}-\mathrm{I}}\left(\underline{\mathbf{X}}^{s}, \mathbf{X}^{t}, \mathbf{W}\right)}_{2}$</li>
<li>seed lexicon：用于学习embedding的种子字典</li>
<li>refinement：用于修正学习到的mapping</li>
<li>retrieval：用于搜索最近邻</li>
</ul>
</li>
</ul>
<h3 id="Sentence-Level-Alignment-Methods"><a href="#Sentence-Level-Alignment-Methods" class="headerlink" title="Sentence-Level Alignment Methods"></a>Sentence-Level Alignment Methods</h3><p>基于平行语料的方法也可以分为4种：</p>
<ul>
<li>Word-alignment based matrix factorization approaches<ul>
<li>基于Word-alignment的方法可以先用FastAlign找到词对齐关系。如果一个source word在target空间中只有一个翻译，那么这个target翻译的embedding应该是确定的一个，但如果它在target空间中有多个翻译，那么应该认为target的embedding应该是这些翻译的一个加权平均。这就是word-alignment方法的基本思路，其目标函数为：$\Omega_{s \rightarrow t}=\left|\mathbf{X}^{t}-\mathbf{A}^{s \rightarrow t} \mathbf{X}^{s}\right|^{2}$，也可以写成：$J=\underbrace{\mathcal{L}_{\mathrm{MML}}\left(\mathbf{X}^{t}\right)}_{1}+\underbrace{\Omega_{s \rightarrow t}\left(\underline{\mathbf{X}}^{t}, \underline{\mathbf{A}}^{s \rightarrow t}, \mathbf{X}^{s}\right)}_{2}$。之所以叫factorization approach是因为有的方法将$A^{s-&gt;t}$看成是共现矩阵并用Glove的目标函数进行分解</li>
</ul>
</li>
<li>Compositional sentence models<ul>
<li>这种方法将平行句子表示的距离最小化，即$E_{\text {dist}}\left(\operatorname{sen} t^{s}, \operatorname{sen} t^{t}\right)=\left|\mathbf{y}^{s}-\mathbf{y}^{t}\right|^{2}$，其中句子的表示使用单词表示的和，使用hinge loss作为目标函数：$\mathcal{L}=\sum_{\left(s e n t^{s}, s e n t^{t}\right) \in \mathcal{C}} \sum_{i=1}^{k} \max \left(0,1+E_{\text {dist}}\left(\operatorname{sent}^{s}, \operatorname{sent}^{t}\right)-E_{\text {dist}}\left(\operatorname{sent}^{s}, s_{i}^{t}\right)\right)$，或者$J=\mathcal{L}\left(\mathbf{X}^{s}, \mathbf{X}^{t}\right)+\Omega\left(\mathbf{X}^{s}\right)+\Omega\left(\mathbf{X}^{t}\right)$</li>
</ul>
</li>
<li>Bilingual autoencoder models<ul>
<li>从这里开始就开始无监督的工作了，Barone开始使用对抗自动编码器将源语言词嵌入转换到目标语言中，然后训练自动编码器以重建源嵌入，同时训练鉴别器以将投射的源嵌入与实际目标嵌入区分开，如下图：</li>
</ul>
</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/word2word/7.png" alt="图片"></p>
<ul>
<li>Bilingual skip-gram models</li>
</ul>
<p>论文中还讲解了Document-Level Alignment Models，训练以及评测。内容太多，也没看，在此不继续写这篇综述了。感兴趣的读者可以再去看看论文。</p>
<p>词向量训练还有BERT等方法，[Devlin et al., 2018] 提出了 Multilingual BERT，与单语 BERT 结构一样，使用共享的 Wordpiece 表示，使用了 104 中语言进行训练。训练时，无输入语言标记，也没有强制对齐的语料有相 同的表示。[Pires et al., 2019] 分析了 Multilingual BERT 的多语言表征能力，得出了几点结论： Multilingual BERT 的多语言表征能力不仅仅依赖于共享的词表，对于没有重叠（overlap）词汇语 言的 zero-shot 任务，也可以完成的很好；语言越相似，效果越好；对于语言顺序（主谓宾或者形 容词名词）不同的语言，效果不是很好；Multilingual BERT 的表示同时包含了多种语言共有的表 示，同时也包含了语言特定的表示，这一结论，[Wu and Dredze, 2019] 在语言分类任务中也指出， Multilingual BERT 由于需要完成语言模型任务，所以需要保持一定的语言特定的表示来在词表中 选择特定语言词语。</p>
<p>[Lample and Conneau, 2019] 提出了基于多种语言预训练的模型 XLMs，首先从单语语料库中 采样一些句子，对于资源稀少的语言可以增加数量，对于资源丰富的语言可以减少数量，将所有语 言使用统一 BPE 进行表示。使用三种语言模型目标来完成学习。前两个是基于单语语料库的，最 后一个是基于双语对齐数据的。第一种是 Causal Language Modeling (CLM)，根据之前的词语预 测下一个词语。第二个是 Masked Language Modeling (MLM)，和 BERT 类似，但是使用一个词 语流，而非句子对。第三种是 Translation Language Modeling (TLM)，可以随机 mask 掉其中一些 两种语言中的一些词语，然后进行预测。其模型如下：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/word2word/8.png" alt="图片"></p>
<h2 id="一些好用的工具"><a href="#一些好用的工具" class="headerlink" title="一些好用的工具"></a>一些好用的工具</h2><p>这里推荐两个，一个是fast-text的align-vector，可以在<a href="https://fasttext.cc/docs/en/aligned-vectors.html" target="_blank" rel="noopener">https://fasttext.cc/docs/en/aligned-vectors.html</a>下载，如果想训练可以参考<a href="https://github.com/facebookresearch/fastText/tree/master/alignment" target="_blank" rel="noopener">https://github.com/facebookresearch/fastText/tree/master/alignment</a>。</p>
<p>另一个是laser，其主项目在<a href="https://github.com/facebookresearch/LASER" target="_blank" rel="noopener">https://github.com/facebookresearch/LASER</a>，如果想直接使用multi-lingual的sentence embedding，可以参考<a href="https://github.com/yannvgn/laserembeddings" target="_blank" rel="noopener">https://github.com/yannvgn/laserembeddings</a>。顺便说下，这两个都是facebook的工作。</p>
]]></content>
      <tags>
        <tag>机器翻译</tag>
      </tags>
  </entry>
  <entry>
    <title>Lucene搭建搜索引擎初探</title>
    <url>/2020/05/10/Lucene%E6%90%AD%E5%BB%BA%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E5%88%9D%E6%8E%A2/</url>
    <content><![CDATA[<p>最近要做例句搜索的优化，因此重新看一看lucene，边学习边搭demo。由于平时使用惯了python，所以这一次使用pylucene做demo。本文着重于lucene的介绍，一些内容主要参考了niyanchun的博客，并增加了几个pylucene的示例代码。</p>
<a id="more"></a>
<h1 id="配置Pylucene环境"><a href="#配置Pylucene环境" class="headerlink" title="配置Pylucene环境"></a>配置Pylucene环境</h1><h3 id="安装pylucene"><a href="#安装pylucene" class="headerlink" title="安装pylucene"></a><strong>安装pylucene</strong></h3><ul>
<li>wget <a href="https://mirror.bit.edu.cn/apache/lucene/pylucene/pylucene-8.1.1-src.tar.gz" target="_blank" rel="noopener">https://mirror.bit.edu.cn/apache/lucene/pylucene/pylucene-8.1.1-src.tar.gz</a></li>
<li>tar zxvf pylucene-8.1.1-src.tar.gz  </li>
</ul>
<h3 id="安装JCC"><a href="#安装JCC" class="headerlink" title="安装JCC"></a><strong>安装JCC</strong></h3><ul>
<li>cd pylucene-8.1.1/jcc</li>
<li>setup.py中修改jdk位置</li>
<li>如果是MACOS<ul>
<li>export CC=/usr/bin/clang</li>
<li>export CXX=/usr/bin/clang++</li>
</ul>
</li>
<li>python setup.py build</li>
<li>python setup.py install</li>
</ul>
<h3 id="安装ANT"><a href="#安装ANT" class="headerlink" title="安装ANT"></a><strong>安装ANT</strong></h3><ul>
<li>wget <a href="https://mirror.bit.edu.cn/apache//ant/binaries/apache-ant-1.9.14-bin.tar.gz" target="_blank" rel="noopener">https://mirror.bit.edu.cn/apache//ant/binaries/apache-ant-1.9.14-bin.tar.gz</a></li>
<li>tar zxvf apache-ant-1.9.14-bin.tar.gz</li>
<li>export ANT_HOME=…/apache-ant-1.9.14</li>
<li>export PATH=$PATH:$ANT_HOME/bin</li>
<li>export ANT_OPTS=”-Xms1300m -Xmx2048m -XX:PermSize=128M -XX:MaxNewSize=256m -XX:MaxPermSize=256m”</li>
</ul>
<h3 id="安装lucene"><a href="#安装lucene" class="headerlink" title="安装lucene"></a><strong>安装lucene</strong></h3><ul>
<li>cd pylucene-8.1.1</li>
<li>vi Makefile<ul>
<li>PREFIX_PYTHON=…./conda-env</li>
<li>PYTHON=$(PREFIX_PYTHON)/bin/python</li>
<li>ANT=…/apache-ant-1.9.14/bin/ant</li>
<li>JCC=$(PYTHON) -m jcc.<strong>main</strong></li>
<li>NUM_FILES=8</li>
</ul>
</li>
<li>make</li>
<li>make install</li>
</ul>
<h1 id="术语总结"><a href="#术语总结" class="headerlink" title="术语总结"></a>术语总结</h1><p>索引整体的逻辑结构图，如下所示：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/1.png" alt="图片"></p>
<h2 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h2><p>对于初学全文检索的人来说，索引这个词非常具有迷惑性，主要原因是它有两个词性：</p>
<ul>
<li>动词：做动词时，一般英文写为“<em>indexing</em>”，比如“<em>索引一个文件</em>”翻译为“<em>indexing a file</em>”，它指的是我们将原始数据经过一系列的处理，最终形成可以高效全文检索（对于Lucene，就是生成倒排索引）的过程。这个过程就称之为<strong>索引（indexing）</strong>。</li>
<li>名词：做名词时，写为“<em>index</em>”。经过indexing最终形成的结果（一般以文件形式存在）称之为<strong>索引（index）</strong>。</li>
</ul>
<p>所以，见到索引这个词，你一定要分清楚是动词还是名词。后面为了清楚，凡是作为动词的时候我使用indexing，作为名词的时候使用index。Index是Lucene中的顶级逻辑结构，它是一个逻辑概念，如果对应到具体的实物，就是一个目录，目录里面所有的文件组成一个index。注意，这个目录里面不会再嵌套目录，只会包含多个文件。具体index的构成细节后面会专门写一篇文章来介绍。对应到代码里面，就是org.apache.lucene.store.Directory这个抽象类。最后要说明一点的是，Lucene中的Index和ElasticSearch里面的Index不是一个概念，ElasticSearch里面的shard对应的才是Lucene的Index。</p>
<h2 id="文档（Document）和字段（Field）"><a href="#文档（Document）和字段（Field）" class="headerlink" title="文档（Document）和字段（Field）"></a>文档（Document）和字段（Field）</h2><p>一个Index里面会包含若干个文档，文档就像MySQL里面的一行（record）或者HBase里面的一列。文档是Lucene里面索引和搜索的原子单位，就像我们在MySQL里面写数据的时候，肯定是以行为单位的；读的时候也是以行为单位的。当然我们可以指定只读/写行里面某些字段，但仍是以行为单位的，Lucene也是一样，以文档为最小单位。代码里面是这样说明的：”<em>Documents are the unit of indexing and search</em>“.每个文档都会有一个唯一的文档ID。</p>
<p>文档是一个灵活的概念，不同的业务场景对应的具体含义不同。对于搜索引擎来说，一个文档可能就代表爬虫爬到的一个网页，很多个网页（文档）组成了一个索引。而对于提供检索功能的邮件客户端来说，一个文档可能就代表一封邮件，很多封邮件（文档）组成了一个索引。再比如假设我们要构建一个带全文检索功能的商品管理系统，那一件商品就是一个文档，很多个商品组成了一个索引。对于日志处理，一般是一行日志代表一个文档。</p>
<p>文档里面包含若干个字段，真正的数据是存储在字段里面的。一个字段包含三个要素：<strong><em>名称、类型、值</em></strong>。我们要索引数据，必须将数据以文本形式存储到字段里之后才可以。Lucene的字段由一个key-value组成，就像map一样。value支持多种类型，如果value是一个map类型，那就是嵌套字段了。</p>
<p>最后需要注意的是，不同于传统的关系型数据库，Lucene不要求一个index里面的所有文档的字段要一样，如果你喜欢，每一条文档的结构都可以不一样（当然实际中不建议这样操作），而且不需要事先定义，这个特性一般称之为“<strong><em>flexible schema</em></strong>”。传统的关系型数据库要求一个表里面的所有字段的结构必须一致，而且必事先定义好，一般称之为“<strong><em>strict schema</em></strong>”或者”<strong><em>fixed schema</em></strong>“。比如，有一个名为“<em>mixture</em>”的索引包含3条Document，如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &#123; &quot;name&quot;: &quot;Ni Yanchun&quot;, &quot;gender&quot;: &quot;male&quot;, &quot;age&quot;: 28  &#125;,</span><br><span class="line">    &#123; &quot;name&quot;: &quot;Donald John Trump&quot;, &quot;gender&quot;: &quot;male&quot;, &quot;birthday&quot;: &quot;1946.06.14&quot;&#125;,</span><br><span class="line">    &#123; &quot;isbn&quot;: &quot;978-1-933988-17-7&quot;, &quot;price&quot;: 60, &quot;publish&quot;: &quot;2010&quot;, &quot;topic&quot;: [&quot;lucene&quot;, &quot;search&quot;]&#125;</span><br></pre></td></tr></table></figure>
<p>}</p>
<p>可以看到，3条Document的字段并不完全一样，这在Lucene中是合法的。</p>
<h2 id="Token和Term"><a href="#Token和Term" class="headerlink" title="Token和Term"></a>Token和Term</h2><p>Token存储在字段中的文本数据经过分词器分词后（准确的说是经过Tokenizer处理之后）产生的一系列词或者词组。比如假设有个”content”字段的存储的值为”My name is Ni Yanchun”，这个字段经过Lucene的标准分词器分词后的结果是：”my”, “name”, “is”, “ni”, “yanchun”。这里的每个词就是一个token，当然实际上除了词自身外，token还会包含一些其它属性。后面的文章中会介绍这些属性。</p>
<p>一个token加上它原来所属的字段的名称构成了<strong>Term</strong>。比如”content”和”my”组成一个term，”content”和”name”组成另外一个term。我们检索的时候搜的就是Term，而不是Token或者Document（但搜到term之后，会找到包含这个term的Document，然后返回整个Document，而不是返回单个Term）。</p>
<h2 id="Index-Segment"><a href="#Index-Segment" class="headerlink" title="Index Segment"></a>Index Segment</h2><p>在上面的图中，Document分别被一些绿框括了起来，这个称为Segment。Indexing的时候，并不是将所有数据写到一起，而是再分了一层，这层就是segment。Indexing的时候，会先将Document缓存，然后定期flush到文件。每次flush就会生成一个Segment。所以一个Index包含若干个Segment，每个Segment包含一部分Document。为了减少文件描述符的使用，这些小的Segment会定期的合并为（merge）大的Segment，数据量不大的时候，合并之后一个index可能只有一个Segment。搜索的时候，会搜索各个Segment，然后合并搜索结果。</p>
<h1 id="Analyzer"><a href="#Analyzer" class="headerlink" title="Analyzer"></a>Analyzer</h1><blockquote>
<p>参考：<br><a href="https://gist.github.com/Sennahoi/740753384999add46fc1" target="_blank" rel="noopener">https://gist.github.com/Sennahoi/740753384999add46fc1</a><br><a href="https://niyanchun.com/lucene-learning-4.html" target="_blank" rel="noopener">https://niyanchun.com/lucene-learning-4.html</a></p>
</blockquote>
<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>Analyzer像一个数据加工厂，输入是原始的文本数据，输出是经过各种工序加工的term，然后这些terms以倒排索引的方式存储起来，形成最终用于搜索的Index。所以Analyzer也是我们控制数据能以哪些方式检索的重要点。</p>
<h3 id="内置的Analyzer对比"><a href="#内置的Analyzer对比" class="headerlink" title="内置的Analyzer对比"></a>内置的Analyzer对比</h3><p>Lucene已经帮我们内置了许多Analyzer，我们先来挑几个常见的对比一下他们的分析效果吧：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">package com.niyanchun;</span><br><span class="line"></span><br><span class="line">import org.apache.lucene.analysis.Analyzer;</span><br><span class="line">import org.apache.lucene.analysis.TokenStream;</span><br><span class="line">import org.apache.lucene.analysis.core.KeywordAnalyzer;</span><br><span class="line">import org.apache.lucene.analysis.core.SimpleAnalyzer;</span><br><span class="line">import org.apache.lucene.analysis.core.WhitespaceAnalyzer;</span><br><span class="line">import org.apache.lucene.analysis.en.EnglishAnalyzer;</span><br><span class="line">import org.apache.lucene.analysis.standard.StandardAnalyzer;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line">import java.util.concurrent.atomic.AtomicInteger;</span><br><span class="line">public class AnalyzerCompare &#123;</span><br><span class="line"></span><br><span class="line">    private static final Analyzer[] ANALYZERS &#x3D; new Analyzer[]&#123;</span><br><span class="line">            new WhitespaceAnalyzer(), &#x2F;&#x2F; 仅根据空白字符（whitespace）进行分词。</span><br><span class="line">            new KeywordAnalyzer(), &#x2F;&#x2F; 不做任何分词，把整个原始输入作为一个token。所以可以看到输出只有1个token，就是原始句子。</span><br><span class="line">            new SimpleAnalyzer(), &#x2F;&#x2F; 根据非字母（non-letters）分词，并且将token全部转换为小写。所以该分词的输出的terms都是由小写字母组成的。</span><br><span class="line">            new StandardAnalyzer(EnglishAnalyzer.getDefaultStopSet()) &#x2F;&#x2F; 基于JFlex进行语法分词，然后删除停用词，并且将token全部转换为小写。标准分词器会处理停用词，但默认其停用词库为空，这里我们使用英文的停用词&#125;;</span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        String content &#x3D; &quot;My name is Ni Yanchun, I&#39;m 28 years old. You can contact me with the email niyanchun@outlook.com&quot;;</span><br><span class="line">        System.out.println(&quot;原始数据:\n&quot; + content + &quot;\n\n分析结果：&quot;);</span><br><span class="line">        for (Analyzer analyzer : ANALYZERS) &#123;</span><br><span class="line">            showTerms(analyzer, content);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private static void showTerms(Analyzer analyzer, String content) throws IOException &#123;</span><br><span class="line"></span><br><span class="line">        try (TokenStream tokenStream &#x3D; analyzer.tokenStream(&quot;content&quot;, content)) &#123;</span><br><span class="line">            StringBuilder sb &#x3D; new StringBuilder();</span><br><span class="line">            AtomicInteger tokenNum &#x3D; new AtomicInteger();</span><br><span class="line">            tokenStream.reset();</span><br><span class="line">            while (tokenStream.incrementToken()) &#123;</span><br><span class="line">                tokenStream.reflectWith(((attClass, key, value) -&gt; &#123;</span><br><span class="line">                    if (&quot;term&quot;.equals(key)) &#123;</span><br><span class="line">                        tokenNum.getAndIncrement();</span><br><span class="line">                        sb.append(&quot;\&quot;&quot;).append(value).append(&quot;\&quot;, &quot;);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;));</span><br><span class="line">            &#125;</span><br><span class="line">            tokenStream.end();</span><br><span class="line"></span><br><span class="line">            System.out.println(analyzer.getClass().getSimpleName() + &quot;:\n&quot; + tokenNum + &quot; tokens: [&quot; + sb.toString().substring(0, sb.toString().length() - 2) + &quot;]&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这段代码的功能是使用常见的四种分词器（WhitespaceAnalyzer，KeywordAnalyzer，SimpleAnalyzer，StandardAnalyzer）对“My name is Ni Yanchun, I’m 28 years old. You can contact me with the email niyanchun@outlook.com”这句话进行analyze，输出最终的terms。其中需要注意的是，标准分词器会去掉停用词（stop word），但其内置的停用词库为空，所以我们传了一个英文默认的停用词库。运行代码之后的输出如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">原始数据:</span><br><span class="line">My name is Ni Yanchun, I&#39;m 28 years old. You can contact me with the email niyanchun@outlook.com</span><br><span class="line"></span><br><span class="line">分析结果：</span><br><span class="line">WhitespaceAnalyzer:</span><br><span class="line">17 tokens: [&quot;My&quot;, &quot;name&quot;, &quot;is&quot;, &quot;Ni&quot;, &quot;Yanchun,&quot;, &quot;I&#39;m&quot;, &quot;28&quot;, &quot;years&quot;, &quot;old.&quot;, &quot;You&quot;, &quot;can&quot;, &quot;contact&quot;, &quot;me&quot;, &quot;with&quot;, &quot;the&quot;, &quot;email&quot;, &quot;niyanchun@outlook.com&quot;]</span><br><span class="line">KeywordAnalyzer:</span><br><span class="line">1 tokens: [&quot;My name is Ni Yanchun, I&#39;m 28 years old. You can contact me with the email niyanchun@outlook.com&quot;]</span><br><span class="line">SimpleAnalyzer:</span><br><span class="line">19 tokens: [&quot;my&quot;, &quot;name&quot;, &quot;is&quot;, &quot;ni&quot;, &quot;yanchun&quot;, &quot;i&quot;, &quot;m&quot;, &quot;years&quot;, &quot;old&quot;, &quot;you&quot;, &quot;can&quot;, &quot;contact&quot;, &quot;me&quot;, &quot;with&quot;, &quot;the&quot;, &quot;email&quot;, &quot;niyanchun&quot;, &quot;outlook&quot;, &quot;com&quot;]</span><br><span class="line">StandardAnalyzer:</span><br><span class="line">15 tokens: [&quot;my&quot;, &quot;name&quot;, &quot;ni&quot;, &quot;yanchun&quot;, &quot;i&#39;m&quot;, &quot;28&quot;, &quot;years&quot;, &quot;old&quot;, &quot;you&quot;, &quot;can&quot;, &quot;contact&quot;, &quot;me&quot;, &quot;email&quot;, &quot;niyanchun&quot;, &quot;outlook.com&quot;]</span><br></pre></td></tr></table></figure>
<h3 id="Analyzer原理"><a href="#Analyzer原理" class="headerlink" title="Analyzer原理"></a>Analyzer原理</h3><p>前面我们说了Analyzer就像一个加工厂，包含很多道工序。这些工序在Lucene里面分为两大类：Tokenizer和TokenFilter。Tokenizer永远是Analyzer的第一道工序，有且只有一个。它的作用是读取输入的原始文本，然后根据工序的内部定义，将其转化为一个个token输出。TokenFilter只能接在Tokenizer之后，因为它的输入只能是token。然后它将输入的token进行加工，输出加工之后的token。一个Analyzer中，TokenFilter可以没有，也可以有多个。也就是说一个Analyzer内部的流水线是这样的：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/2.png" alt="图片"></p>
<p>比如StandardAnalyzer的流水线是这样的：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/3.png" alt="图片"></p>
<p>所以，Analyzer的原理还是比较简单的，Tokenizer读入文本转化为token，然后后续的TokenFilter将token按需加工，输出需要的token。我们可以自由组合已有的Tokenizer和TokenFilter来满足自己的需求，也可以实现自己的Tokenizer和TokenFilter。</p>
<h2 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h2><h3 id="Analyzer和TokenStream"><a href="#Analyzer和TokenStream" class="headerlink" title="Analyzer和TokenStream"></a>Analyzer和TokenStream</h3><p>Analyzer对应的实现类是org.apache.lucene.analysis.Analyzer，这是一个抽象类。它的主要作用是构建一个org.apache.lucene.analysis.TokenStream对象，该对象用于分析文本。代码中的类描述是这样的：</p>
<blockquote>
<p>An Analyzer builds TokenStreams, which analyze text. It thus represents a policy for extracting index terms from text.</p>
</blockquote>
<p>因为它是一个抽象类，所以实际使用的时候需要继承它，实现具体的类。比如第一部分我们使用的4个内置Analyzer都是直接或间接继承的该类。继承的子类需要实现createComponents方法，之前说的一系列工序就是加在这个方法里的，可以认为一道工序就是整个流水线中的一个Component。Analyzer抽象类还实现了一个tokenStream方法，并且是final的。该方法会将一系列工序转化为TokenStream对象输出。比如SimpleAnalyzer的实现如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public final class SimpleAnalyzer extends Analyzer &#123;</span><br><span class="line">  public SimpleAnalyzer() &#123;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  @Override</span><br><span class="line">  protected TokenStreamComponents createComponents(final String fieldName) &#123;</span><br><span class="line">    Tokenizer tokenizer &#x3D; new LetterTokenizer();</span><br><span class="line">    return new TokenStreamComponents(tokenizer, new LowerCaseFilter(tokenizer));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  @Override</span><br><span class="line">  protected TokenStream normalize(String fieldName, TokenStream in) &#123;</span><br><span class="line">    return new LowerCaseFilter(in);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>TokenStream的作用就是流式的产生token。这些token可能来自于indexing时文档里面的字段数据，也可能来自于检索时的检索语句。其实就是之前说的indexing和查询的时候都会调用Analyzer。</p>
<h3 id="Tokenizer和TokenFilter"><a href="#Tokenizer和TokenFilter" class="headerlink" title="Tokenizer和TokenFilter"></a>Tokenizer和TokenFilter</h3><p>TokenStream有两个非常重要的抽象子类：org.apache.lucene.analysis.Tokenizer和org.apache.lucene.analysis.TokenFilter。这两个类的实质其实都是一样的，都是对Token进行处理。不同之处就是前面介绍的，Tokenizer是第一道工序，所以它的输入是原始文本，输出是token；而TokenFilter是后面的工序，它的输入是token，输出也是token。实质都是对token的处理，所以实现它两个的子类都需要实现incrementToken方法，也就是在这个方法里面实现处理token的具体逻辑。incrementToken方法是在TokenStream类中定义的。比如前面提到的StandardTokenizer就是实现Tokenizer的一个具体子类；LowerCaseFilter和StopFilter就是实现TokenFilter的具体子类。</p>
<p>最后要说一下，Analyzer的流程越长，处理逻辑越复杂，性能就越差，实际使用中需要注意权衡。Analyzer的原理及代码就分析到这里，因为篇幅，一些源码没有在文章中全部列出，如果你有兴趣，建议去看下常见的Analyzer的实现的源码，一定会有收获。</p>
<h2 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h2><p>那么在python中我们怎么使用呢？下面举个小例子，使用pylucene其实和java的lucene没有太大差别，因为实际都是调用的java的类库。这里我没自定义过Analyzer，都是将输入进行处理后，变成用空格分好的结果，再送入到WhitespaceAnalyzer。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from org.apache.lucene.analysis.core import WhitespaceAnalyzer</span><br><span class="line">from org.apache.lucene.index import IndexWriter, IndexWriterConfig</span><br><span class="line">from org.apache.lucene.store import SimpleFSDirectory</span><br><span class="line">from org.apache.lucene.document import Document, Field</span><br><span class="line">from org.apache.lucene.document import TextField</span><br><span class="line">from org.apache.lucene.search import BooleanQuery</span><br><span class="line">from org.apache.lucene.queryparser.classic import QueryParser</span><br><span class="line"># 声明</span><br><span class="line">lucene_analyzer &#x3D; WhitespaceAnalyzer()</span><br><span class="line"># 在建索引中使用analyzer</span><br><span class="line">sentence &#x3D; &#39;i am so confused .&#39;</span><br><span class="line">config &#x3D; IndexWriterConfig(self.lucene_analyzer)</span><br><span class="line">index_writer &#x3D; IndexWriter(SimpleFSDirectory(INDEXIDR), config)</span><br><span class="line">document &#x3D; Document()</span><br><span class="line">document.add(Field(&#39;sentence&#39;, sentence, TextField.TYPE_STORED))</span><br><span class="line">index_writer.addDocument(document)</span><br><span class="line"></span><br><span class="line"># 在构建query时使用analyzer</span><br><span class="line">query &#x3D; &#39;confuse&#39;</span><br><span class="line">boolean_query &#x3D; BooleanQuery.Builder()</span><br><span class="line">simple_query &#x3D; QueryParser(&quot;sentence&quot;, lucene_analyzer).parse(query)</span><br></pre></td></tr></table></figure>
<h1 id="倒排索引、Token与词向量"><a href="#倒排索引、Token与词向量" class="headerlink" title="倒排索引、Token与词向量"></a>倒排索引、Token与词向量</h1><h2 id="倒排索引（Inverted-Index）和正向索引（Forward-Index）"><a href="#倒排索引（Inverted-Index）和正向索引（Forward-Index）" class="headerlink" title="倒排索引（Inverted Index）和正向索引（Forward Index）"></a>倒排索引（Inverted Index）和正向索引（Forward Index）</h2><p>我们用一个例子来看什么是倒排索引，什么是正向索引。假设有两个文档（前面的数字为文档ID）：</p>
<ul>
<li>a good student.</li>
<li>a gifted student.</li>
</ul>
<p>这两个文档经过Analyzer之后（这里我们不去停顿词），分别得到以下两个索引表：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/4.png" alt="图片"></p>
<p>这两个表都是key-value形式的Map结构，该数据结构的最大特点就是可以根据key快速访问value。我们分别分析以下这两个表。</p>
<p>表1中，Map的key是一个个词，也就是上文中Analyzer的输出。value是包含该词的文档的ID。这种映射的好处就是如果我们知道了词，就可以很快的查出有哪些文档包含该词。大家想一下我们平时的检索是不是就是这种场景：我们知道一些关键字，然后想查有哪些网页包含该关键词。表1这种<strong>词到文档的映射</strong>结构就称之为<strong>倒排索引</strong>。</p>
<p>表2中，Map的key是文档id，而value是该文档中包含的所有词。这种结构的映射的好处是只要我们知道了文档（ID），就能知道这个文档里面包含了哪些词。这种<strong>文档到词的映射</strong>结构称之为<strong>正向索引</strong>。</p>
<p>倒排索引是文档检索系统最常用的数据结构，Lucene用的就是这种数据结构。那对于检索有了倒排索引是不是就够用了呢？我们来看一个搜索结果：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/5.png" alt="图片"></p>
<p>这里我搜索了我年少时的偶像S.H.E，一个台湾女团，Google返回了一些包含该关键字的网页，同时它将网页中该关键字用红色字体标了出来。几乎所有的搜索引擎都有该功能。大家想一下，使用上述的倒排索引结构能否做到这一点？</p>
<p>答案是做不到的。倒排索引的结构只能让我们快速判断一个文档（上述例子中一个网页就是一个文档）是否包含该关键字，但无法知道关键字出现在文档中的哪个位置。那搜索引擎是如何知道的呢？其实使用的是另外一个结构——词向量，词向量和倒排索引的信息都是在Analyze阶段计算出来的。在介绍词向量之前，我们先来看一下Analyze的输出结果——Token。</p>
<h2 id="Token"><a href="#Token" class="headerlink" title="Token"></a>Token</h2><p>Token除了包含词以外，还存在一些其它属性，下面就让我们来看看完整的token长什么样？</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import org.apache.lucene.analysis.Analyzer;</span><br><span class="line">import org.apache.lucene.analysis.TokenStream;</span><br><span class="line">import org.apache.lucene.analysis.core.WhitespaceAnalyzer;</span><br><span class="line">import org.apache.lucene.analysis.standard.StandardAnalyzer;</span><br><span class="line">import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;</span><br><span class="line"></span><br><span class="line">public class AnalysisDebug &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        Analyzer analyzer &#x3D; new StandardAnalyzer();</span><br><span class="line">        String sentence &#x3D; &quot;a good student, a gifted student.&quot;;</span><br><span class="line">        try (TokenStream tokenStream &#x3D; analyzer.tokenStream(&quot;sentence&quot;, sentence)) &#123;</span><br><span class="line">            tokenStream.reset();</span><br><span class="line"></span><br><span class="line">            while (tokenStream.incrementToken()) &#123;</span><br><span class="line">                System.out.println(&quot;token: &quot; + tokenStream.reflectAsString(false));</span><br><span class="line">            &#125;</span><br><span class="line">            tokenStream.end();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们借助TokenStream对象输出经过StandardAnalyzer处理的数据，程序运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">token: term&#x3D;a,bytes&#x3D;[61],startOffset&#x3D;0,endOffset&#x3D;1,positionIncrement&#x3D;1,positionLength&#x3D;1,type&#x3D;&lt;ALPHANUM&gt;,termFrequency&#x3D;1</span><br><span class="line">token: term&#x3D;good,bytes&#x3D;[67 6f 6f 64],startOffset&#x3D;2,endOffset&#x3D;6,positionIncrement&#x3D;1,positionLength&#x3D;1,type&#x3D;&lt;ALPHANUM&gt;,termFrequency&#x3D;1</span><br><span class="line">token: term&#x3D;student,bytes&#x3D;[73 74 75 64 65 6e 74],startOffset&#x3D;7,endOffset&#x3D;14,positionIncrement&#x3D;1,positionLength&#x3D;1,type&#x3D;&lt;ALPHANUM&gt;,termFrequency&#x3D;1</span><br><span class="line">token: term&#x3D;a,bytes&#x3D;[61],startOffset&#x3D;16,endOffset&#x3D;17,positionIncrement&#x3D;1,positionLength&#x3D;1,type&#x3D;&lt;ALPHANUM&gt;,termFrequency&#x3D;1</span><br><span class="line">token: term&#x3D;gifted,bytes&#x3D;[67 69 66 74 65 64],startOffset&#x3D;18,endOffset&#x3D;24,positionIncrement&#x3D;1,positionLength&#x3D;1,type&#x3D;&lt;ALPHANUM&gt;,termFrequency&#x3D;1</span><br><span class="line">token: term&#x3D;student,bytes&#x3D;[73 74 75 64 65 6e 74],startOffset&#x3D;25,endOffset&#x3D;32,positionIncrement&#x3D;1,positionLength&#x3D;1,type&#x3D;&lt;ALPHANUM&gt;,termFrequency&#x3D;1</span><br></pre></td></tr></table></figure>
<p>这个输出结果是非常值得探究的。可以看到sentence字段的文本数据”a good student, a gifted student”经过StandardAnalyzer分析之后输出了6个token，每个token由一些属性组成，这些属性对应的定义类在org.apache.lucene.analysis.tokenattributes包下面，有兴趣的可以查阅。这里我们简单介绍一下这些属性：</p>
<ul>
<li><strong>term</strong>：解析出来的词。<strong>注意这里的term不同于我们之前介绍的Term，它仅指提取出来的词</strong>。</li>
<li><strong>bytes</strong>：词的字节数组形式。</li>
<li><strong>startOffset, endOffset</strong>：词开始和结束的位置，从0开始计数。大家可以数一下。</li>
<li><strong>positionIncrement</strong>：当前词和上个词的距离，默认为1，表示词是连续的。如果有些token被丢掉了，这个值就会大于1了。可以将上述代码中注释掉的那行放开，同时将原来不带停用词的analyzer注释掉，这样解析出的停用词token就会被移除，你就会发现有些token的该字段的值会变成2。该字段主要用于支持”phrase search”, “span search”以及”highlight”，这些搜索都需要知道关键字在文档中的position，以后介绍搜索的时候再介绍。另外这个字段还有一个非常重要的用途就是支持同义词查询。我们将该某个token的positionIncrement置为0，就表示该token和上个token没有距离，搜索的时候，不论搜这两个token任何一个，都会返回它们两对应的文档。假设第一个token是土豆，下一个token是马铃薯，马铃薯对应的token的positionIncrement为0，那我们搜马铃薯时，也会给出土豆相关的信息，反之亦然。</li>
<li><strong>positionLength</strong>：该字段跨了多少个位置。代码注释中说极少有Analyzer会产生该字段，基本都是使用默认值1.</li>
<li><strong>type</strong>：字段类型。需要注意的是这个类型是由每个Analyzer的Tokenizer定义的，不同的Analyer定义的类型可能不同。比如StandardAnalyzer使用的StandardTokenizer定义了这几种类型：<ALPHANUM>、<NUM>、<SOUTHEAST_ASIAN>、<IDEOGRAPHIC>、<HIRAGANA>、<KATAKANA>、<HANGUL>、<EMOJI>。</li>
<li><strong>termFrequency</strong>：词频。注意这里的词频不是token在句子中出现的频率，而是让用户自定义的，比如我们想让某个token在评分的时候更重要一些，那我们就可以将其词频设置大一些。如果不设置，默认都会初始化为1。比如上面输出结果中有两个”a”字段，词频都为初始值1，这个在后续的流程会合并，合并之后，词频会变为2。</li>
</ul>
<p>除了以上属性外，还有一个可能存在的属性就是payload，我们可以在这个字段里面存储一些信息。以上就是一个完整的Token。</p>
<h2 id="词向量（Term-Vector）"><a href="#词向量（Term-Vector）" class="headerlink" title="词向量（Term Vector）"></a>词向量（Term Vector）</h2><p>Analyzer分析出来的Token并不会直接写入Index，还需要做一些转化：</p>
<ul>
<li>取token中的词，以及包含该词的字段信息、文档信息（doc id），形成词到字段信息、文档信息的映射，也就是我们前面介绍的倒排索引。</li>
<li>取token中的词，以及包含该词的positionIncrement、startOffset、endOffset、termFrequency信息，组成从token到后面四个信息的映射，这就是<strong>词向量</strong>。</li>
</ul>
<p>所以，倒排索引和词向量都是从term到某个value的映射，只是value的值不一样。这里需要注意，倒排索引是所有文档范围内的，而词向量是某个文档范围的。简言之就是一个index对应一个倒排索引，而一个document就有一个词向量。<strong>有了倒排索引，我们就知道搜索关键字包含在index的哪些document的字段中。有了词向量，我们就知道关键字在匹配到的document的具体位置。</strong>下面让我们从代码角度来验证一下上面的理论。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import org.apache.lucene.analysis.Analyzer;</span><br><span class="line">import org.apache.lucene.analysis.standard.StandardAnalyzer;</span><br><span class="line">import org.apache.lucene.document.Document;</span><br><span class="line">import org.apache.lucene.document.Field;</span><br><span class="line">import org.apache.lucene.document.FieldType;</span><br><span class="line">import org.apache.lucene.index.*;</span><br><span class="line">import org.apache.lucene.search.DocIdSetIterator;</span><br><span class="line">import org.apache.lucene.store.Directory;</span><br><span class="line">import org.apache.lucene.store.FSDirectory;</span><br><span class="line"></span><br><span class="line">import java.nio.file.Paths;</span><br><span class="line"></span><br><span class="line">public class TermVectorShow &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        &#x2F;&#x2F; 构建索引</span><br><span class="line">        final String indexPath &#x3D; &quot;indices&#x2F;tv-show&quot;;</span><br><span class="line">        Directory indexDir &#x3D; FSDirectory.open(Paths.get(indexPath));</span><br><span class="line"></span><br><span class="line">        Analyzer analyzer &#x3D; new StandardAnalyzer();</span><br><span class="line">        IndexWriterConfig iwc &#x3D; new IndexWriterConfig(analyzer);</span><br><span class="line">        iwc.setOpenMode(IndexWriterConfig.OpenMode.CREATE);</span><br><span class="line">        IndexWriter writer &#x3D; new IndexWriter(indexDir, iwc);</span><br><span class="line"></span><br><span class="line">        String sentence &#x3D; &quot;a good student, a gifted student&quot;;</span><br><span class="line">        &#x2F;&#x2F; 默认不会保存词向量，这里我们通过一些设置来保存词向量的相关信息</span><br><span class="line">        FieldType fieldType &#x3D; new FieldType();</span><br><span class="line">        fieldType.setStored(true);</span><br><span class="line">        fieldType.setStoreTermVectors(true);</span><br><span class="line">        fieldType.setStoreTermVectorOffsets(true);</span><br><span class="line">        fieldType.setStoreTermVectorPositions(true);</span><br><span class="line">        fieldType.setIndexOptions(</span><br><span class="line">             IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);</span><br><span class="line">        Field field &#x3D; new Field(&quot;content&quot;, sentence, fieldType);</span><br><span class="line">        Document document &#x3D; new Document();</span><br><span class="line">        document.add(field);</span><br><span class="line">        writer.addDocument(document);</span><br><span class="line">        writer.close();</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 从索引读取Term Vector信息</span><br><span class="line">        IndexReader indexReader &#x3D; DirectoryReader.open(indexDir);</span><br><span class="line">        Terms termVector &#x3D; indexReader.getTermVector(0, &quot;content&quot;);</span><br><span class="line">        TermsEnum termIter &#x3D; termVector.iterator();</span><br><span class="line">        while (termIter.next() !&#x3D; null) &#123;</span><br><span class="line">            PostingsEnum postingsEnum &#x3D; termIter.postings(null, PostingsEnum.ALL);</span><br><span class="line">            while (postingsEnum.nextDoc() !&#x3D; DocIdSetIterator.NO_MORE_DOCS) &#123;</span><br><span class="line">                int freq &#x3D; postingsEnum.freq();</span><br><span class="line">                System.out.printf(&quot;term: %s, freq: %d,&quot;, termIter.term().utf8ToString(), freq);</span><br><span class="line">                while (freq &gt; 0) &#123;</span><br><span class="line">                    System.out.printf(&quot; nextPosition: %d,&quot;, postingsEnum.nextPosition());</span><br><span class="line">                    System.out.printf(&quot; startOffset: %d, endOffset: %d&quot;,</span><br><span class="line">                            postingsEnum.startOffset(), postingsEnum.endOffset());</span><br><span class="line">                    freq--;</span><br><span class="line">                &#125;</span><br><span class="line">                System.out.println();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这段代码实现的功能是先indexing 1条document，形成index，然后我们读取index，从中获取那条document content字段的词向量。需要注意，indexing时默认是不存储词向量相关信息的，我们需要通过FieldType做显式的设置，否则你读取出来的Term Vector会是null。我们看一下程序的输出结果：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">term: a, freq: 2, nextPosition: 0, startOffset: 0, endOffset: 1 nextPosition: 3, startOffset: 16, endOffset: 17</span><br><span class="line">term: gifted, freq: 1, nextPosition: 4, startOffset: 18, endOffset: 24</span><br><span class="line">term: good, freq: 1, nextPosition: 1, startOffset: 2, endOffset: 6</span><br><span class="line">term: student, freq: 2, nextPosition: 2, startOffset: 7, endOffset: 14 nextPosition: 5, startOffset: 25, endOffset: 32</span><br></pre></td></tr></table></figure>
<p>这里我们indexing的数据和上一节token部分的数据是一样的，而且都使用的是StandardAnalyzer，所以我们可以对比着看上一节输出的token和这里输出的term vector数据。可以看到，之前重复的token（a和student）到这里已经被合并了，并且词频也相应的变成了2。然后我们看一下position信息和offset信息也是OK的。而像token中的positionLength、type等信息都丢弃了。<br>词向量的信息量比较大，所以默认并不记录，我们想要保存时需要针对每个字段做显式的设置，Lucene 8.2.0中包含如下一些选项（见org.apache.lucene.index.IndexOptions枚举类）：</p>
<ul>
<li>NONE：不索引</li>
<li>DOCS：只索引字段，不保存词频等位置信息</li>
<li>DOCS_AND_FREQS：索引字段并保存词频信息，但不保存位置信息</li>
<li>DOCS_AND_FREQS_AND_POSITIONS：索引字段并保存词频及位置信息</li>
<li>DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS：索引字段并保存词频、位置、偏移量等信息</li>
</ul>
<p>phrase search和span search需要position信息支持，所以一般全文搜索引擎默认会采用DOCS_AND_FREQS_AND_POSITIONS策略，这样基本就能覆盖常用的搜索需求了。而需要高亮等功能的时候，才需要记录offset信息。</p>
<h1 id="字段及其属性"><a href="#字段及其属性" class="headerlink" title="字段及其属性"></a>字段及其属性</h1><p>在创建Field的时候，第一个参数是字段名，第二个是字段值，第三个就是字段属性了。字段的属性决定了字段如何Analyze，以及Analyze之后存储哪些信息，进而决定了以后我们可以使用哪些方式进行检索。</p>
<h2 id="Field类"><a href="#Field类" class="headerlink" title="Field类"></a>Field类</h2><p>Field对应的类是org.apache.lucene.document.Field，该类实现了org.apache.lucene.document.IndexableField接口，代表用于indexing的一个字段。Field类比较底层一些，所以Lucene实现了许多Field子类，用于不同的场景，比如下图是IDEA分析出来的Field的子类：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/6.png" alt="图片"></p>
<p>如果有某个子类能满足我们的场景，那推荐使用子类。在介绍常用子类之前，需要了解一下字段的三大类属性：</p>
<ul>
<li>是否indexing（只有indexing的数据才能被搜索）</li>
<li>是否存储（即是否保存字段的原始值）</li>
<li>是否保存term vector</li>
</ul>
<p>这些属性就是由之前文章中介绍的org.apache.lucene.index.IndexOptions枚举类定义的：</p>
<ul>
<li>NONE：不索引</li>
<li>DOCS：只索引字段，不保存词频等位置信息</li>
<li>DOCS_AND_FREQS：索引字段并保存词频信息，但不保存位置信息</li>
<li>DOCS_AND_FREQS_AND_POSITIONS：索引字段并保存词频及位置信息</li>
<li>DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS：索引字段并保存词频、位置、偏移量等信息</li>
</ul>
<p>Field的各个子类就是实现了对不同类型字段的存储，同时选择了不同的字段属性，这里列举几个常用的：</p>
<ul>
<li>TextField：存储字符串类型的数据。indexing+analyze；不存储原始数据；不保存term vector。适用于需要全文检索的数据，比如邮件内容，网页内容等。</li>
<li>StringField：存储字符串类型的数据。indexing但不analyze，即整个字符串就是一个token，之前介绍的KeywordAnalyzer就属于这种；不存储原始数据；不保存term vector。适用于文章标题、人名、ID等只需精确匹配的字符串。</li>
<li>IntPoint, LongPoint, FloatPoint, DoublePoint：用于存储各种不同类型的数值型数据。indexing；不存储原始数据；不保存term vector。适用于数值型数据的存储。</li>
</ul>
<p>所以，对于Field及其子类我们需要注意以下两点：</p>
<ul>
<li>几乎所有的Field子类（除StoredField）都默认不存储原始数据，如果需要存储原始数据，就要额外增加一个StoredField类型的字段，专门用于存储原始数据。注意该类也是Field的一个子类。当然也有一些子类的构造函数中提供了参数来控制是否存储原始数据，比如StringField，创建实例时可以通过传递Field.Store.YES参数来存储原始数据。</li>
<li>Field子类的使用场景和对应的属性都已经设置好了，如果子类不能满足我们的需求，就需要对字段属性进行自定义，但子类的属性一般是不允许更改的，需要直接使用Field类，再配合FieldType类进行自定义化。</li>
</ul>
<h2 id="FieldType类"><a href="#FieldType类" class="headerlink" title="FieldType类"></a>FieldType类</h2><p>org.apache.lucene.document.FieldType类实现了org.apache.lucene.index.IndexableFieldType接口，用于描述字段的属性，如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">FieldType fieldType &#x3D; new FieldType();</span><br><span class="line">fieldType.setStored(true);</span><br><span class="line">fieldType.setStoreTermVectors(true);</span><br><span class="line">fieldType.setStoreTermVectorOffsets(true);</span><br><span class="line">fieldType.setStoreTermVectorPositions(true);</span><br><span class="line">fieldType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);</span><br><span class="line">Field field &#x3D; new Field(&quot;content&quot;, sentence, fieldType);</span><br></pre></td></tr></table></figure>
<p>该类定义了一些成员变量，这些成员变量就是字段的一些属性，这里列一下代码中的成员变量：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private boolean stored;</span><br><span class="line">private boolean tokenized &#x3D; true;</span><br><span class="line">private boolean storeTermVectors;</span><br><span class="line">private boolean storeTermVectorOffsets;</span><br><span class="line">private boolean storeTermVectorPositions;</span><br><span class="line">private boolean storeTermVectorPayloads;</span><br><span class="line">private boolean omitNorms;</span><br><span class="line">private IndexOptions indexOptions &#x3D; IndexOptions.NONE;</span><br><span class="line">private boolean frozen;</span><br><span class="line">private DocValuesType docValuesType &#x3D; DocValuesType.NONE;</span><br><span class="line">private int dataDimensionCount;</span><br><span class="line">private int indexDimensionCount;</span><br><span class="line">private int dimensionNumBytes;</span><br><span class="line">private Map&lt;String, String&gt; attributes;</span><br></pre></td></tr></table></figure>
<p>大部分属性含义已经比较清楚了，这里再简单介绍一下其含义：</p>
<ul>
<li><em>stored</em>：是否存储字段，默认为false。</li>
<li><em>tokenized</em>：是否analyze，默认为true。</li>
<li><em>storeTermVectors</em>：是否存储TermVector（如果是true，也不存储offset、position、payload信息），默认为false。</li>
<li><em>storeTermVectorOffsets</em>：是否存储offset信息，默认为false。</li>
<li><em>storeTermVectorPositions</em>：是否存储position信息，默认为false。</li>
<li><em>storeTermVectorPayloads</em>：是否存储payload信息，默认为false。</li>
<li><em>omitNorms</em>：是否忽略norm信息，默认为false。那什么是norm信息呢？Norm的全称是“Normalization”，理解起来非常简单，按照TF-IDF的计算方式，包含同一个搜索词的多个文本，文本越短其权重（或者叫相关性）越高。比如有两个文本都包含搜索词，一个文本只有100个词，另外文本一个有1000个词，那按照TF-IDF的算法，第一个文本跟搜索词的相关度比第二个文本高。这个信息就是norm信息。如果我们将其忽略掉，那在计算相关性的时候，会认为长文本和短文本的权重得分是一样的。</li>
<li><em>indexOptions</em>：即org.apache.lucene.index.IndexOptions，已经介绍过了。默认值为DOCS_AND_FREQS_AND_POSITIONS。</li>
<li><em>frozen</em>：该值设置为true之后，字段的各个属性就不允许再更改了，比如Field的TextField、StringField等子类都将该值设置为true了，因为他们已经将字段的各个属性定制好了。</li>
<li><em>dataDimensionCount</em>、<em>indexDimensionCount</em>、<em>dimensionNumBytes</em>：这几个和数值型的字段类型有关系，Lucene 6.0开始，对于数值型都改用Point来组织。dataDimensionCount和indexDimensionCount都可以理解为是Point的维度，类似于数组的维度。dimensionNumBytes则是Point中每个值所使用的字节数，比如IntPoint和FloatPoint是4个字节，LongPoint和DoublePoint则是8个字节。</li>
<li><em>attributes</em>：可以选择性的以key-value的形式给字段增加一些元数据信息，但注意这个key-value的map不是线程安全的。</li>
<li><em>docValuesType</em>：指定字段的值指定以何种类型索引DocValue。那什么是DocValue？DocValue就是从文档到Term的一个正向索引，需要这个东西是因为排序、聚集等操作需要根据文档快速访问文档内的字段。Lucene 4.0之前都是在查询的时候将所有文档的相关字段信息加载到内存缓存，一方面用的时候才加载，所以慢，另一方面对于内存压力很大。4.0中引入了DocValue的概念，在indexing阶段除了创建倒排索引，也可以选择性的创建一个正向索引，这个正向索引就是DocValue，主要用于排序、聚集等操作。其好处是存储在磁盘上，减小了内存压力，而且因为是事先计算好的，所以使用时速度也很快。弊端就是磁盘使用量变大（需要耗费 document个数*每个document的字段数 个字节），同时indexing的速度慢了。</li>
</ul>
<p>对于我们使用（包括ES、Solr等基于Lucene的软件）而言，只需要知道我们检索一个字段的时候可以控制保存哪些信息，以及这些信息在什么场景下使用，能带来什么好处，又会产生什么弊端即可。举个例子：比如我们在设计字段的时候，如果一个字段不会用来排序，也不会做聚集，那就没有必要生成DocValue，既能节省磁盘空间，又能提高写入速度。另外对于norm信息，如果你的场景只关注是否包含，那无需保存norm信息，但如果也关注相似度评分，并且文本长短是一个需要考虑的因素，那就应该保存norm信息。</p>
<h2 id="基本使用-1"><a href="#基本使用-1" class="headerlink" title="基本使用"></a>基本使用</h2><blockquote>
<p>参考：<br><a href="https://blog.51cto.com/8744704/2086852" target="_blank" rel="noopener">https://blog.51cto.com/8744704/2086852</a><br><a href="https://www.cnblogs.com/leeSmall/p/9011405.html" target="_blank" rel="noopener">https://www.cnblogs.com/leeSmall/p/9011405.html</a><br><a href="https://www.amazingkoala.com.cn/Lucene/DocValues/2019/0412/48.html" target="_blank" rel="noopener">https://www.amazingkoala.com.cn/Lucene/DocValues/2019/0412/48.html</a><br><a href="https://www.cnblogs.com/cnjavahome/p/9192467.html" target="_blank" rel="noopener">https://www.cnblogs.com/cnjavahome/p/9192467.html</a></p>
</blockquote>
<p>在例句搜索中主要使用了以下几种FieldType:</p>
<ul>
<li>TextField</li>
<li>IntPoint<ul>
<li>把整型存入索引中，必须同时加入NumericDocValuesField和StoredField，是看IntPoint源码注释中写的，不知道为什么一定要这样写</li>
</ul>
</li>
<li>FloatPoint<ul>
<li>把浮点数存入索引中，必须同时加入FloatDocValuesField和StoredField</li>
</ul>
</li>
<li>数组类型<ul>
<li>复杂类型存储</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">confidence &#x3D; int(data_json[&#39;confidence&#39;])</span><br><span class="line">document.add(IntPoint(&quot;confidence&quot;, confidence))</span><br><span class="line">document.add(NumericDocValuesField(&quot;confidence&quot;, confidence))</span><br><span class="line">document.add(StoredField(&quot;confidence&quot;, confidence))</span><br><span class="line"></span><br><span class="line">score &#x3D; float(data_json[&#39;score&#39;])</span><br><span class="line">document.add(FloatPoint(&quot;score&quot;, score))</span><br><span class="line">document.add(FloatDocValuesField(&quot;score&quot;, score))</span><br><span class="line">document.add(StoredField(&quot;score&quot;, score))</span><br><span class="line"></span><br><span class="line">document.add(SortedSetDocValuesField(&quot;keyword&quot;, BytesRef(keyword_text)))</span><br><span class="line">document.add(StoredField(&quot;keyword&quot;, keyword_text))</span><br><span class="line">document.add(Field(&quot;keyword&quot;, keyword_text, TextField.TYPE_STORED))</span><br></pre></td></tr></table></figure>
<h1 id="索引存储文件介绍"><a href="#索引存储文件介绍" class="headerlink" title="索引存储文件介绍"></a>索引存储文件介绍</h1><h3 id="索引文件格式"><a href="#索引文件格式" class="headerlink" title="索引文件格式"></a>索引文件格式</h3><p>不论是Solr还是ES，底层index的存储都是完全使用Lucene原生的方式，没有做改变，所以本文会以ES为例来介绍。需要注意的是Lucene的index在ES中称为shard，本文中提到的index都指的是Lucene的index，即ES中的shard。先来看一个某个index的数据目录：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/7.png" alt="图片"></p>
<p>可以看到一个索引包含了很多文件，似乎很复杂。但仔细观察之后会发现乱中似乎又有些规律：很多文件前缀一样，只是后缀不同，比如有很多_3c开头的文件。回想一下之前文章的介绍，index由若干个segment组成，而<strong>一个index目录下前缀相同表示这些文件都属于同一个segment</strong>。</p>
<p>那各种各样的后缀又代表什么含义呢？Lucene存储segment时有两种方式：</p>
<ul>
<li><strong>multifile格式</strong>。该模式下会产生很多文件，不同的文件存储不同的信息，其弊端是读取index时需要打开很多文件，可能造成文件描述符超出系统限制。</li>
<li><strong>compound格式</strong>。一般简写为CFS(Compound File System)，该模式下会将很多小文件合并成一个大文件，以减少文件描述符的使用。</li>
</ul>
<p>我们先来介绍multifile格式下的各个文件：</p>
<ul>
<li>write.lock：每个index目录都会有一个该文件，用于防止多个IndexWriter同时写一个文件。</li>
<li>segments_N：该文件记录index所有segment的相关信息，比如该索引包含了哪些segment。IndexWriter每次commit都会生成一个（N的值会递增），新文件生成后旧文件就会删除。所以也说该文件用于保存commit point信息。</li>
</ul>
<p>上面这两个文件是针对当前index的，所以每个index目录下都只会有1个（segments_N可能因为旧的没有及时删除临时存在两个）。下面介绍的文件都是针对segment的，每个segment就会有1个。</p>
<ul>
<li>.si：<em>Segment Info</em>的缩写，用于记录segment的一些元数据信息。</li>
<li>.fnm：<em>Fields</em>，用于记录fields设置类信息，比如字段的index option信息，是否存储了norm信息、DocValue等。</li>
<li>.fdt：<em>Field Data</em>，存储字段信息。当通过StoredField或者Field.Store.YES指定存储原始field数据时，这些数据就会存储在该文件中。</li>
<li>.fdx：<em>Field Index</em>，.fdt文件的索引/指针。通过该文件可以快速从.fdt文件中读取field数据。</li>
<li>.doc：<em>Frequencies</em>，存储了一个documents列表，以及它们的term frequency信息。</li>
<li>.pos：<em>Positions</em>，和.doc类似，但保存的是position信息。</li>
<li>.pay：Payloads<em>，和</em>.doc类似，但保存的是payloads和offset信息。</li>
<li>.tim：<em>Term Dictionary</em>，存储所有文档analyze出来的term信息。同时还包含term对应的document number以及若干指向.doc, .pos, .pay的指针，从而可以快速获取term的term vector信息。。</li>
<li>.tip：<em>Term Index</em>，该文件保存了Term Dictionary的索引信息，使得可以对Term Dictionary进行随机访问。</li>
<li>.nvd, .nvm：<em>Norms</em>，这两个都是用来存储Norms信息的，前者用于存储norms的数据，后者用于存储norms的元数据。</li>
<li>.dvd, .dvm：<em>Per-Document Values</em>，这两个都是用来存储DocValues信息的，前者用于数据，后者用于存储元数据。</li>
<li>.tvd：<em>Term Vector Data</em>，用于存储term vector数据。</li>
<li>.tvx：<em>Term Vector Index</em>，用于存储Term Vector Data的索引数据。</li>
<li>.liv：<em>Live Documents</em>，用于记录segment中哪些documents没有被删除。一般不存在该文件，表示segment内的所有document都是live的。如果有documents被删除，就会产生该文件。以前是使用一个.del后缀的文件来记录被删除的documents，现在改为使用该文件了。</li>
<li>.dim,.dii：<em>Point values</em>，这两个文件用于记录indexing的Point信息，前者保存数据，后者保存索引/指针，用于快速访问前者。</li>
</ul>
<p>上面介绍了很多文件类型，实际中不一定都有，如果indexing阶段不保存字段的term vector信息，那存储term vector的相关文件可能就不存在。如果一个index的segment非常多，那将会有非常非常多的文件，检索时，这些文件都是要打开的，很可能会造成文件描述符不够用，所以Lucene引入了前面介绍的CFS格式，它把上述每个segment的众多文件做了一个合并压缩（.liv和.si没有被合并，依旧单独写文件），最终形成了两个新文件：.cfs和.cfe，前者用于保存数据，后者保存了前者的一个Entry Table，用于快速访问。所以，如果使用CFS的话，最终对于每个segment，最多就只存在.cfs, .cfe, .si, .liv4个文件了。Lucene从1.4版本开始，默认使用CFS来保存segment数据，但开发者仍然可以选择使用multifile格式。一般来说，对于小的segment使用CFS，对于大的segment，使用multifile格式。比如Lucene的org.apache.lucene.index.MergePolicy构造函数中就提供merge时在哪些条件下使用CFS：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">  &#x2F;**</span><br><span class="line">   * Default ratio for compound file system usage. Set to &lt;tt&gt;1.0&lt;&#x2F;tt&gt;, always use </span><br><span class="line">   * compound file system.</span><br><span class="line">   *&#x2F;</span><br><span class="line">  protected static final double DEFAULT_NO_CFS_RATIO &#x3D; 1.0;</span><br><span class="line"></span><br><span class="line">  &#x2F;**</span><br><span class="line">   * Default max segment size in order to use compound file system. Set to &#123;@link Long#MAX_VALUE&#125;.</span><br><span class="line">   *&#x2F;</span><br><span class="line">  protected static final long DEFAULT_MAX_CFS_SEGMENT_SIZE &#x3D; Long.MAX_VALUE;</span><br><span class="line"></span><br><span class="line">  &#x2F;** If the size of the merge segment exceeds this ratio of</span><br><span class="line">   *  the total index size then it will remain in</span><br><span class="line">   *  non-compound format *&#x2F;</span><br><span class="line">  protected double noCFSRatio &#x3D; DEFAULT_NO_CFS_RATIO;</span><br><span class="line">  </span><br><span class="line">  &#x2F;** If the size of the merged segment exceeds</span><br><span class="line">   *  this value then it will not use compound file format. *&#x2F;</span><br><span class="line">  protected long maxCFSSegmentSize &#x3D; DEFAULT_MAX_CFS_SEGMENT_SIZE;</span><br><span class="line"></span><br><span class="line">  &#x2F;**</span><br><span class="line">   * Creates a new merge policy instance.</span><br><span class="line">   *&#x2F;</span><br><span class="line">  public MergePolicy() &#123;</span><br><span class="line">    this(DEFAULT_NO_CFS_RATIO, DEFAULT_MAX_CFS_SEGMENT_SIZE);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  &#x2F;**</span><br><span class="line">   * Creates a new merge policy instance with default settings for noCFSRatio</span><br><span class="line">   * and maxCFSSegmentSize. This ctor should be used by subclasses using different</span><br><span class="line">   * defaults than the &#123;@link MergePolicy&#125;</span><br><span class="line">   *&#x2F;</span><br><span class="line">  protected MergePolicy(double defaultNoCFSRatio, long defaultMaxCFSSegmentSize) &#123;</span><br><span class="line">    this.noCFSRatio &#x3D; defaultNoCFSRatio;</span><br><span class="line">    this.maxCFSSegmentSize &#x3D; defaultMaxCFSSegmentSize;</span><br></pre></td></tr></table></figure>
<h3 id=""><a href="#" class="headerlink" title="}"></a>}</h3><p>栗子</p>
<p>首先在ES中创建一个索引：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PUT nyc-test</span><br><span class="line">&#123;</span><br><span class="line">  &quot;settings&quot;: &#123;</span><br><span class="line">    &quot;number_of_shards&quot;: 1,</span><br><span class="line">    &quot;number_of_replicas&quot;: 0,</span><br><span class="line">    &quot;refresh_interval&quot;: -1</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里设置1个shard，0个副本，并且将refresh_interval设置为-1，表示不自动刷新。创建完之后就可以在es的数据目录找到该索引，es的后台索引的目录结构为：&lt;数据目录&gt;/nodes/0/indices/&lt;索引UUID&gt;/<shard>/index，这里的shard就是Lucene的index。我们看下刚创建的index的目录：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-&gt; % ll</span><br><span class="line">总用量 4.0K</span><br><span class="line">-rw-rw-r-- 1 allan allan 230 10月 11 21:45 segments_2</span><br><span class="line">-rw-rw-r-- 1 allan allan   0 10月 11 21:45 write.lock</span><br></pre></td></tr></table></figure>
<p>可以看到，现在还没有写入任何数据，所以只有index级别的segments_N和write.lock文件，没有segment级别的文件。写入1条数据并查看索引目录的变化：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PUT nyc-test&#x2F;doc&#x2F;1</span><br><span class="line">&#123;</span><br><span class="line">  &quot;name&quot;: &quot;Jack&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 查看索引目录</span><br><span class="line">-&gt; % ll</span><br><span class="line">总用量 4.0K</span><br><span class="line">-rw-rw-r-- 1 allan allan   0 10月 11 22:20 _0.fdt</span><br><span class="line">-rw-rw-r-- 1 allan allan   0 10月 11 22:20 _0.fdx</span><br><span class="line">-rw-rw-r-- 1 allan allan 230 10月 11 22:19 segments_2</span><br><span class="line">-rw-rw-r-- 1 allan allan   0 10月 11 22:19 write.lock</span><br></pre></td></tr></table></figure>
<p>可以看到出现了1个segment的数据，因为ES把数据缓存在内存里面，所以文件大小为0。然后再写入1条数据，并查看目录变化：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PUT nyc-test&#x2F;doc&#x2F;2</span><br><span class="line">&#123;</span><br><span class="line">  &quot;name&quot;: &quot;Allan&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 查看目录</span><br><span class="line">-&gt; % ll</span><br><span class="line">总用量 4.0K</span><br><span class="line">-rw-rw-r-- 1 allan allan   0 10月 11 22:20 _0.fdt</span><br><span class="line">-rw-rw-r-- 1 allan allan   0 10月 11 22:20 _0.fdx</span><br><span class="line">-rw-rw-r-- 1 allan allan 230 10月 11 22:19 segments_2</span><br><span class="line">-rw-rw-r-- 1 allan allan   0 10月 11 22:19 write.lock</span><br></pre></td></tr></table></figure>
<p>因为ES缓存机制的原因，目录没有变化。显式的refresh一下，让内存中的数据落地：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">POST nyc-test&#x2F;_refresh</span><br><span class="line"></span><br><span class="line">-&gt; % ll</span><br><span class="line">总用量 16K</span><br><span class="line">-rw-rw-r-- 1 allan allan  405 10月 11 22:22 _0.cfe</span><br><span class="line">-rw-rw-r-- 1 allan allan 2.5K 10月 11 22:22 _0.cfs</span><br><span class="line">-rw-rw-r-- 1 allan allan  393 10月 11 22:22 _0.si</span><br><span class="line">-rw-rw-r-- 1 allan allan  230 10月 11 22:19 segments_2</span><br><span class="line">-rw-rw-r-- 1 allan allan    0 10月 11 22:19 write.lock</span><br></pre></td></tr></table></figure>
<p>ES的refresh操作会将内存中的数据写入到一个新的segment中，所以refresh之后写入的两条数据形成了一个segment，并且使用CFS格式存储了。然后再插入1条数据，接着update这条数据：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 触发Lucene commit</span><br><span class="line">POST nyc-test&#x2F;_flush?wait_if_ongoing</span><br><span class="line"></span><br><span class="line"># 查看目录</span><br><span class="line">-&gt; % ll</span><br><span class="line">总用量 32K</span><br><span class="line">-rw-rw-r-- 1 allan allan  405 10月 11 22:22 _0.cfe</span><br><span class="line">-rw-rw-r-- 1 allan allan 2.5K 10月 11 22:22 _0.cfs</span><br><span class="line">-rw-rw-r-- 1 allan allan  393 10月 11 22:22 _0.si</span><br><span class="line">-rw-rw-r-- 1 allan allan   67 10月 11 22:24 _1_1.liv</span><br><span class="line">-rw-rw-r-- 1 allan allan  405 10月 11 22:24 _1.cfe</span><br><span class="line">-rw-rw-r-- 1 allan allan 2.5K 10月 11 22:24 _1.cfs</span><br><span class="line">-rw-rw-r-- 1 allan allan  393 10月 11 22:24 _1.si</span><br><span class="line">-rw-rw-r-- 1 allan allan  361 10月 11 22:25 segments_3</span><br><span class="line">-rw-rw-r-- 1 allan allan    0 10月 11 22:19 write.lock</span><br><span class="line"></span><br><span class="line"># 查看segment信息</span><br><span class="line">GET _cat&#x2F;segments&#x2F;nyc-test?v</span><br><span class="line"></span><br><span class="line">index    shard prirep ip        segment generation docs.count docs.deleted  size size.memory committed searchable version compound</span><br><span class="line">nyc-test 0     p      10.8.4.42 _0               0          2            0 3.2kb        1184 true      true       7.4.0   true</span><br><span class="line">nyc-test 0     p      10.8.4.42 _1               1          1            2 3.2kb        1184 true      true       7.4.0   true</span><br></pre></td></tr></table></figure>
<p>触发Lucene commit之后，可以看到segments_2变成了segments_3。然后调用_cat接口查看索引的segment信息也能看到目前有2个segment，而且都已经commit过了，并且compound是true，表示是CFS格式存储的。当然Lucene的segment是可以合并的。我们通过ES的forcemerge接口进行合并，并且将所有segment合并成1个segment，forcemerge的时候会自动调用flush，即会触发Lucene commit：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">POST nyc-test&#x2F;_forcemerge?max_num_segments&#x3D;1</span><br><span class="line"></span><br><span class="line">-&gt; % ll</span><br><span class="line">总用量 60K</span><br><span class="line">-rw-rw-r-- 1 allan allan  69 10月 11 22:27 _2.dii</span><br><span class="line">-rw-rw-r-- 1 allan allan 123 10月 11 22:27 _2.dim</span><br><span class="line">-rw-rw-r-- 1 allan allan 142 10月 11 22:27 _2.fdt</span><br><span class="line">-rw-rw-r-- 1 allan allan  83 10月 11 22:27 _2.fdx</span><br><span class="line">-rw-rw-r-- 1 allan allan 945 10月 11 22:27 _2.fnm</span><br><span class="line">-rw-rw-r-- 1 allan allan 110 10月 11 22:27 _2_Lucene50_0.doc</span><br><span class="line">-rw-rw-r-- 1 allan allan  80 10月 11 22:27 _2_Lucene50_0.pos</span><br><span class="line">-rw-rw-r-- 1 allan allan 287 10月 11 22:27 _2_Lucene50_0.tim</span><br><span class="line">-rw-rw-r-- 1 allan allan 145 10月 11 22:27 _2_Lucene50_0.tip</span><br><span class="line">-rw-rw-r-- 1 allan allan 100 10月 11 22:27 _2_Lucene70_0.dvd</span><br><span class="line">-rw-rw-r-- 1 allan allan 469 10月 11 22:27 _2_Lucene70_0.dvm</span><br><span class="line">-rw-rw-r-- 1 allan allan  59 10月 11 22:27 _2.nvd</span><br><span class="line">-rw-rw-r-- 1 allan allan 100 10月 11 22:27 _2.nvm</span><br><span class="line">-rw-rw-r-- 1 allan allan 572 10月 11 22:27 _2.si</span><br><span class="line">-rw-rw-r-- 1 allan allan 296 10月 11 22:27 segments_4</span><br><span class="line">-rw-rw-r-- 1 allan allan   0 10月 11 22:19 write.lock</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">GET _cat&#x2F;segments&#x2F;nyc-test?v</span><br><span class="line"></span><br><span class="line">index    shard prirep ip        segment generation docs.count docs.deleted  size size.memory committed searchable version compound</span><br><span class="line">nyc-test 0     p      10.8.4.42 _2               2          3            0 3.2kb        1224 true      true       7.4.0   false</span><br></pre></td></tr></table></figure>
<p>可以看到，force merge之后只有一个segment了，并且使用了multifile格式存储，而不是compound。当然这并非Lucene的机制，而是ES自己的设计。<br>最后用图总结一下：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/8.png" alt="图片"></p>
<p>想了解更详细的，可以阅读：<a href="https://lucene.apache.org/core/8_2_0/core/org/apache/lucene/codecs/lucene80/package-summary.html#package.description" target="_blank" rel="noopener">https://lucene.apache.org/core/8_2_0/core/org/apache/lucene/codecs/lucene80/package-summary.html#package.description</a></p>
<h1 id="Query"><a href="#Query" class="headerlink" title="Query"></a>Query</h1><blockquote>
<p>参考：<br><a href="https://niyanchun.com/lucene-learning-8.html" target="_blank" rel="noopener">https://niyanchun.com/lucene-learning-8.html</a><br><a href="https://niyanchun.com/lucene-learning-9.html" target="_blank" rel="noopener">https://niyanchun.com/lucene-learning-9.html</a><br><a href="https://pythonhosted.org/lupyne/examples.html" target="_blank" rel="noopener">https://pythonhosted.org/lupyne/examples.html</a></p>
</blockquote>
<p>在Lucene中，Term是查询的基本单元(unit)，所有查询类的父类是org.apache.lucene.search.Query，本文会介绍下图中这些主要的Query子类：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/9.png" alt="图片"></p>
<p>DisjunctionMaxQuery主要用于控制评分机制，SpanQuery代表一类查询，有很多的实现。这两类查询不是非常常用。</p>
<h2 id="TermQuery"><a href="#TermQuery" class="headerlink" title="TermQuery"></a>TermQuery</h2><p>TermQuery是最基础最常用的的一个查询了，对应的类是org.apache.lucene.search.TermQuery。其功能很简单，就是查询哪些文档中包含指定的term。看下面代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line"> * Query Demo.</span><br><span class="line"> *</span><br><span class="line"> * @author NiYanchun</span><br><span class="line"> **&#x2F;</span><br><span class="line">public class QueryDemo &#123;</span><br><span class="line"></span><br><span class="line">    &#x2F;**</span><br><span class="line">     * 搜索的字段</span><br><span class="line">     *&#x2F;</span><br><span class="line">    private static final String SEARCH_FIELD &#x3D; &quot;contents&quot;;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        &#x2F;&#x2F; 索引保存目录</span><br><span class="line">        final String indexPath &#x3D; &quot;indices&#x2F;poems-index&quot;;</span><br><span class="line">        &#x2F;&#x2F; 读取索引</span><br><span class="line">        IndexReader indexReader &#x3D; DirectoryReader.open(FSDirectory.open(Paths.get(indexPath)));</span><br><span class="line">        IndexSearcher searcher &#x3D; new IndexSearcher(indexReader);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; TermQuery</span><br><span class="line">        termQueryDemo(searcher);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private static void termQueryDemo(IndexSearcher searcher) throws IOException &#123;</span><br><span class="line">        System.out.println(&quot;TermQuery, search for &#39;death&#39;:&quot;);</span><br><span class="line">        TermQuery termQuery &#x3D; new TermQuery(new Term(SEARCH_FIELD, &quot;death&quot;));</span><br><span class="line"></span><br><span class="line">        resultPrint(searcher, termQuery);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private static void resultPrint(IndexSearcher searcher, Query query) throws IOException &#123;</span><br><span class="line">        TopDocs topDocs &#x3D; searcher.search(query, 10);</span><br><span class="line">        if (topDocs.totalHits.value &#x3D;&#x3D; 0) &#123;</span><br><span class="line">            System.out.println(&quot;not found!\n&quot;);</span><br><span class="line">            return;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        ScoreDoc[] hits &#x3D; topDocs.scoreDocs;</span><br><span class="line"></span><br><span class="line">        System.out.println(topDocs.totalHits.value + &quot; result(s) matched: &quot;);</span><br><span class="line">        for (ScoreDoc hit : hits) &#123;</span><br><span class="line">            Document doc &#x3D; searcher.doc(hit.doc);</span><br><span class="line">            System.out.println(&quot;doc&#x3D;&quot; + hit.doc + &quot; score&#x3D;&quot; + hit.score + &quot; file: &quot; + doc.get(&quot;path&quot;));</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面代码先读取索引文件，然后执行了一个term查询，查询所有包含death关键词的文档。为了方便打印，我们封装了一个resultPrint函数用于打印查询结果。<em>On Death</em>一诗包含了<em>death</em>关键字，所以程序执行结果为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">TermQuery, search for &#39;death&#39;:</span><br><span class="line">1 result(s) matched: </span><br><span class="line">doc&#x3D;3 score&#x3D;0.6199532 file: data&#x2F;poems&#x2F;OnDeath.txt</span><br></pre></td></tr></table></figure>
<h2 id="BooleanQuery"><a href="#BooleanQuery" class="headerlink" title="BooleanQuery"></a>BooleanQuery</h2><p>BooleanQuery用于将若干个查询按照与或的逻辑关系组织起来，支持嵌套。目前支持4个逻辑关系：</p>
<ul>
<li><strong><em>SHOULD</em></strong>：逻辑<strong>或</strong>的关系，文档满足任意一个查询即视为匹配。</li>
<li><strong><em>MUST</em></strong>：逻辑<strong>与</strong>的关系，文档必须满足所有查询才视为匹配。</li>
<li><strong><em>FILTER</em></strong>：逻辑<strong>与</strong>的关系，与must的区别是不计算score，所以性能会比must好。如果只关注是否匹配，而不关注匹配程度（即得分），应该优先使用filter。</li>
<li><strong><em>MUST NOT</em></strong>：逻辑与的关系，且取反。文档不满足所有查询的条件才视为匹配。</li>
</ul>
<p>使用方式也比较简单，以下的代码使用BooleanQuery查询contents字段包含<em>love</em>但不包含<em>seek</em>的词：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private static void booleanQueryDemo(IndexSearcher searcher) throws IOException &#123;</span><br><span class="line">    System.out.println(&quot;BooleanQuery, must contain &#39;love&#39; but absolutely not &#39;seek&#39;: &quot;);</span><br><span class="line">    BooleanQuery.Builder builder &#x3D; new BooleanQuery.Builder();</span><br><span class="line">    builder.add(new TermQuery(new Term(SEARCH_FIELD, &quot;love&quot;)), BooleanClause.Occur.MUST);</span><br><span class="line">    builder.add(new TermQuery(new Term(SEARCH_FIELD, &quot;seek&quot;)), BooleanClause.Occur.MUST_NOT);</span><br><span class="line">    BooleanQuery booleanQuery &#x3D; builder.build();</span><br><span class="line"></span><br><span class="line">    resultPrint(searcher, booleanQuery);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><em>Love’s Secret</em>和<em>Freedom and Love</em>两首诗中均包含了<em>love</em>一词，但前者还包含了<em>seek</em>一词，所以最终的搜索结果为<em>Freedom and Love</em>。</p>
<h2 id="PhraseQuery"><a href="#PhraseQuery" class="headerlink" title="PhraseQuery"></a>PhraseQuery</h2><p>PhraseQuery用于搜索term序列，比如搜索“hello world”这个由两个term组成的一个序列。对于Phrase类的查询需要掌握两个点：</p>
<ul>
<li>Phrase查询需要term的position信息，所以如果indexing阶段没有保存position信息，就无法使用phrase类的查询。</li>
<li>理解slop的概念：Slop就是两个term或者两个term序列的edit distance。后面的FuzzyQuery也用到了该概念，这里简单介绍一下。</li>
</ul>
<p>PhraseQuery使用的是Levenshtein distance，且默认的slop值是0，也就是只检索完全匹配的term序列。看下面这个例子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private static void phraseQueryDemo(IndexSearcher searcher) throws IOException &#123;</span><br><span class="line">    System.out.println(&quot;\nPhraseQuery, search &#39;love that&#39;&quot;);</span><br><span class="line"></span><br><span class="line">    PhraseQuery.Builder builder &#x3D; new PhraseQuery.Builder();</span><br><span class="line">    builder.add(new Term(SEARCH_FIELD, &quot;love&quot;));</span><br><span class="line">    builder.add(new Term(SEARCH_FIELD, &quot;that&quot;));</span><br><span class="line">    PhraseQuery phraseQueryWithSlop &#x3D; builder.build();</span><br><span class="line"></span><br><span class="line">    resultPrint(searcher, phraseQueryWithSlop);</span><br><span class="line">&#125;</span><br><span class="line">&#x2F;&#x2F; 运行结果</span><br><span class="line">PhraseQuery, search &#39;love that&#39;</span><br><span class="line">1 result(s) matched: </span><br><span class="line">doc&#x3D;2 score&#x3D;0.7089927 file: data&#x2F;poems&#x2F;Love&#39;sSecret.txt</span><br></pre></td></tr></table></figure>
<p><em>Love‘s Secret</em>里面有这么一句：”<em>Love that never told shall be</em>“，是能够匹配”<em>love that</em>“的。我们也可以修改slop的值，使得与搜索序列的edit distance小于等于slop的文档都可以被检索到，同时距离越小的文档评分越高。看下面例子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private static void phraseQueryWithSlopDemo(IndexSearcher searcher) throws IOException &#123;</span><br><span class="line">    System.out.println(&quot;PhraseQuery with slop: &#39;love &lt;slop&gt; never&quot;);</span><br><span class="line">    PhraseQuery phraseQueryWithSlop &#x3D; new PhraseQuery(1, SEARCH_FIELD, &quot;love&quot;, &quot;never&quot;);</span><br><span class="line"></span><br><span class="line">    resultPrint(searcher, phraseQueryWithSlop);</span><br><span class="line">&#125;</span><br><span class="line">&#x2F;&#x2F; 运行结果</span><br><span class="line">PhraseQuery with slop: &#39;love &lt;slop&gt; never</span><br><span class="line">1 result(s) matched: </span><br><span class="line">doc&#x3D;2 score&#x3D;0.43595996 file: data&#x2F;poems&#x2F;Love&#39;sSecret.txt</span><br></pre></td></tr></table></figure>
<h2 id="MultiPhraseQuery"><a href="#MultiPhraseQuery" class="headerlink" title="MultiPhraseQuery"></a>MultiPhraseQuery</h2><p>不论是官方文档或是网上的资料，对于MultiPhraseQuery讲解的都比较少。但其实它的功能很简单，举个例子就明白了：我们提供两个由term组成的数组：[“love”, “hate”], [“him”, “her”]，然后把这两个数组传给MultiPhraseQuery，它就会去检索 “love him”, “love her”, “hate him”, “hate her”的组合，每一个组合其实就是一个上面介绍的PhraseQuery。当然MultiPhraseQuery也可以接受更高维的组合。</p>
<p>由上面的例子可以看到PhraseQuery其实是MultiPhraseQuery的一种特殊形式而已，如果给MultiPhraseQuery传递的每个数组里面只有一个term，那就退化成PhraseQuery了。在MultiPhraseQuery中，一个数组内的元素匹配时是 <strong>或(OR)</strong> 的关系，也就是这些term共享同一个position。 还记得之前的文章中我们说过在同一个position放多个term，可以实现同义词的搜索。的确MultiPhraseQuery实际中主要用于同义词的查询。比如查询一个“我爱土豆”，那可以构造这样两个数组传递给MultiPhraseQuery查询：[“喜欢”，“爱”], [“土豆”，”马铃薯”，”洋芋”]，这样查出来的结果就会更全面一些。最后来个例子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private static void multiPhraseQueryDemo(IndexSearcher searcher) throws IOException &#123;</span><br><span class="line">    System.out.println(&quot;MultiPhraseQuery:&quot;);</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; On Death 一诗中有这样一句: I know not what into my ear</span><br><span class="line">    &#x2F;&#x2F; Fog 一诗中有这样一句: It sits looking over harbor and city</span><br><span class="line">    &#x2F;&#x2F; 以下的查询可以匹配 &quot;know harbor, know not, over harbor, over not&quot; 4种情况</span><br><span class="line">    MultiPhraseQuery.Builder builder &#x3D; new MultiPhraseQuery.Builder();</span><br><span class="line">    Term[] termArray1 &#x3D; new Term[2];</span><br><span class="line">    termArray1[0] &#x3D; new Term(SEARCH_FIELD, &quot;know&quot;);</span><br><span class="line">    termArray1[1] &#x3D; new Term(SEARCH_FIELD, &quot;over&quot;);</span><br><span class="line">    Term[] termArray2 &#x3D; new Term[2];</span><br><span class="line">    termArray2[0] &#x3D; new Term(SEARCH_FIELD, &quot;harbor&quot;);</span><br><span class="line">    termArray2[1] &#x3D; new Term(SEARCH_FIELD, &quot;not&quot;);</span><br><span class="line">    builder.add(termArray1);</span><br><span class="line">    builder.add(termArray2);</span><br><span class="line">    MultiPhraseQuery multiPhraseQuery &#x3D; builder.build();</span><br><span class="line"></span><br><span class="line">    resultPrint(searcher, multiPhraseQuery);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 程序输出</span><br><span class="line">MultiPhraseQuery:</span><br><span class="line">2 result(s) matched: </span><br><span class="line">doc&#x3D;0 score&#x3D;2.7032354 file: data&#x2F;poems&#x2F;Fog.txt</span><br><span class="line">doc&#x3D;3 score&#x3D;2.4798129 file: data&#x2F;poems&#x2F;OnDeath.txt</span><br></pre></td></tr></table></figure>
<h2 id="PrefixQuery、WildcardQuery、RegexpQuery"><a href="#PrefixQuery、WildcardQuery、RegexpQuery" class="headerlink" title="PrefixQuery、WildcardQuery、RegexpQuery"></a>PrefixQuery、WildcardQuery、RegexpQuery</h2><p>这三个查询提供模糊模糊查询的功能：</p>
<ul>
<li>PrefixQuery只支持指定前缀模糊查询，用户指定一个前缀，查询时会匹配所有该前缀开头的term。</li>
<li>WildcardQuery比PrefixQuery更进一步，支持 <strong>*</strong>（匹配0个或多个字符）和 <strong>?</strong>（匹配一个字符） 两个通配符。从效果上看，PrefixQuery是WildcardQuery的一种特殊情况，但其底层不是基于WildcardQuery，而是另外一种单独的实现。</li>
<li>RegexpQuery是比WildcardQuery更宽泛的查询，它支持正则表达式。支持的正则语法范围见org.apache.lucene.util.automaton.RegExp类。</li>
</ul>
<p>需要注意，WildcardQuery和RegexpQuery的性能会差一些，因为它们需要遍历很多文档。特别是极力不推荐以模糊匹配开头。当然这里的差是相对其它查询来说的，我粗略测试过，2台16C+32G的ES，比较简短的文档，千万级以下的查询也能毫秒级返回。最后看几个使用的例子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private static void prefixQueryDemo(IndexSearcher searcher) throws IOException &#123;</span><br><span class="line">    System.out.println(&quot;PrefixQuery, search terms begin with &#39;co&#39;&quot;);</span><br><span class="line">    PrefixQuery prefixQuery &#x3D; new PrefixQuery(new Term(SEARCH_FIELD, &quot;co&quot;));</span><br><span class="line"></span><br><span class="line">    resultPrint(searcher, prefixQuery);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private static void wildcardQueryDemo(IndexSearcher searcher) throws IOException &#123;</span><br><span class="line">    System.out.println(&quot;WildcardQuery, search terms &#39;har*&#39;&quot;);</span><br><span class="line">    WildcardQuery wildcardQuery &#x3D; new WildcardQuery(new Term(SEARCH_FIELD, &quot;har*&quot;));</span><br><span class="line"></span><br><span class="line">    resultPrint(searcher, wildcardQuery);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private static void regexpQueryDemo(IndexSearcher searcher) throws IOException &#123;</span><br><span class="line">    System.out.println(&quot;RegexpQuery, search regexp &#39;l[ao]*&#39;&quot;);</span><br><span class="line">    RegexpQuery regexpQuery &#x3D; new RegexpQuery(new Term(SEARCH_FIELD, &quot;l[ai].*&quot;));</span><br><span class="line"></span><br><span class="line">    resultPrint(searcher, regexpQuery);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 程序输出</span><br><span class="line">PrefixQuery, search terms begin with &#39;co&#39;</span><br><span class="line">2 result(s) matched: </span><br><span class="line">doc&#x3D;0 score&#x3D;1.0 file: data&#x2F;poems&#x2F;Fog.txt</span><br><span class="line">doc&#x3D;2 score&#x3D;1.0 file: data&#x2F;poems&#x2F;Love&#39;sSecret.txt</span><br><span class="line"></span><br><span class="line">WildcardQuery, search terms &#39;har*&#39;</span><br><span class="line">1 result(s) matched: </span><br><span class="line">doc&#x3D;0 score&#x3D;1.0 file: data&#x2F;poems&#x2F;Fog.txt</span><br><span class="line"></span><br><span class="line">RegexpQuery, search regexp &#39;l[ao]*&#39;</span><br><span class="line">2 result(s) matched: </span><br><span class="line">doc&#x3D;0 score&#x3D;1.0 file: data&#x2F;poems&#x2F;Fog.txt</span><br><span class="line">doc&#x3D;3 score&#x3D;1.0 file: data&#x2F;poems&#x2F;OnDeath.txt</span><br></pre></td></tr></table></figure>
<h2 id="FuzzyQuery"><a href="#FuzzyQuery" class="headerlink" title="FuzzyQuery"></a>FuzzyQuery</h2><p>FuzzyQuery和PhraseQuery一样，都是基于上面介绍的edit distance做匹配的，差异是在PhraseQuery中搜索词的是一个term序列，此时edit distance中定义的一个symbol就是一个词；而FuzzyQuery的搜索词就是一个term，所以它对应的edit distance中的symbol就是一个字符了。另外使用时还有几个注意点：</p>
<ul>
<li>PhraseQuery采用Levenshtein distance计算edit distance，即相邻symbol交换是2个slop，而FuzzyQuery默认使用Damerau–Levenshtein distance，所以相邻symbol交换是1个slop，但支持用户使用Levenshtein distance。</li>
<li>FuzzyQuery限制最大允许的edit distance为2（LevenshteinAutomata.MAXIMUM_SUPPORTED_DISTANCE值限定），因为对于更大的edit distance会匹配出特别多的词，但FuzzyQuery的定位是解决诸如美式英语和英式英语在拼写上的细微差异。</li>
<li>FuzzyQuery匹配的时候还有个要求就是搜索的term和待匹配的term的edit distance必须小于它们二者长度的最小值。比如搜索词为”abcd”，设定允许的maxEdits（允许的最大edit distance）为2，那么按照edit distance的计算方式”ab”这个词是匹配的，因为它们的距离是2，不大于设定的maxEdits。但是，由于 2 &lt; min( len(“abcd”), len(“ab”) ) = 2不成立，所以算不匹配。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private static void fuzzyQueryDemo(IndexSearcher searcher) throws IOException &#123;</span><br><span class="line">    System.out.println(&quot;FuzzyQuery, search &#39;remembre&#39;&quot;);</span><br><span class="line">    &#x2F;&#x2F; 这里把remember拼成了remembre</span><br><span class="line">    FuzzyQuery fuzzyQuery &#x3D; new FuzzyQuery(new Term(SEARCH_FIELD, &quot;remembre&quot;), 1);</span><br><span class="line"></span><br><span class="line">    resultPrint(searcher, fuzzyQuery);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 程序输出</span><br><span class="line">FuzzyQuery, search &#39;remembre&#39;</span><br><span class="line">1 result(s) matched: </span><br><span class="line">doc&#x3D;1 score&#x3D;0.4473783 file: data&#x2F;poems&#x2F;FreedomAndLove.txt</span><br></pre></td></tr></table></figure>
<h2 id="PointRangeQuery"><a href="#PointRangeQuery" class="headerlink" title="PointRangeQuery"></a>PointRangeQuery</h2><p>前面介绍Field的时候，我们介绍过几种常用的数值型Field：IntPoint、LongPoint、FloatPoint、DoublePoint。PointRangeQuery就是给数值型数据提供范围查询的一个Query，功能和原理都很简单，我们直接看一个完整的例子吧：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line"> * Point Query Demo.</span><br><span class="line"> *</span><br><span class="line"> * @author NiYanchun</span><br><span class="line"> **&#x2F;</span><br><span class="line">public class PointQueryDemo &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        &#x2F;&#x2F; 索引保存目录</span><br><span class="line">        final String indexPath &#x3D; &quot;indices&#x2F;point-index&quot;;</span><br><span class="line">        Directory indexDir &#x3D; FSDirectory.open(Paths.get(indexPath));</span><br><span class="line">        IndexWriterConfig iwc &#x3D; new IndexWriterConfig(new StandardAnalyzer());</span><br><span class="line">        iwc.setOpenMode(IndexWriterConfig.OpenMode.CREATE);</span><br><span class="line">        IndexWriter writer &#x3D; new IndexWriter(indexDir, iwc);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 向索引中插入10条document，每个document包含一个field字段，字段值是0~10之间的数字</span><br><span class="line">        for (int i &#x3D; 0; i &lt; 10; i++) &#123;</span><br><span class="line">            Document doc &#x3D; new Document();</span><br><span class="line">            Field pointField &#x3D; new IntPoint(&quot;field&quot;, i);</span><br><span class="line">            doc.add(pointField);</span><br><span class="line">            writer.addDocument(doc);</span><br><span class="line">        &#125;</span><br><span class="line">        writer.close();</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 查询</span><br><span class="line">        IndexReader indexReader &#x3D; DirectoryReader.open(FSDirectory.open(Paths.get(indexPath)));</span><br><span class="line">        IndexSearcher searcher &#x3D; new IndexSearcher(indexReader);</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 查询field字段值在[5, 8]范围内的文档</span><br><span class="line">        Query query &#x3D; IntPoint.newRangeQuery(&quot;field&quot;, 5, 8);</span><br><span class="line">        TopDocs topDocs &#x3D; searcher.search(query, 10);</span><br><span class="line"></span><br><span class="line">        if (topDocs.totalHits.value &#x3D;&#x3D; 0) &#123;</span><br><span class="line">            System.out.println(&quot;not found!&quot;);</span><br><span class="line">            return;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        ScoreDoc[] hits &#x3D; topDocs.scoreDocs;</span><br><span class="line"></span><br><span class="line">        System.out.println(topDocs.totalHits.value + &quot; result(s) matched: &quot;);</span><br><span class="line">        for (ScoreDoc hit : hits) &#123;</span><br><span class="line">            System.out.println(&quot;doc&#x3D;&quot; + hit.doc + &quot; score&#x3D;&quot; + hit.score);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 程序输出</span><br><span class="line">4 result(s) matched: </span><br><span class="line">doc&#x3D;5 score&#x3D;1.0</span><br><span class="line">doc&#x3D;6 score&#x3D;1.0</span><br><span class="line">doc&#x3D;7 score&#x3D;1.0</span><br><span class="line">doc&#x3D;8 score&#x3D;1.0</span><br></pre></td></tr></table></figure>
<h2 id="TermRangeQuery"><a href="#TermRangeQuery" class="headerlink" title="TermRangeQuery"></a>TermRangeQuery</h2><p>TermRangeQuery和PointRangeQuery功能类似，不过它比较的是字符串，而非数值。比较基于org.apache.lucene.util.BytesRef.compareTo(BytesRef other)方法。直接看例子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private static void termRangeQueryDemo(IndexSearcher searcher) throws IOException &#123;</span><br><span class="line">    System.out.println(&quot;TermRangeQuery, search term between &#39;loa&#39; and &#39;lov&#39;&quot;);</span><br><span class="line">    &#x2F;&#x2F; 后面的true和false分别表示 loa &lt;&#x3D; 待匹配的term &lt; lov</span><br><span class="line">    TermRangeQuery termRangeQuery &#x3D; new TermRangeQuery(SEARCH_FIELD, new BytesRef(&quot;loa&quot;), new BytesRef(&quot;lov&quot;), true, false);</span><br><span class="line"></span><br><span class="line">    resultPrint(searcher, termRangeQuery);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 程序输出</span><br><span class="line">TermRangeQuery, search term between &#39;loa&#39; and &#39;lov&#39;</span><br><span class="line">1 result(s) matched: </span><br><span class="line">doc&#x3D;0 score&#x3D;1.0 file: data&#x2F;poems&#x2F;Fog.txt    &#x2F;&#x2F; Fog中的term &#39;looking&#39; 符合搜索条件</span><br></pre></td></tr></table></figure>
<h2 id="ConstantScoreQuery"><a href="#ConstantScoreQuery" class="headerlink" title="ConstantScoreQuery"></a>ConstantScoreQuery</h2><p>ConstantScoreQuery很简单，它的功能是将其它查询包装起来，并将它们查询结果中的评分改为一个常量值（默认为1.0）。上面FuzzyQuery一节里面最后举得例子中返回的查询结果score=0.4473783，现在我们用ConstantScoreQuery包装一下看下效果：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private static void constantScoreQueryDemo(IndexSearcher searcher) throws IOException &#123;</span><br><span class="line">    System.out.println(&quot;ConstantScoreQuery:&quot;);</span><br><span class="line">    ConstantScoreQuery constantScoreQuery &#x3D; new ConstantScoreQuery(</span><br><span class="line">            new FuzzyQuery(new Term(SEARCH_FIELD, &quot;remembre&quot;), 1));</span><br><span class="line"></span><br><span class="line">    resultPrint(searcher, constantScoreQuery);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 运行结果</span><br><span class="line">ConstantScoreQuery:</span><br><span class="line">1 result(s) matched: </span><br><span class="line">doc&#x3D;1 score&#x3D;1.0 file: data&#x2F;poems&#x2F;FreedomAndLove.txt</span><br></pre></td></tr></table></figure>
<p>另外有个知识点需要注意：ConstantScoreQuery嵌套Filter和BooleanQuery嵌套Filter的查询结果不考虑评分的话是一样的，但前面在BooleanQuery中介绍过Filter，其功能与MUST相同，但不计算评分；而ConstantScoreQuery就是用来设置一个评分的。所以两者的查询结果是一样的，但ConstantScoreQuery嵌套Filter返回结果是附带评分的，而BooleanQuery嵌套Filter的返回结果是没有评分的（score字段的值为0）。</p>
<h2 id="MatchAllDocsQuery"><a href="#MatchAllDocsQuery" class="headerlink" title="MatchAllDocsQuery"></a>MatchAllDocsQuery</h2><p>这个查询很简单，就是匹配所有文档，用于没有特定查询条件，只想预览部分数据的场景。直接看例子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private static void matchAllDocsQueryDemo(IndexSearcher searcher) throws IOException &#123;</span><br><span class="line">    System.out.println(&quot;MatchAllDocsQueryDemo:&quot;);</span><br><span class="line">    MatchAllDocsQuery matchAllDocsQuery &#x3D; new MatchAllDocsQuery();</span><br><span class="line"></span><br><span class="line">    resultPrint(searcher, matchAllDocsQuery);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 程序输出</span><br><span class="line">MatchAllDocsQueryDemo:</span><br><span class="line">4 result(s) matched: </span><br><span class="line">doc&#x3D;0 score&#x3D;1.0 file: data&#x2F;poems&#x2F;Fog.txt</span><br><span class="line">doc&#x3D;1 score&#x3D;1.0 file: data&#x2F;poems&#x2F;FreedomAndLove.txt</span><br><span class="line">doc&#x3D;2 score&#x3D;1.0 file: data&#x2F;poems&#x2F;Love&#39;sSecret.txt</span><br><span class="line">doc&#x3D;3 score&#x3D;1.0 file: data&#x2F;poems&#x2F;OnDeath.txt</span><br></pre></td></tr></table></figure>
<p>想看更多资料，可参考：<a href="https://lucene.apache.org/core/8_2_0/core/org/apache/lucene/search/package-summary.html" target="_blank" rel="noopener">https://lucene.apache.org/core/8_2_0/core/org/apache/lucene/search/package-summary.html</a></p>
<h1 id="QueryParser"><a href="#QueryParser" class="headerlink" title="QueryParser"></a>QueryParser</h1><p>QueryParser定义了一些查询语法，通过这些语法几乎可以实现前文介绍的所有Query API提供的功能，但它的存在并不是为了替换那些API，而是用在一些交互式场景中。比如下面代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class SearchFilesMinimal &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        &#x2F;&#x2F; 索引保存目录</span><br><span class="line">        final String indexPath &#x3D; &quot;&#x2F;Users&#x2F;allan&#x2F;Git&#x2F;allan&#x2F;github&#x2F;CodeSnippet&#x2F;Java&#x2F;lucene-learning&#x2F;indices&#x2F;poems-index&quot;;</span><br><span class="line">        &#x2F;&#x2F; 搜索的字段</span><br><span class="line">        final String searchField &#x3D; &quot;contents&quot;;</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 从索引目录读取索引信息</span><br><span class="line">        IndexReader indexReader &#x3D; DirectoryReader.open(FSDirectory.open(Paths.get(indexPath)));</span><br><span class="line">        &#x2F;&#x2F; 创建索引查询对象</span><br><span class="line">        IndexSearcher searcher &#x3D; new IndexSearcher(indexReader);</span><br><span class="line">        &#x2F;&#x2F; 使用标准分词器</span><br><span class="line">        Analyzer analyzer &#x3D; new StandardAnalyzer();</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 从终端获取查询语句</span><br><span class="line">        BufferedReader in &#x3D; new BufferedReader(new InputStreamReader(System.in));</span><br><span class="line">        &#x2F;&#x2F; 创建查询语句解析对象</span><br><span class="line">        QueryParser queryParser &#x3D; new QueryParser(searchField, analyzer);</span><br><span class="line">        while (true) &#123;</span><br><span class="line">            System.out.println(&quot;Enter query: &quot;);</span><br><span class="line"></span><br><span class="line">            String input &#x3D; in.readLine();</span><br><span class="line">            if (input &#x3D;&#x3D; null) &#123;</span><br><span class="line">                break;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            input &#x3D; input.trim();</span><br><span class="line">            if (input.length() &#x3D;&#x3D; 0) &#123;</span><br><span class="line">                break;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F; 解析用户输入的查询语句：build query</span><br><span class="line">            Query query &#x3D; queryParser.parse(input);</span><br><span class="line">            System.out.println(&quot;searching for: &quot; + query.toString(searchField));</span><br><span class="line">            &#x2F;&#x2F; 查询</span><br><span class="line">            TopDocs results &#x3D; searcher.search(query, 10);</span><br><span class="line">            &#x2F;&#x2F; 省略后面查询结果打印的代码</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在这段代码中，先读取了已经创建好的索引文件，然后创建了一个QueryParser实例(queryParser)。接着不断读取用户输入(input)，并传给QueryParser的parse方法，该方法通过用户的输入构建一个Query对象用于查询。<br>QueryParser的构造函数为QueryParser(String f, Analyzer a)，第1个参数指定一个默认的查询字段，如果后面输入的<em>input</em>里面没有指定查询字段，则默认查询该该字段，比如输入hello表示在默认字段中查询”<em>hello</em>“，而content: hello则表示在<em>content</em>字段中查询”<em>hello</em>“。第2个参数指定一个分析器，一般该分析器应该选择和索引阶段同样的Analyzer。</p>
<p><strong>另外有两个点需要特别注意：</strong></p>
<ul>
<li><strong>QueryParser默认使用TermQuery进行多个Term的OR关系查询</strong>（后文布尔查询那里会再介绍）。比如输入hello world，表示先将hello world分词（一般会分为hello和world两个词），然后使用TermQuery查询。如果需要全词匹配（即使用PhraseQuery），则需要将搜索词用<strong>双引号</strong>引起来，比如”hello world”。</li>
<li><strong>指定搜索字段时，该字段仅对紧随其后的第一个词或第一个用双引号引起来的串有效</strong>。比如title:hello world这个输入，<em>title</em>仅对<em>hello</em>有效，即搜索时只会在<em>title</em>字段中搜索<em>hello</em>，然后在默认搜索字段中搜索<em>world</em>。如果想要在一个字段中搜索多个词或多个用双引号引起来的词组时，将这些词用小括号括起来即可，比如title:(hello world)。</li>
</ul>
<h3 id="Wildcard搜索"><a href="#Wildcard搜索" class="headerlink" title="Wildcard搜索"></a>Wildcard搜索</h3><p>通配符搜索和WildcardQuery API一样，仅支持?和<em>两个通配符，前者用于匹配1个字符，后者匹配0到多个字符。输入title:te?t，则可以匹配到</em>title<em>中的”</em>test<em>“、”</em>text*”等词。</p>
<p><strong>注意：使用QueryParser中的wildcard搜索时，不允许以?和*开头，否则会抛异常，但直接使用WildcardQuery API时，允许以通配符开头，只是因为性能原因，不推荐使用。</strong>这样设计的原因我猜是因为QueryParser的输入是面向用户的，用户对于通配符开头造成的后果并不清楚，所以直接禁掉；而WildcardQuery是给开发者使用的，开发者在开发阶段很清楚如果允许这样做造成的后果是否可以接受，如果不能接受，也是可以通过接口禁掉开头就是用通配符的情况。</p>
<h3 id="Regexp搜索"><a href="#Regexp搜索" class="headerlink" title="Regexp搜索"></a>Regexp搜索</h3><p>正则搜索和RegexpQuery一样，不同之处在于QueryParser中输入的正则表达式需要使用<strong>两个斜线</strong>(“/“)包围起来，比如匹配”moat”或”boat”的正则为/[mb]oat/。</p>
<h3 id="Fuzzy搜索"><a href="#Fuzzy搜索" class="headerlink" title="Fuzzy搜索"></a>Fuzzy搜索</h3><p>在QueryParser中，通过在搜索词后面加<strong>波浪字符</strong>来实现FuzzyQuery，比如love~，默认edit distance是2，可以在波浪符后面加具体的整数值来修改默认值，合法的值为0、1、2.</p>
<h3 id="Phrase-slop搜索"><a href="#Phrase-slop搜索" class="headerlink" title="Phrase slop搜索"></a>Phrase slop搜索</h3><p>PhraseQuery中可以指定slop（默认值为0，精确匹配）来实现相似性搜索，QueryParser中同样可以，使用方法与Fuzzy类似——<strong>将搜索字符串用双引号引起来，然后在末尾加上波浪符</strong>，比如”jakarta apache”~10。这里对数edit distance没有限制，合法值为非负数，默认值为0.</p>
<h3 id="Range搜索"><a href="#Range搜索" class="headerlink" title="Range搜索"></a>Range搜索</h3><p>QueryParser的范围搜索同时支持TermRangeQuery和数值型的范围搜索，排序使用的是<strong>字典序</strong>。<strong>开区间使用大括号，闭区间使用方括号</strong>。比如搜索修改日期介于2019年9月份和10月份的文档：mod_date:[20190901 TO 20191031]，再比如搜索标题字段中包含<em>hate</em>到<em>love</em>的词（但不包含这两个词）的文档：title:{hate TO love}.</p>
<h3 id="提升权重-boost"><a href="#提升权重-boost" class="headerlink" title="提升权重(boost)"></a>提升权重(boost)</h3><p>查询时可以通过给搜索的关键字或双引号引起来的搜索串后面添加脱字符(^)及一个正数来提升其计算相关性时的权重（默认为1），比如love^5 China或”love China”^0.3。</p>
<h3 id="Boolean操作符"><a href="#Boolean操作符" class="headerlink" title="Boolean操作符"></a>Boolean操作符</h3><p>QueryParser中提供了5种布尔操作符：AND、+、OR、NOT、-，所有的<strong>操作符必须大写</strong>。</p>
<ul>
<li><strong>OR是默认的操作符</strong>，表示满足任意一个term即可。比如搜索love China，<em>love</em>和<em>China</em>之间就是OR的关系，检索时文档匹配任意一个词即视为匹配。OR也可以使用可用||代替。</li>
<li>AND表示必须满足<strong>所有term</strong>才可以，可以使用&amp;&amp;代替。</li>
<li>+用在term之前，表示该term必须存在。比如+love China表示匹配文档中必须包含<em>love</em>，<em>China</em>则可包含也可不含。</li>
<li>-用在term之前，表示该term必须不存在。比如-“hate China” “love China”表示匹配文档中包含”<em>love China</em>“，但不包含”<em>hate China</em>“的词。</li>
</ul>
<h3 id="分组"><a href="#分组" class="headerlink" title="分组"></a>分组</h3><p>前面已经介绍过，可以使用<strong>小括号</strong>进行分组，通过分组可以表达一些复杂的逻辑。举两个例子：</p>
<ul>
<li>(jakarta OR apache) AND website表示匹配文档中必须包含<em>webiste</em>，同时需要至少包含<em>jakarta</em>或<em>apache</em>二者之一。</li>
<li>title:(+return +”pink panther”)表示匹配文档中的title字段中必须同时存在<em>return</em>和<em>“pink panther”</em>串。</li>
</ul>
<h3 id="特殊字符"><a href="#特殊字符" class="headerlink" title="特殊字符"></a>特殊字符</h3><p>从前面的介绍可知，有很多符号在QueryParser中具有特殊含义，目前所有的特殊符号包括：+- &amp;&amp; || ! ( ) { } [ ] ^ “ ~ <em> ? :  /。如果搜索关键字中存在这些特殊符号，则需要使用反斜线()转义。比如搜索(1+1)</em>2则必须写为(1+1)*2。</p>
<p>相比于Lucene的其它搜索API，QueryParser提供了一种方式，让普通用户可以不需要写代码，只是掌握一些语法就可以进行复杂的搜索，在一些交互式检索场景中，还是非常方便的。</p>
<h3 id="基本使用-2"><a href="#基本使用-2" class="headerlink" title="基本使用"></a>基本使用</h3><p>下面展示一个使用pylucene构建一个基于term和关键词的Query，这里把keyword命中进行了加权，使用分数和长度进行排序，并将结果写进result中的过程：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">boolean_query &#x3D; BooleanQuery.Builder()</span><br><span class="line">simple_query &#x3D; QueryParser(</span><br><span class="line">    &quot;en_tokenized&quot;,</span><br><span class="line">    self.lucene_analyzer).parse(query)</span><br><span class="line">keyword_query &#x3D; QueryParser(</span><br><span class="line">    &quot;keyword&quot;,</span><br><span class="line">    self.lucene_analyzer).parse(query)</span><br><span class="line">boost_keyword_query &#x3D; BoostQuery(keyword_query, 2.0)</span><br><span class="line">boolean_query.add(simple_query, BooleanClause.Occur.SHOULD)</span><br><span class="line">boolean_query.add(boost_keyword_query, BooleanClause.Occur.SHOULD)</span><br><span class="line"># searcher</span><br><span class="line">lucene_searcher &#x3D; IndexSearcher(</span><br><span class="line">    DirectoryReader.open(self.indir))</span><br><span class="line">sorter &#x3D; Sort([</span><br><span class="line">    SortField.FIELD_SCORE,</span><br><span class="line">    SortField(&#39;origin_score&#39;, SortField.Type.FLOAT, True),</span><br><span class="line">    SortField(&#39;en_sent_lenth&#39;, SortField.Type.INT, True)])</span><br><span class="line"></span><br><span class="line"># rerank</span><br><span class="line">collector &#x3D; TopFieldCollector.create(sorter, self.maxrecal, self.maxrecal)</span><br><span class="line">lucene_searcher.search(boolean_query.build(), collector)</span><br><span class="line">scoreDocs &#x3D; collector.topDocs().scoreDocs</span><br><span class="line">result &#x3D; []</span><br><span class="line">for hit in scoreDocs:</span><br><span class="line">    doc &#x3D; lucene_searcher.doc(hit.doc)</span><br><span class="line">    result.append(...)</span><br></pre></td></tr></table></figure>
<h1 id="相似度评分机制"><a href="#相似度评分机制" class="headerlink" title="相似度评分机制"></a>相似度评分机制</h1><h2 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h2><h3 id="Bad-of-Words模型"><a href="#Bad-of-Words模型" class="headerlink" title="Bad-of-Words模型"></a>Bad-of-Words模型</h3><p>先介绍一下NLP和IR领域里面非常简单且使用极其广泛的bag-fo-words model，即词袋模型。假设有这么一句话：<em>“John likes to watch movies. Mary likes movies too.”</em>。那这句话用JSON格式的词袋模型表示的话就是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">BoW &#x3D; &#123;&quot;John&quot;:1,&quot;likes&quot;:2,&quot;to&quot;:1,&quot;watch&quot;:1,&quot;movies&quot;:2,&quot;Mary&quot;:1,&quot;too&quot;:1&#125;;</span><br></pre></td></tr></table></figure>
<p>可以看到，词袋模型关注的是词的出现次数，而没有记录词的位置信息。所以不同的语句甚至相反含义的语句其词袋可能是一样的，比如<em>“Mary is quicker than John”</em>和<em>“John is quicker than Mary”</em>这两句话，其词袋是一样的，但含义是完全相反的。所以凡是完全基于词袋模型的一些算法一般也存在这样该问题。</p>
<h3 id="Term-frequency"><a href="#Term-frequency" class="headerlink" title="Term frequency"></a>Term frequency</h3><p>词频就是一个词（term）在一个文档中（document）出现的次数（frequency），记为tf_{t,d}。这是一种最简单的定义方式，实际使用中还有一些变种：</p>
<ul>
<li>布尔词频：如果词在文档中出现，则tf_{t,d}=1，否则为0。</li>
<li>根据文档长短做调整的词频：tf_{t,d}/lenth，其中length为文档中的总词数。</li>
<li>对数词频：log(1+tf_{t,d})，加1是防止对0求对数（0没有对数）。 一般选取常用对数或者自然对数。</li>
</ul>
<p>词频的优点是简单，但缺点也很显然：</p>
<ol>
<li>词频中没有包含词的位置信息，所以从词频的角度来看，<em>“Mary is quicker than John”</em>和<em>“John is quicker than Mary”</em>两条文档是完全一致的，但显然它们的含义是完全相反的。</li>
<li>词频没有考虑不同词的重要性一般是不一样的，比如停用词的词频都很高，但它们并不重要。</li>
</ol>
<h3 id="Inverse-document-frequency"><a href="#Inverse-document-frequency" class="headerlink" title="Inverse document frequency"></a>Inverse document frequency</h3><p>一个词的逆文档频率用于衡量该词提供了多少信息，计算方式定义如下：$i d f_{t}=\log \frac{N}{d f_{t}}=-\log \frac{d f_{t}}{N}$</p>
<p>其中，t代表term，D代表文档，N代表语料库中文档总数，df_t代表语料库中包含t的文档的数据，即<strong>文档频率</strong>（document frequency）。如果语料库中不包含t，那df_t就等于0，为了避免除零操作，可以采用后面的公式，将df_t作为分子，也有的变种给df_t加了1。</p>
<p>对于固定的语料库，N是固定的，一个词的df_t越大，其idf(t,D)</p>
<p> 就越小。所以那些很稀少的词的idf值会很高，而像停用词这种出现频率很高的词idf值很低。</p>
<h3 id="TF-IDF-Model"><a href="#TF-IDF-Model" class="headerlink" title="TF-IDF Model"></a>TF-IDF Model</h3><p>TF-IDF就是将TF和IDF结合起来，其实就是简单的相乘：$t f i d f(t, d)=t f_{t, d} \cdot i d f_{t}$。从公式可以分析出来，一个词t在某个文档d中的tf-idf值：</p>
<ul>
<li>当该词在<strong>少数文档</strong>中<strong>出现很多次</strong>的时候，其值接近最大值；（<em>场景1</em>）</li>
<li>当该词在文档中出现次数少或者在很多文档中都出现时，其值较小；（<em>场景2</em>）</li>
<li>当该词几乎在所有文档中都出现时，其值接近最小值。（<em>场景3</em>）</li>
</ul>
<p>下面用一个例子来实战一下，还是以文中的4首英文短诗中的前3首为例。假设这3首诗组成了我们的语料库，每首诗就是一个文档（doc1：<em>Fog</em>、doc2：<em>Freedom And Love</em>、doc3：<em>Love’s Secret</em>），诗里面的每个单词就是一个个词（我们把标题也包含在里面）。然后我们选取<em>“the”、 “freedom”、”love”</em>三个词来分别计算它们在每个文档的TF-IDF，计算中使用自然对数形式。</p>
<ul>
<li>“<em>the</em>“在doc1中出现了1次，在doc2中出现了2次，在doc3中出现了1次，整个语料库有3个文档，包含”the”的文档也是3个。所以：</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/14.png" alt="图片"></p>
<ul>
<li>“<em>freedom</em>“在doc1中出现了0次，在doc2中出现了1次，在doc3中出现了0次，语料库中包含”freedom”的文档只有1个。所以：</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/15.png" alt="图片"></p>
<ul>
<li>“<em>love</em>“在doc1中现了0次，在doc2中出现了3次，在doc3中出现了5次，整个语料库有3个文档，包含”love”的文档有2个。所以：</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/16.png" alt="图片"></p>
<p>我们简单分析一下结果：”<em>the</em>“在所有文档中都出现了，所以其tf-idf值最低，为0，验证了上面公式分析中的场景3；”<em>freedom</em>“只有在第2个文档中出现了，所以其它两个的tf-idf值为0，表示不包含该词；”<em>love</em>“在第2、3个文档中都出现了，但在第3个文档中出现的频率更高，所以其tf-idf值最高。所以tf-idf算法的结果还是能很好的表示实际结果的。</p>
<h2 id="Vector-Space-Model"><a href="#Vector-Space-Model" class="headerlink" title="Vector Space Model"></a>Vector Space Model</h2><p>通过TF-IDF算法，我们可以计算出每个词在语料库中的权重，而通过VSM（Vector Space Model），则可以计算两个文档的相似度。</p>
<p>假设有两个文档：</p>
<ul>
<li>文档1：”Jack Ma regrets setting up Alibaba.”</li>
<li>文档2：”Richard Liu does not know he has a beautiful wife.”</li>
</ul>
<p>这是原始的文档，然后通过词袋模型转化后为：</p>
<ul>
<li>BoW1 = {“jack”:1, “ma”:1, “regret”:1, “set”:1, “up”:1, “alibaba”:1}</li>
<li>BoW2 = {“richard”:1, “liu”:1, “does”:1, “not”:1, “know”:1, “he”:1, “has”:1, “a”: 1, “beautiful”:1, “wife”:1}</li>
</ul>
<p>接着，分别用TF-IDF算法计算每个文档词袋中每个词的tf-idf值（值是随便写的，仅供原理说明）：</p>
<ul>
<li>tf-idf_doc1 = { 0.41, 0.12, 0.76, 0.83, 0.21, 0.47 }</li>
<li>tf-idf_doc2 = { 0.12, 0.25, 0.67, 0.98, 0.43, 0.76, 0.89, 0.51, 0.19, 0.37 }</li>
</ul>
<p>如果将上面的tf-idf_doc1和tf-idf_doc2看成是2个向量，那我们就通过上面的方式将原始的文档转换成了向量，这个向量就是VSM中的Vector。在VSM中，一个Vector就代表一个文档，记为V(q)，Vector中的每个值就是原来文档中term的权重（这个权重一般使用tf-idf计算，也可以通过其他方式计算）。这样语料库中的很多文档就会产生很多的向量，这些向量一起构成了一个向量空间，也就是Vector Space。</p>
<p>假设有一个查询语句为”Jack Alibaba”，我们可以用同样的方式将其转化一个向量，假设这个向量叫查询向量V(q)。<strong>这样在语料库中检索和 q相近文档的问题就转换成求语料库中每个向量V(d)与</strong>V(q)<strong>的相似度问题了</strong>。而衡量两个向量相似度最常用的方法就是余弦相似度，用公式表示就是：<script type="math/tex">\operatorname{cosineSimilarity(q,d)}=\frac{V(q) \cdot V(d)}{|V(q)||V(d)|}=v(q) \cdot v(d)</script>，这个就是Vector Space Model。</p>
<h2 id="TfidfSimilarity"><a href="#TfidfSimilarity" class="headerlink" title="TfidfSimilarity"></a>TfidfSimilarity</h2><blockquote>
<p>参考：<br><a href="https://en.wikipedia.org/wiki/Boolean_model_of_information_retrieval" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Boolean_model_of_information_retrieval</a></p>
</blockquote>
<p>Lucene使用<a href="http://en.wikipedia.org/wiki/Standard_Boolean_model" target="_blank" rel="noopener">Boolean model (BM) of Information Retrieval</a>模型来计算一个文档是否和搜索词匹配，对于匹配的文档使用基于VSM的评分算法来计算得分。具体的实现类是org.apache.lucene.search.similarities.TFIDFSimilarity，但做了一些修正。本文不讨论BM算法，只介绍评分算法。TFIDFSimilarity采用的评分公式如下：<script type="math/tex">\operatorname{Score}(q, d)=\sum_{t \in q}\left(t f_{t, d} \cdot i d f_{t}^{2} \cdot t . \text { get } \operatorname{Boost}() \cdot \text { norm }(t, d)\right)</script>，我们从外到内剖析一下这个公式：</p>
<ul>
<li>最外层的累加。搜索语句一般是由多个词组成的，比如”Jack Alibaba”就是有”Jack”和”Alibaba”两个词组成。计算搜索语句和每个匹配文档的得分的时候就是计算搜索语句中每个词和匹配文档的得分，然后累加起来就是搜索语句和该匹配文档的得分。这就是最外层的累加。</li>
<li>t.getBoost()：之前的系列文章中介绍过，在查询或者索引阶段我们可以人为设定某些term的权重，t.getBoost()获取的就是这个阶段设置的权重。所以查询或索引阶段设置的权重也就是在这个时候起作用的。</li>
<li>norm(t, d)：之前的系列文章中也介绍过，查询的时候一个文档的长短也是会影响词的重要性，匹配次数一样的情况下，越长的文档评分越低。这个也好理解，比如我们搜”Alibaba”，有两个文档里面都出现了一次该词，但其中一个文档总共包含100万个词，而另外一个只包含10个词，很显然，极大多数情况下，后者与搜索词的相关度是比前者高的。实际计算的时候使用的公式如下：<script type="math/tex">\operatorname{norm}(t, d)=\frac{1}{\sqrt{\operatorname{length}}}</script>，length是文档d的长度。</li>
<li>计算<script type="math/tex">t f_{t, d} \cdot i d f_{t}^{2}</script>：Lucene假设一个词在搜索语句中的词频为1（即使出现多次也不影响，就是重复计算多次而已），所以可以把这个公式拆开写：<script type="math/tex">t f_{t, d} \cdot i d f_{t}^{2}=t f_{t, d} \cdot i d f_{t} \cdot 1 \cdot i d f_{t}=\left(t f_{t, d} \cdot i d f_{t}\right) \cdot\left(t f_{t, q} \cdot i d f_{t}\right)</script>，这里的<script type="math/tex">\left(t f_{t, d} \cdot i d f_{t}\right) \cdot\left(t f_{t, q} \cdot i d f_{t}\right)</script>就对应上面的<script type="math/tex">-v(d) \cdot v(q)</script>!</li>
<li>在Lucene中，采用的TF计算公式为：<script type="math/tex">t f_{t, d}=\sqrt{\text {frequency}}</script>，IDF计算公式为：<script type="math/tex">i d f_{t}=1+\log \frac{N+1}{d f_{t}+1}</script></li>
</ul>
<p>其实TFIDFSimilarity是一个抽象类，真正实现上述相似度计算的是org.apache.lucene.search.similarities.ClassicSimilarity类，上面列举的公式在其对应的方法中也可以找到。除了基于TFIDF这种方式外，Lucene还支持另外一种相似度算法BM25，并且从6.0.0版本开始，BM25已经替代ClassicSimilarity，作为默认的评分算法。</p>
<h2 id="BM25Similarity"><a href="#BM25Similarity" class="headerlink" title="BM25Similarity"></a>BM25Similarity</h2><p>BM25全称“Best Match 25”，其中“25”是指现在BM25中的计算公式是第25次迭代优化。该算法是几位大牛在1994年TREC-3（Third <strong>T</strong>ext <strong>RE</strong>trieval <strong>C</strong>onference）会议上提出的，它将文本相似度问题转化为概率模型，可以看做是TF-IDF的改良版，我们看下它是如何进行改良的。</p>
<h3 id="对IDF的改良"><a href="#对IDF的改良" class="headerlink" title="对IDF的改良"></a>对IDF的改良</h3><p>BM25中的IDF公式为：<script type="math/tex">i d f_{t}^{B M 25}=\log \left(1+\frac{N-d f_{t}+0.5}{d f_{t}+0.5}\right)</script>。原版BM25的log中是没有加1的，Lucene为了防止产生负值，做了一点小优化。虽然对公式进行了更改，但其实和原来的公式没有实质性的差异，下面是新旧函数曲线对比：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/10.png" alt="图片"></p>
<h3 id="对TF的改良1"><a href="#对TF的改良1" class="headerlink" title="对TF的改良1"></a>对TF的改良1</h3><p>BM25中TF的公式为：<script type="math/tex">t f_{t, d}^{B M 25}=((k+1) * t f) /(k+t f)</script>，其中tf是传统的词频值。先来看下改良前后的函数曲线对比吧（下图中k=1.2）：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/11.png" alt="图片"></p>
<p>可以看到，传统的tf计算公式中，词频越高，tf值就越大，没有上限。但BM中的tf，随着词频的增长，tf值会无限逼近(k+1)，相当于是有上限的。这就是二者的区别。一般 k</p>
<p>k取 1.2，Lucene中也使用1.2作为k的默认值。</p>
<h3 id="对TF的改良2"><a href="#对TF的改良2" class="headerlink" title="对TF的改良2"></a>对TF的改良2</h3><p>在传统的计算公式中，还有一个norm。BM25将这个因素加到了TF的计算公式中，结合了norm因素的BM25中的TF计算公式为：<script type="math/tex">t f_{t, d}^{B M 25}=((k+1) * t f) /(k *(1.0-b+b * L)+t f)</script>，和之前相比，就是给分母上面的k加了一个乘数(1.0 - b + b * L)，其中的L的计算公式为：$L=|d|/avgDl$，其中，|d|是当前文档的长度，avgDl是语料库中所有文档的平均长度。b是一个常数，用来控制L对最总评分影响的大小，一般取0~1之间的数（取0则代表完全忽略L）。Lucene中b的默认值为 0.75.</p>
<p>通过这些细节上的改良，BM25在很多实际场景中的表现都优于传统的TF-IDF，所以从Lucene 6.0.0版本开始，上位成为默认的相似度评分算法。</p>
<h1 id="代码实践"><a href="#代码实践" class="headerlink" title="代码实践"></a>代码实践</h1><h2 id="Java"><a href="#Java" class="headerlink" title="Java"></a>Java</h2><p>原始数据为4首英文短诗，每个诗对应一个文件，文件名为诗名。这里列出内容，方便后面讨论。</p>
<ul>
<li>Fog（迷雾）：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">The fog comes</span><br><span class="line">on little cat feet.</span><br><span class="line">It sits looking over harbor and city</span><br><span class="line">on silent haunches</span><br><span class="line">and then, moves on.</span><br></pre></td></tr></table></figure>
<ul>
<li>Freedom And Love（自由与爱情）：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">How delicious is the winning</span><br><span class="line">Of a kiss at loves beginning,</span><br><span class="line">When two mutual hearts are sighing</span><br><span class="line">For the knot there&#39;s no untying.</span><br><span class="line">Yet remember, &#39;mist your wooing,</span><br><span class="line">Love is bliss, but love has ruining;</span><br><span class="line">Other smiles may make you fickle,</span><br><span class="line">Tears for charm may tickle.</span><br></pre></td></tr></table></figure>
<ul>
<li>Love’s Secret（爱情的秘密）：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Never seek to tell thy love,</span><br><span class="line">Love that never told shall be;</span><br><span class="line">For the gentle wind does move</span><br><span class="line">Silently, invisibly.</span><br><span class="line">I told my love, I told my love,</span><br><span class="line">I told her all my heart,</span><br><span class="line">Trembling, cold, in ghastly fears.</span><br><span class="line">Ah! she did depart!</span><br><span class="line">Soon after she was gone from me,</span><br><span class="line">A traveller came by,</span><br><span class="line">Silently, invisibly:</span><br><span class="line">He took her with a sigh.</span><br></pre></td></tr></table></figure>
<ul>
<li>On Death（死亡）：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Death stands above me, whispering low</span><br><span class="line">I know not what into my ear:</span><br><span class="line">Of his strange language all I know</span><br><span class="line">Is, there is not a word of fear.</span><br></pre></td></tr></table></figure>
<p>因为原始数据已经是文本格式了，所以我们构建索引的流程如下：<br><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/12.png" alt="图片"></p>
<p>其中的<strong>分析</strong>就是我们之前说的分词。然后先看代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 省略包等信息，完整文件见源文件</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * Minimal Index Files code.</span><br><span class="line"> **&#x2F;</span><br><span class="line">public class IndexFilesMinimal &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        &#x2F;&#x2F; 原数据存放路径</span><br><span class="line">        final String docsPath &#x3D; &quot;&#x2F;Users&#x2F;allan&#x2F;Git&#x2F;allan&#x2F;github&#x2F;CodeSnippet&#x2F;Java&#x2F;lucene-learning&#x2F;data&#x2F;poems&quot;;</span><br><span class="line">        &#x2F;&#x2F; 索引保存目录</span><br><span class="line">        final String indexPath &#x3D; &quot;&#x2F;Users&#x2F;allan&#x2F;Git&#x2F;allan&#x2F;github&#x2F;CodeSnippet&#x2F;Java&#x2F;lucene-learning&#x2F;indices&#x2F;poems-index&quot;;</span><br><span class="line"></span><br><span class="line">        final Path docDir &#x3D; Paths.get(docsPath);</span><br><span class="line">        Directory indexDir &#x3D; FSDirectory.open(Paths.get(indexPath));</span><br><span class="line">        &#x2F;&#x2F; 使用标准分析器</span><br><span class="line">        Analyzer analyzer &#x3D; new StandardAnalyzer();</span><br><span class="line"></span><br><span class="line">        IndexWriterConfig iwc &#x3D; new IndexWriterConfig(analyzer);</span><br><span class="line">        &#x2F;&#x2F; 每次都重新创建索引</span><br><span class="line">        iwc.setOpenMode(IndexWriterConfig.OpenMode.CREATE);</span><br><span class="line">        &#x2F;&#x2F; 创建IndexWriter用于写索引</span><br><span class="line">        IndexWriter writer &#x3D; new IndexWriter(indexDir, iwc);</span><br><span class="line"></span><br><span class="line">        System.out.println(&quot;index start...&quot;);</span><br><span class="line">        &#x2F;&#x2F; 遍历数据目录，对目录下的每个文件进行索引</span><br><span class="line">        Files.walkFileTree(docDir, new SimpleFileVisitor&lt;Path&gt;() &#123;</span><br><span class="line">            @Override</span><br><span class="line">            public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException &#123;</span><br><span class="line">                indexDoc(writer, file);</span><br><span class="line">                return FileVisitResult.CONTINUE;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        writer.close();</span><br><span class="line"></span><br><span class="line">        System.out.println(&quot;index ends.&quot;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private static void indexDoc(IndexWriter writer, Path file) throws IOException &#123;</span><br><span class="line">        try (InputStream stream &#x3D; Files.newInputStream(file)) &#123;</span><br><span class="line">            System.out.println(&quot;indexing file &quot; + file);</span><br><span class="line">            &#x2F;&#x2F; 创建文档对象</span><br><span class="line">            Document doc &#x3D; new Document();</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F; 将文件绝对路径加入到文档中</span><br><span class="line">            Field pathField &#x3D; new StringField(&quot;path&quot;, file.toString(), Field.Store.YES);</span><br><span class="line">            doc.add(pathField);</span><br><span class="line">            &#x2F;&#x2F; 将文件内容加到文档中</span><br><span class="line">            Field contentsField &#x3D; new TextField(&quot;contents&quot;, new BufferedReader(new InputStreamReader(stream)));</span><br><span class="line">            doc.add(contentsField);</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F; 将文档写入索引中</span><br><span class="line">            writer.addDocument(doc);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这段代码的功能是遍历4首诗对应的文件，对其进行分词、索引，最终形成索引文件，供以后检索。里面有几个API比较关键，这里稍作一下介绍：</p>
<ul>
<li><em>FSDirectory</em>：该类实现了索引文件存储到文件系统的功能。我们无需关注底层文件系统的类型，该类会帮我们处理好。当然还有其它几个类型的Directory，以后再介绍。</li>
<li><em>StandardAnalyzer</em>：Lucene内置的标准分词器，其分词的方法是去掉停用词（stop word），全部转化为小写，根据空白字符分成一个个词/词组。Lucene还支持好几种其它分词器，我们也可以实现自己的分词器，以后再介绍。</li>
<li><em>IndexWriter</em>：该类是索引（<em>此处为动词</em>）文件的核心类，负责索引的创建和维护。</li>
</ul>
<p>我们可以这样理解Lucene里面的组织形式：索引（Index）是最顶级的概念，可以理解为MySQL里面的表；索引里面包含很多个Document，一个Document可以理解为MySQL中的一行记录；一个Document里面可以包含很多个Field，每一个Field都是一个类似Map的结构，由字段名和字段内容组成，内容可再嵌套。在MySQL中，表结构是确定的，每一行记录的格式都是一样的，但Lucene没有这个要求，每个Document里面的字段可以完全不一样，即所谓的”<em>flexible schema</em>“。</p>
<p>在上述代码运行完之后，我们就生成了一个名叫<strong>poems-index</strong>的索引，该索引里面包含4个Document，每个Document对应一首短诗。每个Document由<em>path</em>和<em>contents</em>两个字段组成，<em>path</em>里面存储的是诗歌文件的绝对路径，<em>contents</em>里面存储的是诗歌的内容。最终生成的索引目录包含如下一些文件：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lucene/13png.png" alt="图片"></p>
<p>这样后台索引构建的工作就算完成了，接下来我们来看一下如何利用索引进行高效的搜索：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 省略包等信息，完整文件见源文件</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * Minimal Search Files code</span><br><span class="line"> **&#x2F;</span><br><span class="line">public class SearchFilesMinimal &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        &#x2F;&#x2F; 索引保存目录</span><br><span class="line">        final String indexPath &#x3D; &quot;&#x2F;Users&#x2F;allan&#x2F;Git&#x2F;allan&#x2F;github&#x2F;CodeSnippet&#x2F;Java&#x2F;lucene-learning&#x2F;indices&#x2F;poems-index&quot;;</span><br><span class="line">        &#x2F;&#x2F; 搜索的字段</span><br><span class="line">        final String searchField &#x3D; &quot;contents&quot;;</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 从索引目录读取索引信息</span><br><span class="line">        IndexReader indexReader &#x3D; DirectoryReader.open(FSDirectory.open(Paths.get(indexPath)));</span><br><span class="line">        &#x2F;&#x2F; 创建索引查询对象</span><br><span class="line">        IndexSearcher searcher &#x3D; new IndexSearcher(indexReader);</span><br><span class="line">        &#x2F;&#x2F; 使用标准分词器</span><br><span class="line">        Analyzer analyzer &#x3D; new StandardAnalyzer();</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 从终端获取查询语句</span><br><span class="line">        BufferedReader in &#x3D; new BufferedReader(new InputStreamReader(System.in));</span><br><span class="line">        &#x2F;&#x2F; 创建查询语句解析对象</span><br><span class="line">        QueryParser queryParser &#x3D; new QueryParser(searchField, analyzer);</span><br><span class="line">        while (true) &#123;</span><br><span class="line">            System.out.println(&quot;Enter query: &quot;);</span><br><span class="line"></span><br><span class="line">            String input &#x3D; in.readLine();</span><br><span class="line">            if (input &#x3D;&#x3D; null) &#123;</span><br><span class="line">                break;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            input &#x3D; input.trim();</span><br><span class="line">            if (input.length() &#x3D;&#x3D; 0) &#123;</span><br><span class="line">                break;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F; 解析用户输入的查询语句：build query</span><br><span class="line">            Query query &#x3D; queryParser.parse(input);</span><br><span class="line">            System.out.println(&quot;searching for: &quot; + query.toString(searchField));</span><br><span class="line">            &#x2F;&#x2F; 查询</span><br><span class="line">            TopDocs results &#x3D; searcher.search(query, 10);</span><br><span class="line">            ScoreDoc[] hits &#x3D; results.scoreDocs;</span><br><span class="line">            if (results.totalHits.value &#x3D;&#x3D; 0) &#123;</span><br><span class="line">                System.out.println(&quot;no result matched!&quot;);</span><br><span class="line">                continue;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F; 输出匹配到的结果</span><br><span class="line">            System.out.println(results.totalHits.value + &quot; results matched: &quot;);</span><br><span class="line">            for (ScoreDoc hit : hits) &#123;</span><br><span class="line">                Document doc &#x3D; searcher.doc(hit.doc);</span><br><span class="line">                System.out.println(&quot;doc&#x3D;&quot; + hit.doc + &quot; score&#x3D;&quot; + hit.score + &quot; file: &quot; + doc.get(&quot;path&quot;));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这段代码的核心流程是先从上一步创建的索引目录加载构建好的索引，然后获取用户输入并解析为查询语句（build query），接着运行查询（run query），如果有匹配到的，就输出匹配的结果。这里对查询比较重要的API做下简单说明：</p>
<ul>
<li><em>IndexReader</em>：打开一个索引；</li>
<li><em>IndexSearcher</em>：搜索<em>IndexReader</em>打开的索引，返回<em>TopDocs</em>对象；</li>
<li><em>QueryParser</em>：该类的parse方法解析用户输入的查询语句，返回一个<em>Query</em>对象；</li>
</ul>
<p>下面我们来运行一下程序：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Enter query: </span><br><span class="line">love</span><br><span class="line">searching for: love</span><br><span class="line">2 results matched: </span><br><span class="line">doc&#x3D;0 score&#x3D;0.48849338 file: &#x2F;Users&#x2F;allan&#x2F;Git&#x2F;allan&#x2F;github&#x2F;CodeSnippet&#x2F;Java&#x2F;lucene-learning&#x2F;data&#x2F;poems&#x2F;Love&#39;sSecret.txt</span><br><span class="line">doc&#x3D;1 score&#x3D;0.41322997 file: &#x2F;Users&#x2F;allan&#x2F;Git&#x2F;allan&#x2F;github&#x2F;CodeSnippet&#x2F;Java&#x2F;lucene-learning&#x2F;data&#x2F;poems&#x2F;FreedomAndLove.txt</span><br></pre></td></tr></table></figure>
<p>我们输入关键字”<em>love</em>“，搜索出来两个Document，分别对应Love’s Secret和Freedom And Love。<em>doc=</em>后面的数字是Document的ID，唯一标识一个Document。<em>score</em>后面的数字是搜索结果与我们搜索的关键字的相关度。然后我们再输入”<em>LOVE</em>“（注意字母都大写了）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Enter query: </span><br><span class="line">Love</span><br><span class="line">searching for: love</span><br><span class="line">2 results matched: </span><br><span class="line">doc&#x3D;0 score&#x3D;0.48849338 file: &#x2F;Users&#x2F;allan&#x2F;Git&#x2F;allan&#x2F;github&#x2F;CodeSnippet&#x2F;Java&#x2F;lucene-learning&#x2F;data&#x2F;poems&#x2F;Love&#39;sSecret.txt</span><br><span class="line">doc&#x3D;1 score&#x3D;0.41322997 file: &#x2F;Users&#x2F;allan&#x2F;Git&#x2F;allan&#x2F;github&#x2F;CodeSnippet&#x2F;Java&#x2F;lucene-learning&#x2F;data&#x2F;poems&#x2F;FreedomAndLove.txt</span><br></pre></td></tr></table></figure>
<p>可以看到搜索结果与之前是一样的，这是因为我们搜索时使用了和构建索引时相同的分词器<em>StandardAnalyzer</em>，该分词器会将所有词转化为小写。然后我们再尝试一下其它搜索：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Enter query: </span><br><span class="line">fog</span><br><span class="line">searching for: fog</span><br><span class="line">1 results matched: </span><br><span class="line">doc&#x3D;2 score&#x3D;0.67580885 file: &#x2F;Users&#x2F;allan&#x2F;Git&#x2F;allan&#x2F;github&#x2F;CodeSnippet&#x2F;Java&#x2F;lucene-learning&#x2F;data&#x2F;poems&#x2F;Fog.txt</span><br><span class="line"></span><br><span class="line">Enter query: </span><br><span class="line">above</span><br><span class="line">searching for: above</span><br><span class="line">1 results matched: </span><br><span class="line">doc&#x3D;3 score&#x3D;0.6199532 file: &#x2F;Users&#x2F;allan&#x2F;Git&#x2F;allan&#x2F;github&#x2F;CodeSnippet&#x2F;Java&#x2F;lucene-learning&#x2F;data&#x2F;poems&#x2F;OnDeath.txt</span><br><span class="line"></span><br><span class="line">Enter query: </span><br><span class="line">death</span><br><span class="line">searching for: death</span><br><span class="line">1 results matched: </span><br><span class="line">doc&#x3D;3 score&#x3D;0.6199532 file: &#x2F;Users&#x2F;allan&#x2F;Git&#x2F;allan&#x2F;github&#x2F;CodeSnippet&#x2F;Java&#x2F;lucene-learning&#x2F;data&#x2F;poems&#x2F;OnDeath.txt</span><br><span class="line"></span><br><span class="line">Enter query: </span><br><span class="line">abc</span><br><span class="line">searching for: abc</span><br><span class="line">no result matched!</span><br></pre></td></tr></table></figure>
<p>都工作正常，最后一个关键字”<em>abc</em>“没有搜到，因为原文中也没有这个词。我们再来看一个复杂点的查询：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Enter query: </span><br><span class="line">+love -seek</span><br><span class="line">searching for: +love -seek</span><br><span class="line">1 results matched: </span><br><span class="line">doc&#x3D;1 score&#x3D;0.41322997 file: &#x2F;Users&#x2F;allan&#x2F;Git&#x2F;allan&#x2F;github&#x2F;CodeSnippet&#x2F;Java&#x2F;lucene-learning&#x2F;data&#x2F;poems&#x2F;FreedomAndLove.txt</span><br></pre></td></tr></table></figure>
<p>这里我们输入的关键字为”<em>+love -seek</em>“，这是一个高级一点的查询，含义是“包含love但不包含seek”，于是就只搜出来Freedom And Love一首诗了。</p>
<h2 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import lucene</span><br><span class="line">from java.nio.file import Paths</span><br><span class="line"># from org.apache.lucene.analysis.cjk import CJKAnalyzer</span><br><span class="line">from org.apache.lucene.document import Document, Field, FieldType, StoredField</span><br><span class="line">from org.apache.lucene.document import TextField, FloatPoint, IntPoint</span><br><span class="line">from org.apache.lucene.document import NumericDocValuesField</span><br><span class="line">from org.apache.lucene.document import FloatDocValuesField</span><br><span class="line">from org.apache.lucene.document import SortedSetDocValuesField</span><br><span class="line">from org.apache.lucene.index import FieldInfo, IndexWriter, IndexWriterConfig</span><br><span class="line">from org.apache.lucene.store import SimpleFSDirectory</span><br><span class="line">from org.apache.lucene.util import Version</span><br><span class="line">from org.apache.lucene.search import IndexSearcher, Sort, SortField</span><br><span class="line">from org.apache.lucene.search import BooleanQuery</span><br><span class="line">from org.apache.lucene.search import BooleanClause</span><br><span class="line">from org.apache.lucene.search import TopFieldCollector, BoostQuery</span><br><span class="line">from org.apache.lucene.queryparser.classic import QueryParser</span><br><span class="line">from org.apache.lucene.index import DirectoryReader</span><br><span class="line">from org.apache.lucene.analysis.core import WhitespaceAnalyzer</span><br><span class="line">from org.apache.lucene.util import BytesRef</span><br><span class="line">from strsimpy.levenshtein import Levenshtein</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class LuceneECSearch(object):</span><br><span class="line">    def __init__(self, indir, seg_model_path, mode&#x3D;&#39;search&#39;,</span><br><span class="line">                 maxrecall&#x3D;10000, maxdoc&#x3D;30):</span><br><span class="line">        lucene.initVM()</span><br><span class="line">        self.indir &#x3D; indir</span><br><span class="line">        self.lucene_analyzer &#x3D; WhitespaceAnalyzer()</span><br><span class="line">        if mode &#x3D;&#x3D; &#39;search&#39;:</span><br><span class="line">            self.indir &#x3D; SimpleFSDirectory(Paths.get(indir))</span><br><span class="line">        self.maxdoc &#x3D; maxdoc</span><br><span class="line">        self.maxrecal &#x3D; maxrecall</span><br><span class="line">        self.levenshtein &#x3D; Levenshtein()</span><br><span class="line"></span><br><span class="line">    def search(self, query):</span><br><span class="line">        # 构造Query</span><br><span class="line">        query &#x3D; self.processor.process(query, &#39;en&#39;)</span><br><span class="line">        boolean_query &#x3D; BooleanQuery.Builder()</span><br><span class="line">        simple_query &#x3D; QueryParser(</span><br><span class="line">            &quot;en_tokenized&quot;,</span><br><span class="line">            self.lucene_analyzer).parse(query)</span><br><span class="line">        keyword_query &#x3D; QueryParser(</span><br><span class="line">            &quot;keyword&quot;,</span><br><span class="line">            self.lucene_analyzer).parse(query)</span><br><span class="line">        boost_keyword_query &#x3D; BoostQuery(keyword_query, 2.0)</span><br><span class="line">        boolean_query.add(simple_query, BooleanClause.Occur.SHOULD)</span><br><span class="line">        boolean_query.add(boost_keyword_query, BooleanClause.Occur.SHOULD)</span><br><span class="line"></span><br><span class="line">        # searcher</span><br><span class="line">        lucene_searcher &#x3D; IndexSearcher(</span><br><span class="line">            DirectoryReader.open(self.indir))</span><br><span class="line">        sorter &#x3D; Sort([</span><br><span class="line">            SortField.FIELD_SCORE,</span><br><span class="line">            SortField(&#39;origin_score&#39;, SortField.Type.FLOAT, True),</span><br><span class="line">            SortField(&#39;en_sent_lenth&#39;, SortField.Type.INT, True)])</span><br><span class="line"></span><br><span class="line">        # rerank</span><br><span class="line">        collector &#x3D; TopFieldCollector.create(</span><br><span class="line">            sorter, self.maxrecal, self.maxrecal)</span><br><span class="line">        lucene_searcher.search(boolean_query.build(), collector)</span><br><span class="line">        scoreDocs &#x3D; collector.topDocs().scoreDocs</span><br><span class="line">        result &#x3D; []</span><br><span class="line">        for hit in scoreDocs:</span><br><span class="line">            doc &#x3D; lucene_searcher.doc(hit.doc)</span><br><span class="line">            keyword_info &#x3D; json.loads(doc.get(&#39;keyword_info&#39;))</span><br><span class="line">            keyword_score &#x3D; 0.0</span><br><span class="line">            for word, score in keyword_info.items():</span><br><span class="line">                word_splits &#x3D; word.split(&#39; &#39;)</span><br><span class="line">                for split in word_splits:</span><br><span class="line">                    if split &#x3D;&#x3D; query:</span><br><span class="line">                        keyword_score +&#x3D; score</span><br><span class="line">            json_answer &#x3D; &#123;</span><br><span class="line">                &#39;en_tokenized&#39;: doc.get(&quot;en_tokenized&quot;),</span><br><span class="line">                &#39;en_sent&#39;: doc.get(&quot;en_sent&quot;),</span><br><span class="line">                &#39;en_sent_lenth&#39;: doc.get(&quot;en_sent_lenth&quot;),</span><br><span class="line">                &#39;cn_tokenized&#39;: doc.get(&quot;cn_tokenized&quot;),</span><br><span class="line">                &#39;cn_sent&#39;: doc.get(&quot;cn_sent&quot;),</span><br><span class="line">                &#39;confidence&#39;: int(doc.get(&quot;confidence&quot;)),</span><br><span class="line">                &#39;origin_score&#39;: float(doc.get(&quot;origin_score&quot;)),</span><br><span class="line">                &#39;new_score&#39;: float(doc.get(&quot;new_score&quot;))&#125;</span><br><span class="line">            result.append(json_answer)</span><br><span class="line">        result &#x3D; self.rerank(query, result)</span><br><span class="line">        return result</span><br><span class="line"></span><br><span class="line">    def rerank(self, query, candidates):</span><br><span class="line">        candidates.sort(key&#x3D;lambda x: x[&quot;origin_score&quot;], reverse&#x3D;True)</span><br><span class="line">        # candidates &#x3D; candidates[:self.maxdoc * 2]</span><br><span class="line">        candidates.sort(key&#x3D;lambda x: x[&quot;new_score&quot;], reverse&#x3D;True)</span><br><span class="line">        # 先把有释义的拿出来</span><br><span class="line">        return candidates</span><br><span class="line"></span><br><span class="line">    def build(self, docdir, modeldir):</span><br><span class="line">        if os.path.exists(self.indir):</span><br><span class="line">            shutil.rmtree(self.indir)</span><br><span class="line">        lucene.initVM()</span><br><span class="line">        INDEXIDR &#x3D; Paths.get(self.indir)</span><br><span class="line">        indexdir &#x3D; SimpleFSDirectory(INDEXIDR)</span><br><span class="line"></span><br><span class="line">        config &#x3D; IndexWriterConfig(self.lucene_analyzer)</span><br><span class="line">        index_writer &#x3D; IndexWriter(indexdir, config)</span><br><span class="line"></span><br><span class="line">        cnt &#x3D; 0</span><br><span class="line">        with open(docdir, &#39;r&#39;, encoding&#x3D;&#39;utf-8&#39;) as f:</span><br><span class="line">            for line in f.readlines():</span><br><span class="line">                line &#x3D; line.strip()</span><br><span class="line">                try:</span><br><span class="line">                    data_json &#x3D; json.loads(line)</span><br><span class="line">                except Exception:</span><br><span class="line">                    print(&#39;Json load error!&#39;)</span><br><span class="line">                    continue</span><br><span class="line">                try:</span><br><span class="line">                    document &#x3D; Document()</span><br><span class="line"></span><br><span class="line">                    en_tokenized &#x3D; data_json[&#39;en_tokenized&#39;]</span><br><span class="line">                    # TODO 去掉停用词</span><br><span class="line">                    document.add(Field(&quot;en_tokenized&quot;, en_tokenized,</span><br><span class="line">                                       TextField.TYPE_STORED))</span><br><span class="line"></span><br><span class="line">                    en_sent &#x3D; data_json[&#39;en_sent&#39;]</span><br><span class="line">                    document.add(Field(&quot;en_sent&quot;, en_sent,</span><br><span class="line">                                       TextField.TYPE_STORED))</span><br><span class="line"></span><br><span class="line">                    en_sent_lenth &#x3D; len(en_tokenized)</span><br><span class="line">                    document.add(IntPoint(&quot;en_sent_lenth&quot;, en_sent_lenth))</span><br><span class="line">                    document.add(NumericDocValuesField(</span><br><span class="line">                        &quot;en_sent_lenth&quot;, en_sent_lenth))</span><br><span class="line">                    document.add(StoredField(&quot;en_sent_lenth&quot;, en_sent_lenth))</span><br><span class="line"></span><br><span class="line">                    cn_tokenized &#x3D; data_json[&#39;cn_tokenized&#39;]</span><br><span class="line">                    document.add(Field(&quot;cn_tokenized&quot;, cn_tokenized,</span><br><span class="line">                                       TextField.TYPE_STORED))</span><br><span class="line"></span><br><span class="line">                    cn_sent &#x3D; data_json[&#39;cn_sent&#39;]</span><br><span class="line">                    document.add(Field(&quot;cn_sent&quot;, cn_sent,</span><br><span class="line">                                       TextField.TYPE_STORED))</span><br><span class="line"></span><br><span class="line">                    confidence &#x3D; int(data_json[&#39;confidence&#39;])</span><br><span class="line">                    document.add(IntPoint(&quot;confidence&quot;, confidence))</span><br><span class="line">                    document.add(NumericDocValuesField(</span><br><span class="line">                        &quot;confidence&quot;, confidence))</span><br><span class="line">                    document.add(StoredField(&quot;confidence&quot;, confidence))</span><br><span class="line"></span><br><span class="line">                    origin_score &#x3D; float(data_json[&#39;origin_score&#39;])</span><br><span class="line">                    document.add(FloatPoint(&quot;origin_score&quot;, origin_score))</span><br><span class="line">                    document.add(FloatDocValuesField(&quot;origin_score&quot;, origin_score))</span><br><span class="line">                    document.add(StoredField(&quot;origin_score&quot;, origin_score))</span><br><span class="line"></span><br><span class="line">                    keyword_info &#x3D; &#123;&#125;</span><br><span class="line">                    for keyword in data_json[&#39;keyword&#39;]:</span><br><span class="line">                        keyword_text &#x3D; keyword[&#39;text&#39;]</span><br><span class="line">                        keyword_score &#x3D; keyword[&#39;score&#39;]</span><br><span class="line">                        document.add(</span><br><span class="line">                            SortedSetDocValuesField(</span><br><span class="line">                                &quot;keyword&quot;, BytesRef(keyword_text)))</span><br><span class="line">                        document.add(</span><br><span class="line">                            StoredField(&quot;keyword&quot;, keyword_text))</span><br><span class="line">                        document.add(</span><br><span class="line">                            Field(&quot;keyword&quot;, keyword_text,</span><br><span class="line">                                  TextField.TYPE_STORED))</span><br><span class="line">                        keyword_info[keyword_text] &#x3D; keyword_score</span><br><span class="line">                    keyword_info_str &#x3D; json.dumps(</span><br><span class="line">                        keyword_info, ensure_ascii&#x3D;False)</span><br><span class="line">                    document.add(</span><br><span class="line">                        Field(</span><br><span class="line">                            &quot;keyword_info&quot;,</span><br><span class="line">                            keyword_info_str,</span><br><span class="line">                            TextField.TYPE_STORED))</span><br><span class="line">                    </span><br><span class="line">                    new_score &#x3D; ... # 经过模型计算出来的        </span><br><span class="line">0                    new_score &#x3D; </span><br><span class="line">                    document.add(FloatPoint(&quot;new_score&quot;, new_score))</span><br><span class="line">                    document.add(</span><br><span class="line">                        FloatDocValuesField(&quot;new_score&quot;, new_score))</span><br><span class="line">                    document.add(</span><br><span class="line">                        StoredField(&quot;new_score&quot;, new_score))</span><br><span class="line">                    index_writer.addDocument(document)</span><br><span class="line">                except Exception:</span><br><span class="line">                    print(&#39;Index write error!&#39;)</span><br><span class="line">                    continue</span><br><span class="line">                cnt +&#x3D; 1</span><br><span class="line">                if cnt % 1000 &#x3D;&#x3D; 0:</span><br><span class="line">                    print(&#39;Writing &#39;, cnt)</span><br><span class="line"></span><br><span class="line">        index_writer.commit()</span><br></pre></td></tr></table></figure>
<pre><code>    index_writer.close()
</code></pre>]]></content>
      <tags>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title>EM学习——基础学习</title>
    <url>/2020/04/19/EM%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>接下来几天将复习Graphical Model的一系列模型。今天先复习一下EM算法。</p>
<a id="more"></a>
<h1 id="Graphical-Model和Latent-Variable"><a href="#Graphical-Model和Latent-Variable" class="headerlink" title="Graphical Model和Latent Variable"></a>Graphical Model和Latent Variable</h1><p>首先，我们了解一下graphical model的技术思路：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/em/1.png" alt="图片"></p>
<p>从上图中可以看到，Graphical Model可以分成两类，一类是有向图，典型代表是HMM；一类是无向图，典型代表是CRF。当我们能找出状态之间的依赖关系时，可以使用HMM，因为HMM可以利用conditional independence条件独立性质；当我们刻画不出依赖关系时，可以使用CRF。其中，从HMM可以推导出MEMM模型，从MEMM可以推导出CRF模型。我们通常使用的CRF是Linear CRF，其中的Linear是从Log Linear Model中得来的。Log Linear Model可推出的另外一个模型是logistic regressioin，区别是逻辑回归针对static data，CRF针对sequencial data。整个Graphical Model的参数估计使用的是EM算法，推理使用Viterbi算法。</p>
<p>在有隐含变量（latent variable）的模型中，我们经常使用EM算法。那什么是latent variable呢？一个Latent variable model可以用z~x表示，意思是用看不见的z去生成看得见的x。以下图为例，假设任务是生成人脸，那么隐含变量就包含性别、眼睛颜色、头发颜色等信息：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/em/2.png" alt="图片"></p>
<p>EM的核心是计算参数theta，如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/em/3.png" alt="图片"></p>
<p>在计算时分为两种情况：</p>
<ul>
<li>Complete case：(z, x)都是可观测的，如逻辑回归（已知x预测$\theta​$），一般使用MLE</li>
<li>Imcomplete case：x是可观测的，z是不可观测的，使用EM style算法</li>
</ul>
<h1 id="MLE-for-Complete-and-Imcomplete-Case"><a href="#MLE-for-Complete-and-Imcomplete-Case" class="headerlink" title="MLE for Complete and Imcomplete Case"></a>MLE for Complete and Imcomplete Case</h1><p>下面我们来看一下MLE的求解思路。MLE的核心思想是求解$argmax_{\theta}(D|\theta)​$，也就是求最大化D情况下的theta值。比如在逻辑回归中就是求解$argmax_{\theta}P({x_i, y_i}_{i=1}^n|\theta)​$。而在graphic model中变为$argmax_{\theta}P({x, z|\theta})​$。在Complete Case和Incomplete Case中MLE可以分别继续写成如下形式（可以看到$logp(z|\theta_z)​$是典型的MLE，我们可以通过数据统计反推出theta值）：<img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/em/4.png" alt="图片"></p>
<blockquote>
<p>【注】：在图模型中联合概率很难表示，有几个技巧可以尝试：<br>（1）使用conditional independence对形如$P(a,b,c,d,e|\theta)$进行转换：例如n-gram语言模型P(I hate ad|y=垃圾)，在朴素贝叶斯中就会近似为P(I|y=垃圾)P(hate|y=垃圾)P(ad|y=垃圾)<br>（2）使用D-seperation或Markov Blanket对形如$P(a|b,c,d,e,f)$进行转换：比如发现d、e、f对a没有影响，转换为P(a|b,c)，进而再转化为函数形式f(\theta)，再使用优化方法找出解或者近似解<br>（3）在Incomplete Case中，由于z也是未知的，所以把z边缘化了<br>（4）上面图中$p(z|\theta)$和$p(x|z, \theta)$也可以认为是两个函数$f(\theta)和g(\theta)$，也可以使用优化算法进行求解。对于Incomplete Case，$f(\theta)和g(\theta)$很难求解，所以使用EM-style算法求解</p>
</blockquote>
<p>在Complete Case中基本是通过统计方式计算的，以下图为例（假设观测到的是单词，隐变量是词性，建立一个HMM模型，我们可以统计词性之间的Transition Probabilty和词性到单词的Emition Probability）：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/em/5.png" alt="图片"></p>
<h1 id="EM-Derivation"><a href="#EM-Derivation" class="headerlink" title="EM Derivation"></a>EM Derivation</h1><p>EM中一共有3类变量：</p>
<ul>
<li>$\theta​$: model parameter</li>
<li>$z​$: latent variable</li>
<li>$x$: obversation</li>
</ul>
<p>目标：用MLE最大化$L(\theta)=logP(x|\theta)​$，也就是求解$argmax_{\theta}L(\theta)=argmax_{\theta}logP(x|\theta)​$。</p>
<p>假设：使用iterative算法，已经求解出$\theta_1$,$\theta_2$, …, $\theta_n$，则如何计算$\theta_{n+1}$?</p>
<p>已知：$\theta_n$是对应于$L(\theta_n)$的解，那么$\theta_{n+1}$就是使得$argmax_{\theta}L(\theta) - argmax_{\theta_{n+1}}L(\theta_{n+1})$最大的值（可以理解为使得目标函数提升最大的值），且$argmax_{\theta}L(\theta) - argmax_{\theta_{n+1}}L(\theta_{n+1}) = logP(x|\theta) - logP(x|\theta_n)$，上面思考过程来源于下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/em/6.png" alt="图片"></p>
<p>从上面图中看到，当我们已经优化到log sum的形式时，就很难继续求解了。所以我们想到用一个不等式去近似它。这个不等式就叫做Jensen’s Inequality：<img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/em/7.png" alt="图片">（【注】: Jensen’s Inequality可以从凸函数的性质得出来）</p>
<p>经过Jensen’s neuqlity简化，上图中式子可推导成：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/em/8.png" alt="图片"></p>
<p>继续推导可以得到下图（第6行是由于是常量所以消去了）：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/em/9.png" alt="图片"></p>
<p>我们可以把最后一项写成Expectation的形式，怎么理解呢？$P(z|x, \theta_n)$是所有可能的z的一个概率分布，我们把每一个可能的z带入到$logP(x, z|\theta)$中，我们就可以计算$logP(x, z|\theta)$的 一个平均值，也就是期望值。由于已知$x$、$\theta_n$，我们可以求出z的期望值。那么对于$logP(x,z|\theta)$来说，由于$x$、$z$、$\theta_n$都是已知的，想求解最大似然，就变成了一个complete case，我们可以通过MLE求解使其最大的$\theta$。</p>
<blockquote>
<p>【注】：你可能想问，已知$x$, $\theta_n$，是怎么求出$P(z|x, \theta_n)$的呢？在具体问题中，我们通常会把$P(z|x, \theta_n)$写成一个函数的形式，自然就可以求出来了</p>
</blockquote>
<p>EM算法其实就是两个步骤的一个循环，已知$x$, $\theta$求$z$的期望，再用这个$z$求解$\theta$，再去迭代得求解$z$，如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/em/10.png" alt="图片"></p>
<p>EM算法一定是收敛的，它本质上是coordinate descent，即按照坐标轴去优化。</p>
<h1 id="K-means"><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h1><p>K-means整体上就是选中心点—&gt;根据与中心点的举例聚类—&gt;重新计算中心点，这样一个迭代的过程，如下图所示：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/em/11.png" alt="图片"></p>
<h2 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h2><p>Kmeans的代价函数如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/em/12.png" alt="图片"></p>
<p>可以看到有两个参数：</p>
<ul>
<li>$\mu_k$表示中心点，可以看成是模型参数</li>
<li>$\gamma_{ik}$表示对$\mu_k$的选择，可以看成是隐变量</li>
</ul>
<h2 id="K-means和EM的关系"><a href="#K-means和EM的关系" class="headerlink" title="K-means和EM的关系"></a>K-means和EM的关系</h2><p>K-means的迭代过程就是先选择$\mu_k$，再计算$\gamma_{ik}$，再根据聚成的簇重新选择$\mu_k$。但K-means是一个EM-style的算法，而不是一个严格意义的EM。因为在EM中，要计算的是隐变量的期望值，即对于$\gamma_{ik}$表示$x_i$属于第$k$个cluster的概率，则$\gamma_{i0}=0.8$, $\gamma_{i1}=0.2$….（有点像GMM哈）。但在K-means中，我们直接令$\gamma_{i0}=1$了。因此Kmeans也叫Hard-Clustering，GMM也叫Soft-Clustering。如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/em/13.png" alt="图片"></p>
]]></content>
      <tags>
        <tag>EM</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：ALBert</title>
    <url>/2020/04/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AALBert/</url>
    <content><![CDATA[<p>Albert在quora question pair上得分最高，我确始终没看过论文，今天就来补一补。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://paperswithcode.com/paper/albert-a-lite-bert-for-self-supervised" target="_blank" rel="noopener">https://paperswithcode.com/paper/albert-a-lite-bert-for-self-supervised</a><br>语篇分析：<a href="http://www.shuang0420.com/2017/09/20/NLP%20%E7%AC%94%E8%AE%B0%20-%20Discourse%20Analysis/" target="_blank" rel="noopener">http://www.shuang0420.com/2017/09/20/NLP%20%E7%AC%94%E8%AE%B0%20-%20Discourse%20Analysis/</a><br><a href="https://kexue.fm/archives/7187" target="_blank" rel="noopener">https://kexue.fm/archives/7187</a></p>
</blockquote>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>ALBERT主要是从减少模型参数的角度对BERT进行优化，主要有如下3个优化点：</p>
<ul>
<li>Factorized embedding parameterization：</li>
<li>Cross-layer parameter sharing</li>
<li>Inter-sentence coherence loss</li>
</ul>
<h1 id="Factorized-embedding-parameterization"><a href="#Factorized-embedding-parameterization" class="headerlink" title="Factorized embedding parameterization"></a>Factorized embedding parameterization</h1><p>在BERT、XLNet、RoBERTa等模型中，由于模型结构的限制，WordePiece embedding的大小$E$总是与隐层大小$H$相同，即$E \equiv H$。从建模的角度考虑，词嵌入学习的是单词与上下文无关的表示，而隐层则是学习与上下文相关的表示。显然后者更加复杂，需要更多的参数，也就是说模型应当增大隐层大小$H$ ，或者说满足$H \gg E$。但实际上词汇表的大小通$V$常非常大，如果$E=H$的话，增加隐层大小$H$后将会使embedding matrix的维度$V \times E$非常巨大。</p>
<p>因此本文想要打破$E$与$H$之间的绑定关系，从而减小模型的参数量，同时提升模型表现。具体做法是将embedding matrix分解为两个大小分别为$V \times E$和$E \times H$矩阵，也就是说先将单词投影到一个低维的embedding空间$E$，再将其投影到高维的隐藏空间$H$ 。这使得embedding matrix的维度从$O(V \times H)$减小到$O(V \times E+E \times H)$。当$H \gg E$时，参数量减少非常明显。在实现时，随机初始化$V \times E$和$E \times H$的矩阵，计算某个单词的表示需用一个单词的one-hot向量乘以$V \times E$维的矩阵（也就是lookup），再用得到的结果乘$E \times H$维的矩阵即可。两个矩阵的参数通过模型学习。</p>
<h1 id="Cross-layer-parameter-sharing"><a href="#Cross-layer-parameter-sharing" class="headerlink" title="Cross-layer parameter sharing"></a>Cross-layer parameter sharing</h1><p>本文提出的另一个减少参数量的方法就是层之间的参数共享，即多个层使用相同的参数。参数共享有三种方式：只共享feed-forward network的参数、只共享attention的参数、共享全部参数。ALBERT默认是共享全部参数的，在后续实验结果中我们可以看到几种方式的模型表现。</p>
<p>如下图所示，实验表明加入参数共享之后，每一层的输入embedding和输出embedding的L2距离和余弦相似度都比BERT稳定了很多。这证明参数共享能够使模型参数更加稳定。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ALBERT/1.png" alt="图片"></p>
<h1 id="Inter-sentence-coherence-loss"><a href="#Inter-sentence-coherence-loss" class="headerlink" title="Inter-sentence coherence loss"></a>Inter-sentence coherence loss</h1><p>除了减少模型参数外，本外还对BERT的预训练任务Next-sentence prediction (NSP)进行了改进。在BERT中，NSP任务的正例是文章中连续的两个句子，而负例则是从两篇文档中各选一个句子构造而成。在先前的研究中，已经证明NSP是并不是一个合适的预训练任务。本文推测其原因是模型在判断两个句子的关系时不仅考虑了两个句子之间的连贯性（coherence），还会考虑到两个句子的话题（topic）。而两篇文档的话题通常不同，模型会更多的通过话题去分析两个句子的关系，而不是句子间的连贯性，这使得NSP任务变成了一个相对简单的任务。</p>
<p>因此本文提出了Sentence-order prediction (SOP)来取代NSP。具体来说，其正例与NSP相同，但负例是通过选择一篇文档中的两个连续的句子并将它们的顺序交换构造的。这样两个句子就会有相同的话题，模型学习到的就更多是句子间的连贯性。</p>
<h1 id="Model-setup"><a href="#Model-setup" class="headerlink" title="Model setup"></a>Model setup</h1><p>本文为ALBERT选择了四种参数设置：base, large, xlarge和xxlarge。如下图所示</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ALBERT/2.png" alt="图片"></p>
<h1 id="Experimental-Results"><a href="#Experimental-Results" class="headerlink" title="Experimental Results"></a>Experimental Results</h1><h2 id="BERT-vs-ALBERT"><a href="#BERT-vs-ALBERT" class="headerlink" title="BERT vs. ALBERT"></a>BERT vs. ALBERT</h2><p>从下图的实验结果可见，ALBERT的训练速度明显比BERT快，ALBERT-xxlarge的表现更是全方面超过了BERT。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ALBERT/3png.png" alt="图片"></p>
<h2 id="Factorized-Embedding-Parameterization"><a href="#Factorized-Embedding-Parameterization" class="headerlink" title="Factorized Embedding Parameterization"></a>Factorized Embedding Parameterization</h2><p>从下图实验结果可见，对于不共享参数的情况，$E$几乎是与大越好；而共享参数之后，$E$太大反而会使模型表现变差，$E=128$模型表现最好，因此ALBERT的默认参数设置中$E=128$。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ALBERT/4.png" alt="图片"></p>
<p>考虑到ALBERT-base的$H=768$ ，那么$E=768$时，模型应该可以看作没有减少embedding参数量的情况。而不共享参数的实验结果表明此时模型表现更好，那么似乎说明了Factorized embedding在一定程度上降低了模型的表现。</p>
<h2 id="Cross-Layer-Parameter-Sharing"><a href="#Cross-Layer-Parameter-Sharing" class="headerlink" title="Cross-Layer Parameter Sharing"></a>Cross-Layer Parameter Sharing</h2><p>如下图所示，可以看出参数共享几乎也是对模型结果有负面影响的。但是考虑到其能够大幅削减参数量，并且对结果影响不是特别大，因此权衡之下选择了参数共享。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ALBERT/5.png" alt="图片"></p>
<h2 id="SOP"><a href="#SOP" class="headerlink" title="SOP"></a>SOP</h2><p>如下图实验结果所示，如果不使用NSP或SOP作为预训练任务的话，模型在NSP和SOP两个任务上表现都很差；如果使用NSP作为预训练任务的话，模型确实能很好的解决NSP问题，但是在SOP问题上表现却很差，几乎相当于随机猜，因此说明NSP任务确实很难学到句子间的连贯性；而如果用SOP作为预训练任务，则模型也可以较好的解决NSP问题，同时模型在下游任务上表现也更好。说明SOP确实是更好的预训练任务。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ALBERT/6.png" alt="图片"></p>
<h2 id="Effect-Of-Network-Depth-And-Width"><a href="#Effect-Of-Network-Depth-And-Width" class="headerlink" title="Effect Of Network Depth And Width"></a>Effect Of Network Depth And Width</h2><p>从下面两个对比试验结果我们可以看出，增加模型的层数或隐层大小确实能够在一定程度上提升模型的表现。但当大小增加到一定量时，反而会使模型表现变差。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ALBERT/7.png" alt="图片"></p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ALBERT/8.png" alt="图片"></p>
<h2 id="Additional-Training-Data-And-Dropout-Effects"><a href="#Additional-Training-Data-And-Dropout-Effects" class="headerlink" title="Additional Training Data And Dropout Effects"></a>Additional Training Data And Dropout Effects</h2><p>ALBERT训练时还加入了XLNet和RoBERTa训练时用的额外数据，实验表明加入额外数据（W additional data）确实会提升模型表现。此外，作者还观察到模型似乎一直没有过拟合数据，因此去除了Dropout，从对比试验可以看出，去除Dropout（W/O Dropout）后模型表现确实更好。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ALBERT/9.png" alt="图片"></p>
<h2 id="Current-State-Of-The-Art-On-NLU-Tasks"><a href="#Current-State-Of-The-Art-On-NLU-Tasks" class="headerlink" title="Current State-Of-The-Art On NLU Tasks"></a>Current State-Of-The-Art On NLU Tasks</h2><p>最后是ALBERT在各个NLU任务上的表现，几乎都达到了state-of-the-art的表现。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ALBERT/10.png" alt="图片"></p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ALBERT/11.png" alt="图片"></p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><ul>
<li>本文提出了Factorized embedding和层之间参数共享两种削减参数量的方式，在大家都想着把模型做大的时候给大家指出了另一条可行的路，意义重大。但本文提出的两种方法实际上都带来了模型效果的下降，也就是说本文似乎也还没有找到BERT中真正的冗余参数，减少模型参数量这方面还需要更多的研究。</li>
<li>本文提出了SOP，很好地替换了NSP作为预训练任务，给模型表现带来了明显提升。</li>
<li>本文的削减参数使模型表现下降，结果更好主要是靠SOP、更大的$H$ 、更多的数据、去除dropout。那么如果不削减参数的话再使用SOP、加更多的数据、去除dropout呢？</li>
<li>本文的削减参数量参数量带来了模型训练速度的提升，但是ALBERT-xxlarge比BERT-xlarge参数量少了约1000M，而训练速度并没有太大的提升（只有1.2倍）。原因应该是更少的参数量的确能带来速度上的提升，但是本文提出的Factorized embedding引入了额外的矩阵运算，并且同时ALBERT-xxlarge大幅增加了$H$，实际上增加了模型的计算量。</li>
<li>本文还有两个小细节可以学习，一个是在模型不会过拟合的情况下不使用dropout，文中有提到batch_norm+dropout可能带来负面效果，dropout存在训练和推断的不一致问题，也就是“严格来讲训练模型和预测模型并不是同一个模型”，模型变大变深时，这种不一致性可能会进一步放大，所以dropout对于超大模型可能并不是一种有效的防止过拟合的方法；另一个则是warm-start，即在训练深层网络（例如12层）时，可以先训练浅层网络（例如6层），再在其基础上做fine-tune，这样可以加快深层模型的收敛。</li>
</ul>
<h1 id="Extend"><a href="#Extend" class="headerlink" title="Extend"></a>Extend</h1><p>在<a href="https://kexue.fm/archives/7187" target="_blank" rel="noopener">https://kexue.fm/archives/7187</a>中提出了一个能提升albert在下游任务中表现的方法：    在下游任务中，放弃albert的权重共享的约束，也就是把albert当bert用。</p>
]]></content>
      <tags>
        <tag>论文阅读</tag>
        <tag>Bert</tag>
      </tags>
  </entry>
  <entry>
    <title>如何对文本后处理之：大小写转换</title>
    <url>/2020/04/15/%E5%A6%82%E4%BD%95%E5%AF%B9%E6%96%87%E6%9C%AC%E5%90%8E%E5%A4%84%E7%90%86%E4%B9%8B%EF%BC%9A%E5%A4%A7%E5%B0%8F%E5%86%99%E8%BD%AC%E6%8D%A2/</url>
    <content><![CDATA[<p>最近工作中文本处理任务特别多，今天特地看一下大小写转换。大小写转换对于文本的后处理很重要，如果做不好，句子看起来很ugly。一开始想通过端到端的方法做，后来想一想感觉不需要上神经网络模型。大小写本身就是跟“是否句子开头”、“是否命名实体”、“是否缩略词”等有关系。因此认为大小写转换过程应该走一个pipeline的流程，看了一些资料发现确实如此。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://www.cs.cmu.edu/~llita/papers/lita.truecasing-acl2003.pdf" target="_blank" rel="noopener">https://www.cs.cmu.edu/~llita/papers/lita.truecasing-acl2003.pdf</a><br><a href="https://towardsdatascience.com/truecasing-in-natural-language-processing-12c4df086c21" target="_blank" rel="noopener">https://towardsdatascience.com/truecasing-in-natural-language-processing-12c4df086c21</a></p>
</blockquote>
<h1 id="论文tRuEcasIng"><a href="#论文tRuEcasIng" class="headerlink" title="论文tRuEcasIng"></a>论文tRuEcasIng</h1><p>大小写对于NER、ASR后处理等任务很重要，比如“the president”和“the President”表达的是不同的含义。大小写跟文本的来源、作者的写作风格都有关系，为了对不同语料库中的文本进行统一归一化，可以使用Truecasing将大小写变为统一规范形式。</p>
<p>在以往相关工作中，关注到的是句首、引号后面和命名实体的大写。论文将词的大小写分为4类：全部小写（LC）、首字母大写（UC）、全部字母大写（CA）、混合大小写（MC）。论文中使用的方法可以关注到word的上下文信息。因此即便遇到UNK（如lenon），也会因为跟lenon有相同上下文的词常采用UC，而把lenon也记为UC类别。</p>
<p>如果我们使用Unigram语言模型的话，可能发现这样的问题：只有12%的词是有多种大小写表示的，那对于new这个词，new后面跟着的词中只有少部分（York、Zealand）是需要大写的，那么new这个词就很容易改写为小写的，使得后面是York时出错了（可以使用贪心法解码Unigram Model）。</p>
<p>为了避免Unigram模型的缺点，需要不仅仅考虑local context，还要考虑整个句子的语义。因此论文使用了HMM对整个句子进行建模和推理：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/bigsmall.png" alt="图片"></p>
<p>其中每个节点包含了如下信息：词的可能的truecase、词的语法信息、词在句子中位置信息、前面两个词的语法信息、前面两个词的truecase信息。也就是说，HMM的隐藏状态$\left(q_{1}, q_{2}, \cdots, q_{n}\right)$是词的大小写和上下文信息组合，观察状态$O_{1} O_{2} \cdots O_{t}$是词的lexical item，使用维特比进行求解：$q_{\tau}^{*}=\operatorname{argmax}_{q_{i 1} q_{i 2} \cdots q_{i t}} P\left(q_{i 1} q_{i 2} \cdots q_{i t} \mid O_{1} O_{2} \cdots O_{t}, \lambda\right)$转移概率lambda是语言模型特征的打分：</p>
<p>$\begin{aligned}<br>P_{\text {model}}\left(w_{3} \mid w_{2}, w_{1}\right) &amp;=\lambda_{\text {trigram}} P\left(w_{3} \mid w_{2}, w_{1}\right) \\<br>&amp;+\lambda_{\text {bigram}} P\left(w_{3} \mid w_{2}\right) \\<br>&amp;+\lambda_{\text {unigram}} P\left(w_{3}\right) \\<br>&amp;+\lambda_{\text {uniform}} P_{0}<br>\end{aligned}$</p>
<p>对于UNK单词，比如‘mispeling’，在训练时被替换为UNKNOWN LC，‘Lenon’被替换为UNKNOWN_UC，这样做的目的是当一个unknown word出现同样上下文的context时，就知道怎样大小写了。</p>
<p>这篇论文有一个简单版的实现：<a href="https://github.com/daltonfury42/truecase" target="_blank" rel="noopener">https://github.com/daltonfury42/truecase</a>，我们也阅读一下这个代码。下面是它的训练过程的主函数：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def train(self, corpus):</span><br><span class="line">  for sentence in corpus:</span><br><span class="line">      # 首先检查句子合法性，去掉全部是大写的句子</span><br><span class="line">      if not self.check_sentence_sanity(sentence):</span><br><span class="line">          continue</span><br><span class="line">      for word_idx, word in enumerate(sentence):</span><br><span class="line">          self.uni_dist[word] +&#x3D; 1</span><br><span class="line">          word_lower &#x3D; word.lower()</span><br><span class="line">          # 把单词的所有大小写可能放到word_casing_lookup中</span><br><span class="line">          if word_lower not in self.word_casing_lookup:</span><br><span class="line">              self.word_casing_lookup[word_lower] &#x3D; set()</span><br><span class="line">          self.word_casing_lookup[word_lower].add(word)</span><br><span class="line">          # 将word的上下文加入bi-gram语言模型统计中</span><br><span class="line">          self.__function_one(sentence, word, word_idx, word_lower)</span><br><span class="line">          # 将word的上下文加入tri-gram语言模型统计中</span><br><span class="line">          self.__function_two(sentence, word, word_idx)</span><br></pre></td></tr></table></figure>
<p>训练好的模型用下面代码存放到pickle中：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def save_to_file(self, file_path):</span><br><span class="line">        pickle_dict &#x3D; &#123;</span><br><span class="line">            &quot;uni_dist&quot;: self.uni_dist,</span><br><span class="line">            &quot;backward_bi_dist&quot;: self.backward_bi_dist,</span><br><span class="line">            &quot;forward_bi_dist&quot;: self.forward_bi_dist,</span><br><span class="line">            &quot;trigram_dist&quot;: self.trigram_dist,</span><br><span class="line">            &quot;word_casing_lookup&quot;: self.word_casing_lookup,</span><br><span class="line">        &#125;</span><br><span class="line">        with open(file_path, &quot;wb&quot;) as fp:</span><br><span class="line">            pickle.dump(pickle_dict, fp)</span><br><span class="line">        print(&quot;Model saved to &quot; + file_path)</span><br></pre></td></tr></table></figure>
<p>模型推理的主函数：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def get_true_case(self, sentence, out_of_vocabulary_token_option&#x3D;&quot;title&quot;):</span><br><span class="line"># outOfVocabulariyTokenOption&#x3D;title将OOV词以大写格式输出</span><br><span class="line"># outOfVocabulariyTokenOption&#x3D;lower将OOV词以小写格式输出</span><br><span class="line"># outOfVocabulariyTokenOption&#x3D;as-is将OOV词以原先格式输出</span><br><span class="line">  </span><br><span class="line">  tokens &#x3D; self.tknzr.tokenize(sentence)</span><br><span class="line">  tokens_true_case &#x3D; []</span><br><span class="line">  for token_idx, token in enumerate(tokens):</span><br><span class="line">      # 标点和数字原样输出</span><br><span class="line">      if token in string.punctuation or token.isdigit():</span><br><span class="line">          tokens_true_case.append(token)</span><br><span class="line">      else:</span><br><span class="line">          token &#x3D; token.lower()</span><br><span class="line">          # 在词表中</span><br><span class="line">          if token in self.word_casing_lookup:</span><br><span class="line">              # 只有一种形式的直接返回</span><br><span class="line">              if len(self.word_casing_lookup[token]) &#x3D;&#x3D; 1:</span><br><span class="line">                  tokens_true_case.append(</span><br><span class="line">                      list(self.word_casing_lookup[token])[0])</span><br><span class="line">              else:</span><br><span class="line">                  prev_token &#x3D; (tokens_true_case[token_idx - 1]</span><br><span class="line">                                if token_idx &gt; 0 else None)</span><br><span class="line">                  next_token &#x3D; (tokens[token_idx + 1]</span><br><span class="line">                                if token_idx &lt; len(tokens) - 1 else None)</span><br><span class="line">                  best_token &#x3D; None</span><br><span class="line">                  highest_score &#x3D; float(&quot;-inf&quot;)</span><br><span class="line">                  # 找到语言模型得分最高的得分组合</span><br><span class="line">                  for possible_token in self.word_casing_lookup[token]:</span><br><span class="line">                      score &#x3D; self.get_score(prev_token, possible_token, next_token)</span><br><span class="line">                      if score &gt; highest_score:</span><br><span class="line">                          best_token &#x3D; possible_token</span><br><span class="line">                          highest_score &#x3D; score</span><br><span class="line">                  tokens_true_case.append(best_token)</span><br><span class="line">              if token_idx &#x3D;&#x3D; 0:</span><br><span class="line">                  tokens_true_case[0] &#x3D; tokens_true_case[0].title()</span><br><span class="line">          else:  # OOV</span><br><span class="line">              if out_of_vocabulary_token_option &#x3D;&#x3D; &quot;title&quot;:</span><br><span class="line">                  tokens_true_case.append(token.title())</span><br><span class="line">              elif out_of_vocabulary_token_option &#x3D;&#x3D; &quot;lower&quot;:</span><br><span class="line">                  tokens_true_case.append(token.lower())</span><br><span class="line">              else:</span><br><span class="line">                  tokens_true_case.append(token)</span><br><span class="line">  return &quot;&quot;.join([</span><br><span class="line">      &quot; &quot; +</span><br><span class="line">      i if not i.startswith(&quot;&#39;&quot;) and i not in string.punctuation else i</span><br><span class="line">      for i in tokens_true_case</span><br></pre></td></tr></table></figure>
<h1 id="一个pipeline的设计"><a href="#一个pipeline的设计" class="headerlink" title="一个pipeline的设计"></a>一个pipeline的设计</h1><p>我们可以设计这样一个pipeline以满足大部分大小写转换的需求：分句 + 命名实体大写 + n-gram语言模型处理其他。分句使用spacy，它利用dependency parse tree和一些规则判断句子的边界。在处理时有一些情况要注意：</p>
<ul>
<li>注意句子开始的标志，一些不以字母为开始的句子，它的第一个字母应该大写：”non-digital items(t-shirts, printed posters, mugs, and cd-roms) require physical shipping.”</li>
<li>注意句子切分，如省略号、双引号后面是否是独立的一句话</li>
<li>注意缩略词的分词</li>
</ul>
]]></content>
      <tags>
        <tag>文本处理</tag>
      </tags>
  </entry>
  <entry>
    <title>句子对齐开源代码解读</title>
    <url>/2020/04/15/%E5%8F%A5%E5%AD%90%E5%AF%B9%E9%BD%90%E5%BC%80%E6%BA%90%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/</url>
    <content><![CDATA[<p>最近需要根据句子对齐，给中英句对进行打分，因此看了一下相关的开源项目。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://blog.csdn.net/ykf173/article/details/86747592" target="_blank" rel="noopener">https://blog.csdn.net/ykf173/article/details/86747592</a><br><a href="http://www.cips-cl.org/static/anthology/CCL-2015/CCL-15-019.pdf" target="_blank" rel="noopener">http://www.cips-cl.org/static/anthology/CCL-2015/CCL-15-019.pdf</a></p>
</blockquote>
<h1 id="Gale和Church的句对齐算法"><a href="#Gale和Church的句对齐算法" class="headerlink" title="Gale和Church的句对齐算法"></a>Gale和Church的句对齐算法</h1><blockquote>
<p>参考：<br><a href="https://zhuanlan.zhihu.com/p/59071889" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/59071889</a><br><a href="https://github.com/NLPpupil/gale_and_church_align" target="_blank" rel="noopener">https://github.com/NLPpupil/gale_and_church_align</a><br><a href="https://www.aclweb.org/anthology/J93-1004.pdf" target="_blank" rel="noopener">https://www.aclweb.org/anthology/J93-1004.pdf</a></p>
</blockquote>
<p>Gale和Church在1993年提出了一个基于长度进行句对齐的算法，并在附录里公开了C源代码。这篇论文相当经典，以至于之后的关于句对齐的论文大多数要引用它。论文的题目是 A Program for Aligning Sentences in Bilingual Corpora。这个方法适合欧美语系，思想就是根据句子的长度来比较的。比较出名的hunalign工具是基于galechurch想法写的，并做了改进。Hunalign可以用于十几种语句的对齐，但是很遗憾，中文不太使用，但是也不是完全不适用，只是效果不太好。LF就是根据它做了一些小的改进，对其效果还可以。</p>
<p>对齐分两步。第一步是段落对其，第二步是在段落内部进行句对齐。段落对齐重要，不过简单，问题在于段落内部的句对齐。所以本文只解析已知段落对齐，怎样在段落内进行句对齐。首先定义几个概念，所有论文中出现的符号都对应定义里的符号。</p>
<ul>
<li><strong>句子</strong> 一个短的字符串。</li>
<li><strong>段落</strong> 语文里的自然段。分为源语言L1的段落和目标语言L2的段落，或称原文段落和译文段落。段落由一个序列的连续句子组成。</li>
<li><strong>片段</strong> 一个序列的连续的句子，是段落的子集。对应论文中的portion of text。</li>
<li><strong>片段对</strong> 原文片段和译文片段组成的对。</li>
<li>$l_1, l_2$分别对应片段对中原文部分和译文部分的字符总数。</li>
<li>$c, s^2$ 假设源语言中的一个字符在目标语言中对应的字符数是一个随机变量，且该随机变量服从正态分布 N(c, s^2) 。（如何估计可参考：<a href="https://www.zhihu.com/question/39080163" target="_blank" rel="noopener">https://www.zhihu.com/question/39080163</a>）</li>
<li>$\delta$ 论文中定义为 $\left(l_{2}-l_{1} c\right) / \sqrt{l_{1} s^{2}}$。每一个片段对都有自己的一个$\delta$$。</li>
<li><strong>对齐模式</strong> 或称<strong>匹配模式</strong>，描述一个句块对由几个原文句子和几个译文句子组成。比如1-2表示一个原文句子翻译成两个译文句子的对齐模式。</li>
<li><strong>match</strong> 对齐模式的概率分布。</li>
<li><strong>距离</strong>（distance measure） 衡量片段对两个片段之间的距离。距离度量是对$-\log (\operatorname{Prob}(\operatorname{match} | \delta))$的估计。当一个片段对确定后，我们就知道它的mathc和$\delta$。距离越大，此片段对对齐的概率越小。</li>
<li><strong>片段对序列</strong> 一个序列的片段对，这些片段对的原文部分的集合是原文段落的一个划分，译文部分的集合是译文段落的一个划分。</li>
<li><strong>距离和</strong> 距离和是片段对序列中所有片段对的</li>
<li><strong>对齐序列</strong> 距离和最小的片段对序列。</li>
</ul>
<p>对齐算法的输入是某一对相互对齐的段落，输出是对齐序列。接下来就变成了动态规划问题，类似最小编辑距离。片段对序列那么多，哪个是对齐序列呢？如果用穷举法，计算量太大，显然不现实。换个角度想，假设我们已经知道了对齐序列，用$D(i, j)$表示该对齐序列的距离和，其中$i$是原文段落最后一个句子的index，$j$是译文段落最后一个句子的index。对齐序列的距离和可以表示成最后一个片段对的距离加上去掉最后一个片段对的剩下的片段对序列的距离和（可以认为对齐序列的子序列也是对齐序列）。最后一个片段对有六种对齐模式，所以要对每种模式分情况讨论，选择结果最小的那个。动态规划的递归式就是这么来的。</p>
<p>$D(i, j)=\min \left\{\begin{array}{ccc}D(i, j-1) &amp; + &amp; d\left(0, t_{j} ; 0,0\right) \ D(i-1, j) &amp; + &amp; d\left(s_{i}, 0 ; 0,0\right) \ D(i-1, j-1) &amp; + &amp; d\left(s_{i}, t_{j} ; 0,0\right) \ D(i-1, j-2) &amp; + &amp; d\left(s_{i}, t_{j} ; 0, t_{j-1}\right) \ D(i-2, j-1) &amp; + &amp; d\left(s_{i}, t_{j} ; s_{i-1}, 0\right) \ D(i-2, j-2) &amp; + &amp; \left.d\left(s_{i}, t_{j};s_{i-1}, t_{j-1}\right)\right\}\end{array}\right.$</p>
<p>递归式的基础情况D(0,0)=0,通过递归式，我们可以求出对齐序列的距离和，在求距离和的过程中，我们顺便记录了对齐轨迹，也就是顺便求出了对齐序列。这就是算法的主干思想。下面就是它的主要代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import math</span><br><span class="line">import scipy.stats</span><br><span class="line"># 先使用统计的方法计算出对齐模式的概率分布</span><br><span class="line">match &#x3D; &#123;(1, 2): 0.023114355231143552,  </span><br><span class="line">         (1, 3): 0.0012165450121654502, </span><br><span class="line">         (2, 2): 0.006082725060827251, </span><br><span class="line">         (3, 1): 0.0006082725060827251, </span><br><span class="line">         (1, 1): 0.9422141119221411, </span><br><span class="line">         (2, 1): 0.0267639902676399&#125;</span><br><span class="line"># 源语言的一个字符对应于目标语言的字符数(正太分布)的均值</span><br><span class="line">c &#x3D; 1.467</span><br><span class="line"># 源语言的一个字符对应于目标语言的字符数(正太分布)的方差</span><br><span class="line">s2 &#x3D; 6.315</span><br><span class="line"></span><br><span class="line">def prob_delta(delta):</span><br><span class="line">    return scipy.stats.norm(0,1).cdf(delta) </span><br><span class="line"></span><br><span class="line">def length(sentence):</span><br><span class="line">    punt_list &#x3D; &#39;,.!?:;~，。！？：；～”“《》&#39;</span><br><span class="line">    sentence &#x3D; sentence</span><br><span class="line">    return sum(1 for char in sentence if char not in punt_list)</span><br><span class="line"></span><br><span class="line">def distance(partition1,partition2,match_prob):</span><br><span class="line">    l1 &#x3D; sum(map(length,partition1))</span><br><span class="line">    l2 &#x3D; sum(map(length,partition2))</span><br><span class="line">    try:</span><br><span class="line">        delta &#x3D; (l2-l1*c)&#x2F;math.sqrt(l1*s2)</span><br><span class="line">    except ZeroDivisionError:</span><br><span class="line">        return float(&#39;inf&#39;)</span><br><span class="line">    prob_delta_given_match &#x3D; 2*(1 - prob_delta(abs(delta)))    </span><br><span class="line">    try:</span><br><span class="line">        return - math.log(prob_delta_given_match) - math.log(match_prob)</span><br><span class="line">    except ValueError:</span><br><span class="line">        return float(&#39;inf&#39;)</span><br><span class="line"></span><br><span class="line">def align(para1,para2):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    输入两个句子序列，生成句对</span><br><span class="line">    句对是倒序的，从段落结尾开始向开头对齐</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    align_trace &#x3D; &#123;&#125; </span><br><span class="line">    for i in range(len(para1) + 1):</span><br><span class="line">        for j in range(len(para2) + 1):     </span><br><span class="line">            if i &#x3D;&#x3D; j &#x3D;&#x3D; 0:</span><br><span class="line">                align_trace[0, 0] &#x3D; (0, 0, 0) </span><br><span class="line">            else:</span><br><span class="line">                align_trace[i,j] &#x3D; (float(&#39;inf&#39;),0,0)</span><br><span class="line">                for (di, dj), match_prob in match.items():</span><br><span class="line">                    if i-di&gt;&#x3D;0 and j-dj&gt;&#x3D;0:</span><br><span class="line">                        align_trace[i,j] &#x3D; min(align_trace[i,j],(align_trace[i-di, j-dj][0] + distance(para1[i-di:i],para2[j-dj:j],match_prob), di, dj))</span><br><span class="line">                </span><br><span class="line">    i, j &#x3D; len(para1), len(para2)</span><br><span class="line">    while True:</span><br><span class="line">        (c, di, dj) &#x3D; align_trace[i, j]</span><br><span class="line">        if di &#x3D;&#x3D; dj &#x3D;&#x3D; 0:</span><br><span class="line">            break</span><br><span class="line">        yield &#39;&#39;.join(para1[i-di:i]), &#39;&#39;.join(para2[j-dj:j])</span><br><span class="line">        i -&#x3D; di</span><br><span class="line">        j -&#x3D; dj</span><br></pre></td></tr></table></figure>
<h1 id="Champollion"><a href="#Champollion" class="headerlink" title="Champollion"></a>Champollion</h1><blockquote>
<p>参考：<br><a href="https://www.aclweb.org/anthology/L06-1465/" target="_blank" rel="noopener">https://www.aclweb.org/anthology/L06-1465/</a><br><a href="https://www.aclweb.org/anthology/C10-2081.pdf" target="_blank" rel="noopener">https://www.aclweb.org/anthology/C10-2081.pdf</a><br><a href="https://github.com/LowResourceLanguages/champollion" target="_blank" rel="noopener">https://github.com/LowResourceLanguages/champollion</a></p>
</blockquote>
<p>Champollion是基于长度和词典的对齐算法，是中国人写的，对于中-英对齐比较好。相比于Gale&amp;Church这种对长度比较敏感（适合于英-法）的算法，Champollion更多关注到了内容。Champollion相比于其他算法的优点如下：</p>
<ul>
<li>Champollion假设输入有很大噪声（以往假设源语言与目标语言的match模式主要为1:1，但在中英语料中，句子对齐噪声非常大），使得删除和插入的次数变得很重要</li>
<li>与其他以词典为基础的算法不同，每个词根据对齐的重要性不同赋予了不同的权重。文中举例如下：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a. Marketplace bombing kills 23 in Iraq</span><br><span class="line">b. 伊拉克 集市 爆炸 造成 23 人 死亡</span><br></pre></td></tr></table></figure>
<p>在这个例子中，(23, 23)这个pair相比于(Iraq, 伊拉克)更重要。因为(Iraq, 伊拉克)相比于(23, 23)更经常出现。<br>Champollion因此赋予更少出现的translation pair更大的权重。</p>
<ul>
<li>Champollion对于每个segment pair（每个segment有1～多句话）进行打分，对于非1～1对齐进行了惩罚</li>
</ul>
<h2 id="句子相似度计算"><a href="#句子相似度计算" class="headerlink" title="句子相似度计算"></a>句子相似度计算</h2><p>论文将计算相似度问题转化为检索系统中计算query跟document相似度的问题。计算stf=segment term frequency（某个term在一个segment中出现的次数），定义$\text {idtf}=\frac{T}{ \text {occurences}_{-} \text {in}_{-} \text {the}_{-} \text {document}}$，其中T是document中的总term数。stf衡量term在segment中重要性，idtf衡量term在document中重要性。 Stf-idtf衡量了一个translate-pair对两个segment对齐的重要性。</p>
<p>Champollion将两个segment看成对顺序不敏感的词袋：$\begin{array}{l}E=\left\{e_{1}, e_{2}, \ldots, e_{m-1}, e_{m}\right\} \ C=\left\{c_{1}, c_{2}, \ldots, c_{n-1}, c_{n}\right\}\end{array}$。</p>
<p>定义k个在两个segment中出现的translate-pair：$P=\left\{\left(e_{1}^{\prime}, c_{1}^{\prime}\right),\left(e_{2}^{\prime}, c_{2}^{\prime}\right) \ldots\left(e_{k}^{\prime}, c_{k}^{\prime}\right)\right\}$，则E和C的相似度定义为：</p>
<p>$\begin{array}{l}\operatorname{sim}(E, C)=\sum_{i=1}^{k} \lg \left(\operatorname{stf}\left(e_{i}^{\prime}, c_{i}^{\prime}\right)^{<em>} i d t f\left(e_{i}^{\prime}\right)\right. \ </em> \text { alignment }_{-} \text {penalty } \ \text { +length_penalty }(E, C)\end{array}$</p>
<p>其中alignment_penalty是一个[0, 1]的值，对于1-1的对齐其值为1。length_penalty是一个函数，对于长度不匹配的翻译要进行一下惩罚。</p>
<h2 id="动态规划算法"><a href="#动态规划算法" class="headerlink" title="动态规划算法"></a>动态规划算法</h2><p>Champollion允许1-0, 0-1, 1-1, 2-1, 1-2, 1-3, 3-1, 1-4 和 4-1对齐，其动态规划转移方程为：</p>
<p>$S(i, j)=\max \left\{\begin{array}{c}S(i-1, j)+\operatorname{sim}(i, \phi) \ S(i, j-1)+\operatorname{sim}(\phi, j) \ S(i-1, j-1)+\operatorname{sim}(i, j) \ S(i-1, j-2)+\operatorname{sim}(i, j-1) \ S(i-2, j-1)+\operatorname{sim}(i-1, j) \ S(i-2, j-2)+\operatorname{sim}(i-1, j-1) \ S(i-1, j-3)+\operatorname{sim}(i, j-2) \ S(i-3, j-1)+\operatorname{sim}(i-2, j) \ S(i-1, j-4)+\operatorname{sim}(i, j-3) \ S(i-4, j-1)+\operatorname{sim}(i-3, j)\end{array}\right.$</p>
<h1 id="YALIGN"><a href="#YALIGN" class="headerlink" title="YALIGN"></a>YALIGN</h1><blockquote>
<p>参考：<br><a href="https://github.com/machinalis/yalign" target="_blank" rel="noopener">https://github.com/machinalis/yalign</a><br><a href="https://mailman.uib.no/public/corpora/2013-September/018912.html" target="_blank" rel="noopener">https://mailman.uib.no/public/corpora/2013-September/018912.html</a></p>
</blockquote>
<p>Yalign工具使用了一下，但效果不太好，它主要提供了两个功能：</p>
<ul>
<li>一个句子相似度度量：给定两个句子，它就会对这两个句子相互翻译的可能性产生一个粗略的估计(0到1之间的一个数字)。</li>
<li>一个序列对齐工具：这样给定两个文档(一个句子列表)，它产生一个对齐，最大化单个(每个句子对)相似度的总和。所以Yalign的主要算法实际上是一个标准序列对齐算法的包装。</li>
</ul>
<p>在序列对齐上，Yalign使用Needleman-Wunch算法的一个变体来查找两个给定文档中的句子之间的最佳对齐。它带来的优点是，使该算法具有多项式时间最坏情况的复杂性，并产生最优对齐。反之其缺点是不能处理相互交叉的对齐或从两个句子到一个句子的对齐。关于Needleman-Wunch算法可参考<a href="https://zhuanlan.zhihu.com/p/26212767" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26212767</a>。</p>
<p>对齐之后，只有翻译概率高的句子才会被包含在最终的对齐中。也就是说，有些结果会被过滤，以提供高质量的校准。使用一个阈值以便在句子相似度度量足够差时排除该对。</p>
<p>对于句子相似度度量，Yalign使用统计分类器的似然输出，并将其调整为0-1范围。分类器被训练来确定一对句子是否互相翻译。Yalign使用支持向量机作为分类器，对齐的质量不仅取决于输入，还取决于经过训练的分类器的质量。</p>
<p>下面是它的一些重要函数或类的说明：</p>
<ul>
<li>Sentence: 训练数据的基本类型，继承list</li>
<li>input_conversation.py：将文本/tmx/html格式的训练数据转化为Sentence</li>
<li>YalignModel: 主类，配合basic_model函数进行模型训练、导入、预测</li>
<li>SentencePairScore：定义句对的特征并进行打分</li>
<li>SequenceAligner：序列对齐类</li>
<li>SVMClassifier：训练一个支持向量机</li>
<li>WordPairScore：词及词翻译的概率，可以使用fast-align获取s</li>
</ul>
<h1 id="Bleualign"><a href="#Bleualign" class="headerlink" title="Bleualign"></a>Bleualign</h1><blockquote>
<p>参考：<br><a href="https://www.aclweb.org/anthology/W11-4624.pdf" target="_blank" rel="noopener">https://www.aclweb.org/anthology/W11-4624.pdf</a><br><a href="https://github.com/rsennrich/Bleualign" target="_blank" rel="noopener">https://github.com/rsennrich/Bleualign</a></p>
</blockquote>
<p>Bleualign借助机器翻译的结果进行对齐。使用机器翻译的目的是用目标语表示原文的大概意思，然后和译文进行比较，其算法的主要流程如下：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%8F%A5%E5%AD%90%E5%AF%B9%E9%BD%90/1.png" alt="图片"></p>
<p>Bleualign没使用过，不知道实际效果怎么样。</p>
<h1 id="Vecalign"><a href="#Vecalign" class="headerlink" title="Vecalign"></a>Vecalign</h1><blockquote>
<p>参考：<br><a href="https://github.com/thompsonb/vecalign" target="_blank" rel="noopener">https://github.com/thompsonb/vecalign</a><br>Vecalign: Improved Sentence Alignment in Linear Time and Space（<a href="https://www.aclweb.org/anthology/D19-1136.pdf）" target="_blank" rel="noopener">https://www.aclweb.org/anthology/D19-1136.pdf）</a></p>
</blockquote>
<p>vecalign计算句子相似度的方法跟之前不同，它使用了facebook开源的laser embedding进行句子相似度计算，计算公式如下：$c(x, y)=\frac{(1-\cos (x, y))_{\text {nSents }}(x) \text { nSents }(y)}{\sum_{s=1}^{S} 1-\cos \left(x, y_{s}\right)+\sum_{s=1}^{S} 1-\cos \left(x_{s}, y\right)}$，且对于非1-1对齐采取了一定的惩罚。下面是它的核心代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def vecalign(vecs0,</span><br><span class="line">             vecs1,</span><br><span class="line">             final_alignment_types,</span><br><span class="line">             del_percentile_frac,</span><br><span class="line">             width_over2,</span><br><span class="line">             max_size_full_dp,</span><br><span class="line">             costs_sample_size,</span><br><span class="line">             num_samps_for_norm,</span><br><span class="line">             norms0&#x3D;None,</span><br><span class="line">             norms1&#x3D;None):</span><br><span class="line">    if width_over2 &lt; 3:</span><br><span class="line">        logger.warning(</span><br><span class="line">            &#39;width_over2 was set to %d, which does not make sense. &#39;</span><br><span class="line">            &#39;increasing to 3.&#39;, width_over2)</span><br><span class="line">        width_over2 &#x3D; 3</span><br><span class="line"></span><br><span class="line">    # make sure input embeddings are norm&#x3D;&#x3D;1</span><br><span class="line">    make_norm1(vecs0)</span><br><span class="line">    make_norm1(vecs1)</span><br><span class="line"></span><br><span class="line">    # save off runtime stats for summary</span><br><span class="line">    runtimes &#x3D; OrderedDict()</span><br><span class="line"></span><br><span class="line">    # Determine stack depth</span><br><span class="line">    s0, s1 &#x3D; vecs0.shape[1], vecs1.shape[1]</span><br><span class="line">    max_depth &#x3D; 0</span><br><span class="line">    while s0 * s1 &gt; max_size_full_dp ** 2:</span><br><span class="line">        max_depth +&#x3D; 1</span><br><span class="line">        s0 &#x3D; s0 &#x2F;&#x2F; 2</span><br><span class="line">        s1 &#x3D; s1 &#x2F;&#x2F; 2</span><br><span class="line"></span><br><span class="line">    # init recursion stack</span><br><span class="line">    # depth is 0-based (full size is 0, 1 is half, 2 is quarter, etc)</span><br><span class="line">    stack &#x3D; &#123;0: &#123;&#39;v0&#39;: vecs0, &#39;v1&#39;: vecs1&#125;&#125;</span><br><span class="line"></span><br><span class="line">    # downsample sentence vectors</span><br><span class="line">    t0 &#x3D; time()</span><br><span class="line">    for depth in range(1, max_depth + 1):</span><br><span class="line">        stack[depth] &#x3D; &#123;&#39;v0&#39;: downsample_vectors(stack[depth - 1][&#39;v0&#39;]),</span><br><span class="line">                        &#39;v1&#39;: downsample_vectors(stack[depth - 1][&#39;v1&#39;])&#125;</span><br><span class="line">    runtimes[&#39;Downsample embeddings&#39;] &#x3D; time() - t0</span><br><span class="line"></span><br><span class="line">    # compute norms for all depths, add sizes, add alignment types</span><br><span class="line">    t0 &#x3D; time()</span><br><span class="line">    for depth in stack:</span><br><span class="line">        stack[depth][&#39;size0&#39;] &#x3D; stack[depth][&#39;v0&#39;].shape[1]</span><br><span class="line">        stack[depth][&#39;size1&#39;] &#x3D; stack[depth][&#39;v1&#39;].shape[1]</span><br><span class="line">        if depth &#x3D;&#x3D; 0:</span><br><span class="line">            stack[depth][&#39;alignment_types&#39;] &#x3D; final_alignment_types</span><br><span class="line">        else:</span><br><span class="line">            stack[depth][&#39;alignment_types&#39;] &#x3D; [(1, 1)]</span><br><span class="line"></span><br><span class="line">        if depth &#x3D;&#x3D; 0 and norms0 is not None:</span><br><span class="line">            if norms0.shape !&#x3D; vecs0.shape[:2]:</span><br><span class="line">                print(&#39;norms0.shape:&#39;, norms0.shape)</span><br><span class="line">                print(&#39;vecs0.shape[:2]:&#39;, vecs0.shape[:2])</span><br><span class="line">                raise Exception(&#39;norms0 wrong shape&#39;)</span><br><span class="line">            stack[depth][&#39;n0&#39;] &#x3D; norms0</span><br><span class="line">        else:</span><br><span class="line">            stack[depth][&#39;n0&#39;] &#x3D; compute_norms(</span><br><span class="line">                stack[depth][&#39;v0&#39;], stack[depth][&#39;v1&#39;], num_samps_for_norm)</span><br><span class="line"></span><br><span class="line">        if depth &#x3D;&#x3D; 0 and norms1 is not None:</span><br><span class="line">            if norms1.shape !&#x3D; vecs1.shape[:2]:</span><br><span class="line">                print(&#39;norms1.shape:&#39;, norms1.shape)</span><br><span class="line">                print(&#39;vecs1.shape[:2]:&#39;, vecs1.shape[:2])</span><br><span class="line">                raise Exception(&#39;norms1 wrong shape&#39;)</span><br><span class="line">            stack[depth][&#39;n1&#39;] &#x3D; norms1</span><br><span class="line">        else:</span><br><span class="line">            stack[depth][&#39;n1&#39;] &#x3D; compute_norms(</span><br><span class="line">                stack[depth][&#39;v1&#39;], stack[depth][&#39;v0&#39;], num_samps_for_norm)</span><br><span class="line"></span><br><span class="line">    runtimes[&#39;Normalize embeddings&#39;] &#x3D; time() - t0</span><br><span class="line"></span><br><span class="line">    # Compute deletion penalty for all depths</span><br><span class="line">    t0 &#x3D; time()</span><br><span class="line">    for depth in stack:</span><br><span class="line">        stack[depth][&#39;del_knob&#39;] &#x3D; make_del_knob(</span><br><span class="line">            e_laser&#x3D;stack[depth][&#39;v0&#39;][0, :, :],</span><br><span class="line">            f_laser&#x3D;stack[depth][&#39;v1&#39;][0, :, :],</span><br><span class="line">            e_laser_norms&#x3D;stack[depth][&#39;n0&#39;][0, :],</span><br><span class="line">            f_laser_norms&#x3D;stack[depth][&#39;n1&#39;][0, :],</span><br><span class="line">            sample_size&#x3D;costs_sample_size)</span><br><span class="line">        stack[depth][&#39;del_penalty&#39;] &#x3D; \</span><br><span class="line">            stack[depth][&#39;del_knob&#39;].percentile_frac_to_del_penalty(</span><br><span class="line">                del_percentile_frac)</span><br><span class="line">        logger.debug(&#39;del_penalty at depth %d: %f&#39;,</span><br><span class="line">                     depth, stack[depth][&#39;del_penalty&#39;])</span><br><span class="line">    runtimes[&#39;Compute deletion penalties&#39;] &#x3D; time() - t0</span><br><span class="line">    tt &#x3D; time() - t0</span><br><span class="line">    logger.debug(</span><br><span class="line">        &#39;%d x %d full DP make features: %.6fs (%.3e per dot product)&#39;,</span><br><span class="line">        stack[max_depth][&#39;size0&#39;], stack[max_depth][&#39;size1&#39;], tt,</span><br><span class="line">        tt &#x2F; (stack[max_depth][&#39;size0&#39;] + 1e-6) &#x2F;</span><br><span class="line">        (stack[max_depth][&#39;size1&#39;] + 1e-6))</span><br><span class="line">    # full DP at maximum recursion depth</span><br><span class="line">    t0 &#x3D; time()</span><br><span class="line">    stack[max_depth][&#39;costs_1to1&#39;] &#x3D; make_dense_costs(stack[max_depth][&#39;v0&#39;],</span><br><span class="line">                                                      stack[max_depth][&#39;v1&#39;],</span><br><span class="line">                                                      stack[max_depth][&#39;n0&#39;],</span><br><span class="line">                                                      stack[max_depth][&#39;n1&#39;])</span><br><span class="line"></span><br><span class="line">    runtimes[&#39;Full DP make features&#39;] &#x3D; time() - t0</span><br><span class="line">    t0 &#x3D; time()</span><br><span class="line">    _, stack[max_depth][&#39;x_y_tb&#39;] &#x3D; dense_dp(</span><br><span class="line">        stack[max_depth][&#39;costs_1to1&#39;], stack[max_depth][&#39;del_penalty&#39;])</span><br><span class="line">    stack[max_depth][&#39;alignments&#39;] &#x3D; dense_traceback(</span><br><span class="line">        stack[max_depth][&#39;x_y_tb&#39;])</span><br><span class="line">    runtimes[&#39;Full DP&#39;] &#x3D; time() - t0</span><br><span class="line"></span><br><span class="line">    # upsample the path up to the top resolution</span><br><span class="line">    compute_costs_times &#x3D; []</span><br><span class="line">    dp_times &#x3D; []</span><br><span class="line">    upsample_depths &#x3D; [0, ] if max_depth &#x3D;&#x3D; 0 else list(</span><br><span class="line">        reversed(range(0, max_depth)))</span><br><span class="line">    for depth in upsample_depths:</span><br><span class="line">        if max_depth &gt; 0:  # upsample previoius alignment to current resolution</span><br><span class="line">            course_alignments &#x3D; upsample_alignment(</span><br><span class="line">                stack[depth + 1][&#39;alignments&#39;])</span><br><span class="line">            # features may have been truncated when downsampleing,</span><br><span class="line">            # so alignment may need extended</span><br><span class="line">            extend_alignments(</span><br><span class="line">                course_alignments, stack[depth][&#39;size0&#39;],</span><br><span class="line">                stack[depth][&#39;size1&#39;])  # in-place</span><br><span class="line">        else:</span><br><span class="line">            # We did a full size 1-1 search,</span><br><span class="line">            # so search same size with more alignment types</span><br><span class="line">            course_alignments &#x3D; stack[0][&#39;alignments&#39;]</span><br><span class="line"></span><br><span class="line">        # convert couse alignments to a searchpath</span><br><span class="line">        stack[depth][&#39;searchpath&#39;] &#x3D; alignment_to_search_path(</span><br><span class="line">            course_alignments)</span><br><span class="line"></span><br><span class="line">        # compute ccosts for sparse DP</span><br><span class="line">        t0 &#x3D; time()</span><br><span class="line">        stack[depth][&#39;a_b_costs&#39;], stack[depth][&#39;b_offset&#39;] &#x3D; \</span><br><span class="line">            make_sparse_costs(stack[depth][&#39;v0&#39;], stack[depth][&#39;v1&#39;],</span><br><span class="line">                              stack[depth][&#39;n0&#39;], stack[depth][&#39;n1&#39;],</span><br><span class="line">                              stack[depth][&#39;searchpath&#39;],</span><br><span class="line">                              stack[depth][&#39;alignment_types&#39;],</span><br><span class="line">                              width_over2)</span><br><span class="line"></span><br><span class="line">        tt &#x3D; time() - t0</span><br><span class="line">        num_dot_products &#x3D; len(stack[depth][&#39;b_offset&#39;]) * \</span><br><span class="line">            len(stack[depth][&#39;alignment_types&#39;]) * width_over2 * 2</span><br><span class="line">        logger.debug(&#39;%d x %d sparse DP (%d alignment types, %d window) &#39;</span><br><span class="line">                     &#39;make features: %.6fs (%.3e per dot product)&#39;,</span><br><span class="line">                     stack[max_depth][&#39;size0&#39;], stack[max_depth][&#39;size1&#39;],</span><br><span class="line">                     len(stack[depth][&#39;alignment_types&#39;]), width_over2 * 2,</span><br><span class="line">                     tt, tt &#x2F; (num_dot_products + 1e6))</span><br><span class="line"></span><br><span class="line">        compute_costs_times.append(time() - t0)</span><br><span class="line">        t0 &#x3D; time()</span><br><span class="line">        # perform sparse DP</span><br><span class="line">        stack[depth][&#39;a_b_csum&#39;], stack[depth][&#39;a_b_xp&#39;], \</span><br><span class="line">            stack[depth][&#39;a_b_yp&#39;], stack[depth][&#39;new_b_offset&#39;] &#x3D; \</span><br><span class="line">            sparse_dp(</span><br><span class="line">                stack[depth][&#39;a_b_costs&#39;], stack[depth][&#39;b_offset&#39;],</span><br><span class="line">                stack[depth][&#39;alignment_types&#39;], stack[depth][&#39;del_penalty&#39;],</span><br><span class="line">                stack[depth][&#39;size0&#39;], stack[depth][&#39;size1&#39;])</span><br><span class="line"></span><br><span class="line">        # performace traceback to get alignments and alignment scores</span><br><span class="line">        # for debugging, avoid overwriting stack[depth][&#39;alignments&#39;]</span><br><span class="line">        akey &#x3D; &#39;final_alignments&#39; if depth &#x3D;&#x3D; 0 else &#39;alignments&#39;</span><br><span class="line">        stack[depth][akey], stack[depth][&#39;alignment_scores&#39;] &#x3D; \</span><br><span class="line">            sparse_traceback(stack[depth][&#39;a_b_csum&#39;],</span><br><span class="line">                             stack[depth][&#39;a_b_xp&#39;],</span><br><span class="line">                             stack[depth][&#39;a_b_yp&#39;],</span><br><span class="line">                             stack[depth][&#39;new_b_offset&#39;],</span><br><span class="line">                             stack[depth][&#39;size0&#39;],</span><br><span class="line">                             stack[depth][&#39;size1&#39;])</span><br><span class="line">        dp_times.append(time() - t0)</span><br><span class="line"></span><br><span class="line">    runtimes[&#39;Upsample DP compute costs&#39;] &#x3D; sum(compute_costs_times[:-1])</span><br><span class="line">    runtimes[&#39;Upsample DP&#39;] &#x3D; sum(dp_times[:-1])</span><br><span class="line"></span><br><span class="line">    runtimes[&#39;Final DP compute costs&#39;] &#x3D; compute_costs_times[-1]</span><br><span class="line">    runtimes[&#39;Final DP&#39;] &#x3D; dp_times[-1]</span><br><span class="line">    return stack</span><br></pre></td></tr></table></figure>
<p>其他相关资源</p>
<ul>
<li><a href="https://github.com/cocoxu/Shakespeare/tree/master/bilingual-sentence-aligner" target="_blank" rel="noopener">https://github.com/cocoxu/Shakespeare/tree/master/bilingual-sentence-aligner</a></li>
<li>[<a href="https://github.com/loomchild/maligna](" target="_blank" rel="noopener">https://github.com/loomchild/maligna](</a></li>
</ul>
]]></content>
      <tags>
        <tag>句子对齐</tag>
      </tags>
  </entry>
  <entry>
    <title>HMM之——基础学习</title>
    <url>/2020/04/12/HMM%E4%B9%8B%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>最近工作中经常要用到HMM，所以专门来复习下，主要讲解的是HMM的Forward和Backward算法，以及参数估计算法。</p>
<a id="more"></a>
<h1 id="HMM总览"><a href="#HMM总览" class="headerlink" title="HMM总览"></a>HMM总览</h1><p>HMM是一个时序的模型，每时每刻都有一个观测者（observation，下图中绿色点，我们已知的）和一个隐式变量（latent variable，下图中灰色点，我们未知的）。下图为一个HMM基本模型，每一条边都是有方向的，隐式变量可以理解为一个状态，每个状态下都会有一个可以观测到的值。横线表示了状态的转移，竖线表示了从状态到观测的生产过程。因此，HMM是一个有向的生成模型（生成观测者），当然也可以把HMM做成一个判别模型。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/hmm-1/1.png" alt="图片"></p>
<h1 id="Forward-Backward算法"><a href="#Forward-Backward算法" class="headerlink" title="Forward/Backward算法"></a>Forward/Backward算法</h1><p>HMM中最参数估计使用的是Forward/Backward算法，解码使用的是Viterbi算法。首先来看一下Forward/Backward算法。</p>
<p>F/B算法的主要目的是计算给定观测值x时，其中某个给定的$z_k$的概率值是多少，即$P(z_k|x)$。Forward算法的目的是计算联合概率$P(z_k, x_{1:k})$，其中$x_1:k = (x_1, x_2, …, x_k)$。Backward算法的目的是计算条件概率$P(x_{k+1:n}|z_k)$。那么为什么要用Forward和Backward算法呢？事实上，我们估计HMM参数时，需要计算$P(z_k|x)$，而$P(z_k|x)$可以拆分成$P(z_k, x_{1:k})$和$P(x_{k+1:n}|z_k)$这两项，具体推导如下：</p>
<ul>
<li>根据贝叶斯定理：$P(z_k|x) = P(z_k, x) / P(x)$，其中$P(x)$对任何$z_k$都是等同的，因此可以认为$P(z_k|x)$是正比于$P(z_k, x)$的。在进行计算时，要注意归一化，即$P(z_k|x) = P(z_k, x) / sum(P(z=j, x))$。</li>
<li>下面我们把$P(z_k, x)$拆分成Forward和Backward的形式<ul>
<li>根据贝叶斯定理：$P(z_k, x) = P(x_{k+1:n}|z_k, x_{1:k}) * P(z_k, x_{1:k})$，其中第二项就是我们的Forward算法</li>
<li>我们看上式中第一项，发现跟Backward算法就差了一个$x_{1:k}$，由于$x_{1:k}$和$x_{k+1:n}$条件独立于$z_k$的（即$x_{1:k}$全部作用在$z_k$上，而不会对$x_{k+1:n}$产生影响）。因此$P(x_{k+1:n}|z_k, x_{1:k}) = P(x_{k+1:n}|z_k) * P(z_k, x_{1:k})$，其中$P(x_{k+1:n}|z_k)$就是Backward算法，$P(z_k, x_{1:k})$是Forward算法</li>
</ul>
</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/hmm-1/2png.png" alt="图片"></p>
<h2 id="Forward算法"><a href="#Forward算法" class="headerlink" title="Forward算法"></a>Forward算法</h2><p>Forward本质是一个动态规划算法，其目标是计算$P(z_k, x_{1:k})$。既然用动归，我们就要想清楚如何构造$P(z_k, x_{1:k})$的子问题。下面讲一下构造动归的具体思路：</p>
<ul>
<li>我们想构造出：<img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/hmm-1/3.png" alt="图片"></li>
<li>为了引入$z_{k-1}$，考虑使用边缘化的性质：<img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/hmm-1/4.png" alt="图片"><ul>
<li>关于边缘化，可参考：<a href="https://cloud.tencent.com/developer/article/1096441" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1096441</a></li>
</ul>
</li>
<li>继续推导可得出：</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/hmm-1/5.png" alt="图片"></p>
<ul>
<li>公式化简时使用了条件独立性，判断是否条件独立可以使用D-seperation方法</li>
<li>状态的初始化如下：</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/hmm-1/6.png" alt="图片"></p>
<h2 id="Backward算法"><a href="#Backward算法" class="headerlink" title="Backward算法"></a>Backward算法</h2><p>Backward算法就是Forward的相反方向，解决问题的思路也是一样的。其目标是计算$P(x_{k+1:n}|z_k)​$，跟Forward一样，我们要把它拆分成更小的子问题，推导过程如下：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/hmm-1/16.png" alt="图片"></p>
<h1 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h1><p>HMM参数估计涉及到的参数包括：</p>
<ul>
<li>A：状态到状态的转移矩阵，假设状态有m个，则$A.shape = m * m$</li>
<li>B：状态到观测值的发射矩阵，假设观测值为单词，且词表大小为V，则$B.shape = m * V$</li>
<li>pi：初始状态值，$pi.shape = m * 1$</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/hmm-1/8png.png" alt="图片"></p>
<h2 id="Complete情况"><a href="#Complete情况" class="headerlink" title="Complete情况"></a>Complete情况</h2><p>Complete case指的是每一个latent variable是知道的，如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/hmm-1/9.png" alt="图片"></p>
<h3 id="估计pi"><a href="#估计pi" class="headerlink" title="估计pi"></a>估计pi</h3><p>统计每个状态初始出现的次数，如上图中有3个状态，次数分别为[2, 1, 0]，转换成概率为[2/3, 1/3, 0]。</p>
<h3 id="估计状态转移概率A"><a href="#估计状态转移概率A" class="headerlink" title="估计状态转移概率A"></a>估计状态转移概率A</h3><p>统计状态间的转移瓷土，上图中的HMM统计如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">1</th>
<th style="text-align:center">2</th>
<th style="text-align:center">3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">2</td>
<td style="text-align:center">1</td>
<td style="text-align:center">2</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">1</td>
<td style="text-align:center">2</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">0</td>
<td style="text-align:center">2</td>
<td style="text-align:center">1</td>
</tr>
</tbody>
</table>
</div>
<p>转换成概率值：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">1</th>
<th style="text-align:center">2</th>
<th style="text-align:center">3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">2/5</td>
<td style="text-align:center">1/5</td>
<td style="text-align:center">2/5</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">1/4</td>
<td style="text-align:center">2/4</td>
<td style="text-align:center">1/4</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">0</td>
<td style="text-align:center">2/3</td>
<td style="text-align:center">1/3</td>
</tr>
</tbody>
</table>
</div>
<h3 id="估计发射概率B"><a href="#估计发射概率B" class="headerlink" title="估计发射概率B"></a>估计发射概率B</h3><p>统计每个状态下，看到观测值的数目，上图中HMM的统计如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">a</th>
<th style="text-align:center">b</th>
<th style="text-align:center">c</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">3</td>
<td style="text-align:center">2</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">3</td>
<td style="text-align:center">0</td>
<td style="text-align:center">3</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">1</td>
<td style="text-align:center">2</td>
<td style="text-align:center">1</td>
</tr>
</tbody>
</table>
</div>
<p>转换成概率值如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">a</th>
<th style="text-align:center">b</th>
<th style="text-align:center">c</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">3/5</td>
<td style="text-align:center">2/5</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">1/2</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1/2</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">1/4</td>
<td style="text-align:center">1/2</td>
<td style="text-align:center">1/4</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Incomplete情况"><a href="#Incomplete情况" class="headerlink" title="Incomplete情况"></a>Incomplete情况</h2><p>Incomplete case指的是latent variable，即z，是不知道的。那么这种情况下怎么计算z呢？如果我们知道A、B、$\pi$这三个参数的话，就可以用Forward/Backward算法计算z的期望值，即$P(z_k|x)$，如下图中红色：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/hmm-1/10.png" alt="图片"></p>
<p>上图中红色部分，每个概率值我们可以认为是一个expectation count，也就是说在上图情况下，第1个状态出现了0.8词，第2个状态出现了0.1次，第3个状态出现了0.1次。</p>
<p>那么反过来，如果已知了z的期望值，通过上面统计的方式我们可以计算出参数值。我们可以用这个参数值，再去更新z的期望值，如此循环，直到收敛。一般情况下，我们是先初始化z，在通过z计算参数。</p>
<h3 id="估计-pi"><a href="#估计-pi" class="headerlink" title="估计$\pi$"></a>估计$\pi$</h3><p>计算$P(z_1|x)$，比如对于$x_1$, $p(z_1=1|x) = 0.7$, $p(z_1=2|x) = 0.2$, $p(z_1=3|x) = 0.3$，这里的概率值我们认为是出现的次数就可以。如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/hmm-1/11.png" alt="图片"></p>
<p>接着我们统计每个状态出现的总数：[0.7 + 0.4 + 0.6, 0.2 + 0.4 + 0.3, 0.1 + 0.2 + 0.1] = [1.7, 0.9, 0.4]，转变为概率为[1.7/3, 0.9/3, 0.4/3]。</p>
<h3 id="估计状态转移概率A-1"><a href="#估计状态转移概率A-1" class="headerlink" title="估计状态转移概率A"></a>估计状态转移概率A</h3><p>A的估计跟语言模型很像。语言模型中，计算bigram概率$P(w_j|w_i)=c(w_i, w_j)/c(w_i)$，其中$c(w_i, w_j)$表示一种联合状态，即$w_i$、$w_j$共现的次数。</p>
<p>在HMM中，我们要计算的是$P(z_k=i, z_{k+1}=j|x)$，而这个值可以使用Forward/Backward计算出来：</p>
<ul>
<li>先计算$P(z_k=1, z_{k+1}=1, x)$、$P(z_k=1, z_{k+1}=2, x)$…$P(z_k=m, z_{k+1}=m, x)$，这是一个$m * m$的矩阵</li>
<li>$P(z_k=i, z_{k+1}=j|x)$是正比于$P(z_k=i, z_{k+1}=j, x)$的</li>
<li>$P(z_k=i, z_{k+1}=j, x) = P(z_k=i, z_{k+1}=j, x_{1:k}, x_{k+1}, x_{k+2:n})$</li>
<li>根据D-separation（如下图），上式可以写成：$P(z_k, x_{1:k}) <em> P(x_{k+2:n}|z_{k+1}) </em> P(z_{k+1}|z_k) * P(x_{k+1}|z_{k+1})$，由此我们把联合概率转化成了条件概率</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/hmm-1/12.png" alt="图片"></p>
<ul>
<li>上式中$P(z_k, x_{1:k})$是Forward算式，$P(x_{k+2:n}|z_{k+1}) $是Backward算式，$P(z_{k+1}|z_k)$是状态转移，$P(x_{k+1}|z_{k+1})$是发射概率</li>
</ul>
<p>举一个具体的例子（A_{12}、A_{13}同理，我们想得到的就是A_{ij}矩阵）：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/hmm-1/13png.png" alt="图片"></p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/hmm-1/14.png" alt="图片"></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">1</th>
<th style="text-align:center">2</th>
<th style="text-align:center">3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">…</td>
<td style="text-align:center">1.72/6</td>
<td style="text-align:center">(0.3+0.2+0.1+0.3+0.4+0.2+0.2+0.1+0.2+0.1+0.01+0.1)/(0.6+.5+0.6+0.7+0.6+0.5+0.4+0.3+0.6+0.3+0.5+0.1+0.3)=2.27/6</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
</tr>
</tbody>
</table>
</div>
<p>转换成概率值：</p>
<h3 id="估计发射概率B-1"><a href="#估计发射概率B-1" class="headerlink" title="估计发射概率B"></a>估计发射概率B</h3><p>计算$P(z_k|x)$，如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/hmm-1/15.png" alt="图片"></p>
<p>统计每个状态下，看到观测值的数目，上图中HMM的统计如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">a</th>
<th style="text-align:center">b</th>
<th style="text-align:center">c</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">0.6+0.4+0.1+0.3+0.3=1.7</td>
<td style="text-align:center">2.9</td>
<td style="text-align:center">1.6</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">1.7</td>
<td style="text-align:center">2.3</td>
<td style="text-align:center">0.7</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">1.6</td>
<td style="text-align:center">1.8</td>
<td style="text-align:center">0.7</td>
</tr>
</tbody>
</table>
</div>
<p>转换成概率值如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">a</th>
<th style="text-align:center">b</th>
<th style="text-align:center">c</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">1.7/6.2</td>
<td style="text-align:center">2.9/6.2</td>
<td style="text-align:center">1.6/6.2</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">1.7/4.7</td>
<td style="text-align:center">2.3/4.7</td>
<td style="text-align:center">0.7/4.7</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">1.6/4.1</td>
<td style="text-align:center">1.8/4.1</td>
<td style="text-align:center">0.7/4.1</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <tags>
        <tag>HMM</tag>
      </tags>
  </entry>
  <entry>
    <title>令人头大之IBM Model</title>
    <url>/2020/03/29/%E4%BB%A4%E4%BA%BA%E5%A4%B4%E5%A4%A7%E4%B9%8BIBM-Model/</url>
    <content><![CDATA[<p>最近一段时间工作中，急需补充giza、fast align算法的背后原理，因此集中补一补这些令人头大的算法。本来打算看完IBM-Model1~Model5和HMM，但后来卡到了Model-3上，准备在后续的博客中继续更新。因此本篇将重点介绍Model-1~Model2。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://wenku.baidu.com/view/4cda374769eae009581becd2.html" target="_blank" rel="noopener">https://wenku.baidu.com/view/4cda374769eae009581becd2.html</a><br><a href="https://www.cl.uni-heidelberg.de/courses/ss15/smt/scribe2.pdf" target="_blank" rel="noopener">https://www.cl.uni-heidelberg.de/courses/ss15/smt/scribe2.pdf</a></p>
</blockquote>
<h1 id="词对齐算法之IBM-Model"><a href="#词对齐算法之IBM-Model" class="headerlink" title="词对齐算法之IBM Model"></a>词对齐算法之IBM Model</h1><p>假设任意一个英语句子e和一个法语句子f，定义f翻译成e的概率为Pr(e|f)，其归一化条件为$\sum_{e} \operatorname{Pr}(e | f)=1$，于是将f翻译成e的问题就变成求解$\hat{e}=\operatorname{argmax} \operatorname{Pr}(e | f)$。我们可以把它理解成一个噪声信道模型，假设我们看到的源语言文本F是由一段目标语言文本E经过某种奇怪的编码得到的，那么翻译的目标就是要将F还原成E，这也就是就是一个解码的过程，如下图：<img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ibmmodel/1.png" alt="图片"></p>
<p>表达成公式如下：$\mathrm{E}=\arg \max _{\mathrm{E}} P(\mathrm{E}) P(\mathrm{F} | \mathrm{E})$。P(E)为语言模型，它反映“E像一个句子”的程度，即流利度；P(F|E)为翻译模型，它反映“F像E”的程度，即忠实度；联合使用两个模型效果好于单独使用翻译模型，因为后者容易导致一些不好的译文。因此，统计机器翻译要解决的是3个问题：</p>
<ul>
<li>语言模型<em>P</em>(E)的建模和参数估计</li>
<li>翻译模型<em>P</em>(F|E)的建模和参数估计</li>
<li>解码（搜索）算法</li>
</ul>
<p>语言模型给出任何一个句子的出现概率$\operatorname{Pr}\left(E=e_{1} e_{2} \ldots e_{n}\right)$，N元语法模型是最简单也是最常见的语言模型，其他语言模型包括：隐马尔科夫模型（HMM）（加入词性标记信息）、概率上下文无关语法（PCFG）（加入短语结构信息）、概率链语法（Probabilistic Link Grammar）（加入链语法的结构信息）。N元语言模型公式如下：</p>
<p>$\begin{aligned} P(w) &amp;=\prod_{i=1}^{n} P\left(w_{i} | w_{1} w_{2} \ldots w_{i-1}\right) \ &amp; \approx \prod_{i=1}^{n} P\left(w_{i} | w_{i-N+1} w_{i-N+2} \ldots w_{i-1}\right) \end{aligned}$</p>
<p>用一张概率转移图可形象表示，如下为一个2元语言模型：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ibmmodel/2.png" alt="图片"></p>
<p>翻译模型<em>P</em>(F|E)反映的是一个源语言句子E翻译成一个目标语言句子F的概率，由于源语言句子和目标语言句子几乎不可能在语料库中出现过，因此这个概率无法直接从语料库统计得到，必须分解成词语翻译的概率和句子结构（或者顺序）翻译的概率。因此翻译模型的计算引入了隐含变量词对齐：$P(\mathrm{F} | \mathrm{E})=\sum_{A} P(\mathrm{F}, \mathrm{A} | \mathrm{E})$</p>
<p>翻译概率<em>P</em>(F|E)的计算转化为对齐概率<em>P</em>(F,A|E)的估计。IBM Model安成了对P(F,A|E)的估计。那么IBM Model从1~3分别有什么不同呢？IBM Model 1仅考虑词对词的互译概率，IBM Model 2加入了词的位置变化的概率，IBM Model 3加入了一个词翻译成多个词。</p>
<p>IBM模型是份经典的研究工作，这5个模型既是当初基于词的统计机器翻译模型的基础，也是现在统计机器翻译中主流技术中的重要一步。作为一个生成模型，IBM模型有着自身”严密”的模型演绎。总的来说，Model 1和2是在一个展开公式下的建模，而Model 3、4和5则是在另一个展开公式下的建模(fertility based model)。IBM Model属于single word based model，它只允许一对一和一对多的对齐，不存在多对一的对齐，这跟phrase based SMT模型不同。当然，从模型的复杂程度上讲，这5个模型之间的关系是1<2<3<4<5，从模型的计算顺序来讲，是1->2-&gt;3-&gt;4-&gt;5。</p>
<h2 id="IBM-Model-1"><a href="#IBM-Model-1" class="headerlink" title="IBM Model-1"></a>IBM Model-1</h2><blockquote>
<p>参考：<br><a href="https://www.nltk.org/_modules/nltk/translate/ibm1.html" target="_blank" rel="noopener">https://www.nltk.org/_modules/nltk/translate/ibm1.html</a><br><a href="http://mt-class.org/jhu/slides/lecture-ibm-model1.pdf" target="_blank" rel="noopener">http://mt-class.org/jhu/slides/lecture-ibm-model1.pdf</a><br><a href="https://zhuanlan.zhihu.com/p/72160554" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/72160554</a></p>
</blockquote>
<p>IBM模型1&amp;2的推导过程：</p>
<ul>
<li>猜测目标语言句子长度</li>
<li>从左至右，对于每个目标语言单词<ul>
<li>首先猜测该单词由哪一个源语言单词翻译而来</li>
<li>再猜测该单词应该翻译成什么目标语言词</li>
</ul>
</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ibmmodel/3.png" alt="图片"></p>
<p>IBM Model-1进行了如下假设：</p>
<ul>
<li>假设翻译的目标语言句子为： $\mathrm{F}=f_{1}^{m}=f_{1} f_{2} \cdots f_{m}$</li>
<li>假设翻译的源语言句子为：$\mathrm{E}=e_{1}^{l}=e_{1} e_{2} \cdots e_{l}$</li>
<li>假设词语对齐表示为：$\mathrm{A}=a_{1}^{m}=a_{1} a_{2} \cdots a_{m}, \forall i \in\{1, \cdots, m\}, a_{i} \in\{0, \cdots, l\}$</li>
<li>那么词语对齐的概率可以表示为：$\operatorname{Pr}(\mathrm{F}, \mathrm{A} | \mathrm{E})=\operatorname{Pr}(m | \mathrm{E}) \prod_{j=1}^{m} \operatorname{Pr}\left(a_{j} | a_{1}^{j-1}, f_{1}^{j-1}, m, \mathrm{E}\right) \operatorname{Pr}\left(f_{j} | a_{1}^{j}, f_{1}^{j-1}, m, \mathrm{E}\right)$</li>
<li>在Model-1中，假设所有翻译长度都是等概率的</li>
<li>假设词语对齐只与源语言长度有关，与其他因素无关：$\operatorname{Pr}\left(a_{j} | a_{1}^{j-1}, f_{1}^{j-1}, m, \mathrm{E}\right)=\frac{1}{l+1}$</li>
<li>假设目标词语的选择只与其对应的源语言词语有关，与其他因素无关：$\operatorname{Pr}\left(f_{j} | a_{1}^{j}, f_{1}^{j-1}, m, \mathrm{E}\right)=t\left(f_{j} | e_{a_{j}}\right)$</li>
<li>那么对齐概率可以表示为：$\operatorname{Pr}(\mathrm{F}, \mathrm{A} | \mathrm{E})=\frac{\varepsilon}{(l+1)^{m}} \prod_{j=1}^{m} t\left(f_{j} | e_{a_{j}}\right)$</li>
<li>对所有可能的对齐求和，那么翻译概率就可以表示为：$\operatorname{Pr}(\mathrm{F} | \mathrm{E})=\sum_{\mathrm{A}} \operatorname{Pr}(\mathrm{F}, \mathrm{A} | \mathrm{E})=\frac{\varepsilon}{(l+1)^{m}} \sum_{a_{1}=1}^{l} \cdots \sum_{a_{m}=1}^{l} \prod_{j=1}^{m} t\left(f_{j} | e_{a_{j}}\right)$</li>
</ul>
<p>这就是IBM Model 1的翻译模型公式，也就是说，给定参数t(f|e)，我们就可以计算出句子E翻译成句子F的概率。对于其中翻译概率表t(f|e)满足归一约束条件：$\sum_{f} t(f | e)=1$</p>
<p>延伸：在IBM-Model2中增加如下假设：</p>
<ul>
<li>假设词语对齐只与源语言长度、目标语言的长度和两个词的位置有关，与其他因素无关：$\operatorname{Pr}\left(a_{j} | a_{1}^{j-1}, f_{1}^{j-1}, m, \mathrm{E}\right)=a\left(a_{j} | j, m, l\right)$，归一化条件为$\sum_{i=0}^{l} a(i | j, m, l)=1$</li>
</ul>
<h3 id="翻译概率的定义"><a href="#翻译概率的定义" class="headerlink" title="翻译概率的定义"></a>翻译概率的定义</h3><p>对于长度为$l_f$的外语句子$f={f_1,…,f_{l_f}}$，长度为$l_e$的英文句子$e={e_1,…,e_{l_e}}$，从英文到外文的词对齐$e_j-&gt;f_i$关系$a:j-&gt;i$，有翻译概率定义：</p>
<p>$p(\mathbf{e}, a | \mathbf{f})=\frac{\epsilon}{\left(l_{f}+1\right)^{l_{e}}} \prod_{j=1}^{l_{e}} t\left(e_{j} | f_{a(j)}\right)$，例如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ibmmodel/4.png" alt="图片"></p>
<h3 id="参数求解"><a href="#参数求解" class="headerlink" title="参数求解"></a>参数求解</h3><p>根据最大似然估计，我们希望得到一组概率分布，使得我们的训练语料库出现的概率最大。也就是说，给定训练语料库E和F，我们要求解一个概率分布t(f|e)，使得翻译概率Pr(F|E)最大。这是一个受约束的极值问题，约束条件即是t(f|e)的归一性条件。为了求解这个问题，我们需要引入拉格朗日乘子，构造一个辅助函数，将上述受约束的极值问题转换成一个不受约束的极值问题。</p>
<p>引入拉格朗日乘子$\Lambda_{e}$，构造辅助函数：$h(t, \lambda) \equiv \frac{\varepsilon}{(l+1)^{m}} \sum_{a_{i}=1}^{l} \cdots \sum_{a_{m}=1}^{l} \prod_{j=1}^{m} t\left(f_{j} | e_{a_{j}}\right)-\sum_{e} \lambda_{e}\left(\sum_{f} t(f | e)-1\right)$。将上述函数对t(f|e)求导得到：$\frac{\partial h(t, \lambda)}{\partial t(f | e)}=\frac{\varepsilon}{(l+1)^{m}} \sum_{a_{1}=1}^{l} \cdots \sum_{a_{n}=1}^{l} \delta\left(f, f_{j}\right) \delta\left(e, e_{a_{j}}\right) \frac{\prod_{k=1}^{m} t\left(f_{k} | e_{a_{k}}\right)}{\mathrm{t}(f | e)}-\lambda_{e}$。</p>
<p>令上式为0，我们得到：$t(f | e)=\lambda_{e}^{-1} \frac{\varepsilon}{(l+1)^{m}} \sum_{a_{1}=1}^{l} \cdots \sum_{a_{m}=1}^{l} \delta\left(f, f_{j}\right) \delta\left(e, e_{a_{j}}\right) \prod_{k=1}^{m} t\left(f_{k} | e_{a_{k}}\right)$。我们看到，这个公式的左边和右边都出现了t(f|e)，我们无法直接用这个公式从给定的语料库(F|E)中计算出t(f|e)，我们可以将这个公式看成是一个迭代公式，给定一个初值t(f|e)，利用这个公式反复迭代，最后可以收敛到一个稳定的t(f|e)值，这就是EM算法。其中$\sum_{j=1}^{m} \delta\left(f, f_{j}\right) \delta\left(e, e_{a_{j}}\right)$表示对齐A中e连接到f的次数。</p>
<p>定义在E和F的所有可能的对齐A下e和f连接数的均值为：$c(f | e ; \mathrm{F}, \mathrm{E}) \equiv \sum_{\mathrm{A}} \operatorname{Pr}(\mathrm{A} | \mathrm{F}, \mathrm{E}) \sum_{j=1}^{m} \delta\left(f, f_{j}\right) \delta\left(e, e_{a_{j}}\right)$，且$\begin{array}{c}c(f | e ; \mathrm{F}, \mathrm{E})=\sum_{\mathrm{A}} \frac{\operatorname{Pr}(\mathrm{F}, \mathrm{A} | \mathrm{E})}{\operatorname{Pr}(\mathrm{F} | \mathrm{E})} \sum_{j=1}^{m} \delta\left(f, f_{j}\right) \delta\left(e, e_{a_{j}}\right) \ =\frac{\sum_{A} \operatorname{Pr}(\mathrm{F}, \mathrm{A} | \mathrm{E}) \sum_{j=1}^{m} \delta\left(f, f_{j}\right) \delta\left(e, e_{a_{j}}\right)}{\operatorname{Pr}(\mathrm{F}[\mathrm{E})}\end{array}$。</p>
<p>将c(f|e;F,E)代入迭代公式，并将Pr(F|E)并入参数λe，我们得到新的迭代公式：$t(f | e)=\lambda_{e}^{-1} c(f | e ; \mathrm{F}, \mathrm{E})$</p>
<p>这个新的迭代公式可以理解为：</p>
<ul>
<li>一旦我们得到了一组参数t(f|e)，我们就可以计算所有的词语对齐的概率Pr(F,A|E)</li>
<li>有了每个词语对齐的概率Pr(F,A|E)，我们就可以计算新的t(f|e)的值，就是所有的出现词语链接(e,f)的词语对齐概率之和，并对e进行归一化。</li>
</ul>
<p>以上就是EM算法的中心思想。</p>
<h3 id="EM算法迭代"><a href="#EM算法迭代" class="headerlink" title="EM算法迭代"></a>EM算法迭代</h3><p>我们可以使用EM算法迭代求解出对齐概率t，下面为EM算法求解过程：</p>
<ul>
<li><p>初始化$t(e|f)$</p>
</li>
<li><p>E-step: probability of alignments</p>
<ul>
<li><p>计算目标函数$p(a | \mathbf{e}, \mathbf{f})=\frac{p(\mathbf{e}, a | \mathbf{f})}{p(\mathbf{e} | \mathbf{f})}$, (使用上面公式计算$p(e,a|f)$)</p>
<ul>
<li><p>计算$p(e|f)$</p>
<p>$\begin{aligned} p(\mathbf{e} | \mathbf{f}) &amp;=\sum_{a} p(\mathbf{e}, a | \mathbf{f}) \ &amp;=\sum_{a(1)=0}^{l_{f}} \ldots \sum_{a\left(l_{e}\right)=0}^{l_{f}} p(\mathbf{e}, a | \mathbf{f}) \ &amp;=\sum_{a(1)=0}^{l_{f}} \ldots \sum_{a\left(l_{e}\right)=0} \frac{\epsilon}{\left(l_{f}+1\right)^{l_{e}}} \prod_{j=1}^{l_{e}} t\left(e_{j} | f_{a(j)}\right) \end{aligned}$</p>
<p>​        $\begin{array}{l}{=\frac{\epsilon}{\left(l_{f}+1\right)^{l_{e}}} \sum_{a(1)=0}^{l_{f}} \ldots \sum_{a\left(l_{e}\right)=0}^{l_{f}} \prod_{j=1}^{l_{e}} t\left(e_{j} | f_{a(j)}\right)} \ {=\frac{\epsilon}{\left(l_{f}+1\right)^{l_{e}}} \prod_{j=1}^{l_{e}} \sum_{i=0}^{l_{f}} t\left(e_{j} | f_{i}\right)}\end{array}​$</p>
</li>
</ul>
<p>​        一个计算例子如下：<br><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ibmmodel/5.png" alt="图片"></p>
</li>
<li><p>计算$p(a|e,f)$</p>
<p>  ​    $\begin{aligned} p(\mathbf{a} | \mathbf{e}, \mathbf{f}) &amp;=p(\mathbf{e}, \mathbf{a} | \mathbf{f}) / p(\mathbf{e} | \mathbf{f}) \ &amp;=\frac{\frac{\epsilon}{\left(l_{f}+1\right)^{l_{e}}} \prod_{j=1}^{l_{e}} t\left(e_{j} | f_{a(j)}\right)}{\frac{\epsilon}{\left(l_{f}+1\right)^{l_{e}}} \prod_{j=1}^{l_{e}} \sum_{i=0}^{l_{f}} t\left(e_{j} | f_{i}\right)} \ &amp;=\prod_{j=1}^{l_{e}} \frac{t\left(e_{j} | f_{a(j)}\right)}{\sum_{i=0}^{l_{f}} t\left(e_{j} | f_{i}\right)} \end{aligned}$</p>
</li>
<li><p>M-step: count collection</p>
<ul>
<li>计算$c(e | f ; \mathbf{e}, \mathbf{f})=\sum_{a} p(a | \mathbf{e}, \mathbf{f}) \sum_{j=1}^{l_{e}} \delta\left(e, e_{j}\right) \delta\left(f, f_{a(j)}\right)​$，并可以简化为$c(e | f ; \mathbf{e}, \mathbf{f})=\frac{t(e | f)}{\sum_{i=0}^{l_{f}} t\left(e | f_{i}\right)} \sum_{j=1}^{l_{e}} \delta\left(e, e_{j}\right) \sum_{i=0}^{l_{f}} \delta\left(f, f_{i}\right)​$</li>
<li>估计模型$t(e | f ; \mathbf{e}, \mathbf{f})=\frac{\left.\sum_{\mathbf{e}} \mathbf{f}_{\mathbf{j}} c(e | f ; \mathbf{e}, \mathbf{f})\right)}{\left.\sum_{e} \sum_{(\mathbf{e}, \mathbf{f})} c(e | f ; \mathbf{e}, \mathbf{f})\right)}$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="计算实例"><a href="#计算实例" class="headerlink" title="计算实例"></a>计算实例</h3><ul>
<li>训练句子：<ul>
<li>sentence1:  the house ||| la maison</li>
<li>sentence2:  house ||| maison</li>
</ul>
</li>
<li>画出所有对齐的可能<ul>
<li>sentence1: <ul>
<li>a1:  the-&gt;la、house-&gt;maison</li>
<li>a2: the-&gt;maison、house-&gt;la</li>
</ul>
</li>
<li>sentence2: <ul>
<li>a3: house-&gt;aison</li>
</ul>
</li>
</ul>
</li>
<li>初始化<ul>
<li>source_side_vocabulary: {the, house}, size=2</li>
<li>t(la|the) = 1/size = 1/2</li>
<li>t(maison|the) = 1/size = 1/2</li>
<li>t(la|house) = 1/size = 1/2</li>
<li>t(maison|house) = 1/size = 1/2</li>
</ul>
</li>
<li>第一次迭代<ul>
<li>Expectation:<ul>
<li>alignment probability<ul>
<li>p(e,a1|f) = t(la|the) <em> t(maison|house) =1/2 </em> 1/2  = 1/4</li>
<li>p(e, a2|f) = t(maison|the) <em> t(la|house) = 1/2 </em> 1/2 = 1/4</li>
<li>p(e, a3|f) = t(maison|house) = 1/2</li>
</ul>
</li>
<li>normalize alignment probability<ul>
<li>p(a1|E,F) = p(e,a1|f)/sum(p(E,a|F)) = p(e,a1|f)/(p(e,a1|f)+p(e, a2|f)) = 1/4/(1/4+1/4) = 1/2</li>
<li>p(a2|E,F) = p(e,a2|f)/(p(e,a1|f)+p(e, a2|f)) = 1/4/(1/4+1/4) = 1/2</li>
<li>p(a3|E,F) = p(e,a3|f)/p(e,a3|f) = 1</li>
</ul>
</li>
</ul>
</li>
<li>Max:<ul>
<li>collect counts<ul>
<li>c(la|the) = p(a1|E,F) <em> count(la|the) = 1/2 </em> 1 = 1/2</li>
<li>c(maison|the) = p(a2|E,F) <em> count(maison|the) = 1/2 </em> 1 = 1/2</li>
<li>c(la|house) = p(a2|E,F) <em> count(la|house) = 1/2 </em> 1 = 1/2</li>
<li>c(maison|house) = p(a1|E,F) <em> count(maison|house) + p(a3|E,F) </em> count(maison|house) = 1/2 <em> 1 + 1 </em> 1 = 3/2</li>
</ul>
</li>
<li>normalize<ul>
<li>t(la|the) = c(la|the)/sum(c(*|the)) = c(la|the)/(c(la|the) + c(maison|the) ) = 1/2/(1/2 + 1/2) = 1/2</li>
<li>t(maison|the) = c(maison|the)/sum(c(*|the)) = c(maison|the)/(c(la|the) + c(maison|the) ) = 1/2/(1/2 + 1/2) = 1/2</li>
<li>t(la|house) = 1/2/(1/2 + 3/2) = 1/4</li>
<li>t(maison|house) = 3/2/(1/2 + 3/2) = 3/4</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>第二次迭代<ul>
<li>Expectation:<ul>
<li>alignment probability<ul>
<li>p(e,a1|f) = 1/2 * 3/4 = 3/8</li>
<li>p(e, a2|f) = 1/2 * 1/4 = 1/8</li>
<li>p(e, a3|f) = 3/4</li>
</ul>
</li>
<li>normalize alignment probability<ul>
<li>p(a1|E,F) = 3/8/(3/8 + 1/8) = 3/4</li>
<li>p(a2|E,F) = 1/8/(3/8 + 1/8) = 1/4</li>
<li>p(a3|E,F) = 3/4/3/4 = 1</li>
</ul>
</li>
</ul>
</li>
<li>Max:<ul>
<li>collect counts<ul>
<li>c(la|the) = 3/4 * 1 = 3/4</li>
<li>c(maison|the) = 1/4 * 1 = 1/4</li>
<li>c(la|house) = 1/4 * 1 = 1/4</li>
<li>c(maison|house) = 3/4 <em> 1 + 1 </em> 1 = 7/4</li>
</ul>
</li>
<li>normalize<ul>
<li>t(la|the) = 3/4/(3/4 + 1/4) = 3/4</li>
<li>t(maison|the) = 1/4/(3/4 + 1/4) = 1/4</li>
<li>t(la|house) = 1/4/(1/4 + 7/4) = 1/8</li>
<li>t(maison|house) = 7/4/(1/4 + 7/4) = 7/8</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="NLTK源码分析"><a href="#NLTK源码分析" class="headerlink" title="NLTK源码分析"></a>NLTK源码分析</h3><ul>
<li>整体代码</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">counts &#x3D; Counts()</span><br><span class="line">for aligned_sentence in parallel_corpus:</span><br><span class="line">    trg_sentence &#x3D; aligned_sentence.words</span><br><span class="line">    src_sentence &#x3D; [None] + aligned_sentence.mots</span><br><span class="line"></span><br><span class="line">    # E step (a): Compute normalization factors to weigh counts</span><br><span class="line">    total_count &#x3D; self.prob_all_alignments(src_sentence, trg_sentence)</span><br><span class="line"></span><br><span class="line">    # E step (b): Collect counts</span><br><span class="line">    for t in trg_sentence:</span><br><span class="line">        for s in src_sentence:</span><br><span class="line">            count &#x3D; self.translation_table[t][s]</span><br><span class="line">            normalized_count &#x3D; count &#x2F; total_count[t]</span><br><span class="line">            counts.t_given_s[t][s] +&#x3D; normalized_count</span><br><span class="line">            counts.any_t_given_s[s] +&#x3D; normalized_count</span><br><span class="line">    </span><br><span class="line">    # M step: Update probabilities with maximum likelihood estimate</span><br><span class="line">    self.maximize_lexical_translation_probabilities(counts)</span><br></pre></td></tr></table></figure>
<ul>
<li>计算$\sum_{a} p(\mathbf{e}, a | \mathbf{f})$</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def prob_all_alignments(self, src_sentence, trg_sentence):</span><br><span class="line">  alignment_prob_for_t &#x3D; defaultdict(lambda: 0.0)</span><br><span class="line">  for t in trg_sentence:</span><br><span class="line">      for s in src_sentence:</span><br><span class="line">          alignment_prob_for_t[t] +&#x3D; self.translation_table[t][s]</span><br><span class="line">  return alignment_prob_for_t</span><br></pre></td></tr></table></figure>
<ul>
<li>计算M-step</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def maximize_lexical_translation_probabilities(self, counts):</span><br><span class="line">    for t, src_words in counts.t_given_s.items():</span><br><span class="line">        for s in src_words:</span><br><span class="line">            estimate &#x3D; counts.t_given_s[t][s] &#x2F; counts.any_t_given_s[s]</span><br><span class="line">            self.translation_table[t][s] &#x3D; max(estimate, IBMModel.MIN_PROB)</span><br></pre></td></tr></table></figure>
<ul>
<li>给定句对计算alignment</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">best_alignment &#x3D; []</span><br><span class="line">for j, trg_word in enumerate(sentence_pair.words):</span><br><span class="line">    best_prob &#x3D; max(self.translation_table[trg_word][None], IBMModel.MIN_PROB)</span><br><span class="line">    best_alignment_point &#x3D; None</span><br><span class="line">    for i, src_word in enumerate(sentence_pair.mots):</span><br><span class="line">        align_prob &#x3D; self.translation_table[trg_word][src_word]</span><br><span class="line">        if align_prob &gt;&#x3D; best_prob:  # prefer newer word in case of tie</span><br><span class="line">            best_prob &#x3D; align_prob</span><br><span class="line">            best_alignment_point &#x3D; i</span><br><span class="line"></span><br><span class="line">    best_alignment.append((j, best_alignment_point))</span><br><span class="line">sentence_pair.alignment &#x3D; Alignment(best_alignment)</span><br></pre></td></tr></table></figure>
<h2 id="IBM-Model-2"><a href="#IBM-Model-2" class="headerlink" title="IBM Model-2"></a>IBM Model-2</h2><blockquote>
<p>参考：<br><a href="https://www.nltk.org/_modules/nltk/translate/ibm2.html" target="_blank" rel="noopener">https://www.nltk.org/_modules/nltk/translate/ibm2.html</a><br><a href="http://www.ai.mit.edu/courses/6.891-nlp/l11.pdf" target="_blank" rel="noopener">http://www.ai.mit.edu/courses/6.891-nlp/l11.pdf</a></p>
</blockquote>
<p>IBM模型1的一个问题是它的重新排序能力非常弱，因为$p(f,a|s)$仅使用词法转换概率$t(t|s)$来计算。因此，如果模型有2个候选$t_1$和$t_2$具有相同的词汇翻译，但是对翻译后的单词进行了不同的重新排序，那么模型对这两个翻译都给出相同的分数。IBM-Model2使用对齐概率模型$p(i|j,s,f)$表示位置i、j的对齐概率，并用它计算$\operatorname{Pr}(t, a | s)=\frac{\epsilon}{(J+1)^{I}} \prod_{j=1}^{J} \operatorname{tr}\left(t_{j} | s_{a(j)}\right) \operatorname{Pr}_{a}(a(j) | j, J, I)$，其中$\operatorname{Pr}_{a}(a(j) | j, J, I)​$模拟一个单词在源句中的位置i被重新排序到目标句中的位置j的概率。</p>
<p>问题定义为：求解概率P(f|e)，其中$e=\{e_1,…,e_l\}$, $f=\{f_1,..,f_m\}$, 对齐定义为$\{a_1, …, a_m\}$且$a_j\in \{0,..,l\}$, 一共有$(l+1)^m$个对齐。IBM Model1的对齐概率认为是等概率的，即$P(\mathbf{a} | \mathbf{e})=C \times \frac{1}{(l+1)^{m}}$，且C是常数$C=\operatorname{prob}(\operatorname{length}(\mathbf{f})=m)$. 对于IBM Model1来说它生成出一个句子的具体过程为：</p>
<ul>
<li>选择长度为f(均等概率C产生的长度)</li>
<li>使用均等概率$1/(l+1)^m$产生对齐a</li>
<li>使用概率$P(\mathbf{f} | \mathbf{a}, \mathbf{e})=\prod_{j=1}^{m} \mathbf{T}\left(f_{j} | e_{a_{j}}\right)$产生目标语言</li>
<li>最终结果$P(\mathbf{f}, \mathbf{a} | \mathbf{e})=P(\mathbf{a} | e) P(\mathbf{f} | \mathbf{a}, e)=\frac{C}{(l+1)^{m}} \prod_{j=1}^{m} \mathbf{T}\left(f_{j} | e_{a_{j}}\right)$</li>
</ul>
<h3 id="翻译概率的定义-1"><a href="#翻译概率的定义-1" class="headerlink" title="翻译概率的定义"></a>翻译概率的定义</h3><p>在IBM Model-2中，定义D(i|j,l,m) = 给定source lenth e和target lenth f，第j个target word和第i个source word对齐的概。因此定义$P\left(\mathbf{a}=\left\{a_{1}, \ldots a_{m}\right\} | \mathbf{e}, l, m\right)=\prod_{j=1}^{m} \mathbf{D}\left(a_{j} | j, l, m\right)$，则翻译概率定义为：$P(\mathbf{f}, \mathbf{a} | \mathbf{e}, l, m)=\prod_{j=1}^{m} \mathbf{D}\left(a_{j} | j, l, m\right) \mathbf{T}\left(f_{j} | e_{a_{j}}\right)$。可以看出来，IBM-Model1是Model2的一个特例，在IBM-Model1中$\mathrm{D}(i | j, l, m)=\frac{1}{l+1}$</p>
<p>因此对于IBM Model2来说它生成出一个句子的具体过程为：</p>
<ul>
<li>选择长度为f(均等概率C产生的长度)</li>
<li>使用概率$\prod_{j=1}^{m} \mathrm{D}\left(a_{j} | j, l, m\right)$产生对齐$a=\{a_1,…,a_m\}$</li>
<li>使用概率$P(\mathbf{f} | \mathbf{a}, \mathbf{e})=\prod_{j=1}^{m} \mathbf{T}\left(f_{j} | e_{a_{j}}\right)$产生目标语言</li>
<li>最终结果$P(\mathbf{f}, \mathbf{a} | \mathbf{e})=P(\mathbf{a} | \mathbf{e}) P(\mathbf{f} | \mathbf{a}, \mathbf{e})=C \prod_{j=1}^{m} \mathbf{D}\left(a_{j} | j, l, m\right) \mathbf{T}\left(f_{j} | e_{a_{j}}\right)$</li>
</ul>
<h3 id="参数求解-1"><a href="#参数求解-1" class="headerlink" title="参数求解"></a>参数求解</h3><p>假设Model-2翻译模型定义为：$\operatorname{Pr}(\mathrm{F} | \mathrm{E})=\varepsilon \prod_{j=1}^{m} \sum_{i=0}^{l} t\left(f_{j} | e_{a_{j}}\right) a\left(a_{j} | j, m, l\right)$，同样通过引入拉格朗日乘子推导可以得到：</p>
<p>$t(f | e)=\lambda_{e}^{-1} c(f | e ; \mathrm{F}, \mathrm{E})$</p>
<p>$a(i | j, m, l)=\mu_{j m l}^{-1} c(i | j, m, l ; \mathrm{F}, \mathrm{E})$</p>
<p>$c(f | e ; \mathrm{F}, \mathrm{E})=\sum_{j=1}^{m} \sum_{i=0}^{l} \frac{t(f | e) a(i | j, m, l) \delta\left(f, f_{j}\right) \delta\left(e, e_{i}\right)}{t\left(f | e_{0}\right) a(0 | j, m, l)+\cdots+t\left(f | e_{l}\right) a(l | j, m, l)}$</p>
<p>$c(i | j, m, l ; \mathrm{F}, \mathrm{E})=\frac{t\left(f_{j} | e_{i}\right) a(i | j, m, l)}{t\left(f_{j} | e_{0}\right) a(0 | j, m, l)+\cdots+t\left(f_{j} | e_{l}\right) a(l | j, m, l)}$</p>
<p>考虑到训练语料库(F|E)是由一系列句子对组成的：$\left(\mathrm{F}^{(1)}, \mathrm{E}^{(\mathrm{i})}\right),\left(\mathrm{F}^{(2)}, \mathrm{E}^{(2)}\right), \cdots,\left(\mathrm{F}^{(\mathrm{s})}, \mathrm{E}^{(\mathrm{s})}\right)$</p>
<p>因此实际计算时我们采用以下公式：</p>
<p>$t(f | e)=\lambda_{e}^{-1} \sum_{s} c\left(f | e ; \mathrm{F}^{(\mathrm{s})}, \mathrm{E}^{(\mathrm{s})}\right)$</p>
<p>$a(i | j, m, l)=\mu_{j m l}^{-1} \sum_{s} c\left(i | j, m, l ; \mathrm{F}^{(\mathrm{s})}, \mathrm{E}^{(\mathrm{s})}\right)$</p>
<p>这里$\Lambda_{e}$和$\mu_{j m}$仅仅起到归一化因子的作用。</p>
<h3 id="EM算法迭代-1"><a href="#EM算法迭代-1" class="headerlink" title="EM算法迭代"></a>EM算法迭代</h3><ul>
<li>初始化$\begin{array}{r}\mathrm{T}(f | e) \ \mathrm{D}(i | j, l, m)\end{array}$</li>
<li>计算对齐概率$a[i, j, k]=\frac{\mathrm{D}\left(a_{j}=i | j, l, m\right) \mathrm{T}\left(f_{j} | e_{i}\right)}{\sum_{i^{\prime}=0}^{l} \mathrm{D}\left(a_{j}=i^{\prime} | j, l, m\right) \mathrm{T}\left(f_{j} | e_{i^{\prime}}\right)}$</li>
<li>E步：<ul>
<li>计算e和f对齐的期望次数：$\text { tcount }(e, f)=\sum_{\{|k, j|=f} a[i, j, k]$</li>
<li>计算e和任意target word对齐的期望次数：$\text { scount }(e)=\sum_{e[k, i]=\ell}^{i, k} \sum_{j=1}^{m[k]} a[i, j, k]$</li>
<li>计算对于source lenth 为l和target lenth 为m的句对ei和fj对齐的期望次数：$\operatorname{acount}(i, j, l, m)=\sum_{, m[k]=m} a_{\vdots=m}[i, j, k]$</li>
<li>计算表示source lenth 为l和target lenth 为m的期望次数：$\operatorname{acount}(j, l, m)=|\{k: l[k]=l, m[k]=m\}|$</li>
</ul>
</li>
<li>M步：<ul>
<li>重新估算翻译概率T(f|e)：$P(f | e)=\frac{\text { tcount }(e, f)}{\text { scount }(e)}$</li>
<li>重新估算对齐概率D(i|j,l,m)：$\left.P\left(a_{j}=i | j, l, m\right)\right)=\frac{a \operatorname{count}(i, j, l, m)}{\operatorname{acount}(j, l, m)}$</li>
</ul>
</li>
</ul>
<h3 id="计算实例-1"><a href="#计算实例-1" class="headerlink" title="计算实例"></a>计算实例</h3><ul>
<li>训练句子：<ul>
<li>sentence1:  the dog ||| le chien</li>
<li>sentence2:  the cat ||| le chat</li>
<li>sentence3: the bus ||| I’ autobus</li>
</ul>
</li>
<li>初始化<ul>
<li>source_side_vocabulary: {the, dog, cat, bus}, size=4</li>
<li>随机初始化<ul>
<li>T(f|e)</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ibmmodel/6.png" alt="图片"></p>
<p>​        * D(i|j,l,m)</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ibmmodel/7.png" alt="图片"></p>
<ul>
<li>E步：<ul>
<li>计算tcount(e,f)</li>
</ul>
</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ibmmodel/8.png" alt="图片"></p>
<p>如tcount(the, le) = a(1, 1, 0) + a(1, 1, 1) = 0.5264 + 0.4665 = 0.9929</p>
<ul>
<li>计算scount(e)</li>
<li>计算account(i,j,l,m)</li>
<li>计算account(j,l,m)</li>
<li>M步：<ul>
<li>重新估算T(f|e)(下图为经过几轮迭代后的变化)</li>
</ul>
</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ibmmodel/9.png" alt="图片"></p>
<ul>
<li>重新估算D(i|j,l,m)</li>
<li>IBM paper中建议用Model-1估计T(f|e)并用来初始化Model-2</li>
</ul>
<h3 id="NLTK源码分析-1"><a href="#NLTK源码分析-1" class="headerlink" title="NLTK源码分析"></a>NLTK源码分析</h3><ul>
<li>整体代码<ul>
<li>初始化</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if probability_tables is None:</span><br><span class="line">    ibm1 &#x3D; IBMModel1(sentence_aligned_corpus, 2 * iterations)</span><br><span class="line">    self.translation_table &#x3D; ibm1.translation_table</span><br><span class="line">    # a(i | j,l,m) &#x3D; 1 &#x2F; (l+1) for all i, j, l, m</span><br><span class="line">    l_m_combinations &#x3D; set()</span><br><span class="line">    for aligned_sentence in sentence_aligned_corpus:</span><br><span class="line">        l &#x3D; len(aligned_sentence.mots)</span><br><span class="line">        m &#x3D; len(aligned_sentence.words)</span><br><span class="line">        if (l, m) not in l_m_combinations:</span><br><span class="line">            l_m_combinations.add((l, m))</span><br><span class="line">            initial_prob &#x3D; 1 &#x2F; (l + 1)</span><br><span class="line">            for i in range(0, l + 1):</span><br><span class="line">                for j in range(1, m + 1):</span><br><span class="line">                    self.alignment_table[i][j][l][m] &#x3D; initial_prob</span><br><span class="line">else:</span><br><span class="line">    self.translation_table &#x3D; probability_tables[&quot;translation_table&quot;]</span><br><span class="line">    # D(i|j,l,m)</span><br><span class="line">    self.alignment_table &#x3D; probability_tables[&quot;alignment_table&quot;]</span><br><span class="line">    for n in range(0, iterations):</span><br><span class="line">        self.train(sentence_aligned_corpus)</span><br><span class="line">    self.align_all(sentence_aligned_corpus)</span><br></pre></td></tr></table></figure>
<ul>
<li>训练</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def train(self, parallel_corpus):</span><br><span class="line">    counts &#x3D; Model2Counts()</span><br><span class="line">    for aligned_sentence in parallel_corpus:</span><br><span class="line">        src_sentence &#x3D; [None] + aligned_sentence.mots</span><br><span class="line">        trg_sentence &#x3D; [&quot;UNUSED&quot;] + aligned_sentence.words</span><br><span class="line">        l &#x3D; len(aligned_sentence.mots)</span><br><span class="line">        m &#x3D; len(aligned_sentence.words)</span><br><span class="line">        # E step (a): Compute normalization factors to weigh counts</span><br><span class="line">        alignment_prob_for_t &#x3D; defaultdict(lambda: 0.0)</span><br><span class="line">        for j in range(1, len(trg_sentence)):</span><br><span class="line">            t &#x3D; trg_sentence[j]</span><br><span class="line">            for i in range(0, len(src_sentence)):</span><br><span class="line">                alignment_prob_for_t[t] +&#x3D; self.prob_alignment_point(</span><br><span class="line">                    i, j, src_sentence, trg_sentence</span><br><span class="line">                )</span><br><span class="line">        total_count &#x3D; alignment_prob_for_t</span><br><span class="line">        # E step (b): Collect counts</span><br><span class="line">        for j in range(1, m + 1):</span><br><span class="line">            t &#x3D; trg_sentence[j]</span><br><span class="line">            for i in range(0, l + 1):</span><br><span class="line">                s &#x3D; src_sentence[i]</span><br><span class="line">                l &#x3D; len(src_sentence) - 1</span><br><span class="line">                m &#x3D; len(trg_sentence) - 1</span><br><span class="line">                s &#x3D; src_sentence[i]</span><br><span class="line">                t &#x3D; trg_sentence[j]</span><br><span class="line">                count &#x3D; self.translation_table[t][s] * self.alignment_table[i][j][l][m]</span><br><span class="line">                normalized_count &#x3D; count &#x2F; total_count[t]</span><br><span class="line">                self.t_given_s[t][s] +&#x3D; normalized_count</span><br><span class="line">                self.any_t_given_s[s] +&#x3D; normalized_count</span><br><span class="line">                self.alignment[i][j][l][m] +&#x3D; normalized_count</span><br><span class="line">                self.alignment_for_any_i[j][l][m] +&#x3D; normalized_count</span><br><span class="line">    # M step: Update probabilities with maximum likelihood estimates</span><br><span class="line">    self.maximize_lexical_translation_probabilities(counts)</span><br><span class="line">    self.maximize_alignment_probabilities(counts)</span><br></pre></td></tr></table></figure>
<ul>
<li>计算M-step</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def maximize_alignment_probabilities(self, counts):</span><br><span class="line">    MIN_PROB &#x3D; IBMModel.MIN_PROB</span><br><span class="line">    for i, j_s in counts.alignment.items():</span><br><span class="line">        for j, src_sentence_lengths in j_s.items():</span><br><span class="line">            for l, trg_sentence_lengths in src_sentence_lengths.items():</span><br><span class="line">                for m in trg_sentence_lengths:</span><br><span class="line">                    estimate &#x3D; (</span><br><span class="line">                        counts.alignment[i][j][l][m]</span><br><span class="line">                        &#x2F; counts.alignment_for_any_i[j][l][m]</span><br><span class="line">                    )</span><br><span class="line">                    self.alignment_table[i][j][l][m] &#x3D; max(estimate, MIN_PROB)</span><br></pre></td></tr></table></figure>
<ul>
<li>给定句对计算alignment：$a_{j}^{*, 2}=\operatorname{argmax}_{j}\left(\mathrm{T}\left(f_{j} | e_{a_{j}}\right) \mathrm{D}(j | i, l, m)\right)​$</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def align(self, sentence_pair):</span><br><span class="line">    best_alignment &#x3D; []</span><br><span class="line">    l &#x3D; len(sentence_pair.mots)</span><br><span class="line">    m &#x3D; len(sentence_pair.words)</span><br><span class="line">    for j, trg_word in enumerate(sentence_pair.words):</span><br><span class="line">        # Initialize trg_word to align with the NULL token</span><br><span class="line">        best_prob &#x3D; (</span><br><span class="line">            self.translation_table[trg_word][None]</span><br><span class="line">            * self.alignment_table[0][j + 1][l][m]</span><br><span class="line">        )</span><br><span class="line">        best_prob &#x3D; max(best_prob, IBMModel.MIN_PROB)</span><br><span class="line">        best_alignment_point &#x3D; None</span><br><span class="line">        for i, src_word in enumerate(sentence_pair.mots):</span><br><span class="line">            align_prob &#x3D; (</span><br><span class="line">                self.translation_table[trg_word][src_word]</span><br><span class="line">                * self.alignment_table[i + 1][j + 1][l][m]</span><br><span class="line">            )</span><br><span class="line">            if align_prob &gt;&#x3D; best_prob:</span><br><span class="line">                best_prob &#x3D; align_prob</span><br><span class="line">                best_alignment_point &#x3D; i</span><br><span class="line"></span><br><span class="line">        best_alignment.append((j, best_alignment_point))</span><br><span class="line"></span><br><span class="line">    sentence_pair.alignment &#x3D; Alignment(best_alignment)</span><br></pre></td></tr></table></figure>
<h2 id="IBM-Model-3"><a href="#IBM-Model-3" class="headerlink" title="IBM Model-3"></a>IBM Model-3</h2><blockquote>
<p>参考：<br><a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1162/handouts/cs224n-lecture3-MT.pdf" target="_blank" rel="noopener">https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1162/handouts/cs224n-lecture3-MT.pdf</a><br><a href="http://www.ai.mit.edu/courses/6.891-nlp/l11.pdf" target="_blank" rel="noopener">http://www.ai.mit.edu/courses/6.891-nlp/l11.pdf</a><br><a href="http://www.ai.mit.edu/courses/6.891-nlp/l12.pdf" target="_blank" rel="noopener">http://www.ai.mit.edu/courses/6.891-nlp/l12.pdf</a><br><a href="https://www.nltk.org/_modules/nltk/translate/ibm3" target="_blank" rel="noopener">https://www.nltk.org/_modules/nltk/translate/ibm3</a>.html<br><a href="http://www1.maths.lth.se/matematiklth/vision/publdb/reports/pdf/schoenemann-ccnll-10.pdf" target="_blank" rel="noopener">http://www1.maths.lth.se/matematiklth/vision/publdb/reports/pdf/schoenemann-ccnll-10.pdf</a></p>
</blockquote>
<p>从Model-3到Model-5，翻译模型如下图所示：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ibmmodel/10.png" alt="图片"></p>
<p>具体步骤是：</p>
<ul>
<li>首先根据源语言词语的繁殖概率，确定每个源语言词翻译成多少个目标语言词</li>
<li>根据每个源语言词语的目标语言词数，将每个源语言词复制若干次</li>
<li>将复制后得到的每个源语言词，根据翻译概率，翻译成一个目标语言词</li>
<li>根据调序概率，将翻译得到的目标语言词重新调整顺序，得到目标语言句子</li>
</ul>
<p>对于Model-3来说，其推导过程为：</p>
<ul>
<li>对于句子中每一个英语单词e，选择一个产出率φ，其概率为n(φ|e)</li>
<li>对于所有单词的产出率求和得到m-prime</li>
<li>按照下面的方式构造一个新的英语单词串：删除产出率为0的单词，复制产出率为1的单词，复制两遍产出率为2的单词，依此类推</li>
<li>在这m-prime个单词的每一个后面，决定是否插入一个空单词NULL，插入和不插入的概率分别为p1和p0</li>
<li>φ0为插入的空单词NULL的个数</li>
<li>设m为目前的总单词数：m-prime+φ0</li>
<li>根据概率表t(f|e)，将每一个单词e替换为外文单词f</li>
<li>对于不是由空单词NULL产生的每一个外语单词，根据概率表d(j|i,l,m)，赋予一个位置。这里j是法语单词在法语串中的位置，i是产生当前这个法语单词的对应英语单词在英语句子中的位置，l是英语串的长度，m是法语串的长度</li>
<li>如果任何一个目标语言位置被多重登录（含有一个以上单词），则返回失败</li>
<li>给空单词NULL产生的单词赋予一个目标语言位置。这些位置必须是空位置（没有被占用）。任何一个赋值都被认为是等概率的，概率值为1/φ0</li>
<li>最后，读出法语串，其概率为上述每一步概率的乘积</li>
</ul>
<h3 id="翻译概率的定义-2"><a href="#翻译概率的定义-2" class="headerlink" title="翻译概率的定义"></a>翻译概率的定义</h3><p>Model-3引入fertility参数来约束一个源语言单词可以产生多少个目标语言单词, 并且从Model-3开始其建模过程和Model-1、Model-2有所不同，其产生句子的总体过程如下：</p>
<ul>
<li>对每个ej产生一个fertility $\phi_{j}$只依赖于ej）</li>
<li>对每个ej生成$\phi_{j}$个target word词（只依赖于ej而不依赖于任何其他context）</li>
<li>给每个target words选择一个位置（只依赖于ej的位置和句子长度）</li>
</ul>
<p>具体过程如下：</p>
<ul>
<li>有英文句子$e=\{e_1,…,e_l\}$, 想要建模$P(f|e)$</li>
<li>使用概率$P\left(\left\{\phi_{0} \ldots \phi_{l}\right\} | \mathbf{e}\right)$选择$l+1$个fertilities$\{\phi_{0} \ldots \phi_{l}\}$<ul>
<li>$P\left(\left\{\phi_{0} \ldots \phi_{l}\right\} | \mathbf{e}\right)=P\left(\phi_{0} | \phi_{1} \ldots \phi_{l}\right) \prod_{i=1}^{l} \mathrm{F}\left(\phi_{i} | e_{i}\right)$<ul>
<li>$F(\phi|e)$表示e和$\phi$个单词对齐的概率：F(0|the)=0.1、F(1|the)=0.9、F(2|the)=0…F(0|not)=0.01、F(1|not)=0.09、F(2|not)=0.9</li>
<li>$P\left(\phi_{0} | \phi_{1} \ldots \phi_{l}\right)$表示以出现$p_1$正面的概率硬币m次，出现$\phi_0$次正面的概率：$P\left(\phi_{0} | \phi_{1} \ldots \phi_{l}\right)=\frac{m !}{\left(m-\phi_{0}\right) ! \phi_{0} !} p_{1}^{\phi_{0}}\left(1-p_{1}\right)^{m-\phi_{0}}$<ul>
<li>$m=\sum_{i=1}^{l} \phi_{i}$，可以这样了理解：假设已经有m个source word产生，则这m个词的每个词都增加一个$p_1$概率的空对齐</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>对于每一个$e_i$, 使用概率$\prod_{i=0}^{l} \prod_{k=1}^{\phi_{i}} \mathrm{R}\left(\pi_{i, k} | i, l, m\right) \mathrm{T}\left(f_{i, k} | e_{i}\right)$选择位置$\pi_{i,k}\in 1…m$和target word $f_{i,k}$<ul>
<li>$R(j|i,l,m)$表示给定source lenth l、target lenth m 和 source position i，产生target position j的概率</li>
<li>$R(j|i,l,m)$要注意与之前的$D(i|j,l,m)$区别，$D(i|j,l,m)$表示给定source lenth l、target lenth m 和 target position j，产生source position i的概率</li>
</ul>
</li>
<li>根据上面推导产生模型:</li>
</ul>
<p>$\begin{aligned} \phi=&amp;\left\{\phi_{0} \ldots \phi_{m}\right\} \ \pi=&amp;\left\{\pi_{i, k}: i=0 \ldots m, k=1 \ldots \phi_{i}\right\} \ \mathbf{f} 2=&amp;\left\{f_{i, k}: i=0 \ldots m, k=1 \ldots \phi_{i}\right\} \ &amp; P(\phi, \pi, \mathbf{f} 2 | \mathbf{e})=\ \frac{m !}{\left(m-\phi_{0}\right) ! \phi_{0} !} p_{1}^{\phi_{0}}\left(1-p_{1}\right)^{m-\phi_{0}} &amp;\left(\prod_{i=0}^{l} \prod_{k=1}^{\phi_{i}} \mathbf{R}\left(\pi_{i, k} | i, l, m\right) \mathbf{T}\left(f_{i, k} | e_{i}\right)\right) \end{aligned}$</p>
<ul>
<li>进一步优化：</li>
</ul>
<p>$\begin{array}{c}P(\mathbf{f}, \mathbf{a} | \mathbf{e})= \ \frac{m !}{\left(m-\phi_{0}\right) ! \phi_{0} !} p_{1}^{\phi_{0}}\left(1-p_{1}\right)^{m-\phi_{0}}\left(\prod_{i=1}^{l} \mathbf{F}\left(\phi_{i} | e_{i}\right) \phi_{i} !\right)\left(\prod_{i=1}^{l} \prod_{k=1}^{\phi_{i}} \mathbf{R}\left(\pi_{i, k} | i, l, m\right)\right)\left(\prod_{i=0}^{l} \prod_{k=1}^{\phi_{i}} \mathbf{T}\left(f_{i, k} | e_{i}\right)\right)\end{array}$</p>
<h3 id="EM算法迭代-2"><a href="#EM算法迭代-2" class="headerlink" title="EM算法迭代"></a>EM算法迭代</h3><ul>
<li>初始化<ul>
<li>fertility_table：$F(\phi | e)$</li>
<li>translation_table：T(f|e)用Model-2的结果初始化</li>
<li>distortion_table：R(j|i,l,m)</li>
<li>p1=0.5</li>
<li>alignment_table：a[i,j,k]用Model-2的结果初始化</li>
</ul>
</li>
<li>E步：Model-1和Model-2可以计算出expect count，Model-3计算量太大，所以使用了一个近似算法<ul>
<li>什么是expect count？以句对e = I do not understand the logic of these people，f = Je ne comprends pas la logique de ces gens -la为例，在给定当前模型参数下，$\phi_{3}$（not 的 fertility）的期望值为$\sum_{\mathbf{a} \in \mathcal{A}} P(\mathbf{a} | \mathbf{f}, \mathbf{e}) \phi_{3}(\mathbf{a})=\sum_{\mathbf{a} \in \mathcal{A}} \frac{P(\mathbf{f}, \mathbf{a} | \mathbf{e})}{\sum_{\mathbf{a}^{\prime} \in \mathcal{A}} P\left(\mathbf{f}, \mathbf{a}^{\prime} | \mathbf{e}\right)} \phi_{3}(\mathbf{a})$，其中$\phi_{3}(\mathbf{a})$表示对齐a中$\phi_{3}(\mathbf{a})$的值</li>
<li>在Model-3中使用high probability alignments $\overline{\mathcal{A}}$计算$\sum_{\mathbf{a} \in \mathcal{A}} \frac{P(\mathbf{f}, \mathbf{a} | \mathbf{e})}{\sum_{\mathbf{a}^{\prime} \in \overline{\mathcal{A}}} P\left(\mathbf{f}, \mathbf{a}^{\prime} | \mathbf{e}\right)} \phi_{3}(\mathbf{a})$减少计算复杂度</li>
</ul>
</li>
<li>M步：重新估算参数概率</li>
</ul>
<h3 id="Viterbi训练"><a href="#Viterbi训练" class="headerlink" title="Viterbi训练"></a>Viterbi训练</h3><blockquote>
<p>参考：<br>词语对齐的对数线性模型：<a href="http://nlp.ict.ac.cn/~liuyang/papers/acl05_chn.pdf" target="_blank" rel="noopener">http://nlp.ict.ac.cn/~liuyang/papers/acl05_chn.pdf</a></p>
</blockquote>
<p>Viterbi参数训练算法的总体思路：</p>
<ul>
<li>给定初始参数</li>
<li>用已有的参数求概率最大（Viterbi）的词语对齐</li>
<li>用得到的概率最大的词语对齐重新计算参数</li>
<li>回到第二步，直到收敛为止</li>
</ul>
<p>在对参数计算公式无法化简的情况下，采用Viterbi参数训练算法是一种可行的做法，这种算法通常可以迅速收敛到一个可以接受的结果。</p>
<p>由于IBM Model 1和2存在简化的迭代公式，实际上在EM算法迭代是并不用真的去计算所有的对齐，而是可以利用迭代公式直接计算下一次的参数；由于IBM Model 3、4、5的翻译模型公式无法化简，理论上应该进行EM迭代。由于实际上由于计算所有词语对齐的代价太大，通常采用Viterbi训练，每次E步骤只生成最好的一个或者若干个对齐。</p>
<p>Generalized Iterative Scaling算法(GIS)。</p>
<h3 id="计算实例-2"><a href="#计算实例-2" class="headerlink" title="计算实例"></a>计算实例</h3><p>以一个实际例子来说明翻译过程：I do not understand the logic of these people翻译成法语。</p>
<ul>
<li>pick fertilities：以概率$\begin{aligned} P\left(\phi_{1} \ldots \phi_{l} | \mathbf{e}\right) &amp;=\prod_{i=1}^{l} \mathbf{F}\left(\phi_{i} | e_{i}\right) \ &amp;=\mathrm{F}(1 | I) \mathrm{F}(0 | d o) \mathrm{F}(2 | n o t) \mathrm{F}(1 | \text { understand }) \mathrm{F}(1 | \text { the }) \ &amp; \mathrm{F}(1 | \text { logic }) \mathrm{F}(1 | \text { of }) \mathrm{F}(1 | \text { these }) \mathrm{F}(1 | \text { people }) \end{aligned}$产生</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ibmmodel/11.png" alt="图片"></p>
<ul>
<li>replace words：以概率$\begin{aligned} \prod_{i=1}^{l} \phi_{i} ! \prod_{k=1}^{\phi_{i}} \mathrm{T}\left(\mathrm{f}_{i, k} | e_{i}\right)=&amp; 1 ! \times 0 ! \times 2 ! \times 1 ! \times 1 ! \times 1 ! \times 1 ! \times 1 ! \times 1 ! \times \ &amp; \mathrm{T}(J e | I) \mathrm{T}(n e | n o t) \mathrm{T}(p a s | n o t) \times \ &amp; \mathrm{T}(\text {comprends} | \text {understand}) \mathrm{T}(l a | \text {the}) \mathrm{T}(\text {logique } | \text {logic}) \times \ &amp; \mathrm{T}(\text {de } | \text {of}) \mathrm{T}(\text {ces} | \text {these}) \mathrm{T}(\text {gens } | \text {people}) \end{aligned}​$产生目标词汇</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ibmmodel/15.png" alt="图片"></p>
<ul>
<li>reorder：以概率$\begin{aligned} \prod_{i=1}^{1} \prod_{k=1}^{\phi_{i}} \mathrm{R}\left(\pi_{i, k} | i, l, m\right)=&amp; \mathrm{R}(j=1 | i=1, l=9, m=10) \mathrm{R}(2 | 3,9,10) \mathrm{R}(3 | 4,9,10) \times \ &amp; \mathrm{R}(4 | 3,9,10) \mathrm{R}(5 | 5,9,10) \mathrm{R}(6 | 6,9,10) \times \ &amp; \mathrm{R}(7 | 7,9,10) \mathrm{R}(8 | 8,9,10) \mathrm{R}(9 | 9,9,10) \end{aligned}$重新排序</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ibmmodel/13.png" alt="图片"></p>
<ul>
<li>spurious words：以概率$\begin{aligned} P\left(\phi_{0} | \phi_{1} \ldots \phi_{1}\right) \prod_{k=1}^{60} \mathrm{T}\left(\mathrm{f}_{0, k} | N U L L\right) &amp;=\frac{n !}{\left(n-\phi_{0}\right) ! \phi_{0} !} p_{1}^{00}\left(1-p_{1}\right)^{n-\phi_{0}} \prod_{k=1}^{60} \mathrm{T}\left(\mathrm{f}_{0, k} | N U L L\right) \ &amp;=\frac{9 !}{811 !} p_{1}\left(1-p_{1}\right)^{8} \mathrm{T}(-l a | N U L L) \ &amp;=9 p_{1}\left(1-p_{1}\right)^{8} \mathrm{T}(-l a | N U L L) \end{aligned}​$产生</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ibmmodel/14.png" alt="图片">，这里$n=\sum_{i=1}^{l} \phi_{l}=m-\phi_{0}$</p>
<ul>
<li>p1是$\phi_{0}$（即空对齐）的概率，spurious words即产生T(f|NULL)</li>
</ul>
<h3 id="NLTK源码分析-2"><a href="#NLTK源码分析-2" class="headerlink" title="NLTK源码分析"></a>NLTK源码分析</h3><ul>
<li>整体代码<ul>
<li>初始化</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if probability_tables is None:</span><br><span class="line">    ibm2 &#x3D; IBMModel2(sentence_aligned_corpus, iterations)</span><br><span class="line">    self.translation_table &#x3D; ibm2.translation_table</span><br><span class="line">    self.alignment_table &#x3D; ibm2.alignment_table</span><br><span class="line">    # d(j | i,l,m) &#x3D; 1 &#x2F; m for all i, j, l, m</span><br><span class="line">    l_m_combinations &#x3D; set()</span><br><span class="line">    for aligned_sentence in sentence_aligned_corpus:</span><br><span class="line">        l &#x3D; len(aligned_sentence.mots)</span><br><span class="line">        m &#x3D; len(aligned_sentence.words)</span><br><span class="line">        if (l, m) not in l_m_combinations:</span><br><span class="line">            l_m_combinations.add((l, m))</span><br><span class="line">            initial_prob &#x3D; 1 &#x2F; m</span><br><span class="line">            for j in range(1, m + 1):</span><br><span class="line">                for i in range(0, l + 1):</span><br><span class="line">                    self.distortion_table[j][i][l][m] &#x3D; initial_prob</span><br><span class="line">     # simple initialization, taken from GIZA++</span><br><span class="line">    self.fertility_table[0] &#x3D; defaultdict(lambda: 0.2)</span><br><span class="line">    self.fertility_table[1] &#x3D; defaultdict(lambda: 0.65)</span><br><span class="line">    self.fertility_table[2] &#x3D; defaultdict(lambda: 0.1)</span><br><span class="line">    self.fertility_table[3] &#x3D; defaultdict(lambda: 0.04)</span><br><span class="line">    MAX_FERTILITY &#x3D; 10</span><br><span class="line">    initial_fert_prob &#x3D; 0.01 &#x2F; (MAX_FERTILITY - 4)</span><br><span class="line">    for phi in range(4, MAX_FERTILITY):</span><br><span class="line">        self.fertility_table[phi] &#x3D; defaultdict(lambda: initial_fert_prob)</span><br><span class="line">    self.p1 &#x3D; 0.5</span><br><span class="line">else:</span><br><span class="line">    self.translation_table &#x3D; probability_tables[&quot;translation_table&quot;]</span><br><span class="line">    self.alignment_table &#x3D; probability_tables[&quot;alignment_table&quot;]</span><br><span class="line">    self.fertility_table &#x3D; probability_tables[&quot;fertility_table&quot;]</span><br><span class="line">    self.p1 &#x3D; probability_tables[&quot;p1&quot;]</span><br><span class="line">    self.distortion_table &#x3D; probability_tables[&quot;distortion_table&quot;]</span><br></pre></td></tr></table></figure>
<ul>
<li>训练</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def train(self, parallel_corpus):</span><br><span class="line">        counts &#x3D; Model3Counts()</span><br><span class="line">        for aligned_sentence in parallel_corpus:</span><br><span class="line">            l &#x3D; len(aligned_sentence.mots)</span><br><span class="line">            m &#x3D; len(aligned_sentence.words)</span><br><span class="line">            # Sample the alignment space</span><br><span class="line">            sampled_alignments, best_alignment &#x3D; self.sample(aligned_sentence)</span><br><span class="line">            # Record the most probable alignment</span><br><span class="line">            aligned_sentence.alignment &#x3D; Alignment(best_alignment.zero_indexed_alignment())</span><br><span class="line">            # E step (a): Compute normalization factors to weigh counts, https:&#x2F;&#x2F;github.com&#x2F;nltk&#x2F;nltk&#x2F;blob&#x2F;5023d6b933ef1a5b1f25fba1d5ed11a8a43a47e4&#x2F;nltk&#x2F;translate&#x2F;ibm_model.py中有详细计算</span><br><span class="line">            total_count &#x3D; self.prob_of_alignments(sampled_alignments)</span><br><span class="line">            # E step (b): Collect counts</span><br><span class="line">            for alignment_info in sampled_alignments:</span><br><span class="line">                count &#x3D; self.prob_t_a_given_s(alignment_info)</span><br><span class="line">                normalized_count &#x3D; count &#x2F; total_count</span><br><span class="line">                for j in range(1, m + 1):</span><br><span class="line">                   counts.update_lexical_translation(normalized_count, alignment_info, j)</span><br><span class="line">                   counts.update_distortion(normalized_count, alignment_info, j, l, m)</span><br><span class="line">                counts.update_null_generation(normalized_count, alignment_info)</span><br><span class="line">                counts.update_fertility(normalized_count, alignment_info)</span><br><span class="line">        # M step:</span><br><span class="line">        # If any probability is less than MIN_PROB, clamp it to MIN_PROB</span><br><span class="line">        existing_alignment_table &#x3D; self.alignment_table</span><br><span class="line">        self.reset_probabilities()</span><br><span class="line">        self.alignment_table &#x3D; existing_alignment_table  # don&#39;t retrain</span><br><span class="line"></span><br><span class="line">        self.maximize_lexical_translation_probabilities(counts)</span><br><span class="line">        self.maximize_distortion_probabilities(counts)</span><br><span class="line">        self.maximize_fertility_probabilities(counts)</span><br><span class="line">        self.maximize_null_generation_probabilities(counts)</span><br></pre></td></tr></table></figure>
<ul>
<li>计算M-step<ul>
<li>最大化distortion概率</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def maximize_distortion_probabilities(self, counts):</span><br><span class="line">    MIN_PROB &#x3D; IBMModel.MIN_PROB</span><br><span class="line">    for j, i_s in counts.distortion.items():</span><br><span class="line">        for i, src_sentence_lengths in i_s.items():</span><br><span class="line">            for l, trg_sentence_lengths in src_sentence_lengths.items():</span><br><span class="line">                for m in trg_sentence_lengths:</span><br><span class="line">                    estimate &#x3D; (counts.distortion[j][i][l][m]&#x2F; counts.distortion_for_any_j[i][l][m])</span><br><span class="line">                    self.distortion_table[j][i][l][m] &#x3D; max(estimate, MIN_PROB)</span><br></pre></td></tr></table></figure>
<ul>
<li>计算给定source sentence，产生target sentence和alignment的概率</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def prob_t_a_given_s(self, alignment_info):</span><br><span class="line">    src_sentence &#x3D; alignment_info.src_sentence</span><br><span class="line">    trg_sentence &#x3D; alignment_info.trg_sentence</span><br><span class="line">    l &#x3D; len(src_sentence) - 1  # exclude NULL</span><br><span class="line">    m &#x3D; len(trg_sentence) - 1</span><br><span class="line">    p1 &#x3D; self.p1</span><br><span class="line">    p0 &#x3D; 1 - p1</span><br><span class="line"></span><br><span class="line">    probability &#x3D; 1.0</span><br><span class="line">    MIN_PROB &#x3D; IBMModel.MIN_PROB</span><br><span class="line"></span><br><span class="line">    # Combine NULL insertion probability</span><br><span class="line">    null_fertility &#x3D; alignment_info.fertility_of_i(0)</span><br><span class="line">    probability *&#x3D; pow(p1, null_fertility) * pow(p0, m - 2 * null_fertility)</span><br><span class="line">    if probability &lt; MIN_PROB:</span><br><span class="line">        return MIN_PROB</span><br><span class="line"></span><br><span class="line">    # Compute combination (m - null_fertility) choose null_fertility</span><br><span class="line">    for i in range(1, null_fertility + 1):</span><br><span class="line">        probability *&#x3D; (m - null_fertility - i + 1) &#x2F; i</span><br><span class="line">        if probability &lt; MIN_PROB:</span><br><span class="line">            return MIN_PROB</span><br><span class="line"></span><br><span class="line">    # Combine fertility probabilities</span><br><span class="line">    for i in range(1, l + 1):</span><br><span class="line">        fertility &#x3D; alignment_info.fertility_of_i(i)</span><br><span class="line">        probability *&#x3D; (</span><br><span class="line">            factorial(fertility) * self.fertility_table[fertility][src_sentence[i]]</span><br><span class="line">        )</span><br><span class="line">        if probability &lt; MIN_PROB:</span><br><span class="line">            return MIN_PROB</span><br><span class="line"></span><br><span class="line">    # Combine lexical and distortion probabilities</span><br><span class="line">    for j in range(1, m + 1):</span><br><span class="line">        t &#x3D; trg_sentence[j]</span><br><span class="line">        i &#x3D; alignment_info.alignment[j]</span><br><span class="line">        s &#x3D; src_sentence[i]</span><br><span class="line">        probability *&#x3D; (self.translation_table[t][s] * self.distortion_table[j][i][l][m])</span><br><span class="line">        if probability &lt; MIN_PROB:</span><br><span class="line">            return MIN_PROB</span><br><span class="line"></span><br><span class="line">    return probability</span><br></pre></td></tr></table></figure>
<h3 id="计算alignment"><a href="#计算alignment" class="headerlink" title="计算alignment"></a>计算alignment</h3><ul>
<li>用Model-2计算最可能的alignment：$\mathbf{a}^{<em>, 2}=\operatorname{argmax}_{\mathbf{a} \in \mathcal{A}} P_{2}(\mathbf{f}, \mathbf{a} | \mathbf{e})$、$a_{j}^{</em> 2}=\operatorname{argmax}_{j}\left(\mathrm{T}\left(f_{j} | e_{a_{j}}\right) \mathrm{D}(j | i, l, m)\right)$</li>
<li>计算a2的邻居（邻居指通过替换a2中某个对齐或交换某两个对齐产生的新的对齐）</li>
<li>迭代计算a3（初值设置为a2）：$\mathbf{a}^{<em>, 3}=\operatorname{argmax}_{\mathbf{a} \in \mathcal{N}\left(\mathbf{a}^{</em>, 3}\right)} \quad P_{3}(\mathbf{a}, \mathbf{f} | \mathbf{e})​$<ul>
<li>上式等价于求解概率的负对数</li>
<li>对于所有的source position $j \in\{1, \ldots, J\}$和target position $i \in\{0, \ldots, \tilde{I}\}$,引入$x_{i j} \in\{0,1\}$表示i和j是否对齐，且由于每个i只能和1个j对齐，因此存在约束$\sum_{i} x_{i j}=1 \quad, j=1, \ldots, J$，从而求解目标为：$c_{i j}^{x}=-\log \left[p\left(f_{j} | e_{i}\right) \cdot p(j | i)\right]$</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>SMT</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：《Jointly Learning to Align and Translate with Transformer》</title>
    <url>/2020/03/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E3%80%8AJointly-Learning-to-Align-and-Translate-with-Transformer%E3%80%8B/</url>
    <content><![CDATA[<p>最近在对齐方面看的比较多，这一篇是去年看到的使用多任务学习提高对齐效果的文章。今天仔细读一遍。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://arxiv.org/pdf/1909.02074.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1909.02074.pdf</a></p>
</blockquote>
<h1 id="论文简介"><a href="#论文简介" class="headerlink" title="论文简介"></a>论文简介</h1><p>神经机器翻译已经统治了翻译领域，其中的attention机制是从词对齐借鉴出来的，但是NMT中的attention和词对齐又有很大不同。Attention更倾向于attend到context word而不是source word本身，而且现在的multi-layer、multi-head机制又使得attention非常复杂。</p>
<p>由于词对齐可以用在很多地方，比如对于实体名词的翻译，或者对于low-resource语言的借助词表的翻译有很大作用。因此本文提出了一个多任务学习的方法，使用NMT的negative log likelihood（NLL）loss和alignment loss结合作为多任务学习的loss。而且和NMT的auto-regressive式的模型不同，NMT在翻译时需要借助past target context的信息，而对于词对齐来说是不够用的，因此本文在多任务中使用了不同的context信息进行生成。</p>
<h1 id="问题定义和基线模型"><a href="#问题定义和基线模型" class="headerlink" title="问题定义和基线模型"></a>问题定义和基线模型</h1><p>给定source sentence：f(1,J)=f1,….,fj,…fJ和target translation：e(1,I)=f1,….,fj,…fI，则对齐就是位置的笛卡尔序列：$\mathcal{A} \subseteq\{(j, i): j=1, \ldots, J ; i=1, \ldots, I\}$。词对齐任务就是找到这样的多对多的对应关系。Transformer模型计算过程如下：</p>
<p>$\tilde{\mathbf{q}}_{n}^{i}=\mathbf{q}^{i} W_{n}^{Q}, \tilde{K}_{n}=K W_{n}^{K}, \tilde{V}_{n}=V W_{n}^{V}​$</p>
<p>$H_{n}^{i}=\text { Attention }\left(\tilde{\mathbf{q}}_{n}^{i}, \tilde{K}_{n}, \tilde{V}_{n}\right)$</p>
<p>$\mathcal{M}\left(\mathbf{q}^{i}, K, V\right)=\operatorname{Concat}\left(H_{1}^{i}, \ldots, H_{N}^{i}\right) W^{O}$</p>
<p>$\text { Attention }\left(\tilde{\mathbf{q}}_{n}^{i}, \tilde{K}_{n}, \tilde{V}_{n}\right)=\mathbf{a}_{n}^{i} \tilde{V}_{n}$</p>
<p>$\mathbf{a}_{n}^{i}=\operatorname{softmax}\left(\frac{\tilde{\mathbf{q}}_{n}^{i} \tilde{K}_{n}^{T}}{\sqrt{d_{k}}}\right)​$</p>
<p>其中$\mathbf{a}_{n}^{i} \in \mathbb{R}^{1 \times J}$表示第i个source token和全部target token的关系，整个attention matrix为$A_{I \times J}$。基线模型就是Transformer的attention矩阵抽取出的对齐， 论文在这里介绍了两个前人工作，这里不做介绍了。</p>
<h1 id="本文方法"><a href="#本文方法" class="headerlink" title="本文方法"></a>本文方法</h1><h2 id="Averaging-Layer-wise-Attention-Scores"><a href="#Averaging-Layer-wise-Attention-Scores" class="headerlink" title="Averaging Layer-wise Attention Scores"></a>Averaging Layer-wise Attention Scores</h2><p>单一的attention矩阵是对称的，但是不同层、不同head的attention学习到的是不同的东西，因此我们把所有head的attention矩阵加起来做平均，这样能更好地观察对齐。并且我们发现倒数第二层的attention矩阵G更好得表达了对齐。</p>
<h2 id="Multi-task-Learning"><a href="#Multi-task-Learning" class="headerlink" title="Multi-task Learning"></a>Multi-task Learning</h2><p>由于标注alignment是个很费力的事，本文使用G来指导attention。Gij是一个0-1矩阵（可以使用layer-wise attention或giza++产生的alignment），Aij是某一个head的attention，通过最小化Gij和Aij的KL散度来进行优化：$\mathcal{L}_{a}(A)=-\frac{1}{I} \sum_{i=1}^{I} \sum_{j=1}^{J} G_{i, j}^{p} \log \left(A_{i, j}\right)$。整个模型的损失函数为：$\mathcal{L}=\mathcal{L}_{t}+\lambda \mathcal{L}_{a}(A)$，其中Lt是翻译的NLL loss。</p>
<h2 id="Providing-Full-Target-Context"><a href="#Providing-Full-Target-Context" class="headerlink" title="Providing Full Target Context"></a>Providing Full Target Context</h2><p>训练翻译模型时用的时auto-regressive进行解码，也就是生成每个target tokens时只依赖于source tokens和之前生成的target token，但对于对齐任务来说，需要指导全部的target tokens。本文使用的方法是对于不同的loss，使用不同的context计算（计算两次前向）：</p>
<p>$\mathcal{L}_{t}=-\frac{1}{I} \sum_{i=1}^{I} \log \left(p\left(e_{i} | f_{1}^{J}, e_{1}^{i-1}\right)\right)$</p>
<p>$\mathcal{L}_{a}^{\prime}=\mathcal{L}_{a}\left(A | f_{1}^{J}, e_{1}^{I}\right)$</p>
<h1 id="实验部分"><a href="#实验部分" class="headerlink" title="实验部分"></a>实验部分</h1><p>评价指标选择alignment error rate（AER）。Transformer选择base model参数设置如下：</p>
<ul>
<li>embed_size=512</li>
<li>6 encoder + 6 decoder</li>
<li>8 attention head</li>
<li>share input and output embedding</li>
<li>relu activation</li>
<li>sinusoidal positional embedding（参考：<a href="https://www.zhihu.com/question/307293465" target="_blank" rel="noopener">https://www.zhihu.com/question/307293465</a>）</li>
<li>validation translation loss for early stopping</li>
<li>Adam, learning rate=3e-4, beta1=0.9, beta2=0.98</li>
<li>warmup step=4000</li>
<li>learning rate scheduler = inverse square root</li>
<li>dropout=0.1</li>
<li>label smooth=0.1</li>
</ul>
<p>本文选择的Statistical Baseline设置如下：</p>
<ul>
<li>5次迭代IBM1 + HMM + IBM3 + IBM4</li>
<li>使用grow-diagonal将两个方向的对齐合并</li>
</ul>
<p>最终实验结果如下：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/temp1.png" alt="图片"></p>
<p>在实验中我们发现，模型更容易将代词和名词对齐，提示我们可以将对齐分为sure和possible两种。在基于统计的GIZA++对齐中possible对齐较少出现，这可能是因为giza是通过统计共现来实现的。可以认为，将context进行更好得建模有助于对齐。</p>
]]></content>
      <tags>
        <tag>论文阅读</tag>
        <tag>LaserTagger</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：《Encode, Tag, Realize_ High-Precision Text Editing》</title>
    <url>/2020/03/28/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E3%80%8AEncode-Tag-Realize-High-Precision-Text-Editing%E3%80%8B/</url>
    <content><![CDATA[<p>最近想看一下语法检查的东西，关注到了这一篇谷歌去年出的论文。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://arxiv.org/abs/1909.01187" target="_blank" rel="noopener">https://arxiv.org/abs/1909.01187</a><br><a href="https://zhuanlan.zhihu.com/p/82196470" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/82196470</a></p>
</blockquote>
<h1 id="论文简介"><a href="#论文简介" class="headerlink" title="论文简介"></a>论文简介</h1><p>论文提出了一种用于sequence tagging的模型LASERTAGGER，它的基本思想是将文本生成任务转化为3种token操作的组合：keep、delete和add。它使用了bert作为编码器，transformer作为解码器，并将该模型用于 sentence fusion、sentence splitting、abstractive summarization和grammar correction任务中。LASERTAGGER在训练数据量少的情况下依然能达到不错的效果，且速度提升了很多倍。</p>
<p>研究人员是怎么想到的这个方法的呢？其实在很多text generation任务中，output和input有很大重合， 在Incorporating copying mechanism in sequence-to-sequence learning这篇论文中使用了copy机制用于在解码时选择copy一个source端词汇，抑或是生成一个新的词。但是这样的模型依旧需要庞大的训练集以保证解码端的vocabulary size是足够的。</p>
<p>通过研究发现，使用一个相对较小的output tags的集合来表示文本的deletion、rephrasing和reordering，对于生成训练语料中的大部分文本是足够的。这样就会使模型可以使用比较小的vocabulary来训练，并且由于输出长度只跟输入长度有关，就可以使用更少的语料达到精确的结果。</p>
<p>模型的主要流程如下图所式：先使用encode模块构建输入的表示，tag模块产生edit tags，realize模块通过规则将tags转换成输出tokens。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lasertagger/1.png" alt="图片"></p>
<p>本文提出两种LASERTAGGER架构，一种只使用BERT，另一种使用了BERT encoder+transformer deocder。实验表明LASERTAGGER有更快的推理速度，需要更少的训练语料，相比于seq2seq更可控（因为词表小了），更不易产生hallucination问题（生成的输出不受输入文本支持）。</p>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><h2 id="Text-Simplification"><a href="#Text-Simplification" class="headerlink" title="Text Simplification"></a>Text Simplification</h2><p>这个任务很适用edit operations modeling的方法解决。Dong等人提出了一个文本编辑模型，类似于本文，主要差异是：</p>
<ul>
<li>使用了interpreter module语言模型，来实现本文Realize部分的功能</li>
<li>使用了full vocabulary来生成added tokens，而本文使用了optimized set of frequently added phrases集合</li>
</ul>
<p>虽然Dong的模型生成更多样化的输出，但它可能会在推理时间、精度和数据效率上产生负面影响。另外一个跟本文相似的论文为Gu等人提出的Levenshtein Transformer模型，通过sequence of deletion and insertion actions来生成文本。</p>
<h2 id="Single-document-summarization"><a href="#Single-document-summarization" class="headerlink" title="Single-document summarization"></a>Single-document summarization</h2><p>这个任务可以使用在token-level和sentence-level上的deletion-based方法解决。也有论文使用了seq2seq做抽象摘要，但其缺陷是产生的操作不仅仅是删除。因此，Jing和McKeown(2000)的工作使用还原、组合、句法转换、词汇释义、泛化和重新排序操作解决。也有论文使用copy机制来使模型更容易复制source端词汇。</p>
<h2 id="Grammatical-Error-Correction"><a href="#Grammatical-Error-Correction" class="headerlink" title="Grammatical Error Correction"></a>Grammatical Error Correction</h2><p>此类任务需要利用task-specific knowledge，比如无监督得对某类错误类型构造分类器。这种error detection任务也可以使用sequence label方法，或者seq2seq方法（需要大量数据）。</p>
<h1 id="序列标注任务"><a href="#序列标注任务" class="headerlink" title="序列标注任务"></a>序列标注任务</h1><p>本文使用的方法是将text edit转换为序列标注的问题，主要包含3个部分：定义tagging operation；将训练数据的plain-text target转换为tagging格式；将tag转变为输出文本。</p>
<h2 id="定义Tagging-Operations"><a href="#定义Tagging-Operations" class="headerlink" title="定义Tagging Operations"></a>定义Tagging Operations</h2><h3 id="基本定义"><a href="#基本定义" class="headerlink" title="基本定义"></a>基本定义</h3><p>一个Tag包含两个部分：base tag（KEEP/DELETE） + added phrase（表示在token前需要增加phrase，可以为空）。Added phrase事先在词表V中定义，确保通过在input中加入后可以转换成output。这种tag+phrase的组合定义为B^P，数量大约有2^V个。</p>
<p>不同任务可以有task-specific的tag，比如在sentence fusion任务中可以把SWAP标记添加到第一句话的最后部分（如下图）；还比如为了用代词替换命名实体，可以用PRONOMINALIZE标签，在realize阶段通过查找知识库中命名实体的gender信息来用she、he、they替换（这比用she^DELETE, he^DELETE, they^DELETE要好）</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lasertagger/2.png" alt="图片"></p>
<h3 id="优化Phrase-Vocabulary"><a href="#优化Phrase-Vocabulary" class="headerlink" title="优化Phrase Vocabulary"></a>优化Phrase Vocabulary</h3><p>从词表V中选择出一个子集P，使其覆盖到所有候选added phrase且P最大为l，这个问题类似于minimum k-union问题（参考<a href="https://xbuba.com/questions/12424155" target="_blank" rel="noopener">https://xbuba.com/questions/12424155</a>）。可以证明这个问题是NP-hard问题。具体实现时本文时这样做的：先使用最长公共字串将source text和target text对齐，将没对齐的n-grams加入到P中，再从P中选择最频繁出现的l个phrase作为phrase vocabulary。（本文也尝试过使用贪心策略，每次从P中选择对覆盖率增加最多的token，但发现一个问题，如’(‘和’)’这样的token通常成对出现，但增加’(‘和’)’都对覆盖率影响很小，但如果成对增加’()’则会覆盖到很多）</p>
<h2 id="Train-targets转Tags"><a href="#Train-targets转Tags" class="headerlink" title="Train targets转Tags"></a>Train targets转Tags</h2><p>具体算法如下：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lasertagger/3.png" alt="图片"></p>
<p>本文特别强调了，即便在转换时遇到了想要增加的phrase不在V中，也不会影响整个模型的质量。比如虽然’;’不在V中，也会用’,’替代。</p>
<h2 id="Tags转输出文本"><a href="#Tags转输出文本" class="headerlink" title="Tags转输出文本"></a>Tags转输出文本</h2><p>对于不同任务，可以使用不同realization。比如上面提到的关于entity mention的替换方法，可以让我们对于代词的替换更confidence。另外一个优点是specific loss patterns can be addressed by adding specialized realization rules。比如说对于模式entity mention’s，使用his^DELETE，则对于这种模式模型更会DELETE掉entity mention后面的’s。</p>
<h1 id="模型整体框架"><a href="#模型整体框架" class="headerlink" title="模型整体框架"></a>模型整体框架</h1><p>模型使用encoder+decoder架构，encoder采用了bert-base架构，用pretrained case-sensitive BERT-base model初始化。原始的bert在encoder logits上做argmax作为解码输出，忽略了解码端前后的dependency。为了更好地建模output tags之间的依赖关系，在bert encoder上面增加一层transformer decoder。在计算decoder和encoder的联系时有两种方式，一种是计算全部的attention activation，另一种是只计算当前step的encoder activation。本文发现使用后一种方法效果更好，速度也更快。整体框架图如下：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lasertagger/4.png" alt="图片"></p>
<h1 id="实验部分"><a href="#实验部分" class="headerlink" title="实验部分"></a>实验部分</h1><h2 id="Sentence-Fusion"><a href="#Sentence-Fusion" class="headerlink" title="Sentence Fusion"></a>Sentence Fusion</h2><p>实验首先分析了不同的vocabulary size对结果的影响，发现vocabulary size上升到一定值后，提升就很小了。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lasertagger/5.png" alt="图片"></p>
<p>同时也对不同的baseline进行了比较，为了公平，本文也重新训练了一个用BERT结构的seq2seq模型，来说明本文方法的效果。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lasertagger/6.png" alt="图片"></p>
<p>同时本文也发现，当训练数据量减少到450或4500时，本文方法仍能表现良好。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lasertagger/7.png" alt="图片"></p>
<h2 id="Abstractive-Summarization"><a href="#Abstractive-Summarization" class="headerlink" title="Abstractive Summarization"></a>Abstractive Summarization</h2><p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lasertagger/8.png" alt="图片"></p>
<h2 id="Grammatical-Error-Correction-1"><a href="#Grammatical-Error-Correction-1" class="headerlink" title="Grammatical Error Correction"></a>Grammatical Error Correction</h2><p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lasertagger/9.png" alt="图片"></p>
<h2 id="推理时间比较"><a href="#推理时间比较" class="headerlink" title="推理时间比较"></a>推理时间比较</h2><p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lasertagger/10.png" alt="图片"></p>
<h2 id="质量评测"><a href="#质量评测" class="headerlink" title="质量评测"></a>质量评测</h2><p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/lasertagger/11.png" alt="图片"></p>
]]></content>
      <tags>
        <tag>论文阅读</tag>
        <tag>LaserTagger</tag>
      </tags>
  </entry>
  <entry>
    <title>字符串模糊匹配的方法都有哪些</title>
    <url>/2020/03/11/%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%A8%A1%E7%B3%8A%E5%8C%B9%E9%85%8D%E7%9A%84%E6%96%B9%E6%B3%95%E9%83%BD%E6%9C%89%E5%93%AA%E4%BA%9B/</url>
    <content><![CDATA[<p>工作中经常遇到文本处理上的两个问题，一个是如何在长的文本串中找到跟短文本串最像的子串；另一个是如何将两个文本串进行对齐，忽略掉其中不同的部分。准备专门写一个工具来解决这些问题，因此先调研了模糊匹配和字符串对齐的工具。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://zhuanlan.zhihu.com/p/53135935" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/53135935</a><br><a href="https://www.thinbug.com/q/17740833" target="_blank" rel="noopener">https://www.thinbug.com/q/17740833</a><br><a href="https://github.com/eseraygun/python-alignment" target="_blank" rel="noopener">https://github.com/eseraygun/python-alignment</a><br><a href="https://pypi.org/project/StringDist/" target="_blank" rel="noopener">https://pypi.org/project/StringDist/</a><br><a href="https://pypi.org/project/edlib/" target="_blank" rel="noopener">https://pypi.org/project/edlib/</a><br><a href="https://pypi.org/project/strsimpy/" target="_blank" rel="noopener">https://pypi.org/project/strsimpy/</a><br><a href="https://github.com/gfairchild/pyxDamerauLevenshtein" target="_blank" rel="noopener">https://github.com/gfairchild/pyxDamerauLevenshtein</a><br><a href="https://github.com/mbreese/swalign/" target="_blank" rel="noopener">https://github.com/mbreese/swalign/</a><br>打印表格：<a href="https://pypi.org/project/tabulate/" target="_blank" rel="noopener">https://pypi.org/project/tabulate/</a><br><a href="https://pypi.org/project/weighted-levenshtein/" target="_blank" rel="noopener">https://pypi.org/project/weighted-levenshtein/</a><br><a href="https://pypi.org/project/nwalign/" target="_blank" rel="noopener">https://pypi.org/project/nwalign/</a><br><a href="https://pypi.org/project/pyhacrf-datamade/" target="_blank" rel="noopener">https://pypi.org/project/pyhacrf-datamade/</a><br>打印表格：<a href="https://pypi.org/project/Frmt/" target="_blank" rel="noopener">https://pypi.org/project/Frmt/</a></p>
</blockquote>
<h1 id="difflib"><a href="#difflib" class="headerlink" title="difflib"></a>difflib</h1><blockquote>
<p>参考：<br><a href="https://blog.csdn.net/Lockey23/article/details/77913855" target="_blank" rel="noopener">https://blog.csdn.net/Lockey23/article/details/77913855</a><br><a href="https://blog.csdn.net/gavin_john/article/details/78951698" target="_blank" rel="noopener">https://blog.csdn.net/gavin_john/article/details/78951698</a><br><a href="https://docs.python.org/3.5/library/difflib.html" target="_blank" rel="noopener">https://docs.python.org/3.5/library/difflib.html</a></p>
</blockquote>
<p>difflib模块提供的类和方法用来进行序列的差异化比较，它能够比对文件并生成差异结果文本或者html格式的差异化比较页面。</p>
<h2 id="SequenceMatcher"><a href="#SequenceMatcher" class="headerlink" title="SequenceMatcher"></a>SequenceMatcher</h2><p>SequenceMatcher类可以用来比较两个任意类型的数据，只要是可以哈希的。它使用一个算法来计算序列的最长连续子序列，并且忽略没有意义的“无用数据”。下面代码计算了模糊匹配的相似度：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; import difflib</span><br><span class="line">&gt;&gt;&gt; difflib.SequenceMatcher(None,&quot;amazing&quot;,&quot;amaging&quot;).ratio()</span><br><span class="line">0.8571428571428571</span><br></pre></td></tr></table></figure>
<p>其基本算法比Ratcliff和Obershelp在20世纪80年代末发表的“格式塔模式匹配”(gestalt pattern matching)算法更早，也更新奇。其思想是寻找不包含“垃圾”元素的最长连续匹配子序列;这些“垃圾”元素在某种意义上是无趣的，比如空白行或空白(垃圾信息处理是Ratcliff和Obershelp算法的扩展)。然后，将相同的思想递归地应用到匹配子序列的左子序列和右子序列。这不会产生最小的编辑序列，但是会产生人们“看起来正确”的匹配。<br>在时间复杂度上，基本的Ratcliff-Obershelp算法在最坏情况下是三次时间，在期望情况下是二次时间。SequenceMatcher是最坏情况下的二次时间，它的期望情况行为以一种复杂的方式依赖于序列有多少个公共元素;最好的情况是时间是线性的。</p>
<p>SequenceMatcher支持一种自动将某些序列项视为垃圾的启发式方法。启发式计算每个单独的项目在序列中出现的次数。如果一个项目的重复项(在第一个之后)占序列的1%以上，并且序列至少有200个项目长，则该项目将被标记为“popular”，并被视为垃圾，以便进行序列匹配。在创建SequenceMatcher时，可以通过将autojunk参数设置为False来关闭这种启发式。下面代码可以得到编辑距离的所有操作：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import difflib</span><br><span class="line">s1 &#x3D; [1, 2, 3, 5, 6, 4]</span><br><span class="line">s2 &#x3D; [2, 3, 5, 4, 6, 1]</span><br><span class="line"># 忽略所有空格</span><br><span class="line"># SequenceMatcher(lambda x: x &#x3D;&#x3D; &#39; &#39;, A, B)</span><br><span class="line">matcher &#x3D; difflib.SequenceMatcher(None, s1, s2)</span><br><span class="line">for tag, i1, i2, j1, j2 in reversed(matcher.get_opcodes()):</span><br><span class="line">    if tag &#x3D;&#x3D; &#39;delete&#39;:</span><br><span class="line">        print(&#39;Remove &#123;&#125; from positions [&#123;&#125;:&#123;&#125;]&#39;.format(</span><br><span class="line">            s1[i1:i2], i1, i2))</span><br><span class="line">        print(&#39;  before &#x3D;&#39;, s1)</span><br><span class="line">        del s1[i1:i2]</span><br><span class="line">    elif tag &#x3D;&#x3D; &#39;equal&#39;:</span><br><span class="line">        print(&#39;s1[&#123;&#125;:&#123;&#125;] and s2[&#123;&#125;:&#123;&#125;] are the same&#39;.format(</span><br><span class="line">            i1, i2, j1, j2))</span><br><span class="line">    elif tag &#x3D;&#x3D; &#39;insert&#39;:</span><br><span class="line">        print(&#39;Insert &#123;&#125; from s2[&#123;&#125;:&#123;&#125;] into s1 at &#123;&#125;&#39;.format(</span><br><span class="line">            s2[j1:j2], j1, j2, i1))</span><br><span class="line">        print(&#39;  before &#x3D;&#39;, s1)</span><br><span class="line">        s1[i1:i2] &#x3D; s2[j1:j2]</span><br><span class="line">    elif tag &#x3D;&#x3D; &#39;replace&#39;:</span><br><span class="line">        print((&#39;Replace &#123;&#125; from s1[&#123;&#125;:&#123;&#125;] &#39;</span><br><span class="line">               &#39;with &#123;&#125; from s2[&#123;&#125;:&#123;&#125;]&#39;).format(</span><br><span class="line">                   s1[i1:i2], i1, i2, s2[j1:j2], j1, j2))</span><br><span class="line">        print(&#39;  before &#x3D;&#39;, s1)</span><br><span class="line">        s1[i1:i2] &#x3D; s2[j1:j2]</span><br><span class="line">    print(&#39;   after &#x3D;&#39;, s1, &#39;\n&#39;)</span><br><span class="line">print(&#39;s1 &#x3D;&#x3D; s2:&#39;, s1 &#x3D;&#x3D; s2)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Replace [4] from s1[5:6] with [1] from s2[5:6]</span><br><span class="line">  before &#x3D; [1, 2, 3, 5, 6, 4]</span><br><span class="line">   after &#x3D; [1, 2, 3, 5, 6, 1] </span><br><span class="line"></span><br><span class="line">s1[4:5] and s2[4:5] are the same</span><br><span class="line">   after &#x3D; [1, 2, 3, 5, 6, 1] </span><br><span class="line"></span><br><span class="line">Insert [4] from s2[3:4] into s1 at 4</span><br><span class="line">  before &#x3D; [1, 2, 3, 5, 6, 1]</span><br><span class="line">   after &#x3D; [1, 2, 3, 5, 4, 6, 1] </span><br><span class="line"></span><br><span class="line">s1[1:4] and s2[0:3] are the same</span><br><span class="line">   after &#x3D; [1, 2, 3, 5, 4, 6, 1] </span><br><span class="line"></span><br><span class="line">Remove [1] from positions [0:1]</span><br><span class="line">  before &#x3D; [1, 2, 3, 5, 4, 6, 1]</span><br><span class="line">   after &#x3D; [2, 3, 5, 4, 6, 1]</span><br></pre></td></tr></table></figure>
<h2 id="get-matching-blocks"><a href="#get-matching-blocks" class="headerlink" title="get_matching_blocks"></a>get_matching_blocks</h2><p>返回匹配子序列的三元组列表。每个三元组的形式是(i, j, n)，表示a[i:i+n] == b[j:j+n]。在i和j中，三元组是单调递增的。最后一个三元组是一个哑元，它的值是(len(a)， len(b)， 0)，它是唯一一个n == 0的三元组。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; s &#x3D; SequenceMatcher(None, &quot;abxcd&quot;, &quot;abcd&quot;)</span><br><span class="line">&gt;&gt;&gt; s.get_matching_blocks()</span><br><span class="line">[Match(a&#x3D;0, b&#x3D;0, size&#x3D;2), Match(a&#x3D;3, b&#x3D;2, size&#x3D;2), Match(a&#x3D;5, b&#x3D;4, size&#x3D;0)]</span><br></pre></td></tr></table></figure>
<p>如果想获得所有match的子串，可以这样写：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import difflib</span><br><span class="line">def matches(large_string, query_string, threshold):</span><br><span class="line">    words &#x3D; large_string.split()</span><br><span class="line">    for word in words:</span><br><span class="line">        s &#x3D; difflib.SequenceMatcher(None, word, query_string)</span><br><span class="line">        match &#x3D; &#39;&#39;.join(word[i:i+n] for i, j, n in s.get_matching_blocks() if n)</span><br><span class="line">        if len(match) &#x2F; float(len(query_string)) &gt;&#x3D; threshold:</span><br><span class="line">            yield match</span><br><span class="line">large_string &#x3D; &quot;thelargemanhatanproject is a great project in themanhattincity&quot;</span><br><span class="line">query_string &#x3D; &quot;manhattan&quot;</span><br></pre></td></tr></table></figure>
<p>结果：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; print(list(matches(large_string, query_string, 0.8)))</span><br><span class="line">[&#39;manhatan&#39;, &#39;manhattn&#39;]</span><br></pre></td></tr></table></figure>
<h1 id="fuzzywuzzy"><a href="#fuzzywuzzy" class="headerlink" title="fuzzywuzzy"></a>fuzzywuzzy</h1><blockquote>
<p>参考：<br><a href="https://github.com/seatgeek/fuzzywuzzy" target="_blank" rel="noopener">https://github.com/seatgeek/fuzzywuzzy</a><br><a href="https://zhuanlan.zhihu.com/p/77166627" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/77166627</a><br><a href="https://blog.csdn.net/laobai1015/article/details/80451371" target="_blank" rel="noopener">https://blog.csdn.net/laobai1015/article/details/80451371</a><br><a href="https://stackoverflow.com/questions/48671270/use-sklearn-tfidfvectorizer-with-already-tokenized-inputs" target="_blank" rel="noopener">https://stackoverflow.com/questions/48671270/use-sklearn-tfidfvectorizer-with-already-tokenized-inputs</a><br><a href="https://github.com/ing-bank/sparse_dot_topn" target="_blank" rel="noopener">https://github.com/ing-bank/sparse_dot_topn</a></p>
</blockquote>
<p>FuzzyWuzzy 是一个简单易用的模糊字符串匹配工具包。它依据 Levenshtein Distance 算法 计算两个序列之间的差异。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from fuzzywuzzy import fuzz</span><br><span class="line">from fuzzywuzzy import process</span><br><span class="line"></span><br><span class="line"># 简单匹配</span><br><span class="line">fuzz.ratio(&quot;this is a test&quot;, &quot;this is a test!&quot;)</span><br><span class="line"></span><br><span class="line"># 非完全匹配</span><br><span class="line">fuzz.partial_ratio(&quot;this is a test&quot;, &quot;this is a test!&quot;)</span><br><span class="line"></span><br><span class="line"># 忽略顺序匹配</span><br><span class="line">fuzz.ratio(&quot;fuzzy wuzzy was a bear&quot;, &quot;wuzzy fuzzy was a bear&quot;)</span><br><span class="line">fuzz.token_sort_ratio(&quot;fuzzy wuzzy was a bear&quot;, &quot;wuzzy fuzzy was a bear&quot;)</span><br><span class="line"></span><br><span class="line"># 去重子集匹配</span><br><span class="line">fuzz.token_sort_ratio(&quot;fuzzy was a bear&quot;, &quot;fuzzy fuzzy was a bear&quot;)</span><br><span class="line">fuzz.token_set_ratio(&quot;fuzzy was a bear&quot;, &quot;fuzzy fuzzy was a bear&quot;)</span><br><span class="line"></span><br><span class="line"># 返回模糊匹配的字符串和相似度</span><br><span class="line">choices &#x3D; [&quot;Atlanta Falcons&quot;, &quot;New York Jets&quot;, &quot;New York Giants&quot;, &quot;Dallas Cowboys&quot;]</span><br><span class="line">process.extract(&quot;new york jets&quot;, choices, limit&#x3D;2)</span><br><span class="line"># 返回：[(&#39;New York Jets&#39;, 100), (&#39;New York Giants&#39;, 78)]</span><br><span class="line">process.extractOne(&quot;cowboys&quot;, choices)</span><br><span class="line"># 返回：(&quot;Dallas Cowboys&quot;, 90)</span><br><span class="line"># 可以传入附加参数到 extractOne 方法来设置使用特定的匹配模式，一个典型的用法是来匹配文件路径</span><br><span class="line">process.extractOne(&quot;System of a down - Hypnotize - Heroin&quot;, songs)</span><br><span class="line"># 返回：(&#39;&#x2F;music&#x2F;library&#x2F;good&#x2F;System of a Down&#x2F;2005 - Hypnotize&#x2F;01 - Attack.mp3&#39;, 86)</span><br><span class="line">process.extractOne(&quot;System of a down - Hypnotize - Heroin&quot;, songs, scorer&#x3D;fuzz.token_sort_ratio)</span><br><span class="line"># 返回：(&quot;&#x2F;music&#x2F;library&#x2F;good&#x2F;System of a Down&#x2F;2005 - Hypnotize&#x2F;10 - She&#39;s Like Heroin.mp3&quot;, 61)</span><br></pre></td></tr></table></figure>
<p>上面方法可以用于在候选answers中找到最接近query的answer，但在面临大数据时，会遇到速度慢的问题。我们可以通过先确定一个候选answers的子集，再进行fuzzywuzzy的方式缩短运行时间。<br>首先，我们先将候选answers转换成tf-idf向量：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from sklearn.feature_extraction.text import TfidfVectorizer</span><br><span class="line">import nltk</span><br><span class="line">nltk.download(&#39;stopwords&#39;)</span><br><span class="line">from nltk.corpus import stopwords</span><br><span class="line">stop_words &#x3D; set(stopwords.words(&#39;english&#39;))</span><br><span class="line">choices &#x3D; [[&quot;candle&quot;], [&quot;Don&#39;t&quot;, &quot;trouble&quot;, &quot;trouble&quot;, &quot;until&quot;, &quot;trouble&quot;, &quot;troubles&quot;, &quot;you.&quot;], [&quot;A&quot;, &quot;bad&quot;, &quot;excuse&quot;, &quot;is&quot;, &quot;better&quot;, &quot;than&quot;, &quot;none&quot;, &quot;at&quot;, &quot;all.&quot;], [&quot;Bad&quot;, &quot;excuses&quot;, &quot;are&quot;, &quot;worse&quot;, &quot;than&quot;, &quot;none.&quot;], [&quot;A&quot;, &quot;bribe&quot;, &quot;in&quot;, &quot;hand&quot;, &quot;betrays&quot;, &quot;mischief&quot;, &quot;at&quot;, &quot;heart.&quot;], [&quot;A&quot;, &quot;candle&quot;, &quot;lights&quot;, &quot;others&quot;, &quot;and&quot;, &quot;consumes&quot;, &quot;itself.&quot;], [&quot;Don&#39;t&quot;, &quot;teach&quot;, &quot;your&quot;, &quot;grandmother&quot;, &quot;to&quot;, &quot;suck&quot;, &quot;eggs.&quot;], [&quot;A&quot;, &quot;teacher&quot;, &quot;is&quot;, &quot;just&quot;, &quot;a&quot;, &quot;candle&quot;, &quot;,&quot;, &quot;lights&quot;, &quot;others&quot;, &quot;and&quot;, &quot;consumes&quot;, &quot;itself.&quot;], [&quot;A&quot;, &quot;a&quot;, &quot;candle&quot;, &quot;lights&quot;, &quot;others&quot;, &quot;and&quot;, &quot;consumes&quot;, &quot;itself.&quot;]]</span><br><span class="line"># 按word分，还可以按char、char_wb处理：</span><br><span class="line"># vectorizer &#x3D; TfidfVectorizer(min_df&#x3D;1, analyzer&#x3D;&#39;word&#39;)</span><br><span class="line"># 也可以使用自定义的分词</span><br><span class="line">vectorizer &#x3D; TfidfVectorizer(analyzer&#x3D;lambda x:[w for w in x if w not in stop_words])</span><br><span class="line">tf_idf_matrix_candidates &#x3D; vectorizer.fit_transform(choices)</span><br><span class="line">tf_idf_matrix_queries &#x3D; tf_idf_matrix_candidates[-1]</span><br><span class="line">tf_idf_matrix_candidates &#x3D; tf_idf_matrix_candidates[:-1]</span><br><span class="line"># vectorizer.get_feature_names()可以看到所有的token</span><br></pre></td></tr></table></figure>
<p>其次使用sparse_dot_topn找到相似的字符串：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from scipy.sparse import csr_matrix</span><br><span class="line">import sparse_dot_topn.sparse_dot_topn as ct</span><br><span class="line"></span><br><span class="line">def awesome_cossim_top(A, B, ntop, lower_bound&#x3D;0):</span><br><span class="line">    # force A and B as a CSR matrix.</span><br><span class="line">    # If they have already been CSR,there is no overhead</span><br><span class="line">    A &#x3D; A.tocsr()</span><br><span class="line">    B &#x3D; B.tocsr()</span><br><span class="line">    M, _ &#x3D; A.shape</span><br><span class="line">    _, N &#x3D; B.shape</span><br><span class="line">    idx_dtype &#x3D; np.int32</span><br><span class="line">    nnz_max &#x3D; M*ntop</span><br><span class="line">    indptr &#x3D; np.zeros(M+1,dtype&#x3D;idx_dtype)</span><br><span class="line">    indices &#x3D; np.zeros(nnz_max,dtype&#x3D;idx_dtype)</span><br><span class="line">    data &#x3D; np.zeros(nnz_max,dtype&#x3D;A.dtype)</span><br><span class="line">    ct.sparse_dot_topn(M, N, np.asarray(A.indptr,dtype&#x3D;idx_dtype), np.asarray(A.indices,dtype&#x3D;idx_dtype), A.data, np.asarray(B.indptr,dtype&#x3D;idx_dtype), np.asarray(B.indices,dtype&#x3D;idx_dtype), B.data, ntop, lower_bound, indptr, indices, data)</span><br><span class="line">    return csr_matrix((data,indices,indptr),shape&#x3D;(M,N))</span><br><span class="line">    </span><br><span class="line">matches &#x3D; awesome_cossim_top(tf_idf_matrix_candidates, tf_idf_matrix_queries.transpose(),1,0.0).todense()</span><br><span class="line">matches &#x3D; np.squeeze(matches)</span><br><span class="line">match_score_index &#x3D; np.argsort(-matches)</span><br></pre></td></tr></table></figure>
<h1 id="alignment"><a href="#alignment" class="headerlink" title="alignment"></a>alignment</h1><p>alignment主要用于字符串之间对齐，其使用方法如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from alignment.sequence import Sequence</span><br><span class="line">from alignment.vocabulary import Vocabulary</span><br><span class="line">from alignment.sequencealigner import SimpleScoring, GlobalSequenceAligner</span><br><span class="line"># Create sequences to be aligned.</span><br><span class="line">a &#x3D; Sequence(&#39;what a beautiful day&#39;.split())</span><br><span class="line">b &#x3D; Sequence(&#39;what a disappointingly bad day&#39;.split())</span><br><span class="line"># Create a vocabulary and encode the sequences.</span><br><span class="line">v &#x3D; Vocabulary()</span><br><span class="line">aEncoded &#x3D; v.encodeSequence(a)</span><br><span class="line">bEncoded &#x3D; v.encodeSequence(b)</span><br><span class="line"># Create a scoring and align the sequences using global aligner.</span><br><span class="line">scoring &#x3D; SimpleScoring(1, -1)</span><br><span class="line">aligner &#x3D; GlobalSequenceAligner(scoring, -1)</span><br><span class="line">score, encodeds &#x3D; aligner.align(aEncoded, bEncoded, backtrace&#x3D;True)</span><br><span class="line"># Iterate over optimal alignments and print them.</span><br><span class="line">for encoded in encodeds:</span><br><span class="line">    alignment &#x3D; v.decodeSequenceAlignment(encoded)</span><br><span class="line">    print(alignment)</span><br><span class="line">    print(&#39;Alignment score:&#39;, alignment.score)</span><br><span class="line">    print(&#39;Percent identity:&#39;, alignment.percentIdentity())</span><br></pre></td></tr></table></figure>
<h1 id="strsimpy"><a href="#strsimpy" class="headerlink" title="strsimpy"></a>strsimpy</h1><p>这是一个用于计算各种字符串距离的包。其使用方法如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from strsimpy.levenshtein import Levenshtein</span><br><span class="line">levenshtein &#x3D; Levenshtein()</span><br><span class="line">print(levenshtein.distance(&#39;My string&#39;, &#39;My $string&#39;))</span><br><span class="line">from strsimpy.normalized_levenshtein import NormalizedLevenshtein</span><br><span class="line">normalized_levenshtein &#x3D; NormalizedLevenshtein()</span><br><span class="line">print(normalized_levenshtein.distance(&#39;My string&#39;, &#39;My $string&#39;))</span><br><span class="line">print(normalized_levenshtein.similarity(&#39;My string&#39;, &#39;My $string&#39;))</span><br><span class="line"></span><br><span class="line"># 带权重的编辑距离</span><br><span class="line">from strsimpy.weighted_levenshtein import WeightedLevenshtein</span><br><span class="line">from strsimpy.weighted_levenshtein import CharacterSubstitutionInterface</span><br><span class="line">class CharacterSubstitution(CharacterSubstitutionInterface):</span><br><span class="line">    def cost(self, c0, c1):</span><br><span class="line">        if c0&#x3D;&#x3D;&#39;t&#39; and c1&#x3D;&#x3D;&#39;r&#39;:</span><br><span class="line">            return 0.5</span><br><span class="line">        return 1.0</span><br><span class="line">weighted_levenshtein &#x3D; WeightedLevenshtein(CharacterSubstitution())</span><br><span class="line">print(weighted_levenshtein.distance(&#39;String1&#39;, &#39;String2&#39;))</span><br><span class="line">from strsimpy.damerau import Damerau</span><br><span class="line">damerau &#x3D; Damerau()</span><br><span class="line">print(damerau.distance(&#39;ABCDEF&#39;, &#39;ABDCEF&#39;))</span><br><span class="line"></span><br><span class="line"># 最优化对齐后的编辑距离</span><br><span class="line">from strsimpy.optimal_string_alignment import OptimalStringAlignment</span><br><span class="line">optimal_string_alignment &#x3D; OptimalStringAlignment()</span><br><span class="line">print(optimal_string_alignment.distance(&#39;CA&#39;, &#39;ABC&#39;))</span><br><span class="line"></span><br><span class="line">from strsimpy.jaro_winkler import JaroWinkler</span><br><span class="line">jarowinkler &#x3D; JaroWinkler()</span><br><span class="line">print(jarowinkler.similarity(&#39;My string&#39;, &#39;My tsring&#39;))</span><br><span class="line"></span><br><span class="line"># 最长公共子序列</span><br><span class="line">from strsimpy.longest_common_subsequence import LongestCommonSubsequence</span><br><span class="line">lcs &#x3D; LongestCommonSubsequence()</span><br><span class="line">print(lcs.distance(&#39;AGCAT&#39;, &#39;GAC&#39;))</span><br><span class="line"></span><br><span class="line">from strsimpy.metric_lcs import MetricLCS</span><br><span class="line">metric_lcs &#x3D; MetricLCS()</span><br><span class="line">s1 &#x3D; &#39;ABCDEFG&#39;</span><br><span class="line">s2 &#x3D; &#39;ABCDEFHJKL&#39;</span><br><span class="line">print(metric_lcs.distance(s1, s2))</span><br><span class="line"></span><br><span class="line"># ngram</span><br><span class="line">from strsimpy.ngram import NGram</span><br><span class="line">twogram &#x3D; NGram(2)</span><br><span class="line">print(twogram.distance(&#39;ABCD&#39;, &#39;ABTUIO&#39;))</span><br><span class="line">s1 &#x3D; &#39;Adobe CreativeSuite 5 Master Collection from cheap 4zp&#39;</span><br><span class="line">s2 &#x3D; &#39;Adobe CreativeSuite 5 Master Collection from cheap d1x&#39;</span><br><span class="line">fourgram &#x3D; NGram(4)</span><br><span class="line">print(fourgram.distance(s1, s2))</span><br><span class="line"></span><br><span class="line">from strsimpy.qgram import QGram</span><br><span class="line">qgram &#x3D; QGram(2)</span><br><span class="line">print(qgram.distance(&#39;ABCD&#39;, &#39;ABCE&#39;))</span><br><span class="line"></span><br><span class="line">from strsimpy.cosine import Cosine</span><br><span class="line">cosine &#x3D; Cosine(2)</span><br><span class="line">s0 &#x3D; &#39;My first string&#39;</span><br><span class="line">s1 &#x3D; &#39;My other string...&#39;</span><br><span class="line">p0 &#x3D; cosine.get_profile(s0)</span><br><span class="line">p1 &#x3D; cosine.get_profile(s1)</span><br><span class="line">print(cosine.similarity_profiles(p0, p1))</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title>大话交叉熵损失函数</title>
    <url>/2020/03/10/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<p>使用keras进行二分类时，常使用binary_crossentropy作为损失函数。那么它的原理是什么，跟categorical_crossentropy、sparse_categorical_crossentropy有什么区别？在进行文本分类时，如何选择损失函数，有哪些优化损失函数的方式？本文将从原理到实现进行一一介绍。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a" target="_blank" rel="noopener">https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a</a></p>
</blockquote>
<h1 id="binary-crossentropy"><a href="#binary-crossentropy" class="headerlink" title="binary_crossentropy"></a>binary_crossentropy</h1><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>假设我们想做一个二分类，输入有10个点：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x &#x3D; [-2.2, -1.4, -0.8, 0.2, 0.4, 0.8, 1.2, 2.2, 2.9, 4.6]</span><br></pre></td></tr></table></figure>
<p>输出有两类，分别为红色、绿色：<br><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5/1.png" alt="图片"></p>
<p>我们可以将问题描述成“这个点是绿色的吗?”，或者“这个点是绿色的概率是多少?”。理想情况下，绿点的概率是1.0，而红点的（是绿色的）概率是0.0。从而，绿色就是正样本，红色就是负样本。</p>
<p>如果我们拟合一个模型来执行这种分类，它将预测我们每个点的绿色概率。那么我们如何评估预测概率的好坏呢？这就是损失函数的意义，</p>
<p>Binary CrossEntorpy的计算如下：</p>
<p>$H_{p}(q)=-\frac{1}{N} \sum_{i=1}^{N} y_{i} \cdot \log \left(p\left(y_{i}\right)\right)+\left(1-y_{i}\right) \cdot \log \left(1-p\left(y_{i}\right)\right)$</p>
<p>其中y是标签(1代表绿色点，0代表红色点)，p(y)是所有N个点都是绿色的预测概率。看到这个计算式，发现对于每一个绿点(y=1)它增加了log(p(y))的损失（概率越大，增加的越小），也就是它是绿色的概率。下面我们可视化地看一下这个损失函数。</p>
<p>假设我们训练一个逻辑回归模型来进行分类，那么训练出的函数趋近于一个sigmoid曲线，曲线上每个点表示对于每个x是绿色点的概率：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5/2.png" alt="图片"></p>
<p>那么对于这些绿色的点，他们预测为绿色的概率是多少呢？实际下面图片中绿色的bar：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5/3.png" alt="图片"></p>
<p>那么红色点预测为红色的概率是多少呢？实际就是下面图片中红色的bar：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5/4.png" alt="图片"></p>
<p>我们把图片绘制得更好看一下，如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5/5.png" alt="图片"></p>
<p>因为我们要计算损失，我们需要惩罚错误的预测。如果与正例相关的概率是1.0，我们需要它的损失为零。相反，如果概率很低，比如0.01，我们需要它的损失是巨大的！取概率的(负)对数非常适合我们的目的(由于0.0和1.0之间的值的对数是负的，我们取负对数来获得正的损失值)。下面这个图展示了当正例的概率逐渐趋近于0时loss的变化：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5/6.png" alt="图片"></p>
<p>下面这个图表示了，我们使用负对数时每个点的损失，我们计算其平均值，就是binary cross entropy了！</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5/7.png" alt="图片"></p>
<h2 id="keras实现"><a href="#keras实现" class="headerlink" title="keras实现"></a>keras实现</h2><p>tf2.1的bce用法如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bce &#x3D; tf.keras.losses.BinaryCrossentropy()</span><br><span class="line">loss &#x3D; bce([0., 0., 1., 1.], [1., 1., 1., 0.])</span><br><span class="line">print(&#39;Loss: &#39;, loss.numpy())  # Loss: 11.522857</span><br></pre></td></tr></table></figure>
<p>或者：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">model &#x3D; tf.keras.Model(inputs, outputs)</span><br><span class="line">model.compile(&#39;sgd&#39;, loss&#x3D;tf.keras.losses.BinaryCrossentropy())</span><br></pre></td></tr></table></figure>
<p>具体实现如下（tensorflow.python.keras/losses）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class BinaryCrossentropy(LossFunctionWrapper):</span><br><span class="line">    def __init__(self, from_logits&#x3D;False,</span><br><span class="line">                  label_smoothing&#x3D;0,</span><br><span class="line">                  reduction&#x3D;losses_utils.ReductionV2.AUTO,</span><br><span class="line">                  name&#x3D;&#39;binary_crossentropy&#39;):</span><br><span class="line">        super(BinaryCrossentropy, self).__init__(</span><br><span class="line">              binary_crossentropy,</span><br><span class="line">              name&#x3D;name,</span><br><span class="line">              reduction&#x3D;reduction,</span><br><span class="line">              from_logits&#x3D;from_logits,</span><br><span class="line">              label_smoothing&#x3D;label_smoothing)</span><br><span class="line">        self.from_logits &#x3D; from_logits</span><br><span class="line"></span><br><span class="line">def binary_crossentropy(y_true, y_pred, from_logits&#x3D;False, label_smoothing&#x3D;0):</span><br><span class="line">    y_pred &#x3D; ops.convert_to_tensor_v2(y_pred)</span><br><span class="line">    y_true &#x3D; math_ops.cast(y_true, y_pred.dtype)</span><br><span class="line">    label_smoothing &#x3D; ops.convert_to_tensor_v2(label_smoothing, dtype&#x3D;K.floatx())</span><br><span class="line">    </span><br><span class="line">    def _smooth_labels():</span><br><span class="line">      return y_true * (1.0 - label_smoothing) + 0.5 * label_smoothing</span><br><span class="line">    </span><br><span class="line">    y_true &#x3D; smart_cond.smart_cond(label_smoothing, _smooth_labels, lambda: y_true)</span><br><span class="line">    return K.mean(K.binary_crossentropy(y_true, y_pred, from_logits&#x3D;from_logits), axis&#x3D;-1)</span><br></pre></td></tr></table></figure>
<p>在上面代码中，如果from_logits=True，则认为y_predit是tensor（可以认为是[0,1]之间的概率值），使用from_logits=True可以更稳定一些。label_smoothing在[0,1]之间。reduction的默认值是AUTO，表示根据上下文确定；如果是SUM_OVER_BATCH_SIZE表示整个batch的结果相加。<br>其中K.binary_crossentropy实现如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def binary_crossentropy(target, output, from_logits&#x3D;False):</span><br><span class="line">  if from_logits:</span><br><span class="line">    return nn.sigmoid_cross_entropy_with_logits(labels&#x3D;target, logits&#x3D;output)</span><br><span class="line"></span><br><span class="line">  if not isinstance(output, (ops.EagerTensor, variables_module.Variable)):</span><br><span class="line">    output &#x3D; _backtrack_identity(output)</span><br><span class="line">    if output.op.type &#x3D;&#x3D; &#39;Sigmoid&#39;:</span><br><span class="line">      assert len(output.op.inputs) &#x3D;&#x3D; 1</span><br><span class="line">      output &#x3D; output.op.inputs[0]</span><br><span class="line">      return nn.sigmoid_cross_entropy_with_logits(labels&#x3D;target, logits&#x3D;output)</span><br><span class="line"></span><br><span class="line">  epsilon_ &#x3D; _constant_to_tensor(epsilon(), output.dtype.base_dtype)</span><br><span class="line">  output &#x3D; clip_ops.clip_by_value(output, epsilon_, 1. - epsilon_)</span><br><span class="line"></span><br><span class="line">  bce &#x3D; target * math_ops.log(output + epsilon())</span><br><span class="line">  bce +&#x3D; (1 - target) * math_ops.log(1 - output + epsilon())</span><br><span class="line">  return -bce</span><br></pre></td></tr></table></figure>
<p>sigmoid_cross_entropy_with_logits实现如下：（该函数适用于不同类标签之间相互独立的情况，例如一个图片可以既包含大象也包含狗）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def sigmoid_cross_entropy_with_logits(_sentinel&#x3D;None, labels&#x3D;None, logits&#x3D;None, name&#x3D;None):</span><br><span class="line">    zeros &#x3D; array_ops.zeros_like(logits, dtype&#x3D;logits.dtype)</span><br><span class="line">    cond &#x3D; (logits &gt;&#x3D; zeros)</span><br><span class="line">    relu_logits &#x3D; array_ops.where(cond, logits, zeros)</span><br><span class="line">    neg_abs_logits &#x3D; array_ops.where(cond, -logits, logits)</span><br><span class="line">    return math_ops.add(relu_logits - logits * labels, math_ops.log1p(math_ops.exp(neg_abs_logits)), name&#x3D;name)</span><br></pre></td></tr></table></figure>
<p>对于上面代码，解释如下：<br>对于x=logits, z=labels，logistic损失定义为</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))</span><br><span class="line">&#x3D; z * -log(1 &#x2F; (1 + exp(-x))) + (1 - z) * -log(exp(-x) &#x2F; (1 + exp(-x)))</span><br><span class="line">&#x3D; z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))</span><br><span class="line">&#x3D; z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))</span><br><span class="line">&#x3D; (1 - z) * x + log(1 + exp(-x))</span><br><span class="line">&#x3D; x - x * z + log(1 + exp(-x))</span><br></pre></td></tr></table></figure>
<p>对于x&lt;0，为了防止exp(-x)溢出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x - x * z + log(1 + exp(-x))</span><br><span class="line">&#x3D; log(exp(x)) - x * z + log(1 + exp(-x))</span><br><span class="line">&#x3D; - x * z + log(1 + exp(x))</span><br></pre></td></tr></table></figure>
<p>为了保证稳定和不溢出，在实现过程中使用了如下等式：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">max(x, 0) - x * z + log(1 + exp(-abs(x)))</span><br></pre></td></tr></table></figure>
<h1 id="categorical-crossentropy"><a href="#categorical-crossentropy" class="headerlink" title="categorical_crossentropy"></a>categorical_crossentropy</h1><h2 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h2><p>CrossEntropy可用于多分类任务，且label且one-hot形式。它的计算式如下：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5/8.png" alt="图片"></p>
<h2 id="keras实现-1"><a href="#keras实现-1" class="headerlink" title="keras实现"></a>keras实现</h2><p>tf2.1的ce用法如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">y_true &#x3D; [[0, 1, 0], [0, 0, 1]]</span><br><span class="line">y_pred &#x3D; [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]</span><br><span class="line">cce &#x3D; tf.keras.losses.CategoricalCrossentropy()</span><br><span class="line"># Using &#39;auto&#39;&#x2F;&#39;sum_over_batch_size&#39; reduction type.</span><br><span class="line">cce(y_true, y_pred).numpy()</span><br><span class="line"># Calling with &#39;sample_weight&#39;.</span><br><span class="line">cce(y_true, y_pred, sample_weight&#x3D;tf.constant([0.3, 0.7])).numpy()</span><br><span class="line"># Using &#39;sum&#39; reduction type.</span><br><span class="line">cce &#x3D; tf.keras.losses.CategoricalCrossentropy(reduction&#x3D;tf.keras.losses.Reduction.NONE)</span><br><span class="line">cce(y_true, y_pred).numpy()</span><br><span class="line"># Usage with the &#96;compile&#96; API</span><br><span class="line">model &#x3D; tf.keras.Model(inputs, outputs)</span><br><span class="line">model.compile(&#39;sgd&#39;, loss&#x3D;tf.keras.losses.CategoricalCrossentropy())</span><br></pre></td></tr></table></figure>
<p>具体实现如下（tensorflow.python.keras/losses）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class CategoricalCrossentropy(LossFunctionWrapper):</span><br><span class="line">    def __init__(self,</span><br><span class="line">                   from_logits&#x3D;False,</span><br><span class="line">                   label_smoothing&#x3D;0,</span><br><span class="line">                   reduction&#x3D;losses_utils.ReductionV2.AUTO,</span><br><span class="line">                   name&#x3D;&#39;categorical_crossentropy&#39;):</span><br><span class="line">        super(CategoricalCrossentropy, self).__init__(</span><br><span class="line">                categorical_crossentropy,</span><br><span class="line">                name&#x3D;name,</span><br><span class="line">                reduction&#x3D;reduction,</span><br><span class="line">                from_logits&#x3D;from_logits,</span><br><span class="line">                label_smoothing&#x3D;label_smoothing)</span><br><span class="line"></span><br><span class="line">    def categorical_crossentropy(y_true,</span><br><span class="line">                                 y_pred,</span><br><span class="line">                                 from_logits&#x3D;False,</span><br><span class="line">                                 label_smoothing&#x3D;0):</span><br><span class="line">        y_pred &#x3D; ops.convert_to_tensor_v2(y_pred)</span><br><span class="line">        y_true &#x3D; math_ops.cast(y_true, y_pred.dtype)</span><br><span class="line">        label_smoothing &#x3D; ops.convert_to_tensor_v2(label_smoothing, dtype&#x3D;K.floatx())</span><br><span class="line">        def _smooth_labels():</span><br><span class="line">            num_classes &#x3D; math_ops.cast(array_ops.shape(y_true)[1], y_pred.dtype)</span><br><span class="line">            return y_true * (1.0 - label_smoothing) + (label_smoothing &#x2F; num_classes)</span><br><span class="line">        y_true &#x3D; smart_cond.smart_cond(label_smoothing, _smooth_labels, lambda: y_true)</span><br><span class="line">        return K.categorical_crossentropy(y_true, y_pred, from_logits&#x3D;from_logits)</span><br></pre></td></tr></table></figure>
<p>其中K.categorical_crossentropy实现如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def categorical_crossentropy(target, output, from_logits&#x3D;False, axis&#x3D;-1):</span><br><span class="line">    if from_logits:</span><br><span class="line">        return nn.softmax_cross_entropy_with_logits_v2(</span><br><span class="line">            labels&#x3D;target, logits&#x3D;output, axis&#x3D;axis)</span><br><span class="line">    if not isinstance(output, (ops.EagerTensor, variables_module.Variable)):</span><br><span class="line">        output &#x3D; _backtrack_identity(output)</span><br><span class="line">        if output.op.type &#x3D;&#x3D; &#39;Softmax&#39;:</span><br><span class="line">            output &#x3D; output.op.inputs[0]</span><br><span class="line">            return nn.softmax_cross_entropy_with_logits_v2(</span><br><span class="line">                labels&#x3D;target, logits&#x3D;output, axis&#x3D;axis)</span><br><span class="line">    # scale preds so that the class probas of each sample sum to 1</span><br><span class="line">    output &#x3D; output &#x2F; math_ops.reduce_sum(output, axis, True)</span><br><span class="line">    # Compute cross entropy from probabilities.</span><br><span class="line">    epsilon_ &#x3D; _constant_to_tensor(epsilon(), output.dtype.base_dtype)</span><br><span class="line">    output &#x3D; clip_ops.clip_by_value(output, epsilon_, 1. - epsilon_)</span><br><span class="line">    return -math_ops.reduce_sum(target * math_ops.log(output), axis)</span><br></pre></td></tr></table></figure>
<h1 id="sparse-categorical-crossentropy"><a href="#sparse-categorical-crossentropy" class="headerlink" title="sparse_categorical_crossentropy"></a>sparse_categorical_crossentropy</h1><h2 id="原理-2"><a href="#原理-2" class="headerlink" title="原理"></a>原理</h2><p>跟categorical_crossentropy的区别是其标签不是one-hot，而是integer。比如在categorical_crossentropy是[1,0,0]，在sparse_categorical_crossentropy中是3.</p>
<h2 id="keras实现-2"><a href="#keras实现-2" class="headerlink" title="keras实现"></a>keras实现</h2><p>tf2.1中使用方法如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">y_true &#x3D; [1, 2]</span><br><span class="line">y_pred &#x3D; [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]</span><br><span class="line">loss &#x3D; tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)</span><br></pre></td></tr></table></figure>
<h1 id="其他技巧"><a href="#其他技巧" class="headerlink" title="其他技巧"></a>其他技巧</h1><h2 id="focal-loss"><a href="#focal-loss" class="headerlink" title="focal loss"></a>focal loss</h2><blockquote>
<p>参考：<br><a href="https://ldzhangyx.github.io/2018/11/16/focal-loss/" target="_blank" rel="noopener">https://ldzhangyx.github.io/2018/11/16/focal-loss/</a></p>
</blockquote>
<p>Focal Loss的出现是为了解决训练集正负样本极度不平衡的情况，通过reshape标准交叉熵损失解决类别不均衡（Class Imbalance）,这样它就能降低容易分类的样例的比重（Well-classified Examples）。这个方法专注训练在Hard Examples的稀疏集合上，能够防止大量的Easy Negatives在训练中压倒训练器。其公式为：</p>
<p>$F L\left(p_{t}\right)=-\left(1-p_{t}\right)^{\gamma} \log \left(p_{t}\right)​$</p>
<p>其中参数为0的时候，Focal Loss退化为交叉熵CE。当这个参数不同时，对loss的影响如下：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/%E5%A4%A7%E8%AF%9D%E4%BA%A4%E5%8F%89%E7%86%B5/0.png" alt="图片"></p>
<p>p_t越大，FL越小，其对总体loss所做的贡献就越小；反过来说，p_t越小（小于0.5的情况也就是被误分类），越能反映在总体loss上。</p>
<p>Focal Loss的tensorflow api：<a href="https://www.tensorflow.org/addons/api_docs/python/tfa/losses/SigmoidFocalCrossEntropy" target="_blank" rel="noopener">https://www.tensorflow.org/addons/api_docs/python/tfa/losses/SigmoidFocalCrossEntropy</a></p>
<h2 id="label-smooth"><a href="#label-smooth" class="headerlink" title="label smooth"></a>label smooth</h2><blockquote>
<p>参考：<br><a href="https://towardsdatascience.com/what-is-label-smoothing-108debd7ef06" target="_blank" rel="noopener">https://towardsdatascience.com/what-is-label-smoothing-108debd7ef06</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/76587755" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/76587755</a></p>
</blockquote>
<p>在使用深度学习模型进行分类任务时，我们通常会遇到以下问题：overfit和over confidence。Overfit问题得到了很好的研究，可以通过earlystop、dropout、正则化等方法来解决。另一方面，我们over confidence的工具较少。标签平滑是一种正则化技术，解决了这两个问题。</p>
<p>Label Smooth将y_hot和均匀分布的混合来代替一个hot编码的标签向量y_hot:</p>
<p>$y_{-} l s=(1-\alpha) * y_{-} h o t+\alpha / K$</p>
<p>K是标签类的数目，α是一个决定平滑的超参数。如果α= 0，我们获得最初的一个原始的y_hot编码。如果α= 1，我们得到均匀分布。</p>
<p>当损失函数为交叉熵时，使用标签平滑，模型将softmax函数应用于倒数第二层的logit向量z，计算其输出概率p。在这种情况下，交叉熵损失函数相对于logit的梯度为：</p>
<p>$\nabla C E=p-y=\operatorname{softmax}(z)-y$</p>
<p>其中y是标签分布，并且：</p>
<ul>
<li>梯度下降会使p尽可能接近y</li>
<li>梯度在-1和1之间有界</li>
</ul>
<p>一个标准的ont-hot希望有更大的logit gaps输入到里面。直观地说，较大的logit gap加上有界的梯度会使模型的自适应性降低，并且对其预测过于自信。相反，平滑的标签鼓励小的logit差距，可以得到更好的模型校准，并防止过度自信的预测。</p>
<p>下面我们使用一个例子说明：假设我们有K = 3类，我们的标签属于第一类。令[a, b, c]为logit向量。如果我们不使用标签平滑，那么标签向量就是一个one-hot向量[1,0,0]。我们的模型将a≫b和a≫c。例如,应用softmax分对数向量(10,0,0)给(0.9999,0,0)的4位小数。</p>
<p>如果我们使用标签的平滑与α= 0.1,平滑标签向量≈(0.9333,0.0333,0.0333)。logit向量[3.3322,0,0]在softmax之后将经过平滑处理的标签向量近似为小数点后4位，并且它的差距更小。这就是为什么我们称平滑标签为一种正则化技术，因为它可以防止最大的logit变得比其他的更大。</p>
<p>更形象地说，对于label_smoothing=0.2，则意味着标签0的概率是0.1，标签1的概率是0.9。</p>
<p>另一种解释（来自<a href="https://zhuanlan.zhihu.com/p/76587755" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/76587755</a>）：</p>
<p>​    在常见的多分类问题中，先经过softmax处理后进行交叉熵计算，原理很简单可以将计算loss理解为，为了使得网络对测试集预测的概率分布和其真实分布接近，常用的做法是使用one-hot对真实标签进行编码，作者认为这种将标签强制one-hot的方式使网络过于自信会导致过拟合，因此软化这种编码方式：$q^{\prime}(k | x)=(1-\epsilon) \delta_{k, y}+\epsilon u(k)​$</p>
<p>​    等号左侧：是一种新的预测的分布；等号右侧：前半部分是对原分布乘一个权重，$\epsilon$是一个超参，需要自己设定，取值在0到1范围内。后半部分u是一个均匀分布，k表示模型的类别数。</p>
<p>​    由以上公式可以看出，这种方式使label有$\epsilon$概率来自于均匀分布， $1-\epsilon$概率来自于原分布。这就相当于在原label上增加噪声，让模型的预测值不要过度集中于概率较高的类别，把一些概率放在概率较低的类别。因此，交叉熵可以替换为：$H\left(q^{\prime}, p\right)=-\sum_{k=1}^{K} \log p(k) q^{\prime}(k)=(1-\epsilon) H(q, p)+\epsilon H(u, p)$</p>
<p>​    可以理解为：loss为对<strong>“预测的分布与真实分布”</strong>及<strong>“预测分布与先验分布（均匀分布）”</strong>的惩罚。个人理解：label smooth的思路“做软化、防止过拟合、增加扰动”是好的，个人认为用均匀分布做先验分布有待商榷。</p>
]]></content>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：《Reformer: The Efficient Transformer》</title>
    <url>/2020/03/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E3%80%8AReformer-The-Efficient-Transformer%E3%80%8B/</url>
    <content><![CDATA[<p>本论文为谷歌近期发表的对Transformer改进的一篇论文，论文名字中的Efficient Transformer解释了论文的主要目的。过去一些基于Transformer结构的论文，一看到模型的总参数量就让人望而生畏，有些模型在我们的单卡GPU上根本跑不起来，因此就看了一下这篇论文。论文感觉比较偏工程，了解下它的大致思想就好。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://arxiv.org/pdf/2001.04451.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2001.04451.pdf</a><br><a href="https://github.com/google/trax/blob/master/trax/models/reformer/reformer.py" target="_blank" rel="noopener">https://github.com/google/trax/blob/master/trax/models/reformer/reformer.py</a><br><a href="https://zhuanlan.zhihu.com/p/92153420" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/92153420</a></p>
</blockquote>
<h1 id="论文摘要"><a href="#论文摘要" class="headerlink" title="论文摘要"></a>论文摘要</h1><p>为了解决Transformer模型在处理长序列时的GPU资源消耗问题，提出了更省内存和更快的Transformer模型结构。其改进主要有两点：</p>
<ul>
<li>使用locality-sensitive hashing代替dot-product attention，使得计算复杂度由 $O(L^2)$直接降为$O(LlogL)$，其中L为序列长度</li>
<li>使用reversible residual layers来代替传统的残差层，使得训练过程中对激活函数的值的存储由N次降低为1次，其中N是层数。</li>
</ul>
<h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h1><p>比较大的Transformer模型里的每一层有0.5Billion的参数，最多可达到64层。并且随着序列长度增加，单个文本train example需要能处理11k左右的token。对于音乐、图像等数据，序列可能会更长，因此有些模型只能在大型GPU集群中进行并行训练。受GPU显存限制，有的模型也很难在单个GPU机器上进行微调。</p>
<p>因此会有这样一个疑问，这么大的Transformer模型到底在哪里消耗了这么多资源？我们不妨计算一下：</p>
<ul>
<li>每层0.5Billion的参数需要2GB的存储</li>
<li>使用1024 embedding size和8 batch size训练的64k token的激活函数值需要64K <em> 1K </em> 8 = 0.5Billion的参数，即2GB的存储</li>
<li>N层网络需要将激活值存储N次(为了back-propagation时进行计算)</li>
<li>Feed-forward层的维度通常要比d_model大很多</li>
<li>对于序列长度L来说计算attention所需要的时空复杂度为O(L^2)</li>
</ul>
<p>具体计算过程如下：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/reformer1.png" alt="图片"></p>
<ul>
<li><p>Transformer Block</p>
<p>$h_{m i d}=\text { LayerNorm }\left(h_{i n}+\text { MultiHead }\left(h_{i n}\right)\right)$</p>
<p>$h_{\text {out}}=\text {LayerNorm }\left(h_{\text {mid}}+\mathrm{FFN}\left(h_{\text {mid}}\right)\right)$</p>
</li>
<li><p>Multi-head Attention</p>
</li>
</ul>
<p>$\begin{array}{l}\text {head}_{i}=\text { Attention }\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right) \ \text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V \ \text { MultiHead }(Q, K, V)=\text {Concat}\left(\text {head}_{1}, \ldots, \text {head}_{h}\right) W^{O}\end{array}$</p>
<ul>
<li>Attention输入：<ul>
<li>Q: (batch_size, seq_q, d_model)</li>
<li>K: (batch_size, seq_k, d_model)</li>
<li>V: (batch_size, seq_k, d_model)</li>
</ul>
</li>
<li>Attention输出：<ul>
<li>(batch_size, q_seq_len, d_model)</li>
</ul>
</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/reformer2.png" alt="图片"></p>
<ul>
<li><p>Feed-Forward</p>
<p>$\operatorname{FFN}(h)=\operatorname{ReLU}\left(h W_{1}+b_{1}\right) W_{2}+b_{2}$</p>
</li>
<li><p>FFN输入：</p>
<ul>
<li>(batch_size, q_seq_len, d_model)</li>
</ul>
</li>
<li><p>FFN输出：</p>
<ul>
<li>(batch_size, q_seq_len, d_ff)</li>
</ul>
</li>
</ul>
<p>本论文提出了Reformer模型，使用了下面方法解决了内存和速度的问题：</p>
<ul>
<li>Reversible layers：整个模型只需保存一次activations，使因层数导致的内存问题解决</li>
<li>FFN层分块并行处理：降低d_ff产生的内存消耗</li>
<li>局部敏感哈希(locality-sensitive hashing)：代替dot-product attention带来的O(L^2)计算和内存复杂度，使得能处理更长的序列</li>
</ul>
<h1 id="局部敏感哈希"><a href="#局部敏感哈希" class="headerlink" title="局部敏感哈希"></a>局部敏感哈希</h1><blockquote>
<p>参考：<br><a href="https://arxiv.org/abs/1509.02897" target="_blank" rel="noopener">https://arxiv.org/abs/1509.02897</a><br><a href="https://www.cnblogs.com/maybe2030/p/4953039.html" target="_blank" rel="noopener">https://www.cnblogs.com/maybe2030/p/4953039.html</a></p>
</blockquote>
<p>Attention计算中最耗时和消耗内存的是QK^T([batch size, length, length])。我们其实关注的是softmax(QK^T)，而softmax的取值主要被其中较大的元素主导，因此对Q的每个向量qi，只需要关注K中哪个向量最接近qi。比如说如果K的长度是64K，对于每个qi，我们只需要关注其中跟qi距离最近的32或64个kj。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/reformer3.png" alt="图片"></p>
<p>我们首先想到的是 locality-sensitive hashing，其特点是对于每个向量x，在经过哈希函数h(x)后，在原来的空间中挨的近的向量有更大的概率获得相同的哈希值。就像上面这张图，经过旋转(映射)后，距离远得点(第一行)有很大概率分到不同得桶中，而距离近得点(第二行)很大概率分到相同得桶中。</p>
<p>在实现时我们使用了一个随机产生的大小为(dk, b/2)的矩阵R，定义$h(x)=\arg{\max }([x R ;-x R])​$为哈希函数，这样所有x，可以把它们分配到b个哈希桶里。具体的计算和证明在另一片论文(Practical and Optimal LSH for Angular Distance)中。</p>
<p>下面这张图说明了LSH具体的计算流程：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/reformer4.png" alt="图片"></p>
<p>在上图中，不同的颜色表示不同的哈希值，相似的词则具有相同的颜色。分配哈希值后，序列重新排列，将具有相同哈希值的元素放在一起，再分为多个片段（或多个区块）以实现并行处理。然后在这些短得多的区块（及其相近邻块以覆盖溢出）内应用注意力，从而大大降低计算负载。</p>
<p>上图右侧（a-b）是和传统注意力的比较。(a)表明传统的注意力是很稀疏的，也就是说大多数的字符其实都不用关注；(b) k和q根据它们的哈希桶（注意随机的那个矩阵R是共享的）排序好，然后再使用。</p>
<p>由于哈希桶的大小很可能不均匀，所以我们首先令$k_{j}=\frac{q_{i}}{\left|q_{i}\right|}​$来保证$h\left(k_{j}\right)=h\left(q_{j}\right)​$，然后再从小到大给Q的哈希桶排序，在每个桶内部，按照位置先后排序。这实际上定义了一个置换$i \mapsto s_{i}​$。在排序后的注意力矩阵中，来自同一个哈希桶的(q,k)对会聚集在矩阵的对角(上图右c)。最后，把它们分组，每组m个，在各组内相互关注。</p>
<p>为了进一步减小桶分布不均的情况，可以用不同的哈希函数进行多轮哈希。下表是几种注意力方式的时空复杂度：(l: 序列长度，b: batch_size, $n_h$: num of heads, $n_c$: num of LSH chunk, $n_r$: num of hash repetition)<img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/reformer5.png" alt="图片"></p>
<h1 id="可逆层"><a href="#可逆层" class="headerlink" title="可逆层"></a>可逆层</h1><blockquote>
<p>参考：<br><a href="https://arxiv.org/pdf/1707.04585.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1707.04585.pdf</a><br><a href="https://zhuanlan.zhihu.com/p/60479586" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/60479586</a><br><a href="https://www.cnblogs.com/gczr/p/12181354.html" target="_blank" rel="noopener">https://www.cnblogs.com/gczr/p/12181354.html</a></p>
</blockquote>
<p>通过LSH可以将attention的复杂度减少为序列长度的线性级，但是参数量占的复杂度依旧很高，我们想要进一步减少。在上面表中我们看出，每一层的输入前都至少有$b \cdot l \cdot d_{\text {model}}$的激活输出值，$n_l$层则至少有个$b \cdot l \cdot d_{\bmod e l} \cdot n_{l}$。而且光是FFN层就会产生$b \cdot l \cdot d_{f f} \cdot n_{l}$的激活输出，对于一些大模型，这个$d_ff$会比较大(4K甚至64K)，甚至消耗掉16GB的内存。因此采用可逆层来解决$n_l$和$d_ff$的问题。</p>
<h2 id="可逆Transformer"><a href="#可逆Transformer" class="headerlink" title="可逆Transformer"></a>可逆Transformer</h2><p>可逆残差网络的前向传播和反向计算过程如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/reformer6.png" alt="图片"></p>
<p>前向：</p>
<p>$\begin{array}{l}y_{1}=x_{1}+\mathcal{F}\left(x_{2}\right) \ y_{2}=x_{2}+\mathcal{G}\left(y_{1}\right)\end{array}​$</p>
<p>逆向：</p>
<p>$\begin{array}{l}x_{2}=y_{2}-\mathcal{G}\left(y_{1}\right) \ x_{1}=y_{1}-\mathcal{F}\left(x_{2}\right)\end{array}$</p>
<p>在典型的残差网络中，通过网络传递的输入将会向堆栈中的每一层不断添加至向量。相反，可逆层中每个层有两组激活。一组遵循刚才描述的标准过程，从一层逐步更新到下一层，但是另一组仅捕获第一层的变更。因此，若要反向运行网络，只需简单地减去每一层应用的激活。</p>
<p>简单来说，可逆层将输入分成两部分，使得每一层的值可以由它下一层的输出推导出来。因此整个网络只需要存储最后一层的值即可。</p>
<p>具体的解释可参考论文：The Reversible Residual Network: Backpropagation Without Storing Activations.</p>
<p>在Transformer中我们这样应用可逆层：</p>
<p>$Y_{1}=X_{1}+\text { Attention }\left(X_{2}\right)​$</p>
<p>$Y_{2}=X_{2}+\text { FeedForward }\left(Y_{1}\right)$</p>
<h2 id="FF层分组"><a href="#FF层分组" class="headerlink" title="FF层分组"></a>FF层分组</h2><p>由于FFN层的计算不依赖于位置信息，可以将计算进行分块处理：$Y_{2}=\left[Y_{2}^{(1)} ; \ldots ; Y_{2}^{(c)}\right]=\left[X_{2}^{(1)}+\text { FeedForward }\left(Y_{1}^{(1)}\right) ; \ldots ; X_{2}^{(c)}+\text { FeedForward }\left(Y_{1}^{(c)}\right)\right]$</p>
<p>论文中特别强调，虽然通过分块和可逆层使得激活值是独立于层数的，但是对参数来说可不是这样，参数会随着层的增长而增长。好在我们可以利用CPU的内存，在逐层计算时将暂不使用的参数存储到CPU内存中，当需要时再交换回来。虽说从GPU到CPU的传输是比较慢的，但这对于Reformer来说，其batch_size * lenth已经达到可以忽略到这种参数传输的成本。</p>
<p>下表是所有变体的复杂度：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/reformer7.png" alt="图片"></p>
<h1 id="实验效果"><a href="#实验效果" class="headerlink" title="实验效果"></a>实验效果</h1><blockquote>
<p>参考：<br><a href="https://colab.research.google.com/github/google/trax/blob/master/trax/models/reformer/image_generation.ipynb" target="_blank" rel="noopener">https://colab.research.google.com/github/google/trax/blob/master/trax/models/reformer/image_generation.ipynb</a><br><a href="https://colab.research.google.com/github/google/trax/blob/master/trax/models/reformer/text_generation.ipynb" target="_blank" rel="noopener">https://colab.research.google.com/github/google/trax/blob/master/trax/models/reformer/text_generation.ipynb</a><br><a href="https://colab.research.google.com/github/google/trax/blob/master/trax/intro.ipynb" target="_blank" rel="noopener">https://colab.research.google.com/github/google/trax/blob/master/trax/intro.ipynb</a></p>
</blockquote>
<p>论文中的实验结论主要是为了证实Reformer可以更高效，且对精度几乎没有损失。这里贴一张Colab上对Reformer应用的效果图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/reformer8.png" alt="图片"></p>
<p>上图使用Reformer逐像素生成全画幅图像。</p>
]]></content>
      <tags>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>PLY教程及例子</title>
    <url>/2020/03/03/PLY%E6%95%99%E7%A8%8B%E5%8F%8A%E4%BE%8B%E5%AD%90/</url>
    <content><![CDATA[<p>最近需要重改语音助手中的计算器模块，打算用yacc&amp;lex实现，在这里记录一下学习和使用过程。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://github.com/PChou/python-lex-yacc" target="_blank" rel="noopener">https://github.com/PChou/python-lex-yacc</a></p>
</blockquote>
<h1 id="PLY教程"><a href="#PLY教程" class="headerlink" title="PLY教程"></a>PLY教程</h1><h2 id="PLY简介"><a href="#PLY简介" class="headerlink" title="PLY简介"></a>PLY简介</h2><p>PLY 是纯粹由 Python 实现的 Lex 和 yacc（流行的编译器构建工具）。PLY 的设计目标是尽可能的沿袭传统 lex 和 yacc 工具的工作方式，包括支持 LALR(1)分析法、提供丰富的输入验证、错误报告和诊断。因此，如果你曾经在其他编程语言下使用过 yacc，你应该能够很容易的迁移到 PLY 上。</p>
<p>PLY 包含两个独立的模块：lex.py 和 yacc.py，都定义在 ply 包下。lex.py 模块用来将输入字符通过一系列的正则表达式分解成标记序列，yacc.py 通过一些上下文无关的文法来识别编程语言语法。yacc.py 使用 LR 解析法，并使用 LALR(1)算法（默认）或者 SLR 算法生成分析表。</p>
<p>这两个工具是为了一起工作的。lex.py 通过向外部提供token()方法作为接口，方法每次会从输入中返回下一个有效的标记。yacc.py 将会不断的调用这个方法来获取标记并匹配语法规则。yacc.py 的功能通常是生成抽象语法树(AST)，不过，这完全取决于用户，如果需要，yacc.py 可以直接用来完成简单的翻译。</p>
<p>就像相应的 unix 工具，yacc.py 提供了大多数你期望的特性，其中包括：丰富的错误检查、语法验证、支持空产生式、错误的标记、通过优先级规则解决二义性。事实上，传统 yacc 能够做到的 PLY 都应该支持。</p>
<p>yacc.py 与 Unix 下的 yacc 的主要不同之处在于，yacc.py 没有包含一个独立的代码生成器，而是在 PLY 中依赖反射来构建词法分析器和语法解析器。不像传统的 lex/yacc 工具需要一个独立的输入文件，并将之转化成一个源文件，Python 程序必须是一个可直接可用的程序，这意味着不能有额外的源文件和特殊的创建步骤（像是那种执行 yacc 命令来生成 Python 代码）。又由于生成分析表开销较大，PLY 会缓存生成的分析表，并将它们保存在独立的文件中，除非源文件有变化，会重新生成分析表，否则将从缓存中直接读取。</p>
<h2 id="LEX简介"><a href="#LEX简介" class="headerlink" title="LEX简介"></a>LEX简介</h2><p>lex.py是用来将输入字符串标记化。例如，假设你正在设计一个编程语言，用户的输入字符串如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x &#x3D; 3 + 42 * (s - t)</span><br></pre></td></tr></table></figure>
<p>标记器将字符串分割成独立的标记：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#39;x&#39;,&#39;&#x3D;&#39;, &#39;3&#39;, &#39;+&#39;, &#39;42&#39;, &#39;*&#39;, &#39;(&#39;, &#39;s&#39;, &#39;-&#39;, &#39;t&#39;, &#39;)&#39;</span><br></pre></td></tr></table></figure><br>标记通常用一组名字来命名和表示：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#39;ID&#39;,&#39;EQUALS&#39;,&#39;NUMBER&#39;,&#39;PLUS&#39;,&#39;NUMBER&#39;,&#39;TIMES&#39;,&#39;LPAREN&#39;,&#39;ID&#39;,&#39;MINUS&#39;,&#39;ID&#39;,&#39;RPAREN&#39;</span><br></pre></td></tr></table></figure><br>将标记名和标记值本身组合起来：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(&#39;ID&#39;,&#39;x&#39;), (&#39;EQUALS&#39;,&#39;&#x3D;&#39;), (&#39;NUMBER&#39;,&#39;3&#39;),(&#39;PLUS&#39;,&#39;+&#39;), (&#39;NUMBER&#39;,&#39;42), (&#39;TIMES&#39;,&#39;*&#39;),(&#39;LPAREN&#39;,&#39;(&#39;), (&#39;ID&#39;,&#39;s&#39;),(&#39;MINUS&#39;,&#39;-&#39;),(&#39;ID&#39;,&#39;t&#39;), (&#39;RPAREN&#39;,&#39;)</span><br></pre></td></tr></table></figure></p>
<h3 id="LEX例子"><a href="#LEX例子" class="headerlink" title="LEX例子"></a>LEX例子</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import ply.lex as lex</span><br><span class="line"></span><br><span class="line"># List of token names.   This is always required</span><br><span class="line">tokens &#x3D; (</span><br><span class="line">   &#39;NUMBER&#39;,</span><br><span class="line">   &#39;PLUS&#39;,</span><br><span class="line">   &#39;MINUS&#39;,</span><br><span class="line">   &#39;TIMES&#39;,</span><br><span class="line">   &#39;DIVIDE&#39;,</span><br><span class="line">   &#39;LPAREN&#39;,</span><br><span class="line">   &#39;RPAREN&#39;,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># Regular expression rules for simple tokens</span><br><span class="line">t_PLUS    &#x3D; r&#39;\+&#39;</span><br><span class="line">t_MINUS   &#x3D; r&#39;-&#39;</span><br><span class="line">t_TIMES   &#x3D; r&#39;\*&#39;</span><br><span class="line">t_DIVIDE  &#x3D; r&#39;&#x2F;&#39;</span><br><span class="line">t_LPAREN  &#x3D; r&#39;\(&#39;</span><br><span class="line">t_RPAREN  &#x3D; r&#39;\)&#39;</span><br><span class="line"></span><br><span class="line"># A regular expression rule with some action code</span><br><span class="line">def t_NUMBER(t):</span><br><span class="line">    r&#39;\d+&#39;</span><br><span class="line">    t.value &#x3D; int(t.value)    </span><br><span class="line">    return t</span><br><span class="line"></span><br><span class="line"># Define a rule so we can track line numbers</span><br><span class="line">def t_newline(t):</span><br><span class="line">    r&#39;\n+&#39;</span><br><span class="line">    t.lexer.lineno +&#x3D; len(t.value)</span><br><span class="line"></span><br><span class="line"># A string containing ignored characters (spaces and tabs)</span><br><span class="line">t_ignore  &#x3D; &#39; \t&#39;</span><br><span class="line"></span><br><span class="line"># Error handling rule</span><br><span class="line">def t_error(t):</span><br><span class="line">    print(&quot;Illegal character &#39;%s&#39;&quot; % t.value[0])</span><br><span class="line">    t.lexer.skip(1)</span><br><span class="line"></span><br><span class="line"># Build the lexer</span><br><span class="line">lexer &#x3D; lex.lex()</span><br></pre></td></tr></table></figure>
<p>为了使 lexer 工作，你需要给定一个输入，并传递给input()方法。然后，重复调用token()方法来获取标记序列：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Test it out</span><br><span class="line">data &#x3D; &#39;&#39;&#39;</span><br><span class="line">3 + 4 * 10</span><br><span class="line">  + -20 *2</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line"></span><br><span class="line"># Give the lexer some input</span><br><span class="line">lexer.input(data)</span><br><span class="line"></span><br><span class="line"># Tokenize</span><br><span class="line">for tok in lexer:</span><br><span class="line">    if not tok: break      # No more input</span><br><span class="line">    print(tok.type, tok.value, tok.lineno, tok.lexpos)</span><br></pre></td></tr></table></figure><br>程序执行，将给出如下输出：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">LexToken(NUMBER,3,2,1)</span><br><span class="line">LexToken(PLUS,&#39;+&#39;,2,3)</span><br><span class="line">LexToken(NUMBER,4,2,5)</span><br><span class="line">LexToken(TIMES,&#39;*&#39;,2,7)</span><br><span class="line">LexToken(NUMBER,10,2,10)</span><br><span class="line">LexToken(PLUS,&#39;+&#39;,3,14)</span><br><span class="line">LexToken(MINUS,&#39;-&#39;,3,16)</span><br><span class="line">LexToken(NUMBER,20,3,18)</span><br><span class="line">LexToken(TIMES,&#39;*&#39;,3,20)</span><br><span class="line">LexToken(NUMBER,2,3,21)</span><br></pre></td></tr></table></figure><br>tok.type和tok.value属性表示标记本身的类型和值。tok.line和tok.lexpos属性包含了标记的位置信息，tok.lexpos表示标记相对于输入串起始位置的偏移。</p>
<h3 id="标记列表"><a href="#标记列表" class="headerlink" title="标记列表"></a>标记列表</h3><p>词法分析器必须提供一个标记的列表，这个列表将所有可能的标记告诉分析器，用来执行各种验证，同时也提供给 yacc.py 作为终结符。在上面的例子中，标记列表是由tokens指定的。</p>
<h3 id="标记的规则"><a href="#标记的规则" class="headerlink" title="标记的规则"></a>标记的规则</h3><p>每种标记用一个正则表达式规则来表示，每个规则需要以”t_”开头声明，表示该声明是对标记的规则定义。对于简单的标记，可以定义成这样（在 Python 中使用 raw string 能比较方便的书写正则表达式）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">t_PLUS &#x3D; r&#39;\+&#39;</span><br></pre></td></tr></table></figure>
<p>这里，紧跟在 t_ 后面的单词，必须跟标记列表中的某个标记名称对应。如果需要执行动作的话，规则可以写成一个方法。例如，下面的规则匹配数字字串，并且将匹配的字符串转化成 Python 的整型：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def t_NUMBER(t):</span><br><span class="line">    r&#39;\d+&#39;</span><br><span class="line">    t.value &#x3D; int(t.value)</span><br><span class="line">    return t</span><br></pre></td></tr></table></figure><br>如果使用方法的话，正则表达式写成方法的文档字符串。方法总是需要接受一个 LexToken 实例的参数，该实例有一个 t.type 的属性（字符串表示）来表示标记的类型名称，t.value 是标记值（匹配的实际的字符串），t.lineno 表示当前在源输入串中的作业行，t.lexpos 表示标记相对于输入串起始位置的偏移。默认情况下，t.type 是以t_开头的变量或方法的后面部分。方法可以在方法体里面修改这些属性。但是，如果这样做，应该返回结果 token，否则，标记将被丢弃。<br>在 lex 内部，lex.py 用re模块处理模式匹配，在构造最终的完整的正则式的时候，用户提供的规则按照下面的顺序加入：</p>
<ul>
<li>所有由方法定义的标记规则，按照他们的出现顺序依次加入</li>
<li>由字符串变量定义的标记规则按照其正则式长度倒序后，依次加入（长的先入）</li>
<li>顺序的约定对于精确匹配是必要的。比如，如果你想区分‘=’和‘==’，你需要确保‘==’优先检查。如果用字符串来定义这样的表达式的话，通过将较长的正则式先加入，可以帮助解决这个问题。用方法定义标记，可以显示地控制哪个规则优先检查。</li>
</ul>
<p>为了处理保留字，你应该写一个单一的规则来匹配这些标识，并在方法里面作特殊的查询：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">reserved &#x3D; &#123;</span><br><span class="line">   &#39;if&#39; : &#39;IF&#39;,</span><br><span class="line">   &#39;then&#39; : &#39;THEN&#39;,</span><br><span class="line">   &#39;else&#39; : &#39;ELSE&#39;,</span><br><span class="line">   &#39;while&#39; : &#39;WHILE&#39;,</span><br><span class="line">   ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">tokens &#x3D; [&#39;LPAREN&#39;,&#39;RPAREN&#39;,...,&#39;ID&#39;] + list(reserved.values())</span><br><span class="line"></span><br><span class="line">def t_ID(t):</span><br><span class="line">    r&#39;[a-zA-Z_][a-zA-Z_0-9]*&#39;</span><br><span class="line">    t.type &#x3D; reserved.get(t.value,&#39;ID&#39;)    # Check for reserved words</span><br></pre></td></tr></table></figure>
<pre><code>return t
</code></pre><p>这样做可以大大减少正则式的个数，并稍稍加快处理速度。注意：你应该避免为保留字编写单独的规则，例如，如果你像下面这样写：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">t_FOR   &#x3D; r&#39;for&#39;</span><br><span class="line">t_PRINT &#x3D; r&#39;print&#39;</span><br></pre></td></tr></table></figure><br>但是，这些规则照样也能够匹配以这些字符开头的单词，比如’forget’或者’printed’，这通常不是你想要的。</p>
<h3 id="标记的值"><a href="#标记的值" class="headerlink" title="标记的值"></a>标记的值</h3><p>标记被 lex 返回后，它们的值被保存在value属性中。正常情况下，value 是匹配的实际文本。事实上，value 可以被赋为任何 Python 支持的类型。例如，当扫描到标识符的时候，你可能不仅需要返回标识符的名字，还需要返回其在符号表中的位置，可以像下面这样写：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def t_ID(t):</span><br><span class="line">    ...</span><br><span class="line">    # Look up symbol table information and return a tuple</span><br><span class="line">    t.value &#x3D; (t.value, symbol_lookup(t.value))</span><br><span class="line">    ...</span><br><span class="line">    return t</span><br></pre></td></tr></table></figure>
<p>需要注意的是，不推荐用其他属性来保存值，因为 yacc.py 模块只会暴露出标记的 value属 性，访问其他属性会变得不自然。如果想保存多种属性，可以将元组、字典、或者对象实例赋给 value。</p>
<h3 id="丢弃标记"><a href="#丢弃标记" class="headerlink" title="丢弃标记"></a>丢弃标记</h3><p>想丢弃像注释之类的标记，只要不返回 value 就行了，像这样：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def t_COMMENT(t):</span><br><span class="line">    r&#39;\#.*&#39;</span><br><span class="line">    pass</span><br><span class="line">    # No return value. Token discarded</span><br></pre></td></tr></table></figure>
<p>为标记声明添加”ignore_”前缀同样可以达到目的：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">t_ignore_COMMENT &#x3D; r&#39;\#.*&#39;</span><br></pre></td></tr></table></figure><br>如果有多种文本需要丢弃，建议使用方法来定义规则，因为方法能够提供更精确的匹配优先级控制（方法根据出现的顺序，而字符串的正则表达式依据正则表达式的长度）</p>
<h3 id="行号和位置信息"><a href="#行号和位置信息" class="headerlink" title="行号和位置信息"></a>行号和位置信息</h3><p>默认情况下，lex.py 对行号一无所知。因为 lex.py 根本不知道何为”行”的概念（换行符本身也作为文本的一部分）。不过，可以通过写一个特殊的规则来记录行号：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Define a rule so we can track line numbers</span><br><span class="line">def t_newline(t):</span><br><span class="line">    r&#39;\n+&#39;</span><br><span class="line">    t.lexer.lineno +&#x3D; len(t.value)</span><br></pre></td></tr></table></figure>
<p>在这个规则中，当前 lexer 对象 t.lexer 的 lineno 属性被修改了，而且空行被简单的丢弃了，因为没有任何的返回。<br>lex.py 也不自动做列跟踪。但是，位置信息被记录在了每个标记对象的lexpos属性中，这样，就有可能来计算列信息了。例如：每当遇到新行的时候就重置列值：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Compute column. input is the input text string token is a token instance</span><br><span class="line">def find_column(input,token):</span><br><span class="line">    last_cr &#x3D; input.rfind(&#39;\n&#39;,0,token.lexpos)</span><br><span class="line">    if last_cr &lt; 0:</span><br><span class="line">        last_cr &#x3D; 0</span><br><span class="line">    column &#x3D; (token.lexpos - last_cr) + 1</span><br><span class="line">    return column</span><br></pre></td></tr></table></figure>
<p>通常，计算列的信息是为了指示上下文的错误位置，所以只在必要时有用。</p>
<h3 id="忽略字符"><a href="#忽略字符" class="headerlink" title="忽略字符"></a>忽略字符</h3><p>t_ignore规则比较特殊，是lex.py所保留用来忽略字符的，通常用来跳过空白或者不需要的字符。虽然可以通过定义像t_newline()这样的规则来完成相同的事情，不过使用t_ignore能够提供较好的词法分析性能，因为相比普通的正则式，它被特殊化处理了。用PLY写一个简单计算器</p>
<h3 id="字面字符"><a href="#字面字符" class="headerlink" title="字面字符"></a>字面字符</h3><p>字面字符可以通过在词法模块中定义一个literals变量做到，例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">literals &#x3D; [ &#39;+&#39;,&#39;-&#39;,&#39;*&#39;,&#39;&#x2F;&#39; ]</span><br></pre></td></tr></table></figure>
<p>或者<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">literals &#x3D; &quot;+-*&#x2F;&quot;</span><br></pre></td></tr></table></figure><br>字面字符是指单个字符，表示把字符本身作为标记，标记的type和value都是字符本身。不过，字面字符是在其他正则式之后被检查的，因此如果有规则是以这些字符开头的，那么这些规则的优先级较高。</p>
<h3 id="错误处理"><a href="#错误处理" class="headerlink" title="错误处理"></a>错误处理</h3><p>最后，在词法分析中遇到非法字符时，t_error()用来处理这类错误。这种情况下，t.value包含了余下还未被处理的输入字串，在之前的例子中，错误处理方法是这样的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Error handling rule</span><br><span class="line">def t_error(t):</span><br><span class="line">    print &quot;Illegal character &#39;%s&#39;&quot; % t.value[0]</span><br><span class="line">    t.lexer.skip(1)</span><br></pre></td></tr></table></figure>
<p>这个例子中，我们只是简单的输出不合法的字符，并且通过调用t.lexer.skip(1)跳过一个字符。</p>
<h3 id="构建和使用-lexer"><a href="#构建和使用-lexer" class="headerlink" title="构建和使用 lexer"></a>构建和使用 lexer</h3><p>函数lex.lex()使用 Python 的反射机制读取调用上下文中的正则表达式，来创建 lexer。lexer 一旦创建好，有两个方法可以用来控制 lexer 对象：</p>
<ul>
<li>lexer.input(data) 重置 lexer 和输入字串</li>
<li>lexer.token() 返回下一个 LexToken 类型的标记实例，如果进行到输入字串的尾部时将返回None</li>
</ul>
<p>推荐直接在 lex() 函数返回的 lexer 对象上调用上述接口，尽管也可以向下面这样用模块级别的 lex.input() 和 lex.token()：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lex.lex()</span><br><span class="line">lex.input(sometext)</span><br><span class="line">while 1:</span><br><span class="line">    tok &#x3D; lex.token()</span><br><span class="line">    if not tok: break</span><br><span class="line">    print tok</span><br></pre></td></tr></table></figure>
<p>在这个例子中，lex.input() 和 lex.token() 是模块级别的方法，在 lex 模块中，input() 和 token() 方法绑定到最新创建的 lexer 对象的对应方法上。最好不要这样用，因为这种接口可能不知道在什么时候就失效（例如垃圾回收）。</p>
<h3 id="TOKEN-装饰器"><a href="#TOKEN-装饰器" class="headerlink" title="@TOKEN 装饰器"></a>@TOKEN 装饰器</h3><p>在一些应用中，你可能需要定义一系列辅助的记号来构建复杂的正则表达式，例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">digit            &#x3D; r&#39;([0-9])&#39;</span><br><span class="line">nondigit         &#x3D; r&#39;([_A-Za-z])&#39;</span><br><span class="line">identifier       &#x3D; r&#39;(&#39; + nondigit + r&#39;(&#39; + digit + r&#39;|&#39; + nondigit + r&#39;)*)&#39;        </span><br><span class="line"></span><br><span class="line">def t_ID(t):</span><br><span class="line">    # want docstring to be identifier above. ?????</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p>在这个例子中，我们希望 ID 的规则引用上面的已有的变量。然而，使用文档字符串无法做到，为了解决这个问题，你可以使用@TOKEN装饰器：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from ply.lex import TOKEN</span><br><span class="line"></span><br><span class="line">@TOKEN(identifier)</span><br><span class="line">def t_ID(t):</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><br>装饰器可以将 identifier 关联到 t_ID() 的文档字符串上以使 lex.py 正常工作，一种等价的做法是直接给文档字符串赋值：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def t_ID(t):</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">t_ID.__doc__ &#x3D; identifier</span><br></pre></td></tr></table></figure></p>
<h3 id="优化模式"><a href="#优化模式" class="headerlink" title="优化模式"></a>优化模式</h3><p>为了提高性能，你可能希望使用 Python 的优化模式（比如，使用 -o 选项执行 Python）。然而，这样的话，Python 会忽略文档字串，这是 lex.py 的特殊问题，可以通过在创建 lexer 的时候使用 optimize 选项：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lexer &#x3D; lex.lex(optimize&#x3D;1)</span><br></pre></td></tr></table></figure>
<p>接着，用 Python 常规的模式运行，这样，lex.py 会在当前目录下创建一个 lextab.py 文件，这个文件会包含所有的正则表达式规则和词法分析阶段的分析表。然后，lextab.py 可以被导入用来构建 lexer。这种方法大大改善了词法分析程序的启动时间，而且可以在 Python 的优化模式下工作。<br>想要更改生成的文件名，使用如下参数：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lexer &#x3D; lex.lex(optimize&#x3D;1,lextab&#x3D;&quot;footab&quot;)</span><br></pre></td></tr></table></figure>
<p>在优化模式下执行，需要注意的是 lex 会被禁用大多数的错误检查。因此，建议只在确保万事俱备准备发布最终代码时使用。</p>
<h3 id="调试"><a href="#调试" class="headerlink" title="调试"></a>调试</h3><p>如果想要调试，可以使 lex() 运行在调试模式：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lexer &#x3D; lex.lex(debug&#x3D;1)</span><br></pre></td></tr></table></figure>
<p>这将打出一些调试信息，包括添加的规则、最终的正则表达式和词法分析过程中得到的标记。除此之外，lex.py 有一个简单的主函数，不但支持对命令行参数输入的字串进行扫描，还支持命令行参数指定的文件名：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;</span><br><span class="line">     lex.runmain()</span><br></pre></td></tr></table></figure></p>
<h3 id="其他方式定义词法规则"><a href="#其他方式定义词法规则" class="headerlink" title="其他方式定义词法规则"></a>其他方式定义词法规则</h3><p>上面的例子，词法分析器都是在单个的 Python 模块中指定的。如果你想将标记的规则放到不同的模块，使用 module 关键字参数。例如，你可能有一个专有的模块，包含了标记的规则：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># module: tokrules.py</span><br><span class="line"># This module just contains the lexing rules</span><br><span class="line"></span><br><span class="line"># List of token names.   This is always required</span><br><span class="line">tokens &#x3D; (</span><br><span class="line">   &#39;NUMBER&#39;,</span><br><span class="line">   &#39;PLUS&#39;,</span><br><span class="line">   &#39;MINUS&#39;,</span><br><span class="line">   &#39;TIMES&#39;,</span><br><span class="line">   &#39;DIVIDE&#39;,</span><br><span class="line">   &#39;LPAREN&#39;,</span><br><span class="line">   &#39;RPAREN&#39;,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># Regular expression rules for simple tokens</span><br><span class="line">t_PLUS    &#x3D; r&#39;\+&#39;</span><br><span class="line">t_MINUS   &#x3D; r&#39;-&#39;</span><br><span class="line">t_TIMES   &#x3D; r&#39;\*&#39;</span><br><span class="line">t_DIVIDE  &#x3D; r&#39;&#x2F;&#39;</span><br><span class="line">t_LPAREN  &#x3D; r&#39;\(&#39;</span><br><span class="line">t_RPAREN  &#x3D; r&#39;\)&#39;</span><br><span class="line"></span><br><span class="line"># A regular expression rule with some action code</span><br><span class="line">def t_NUMBER(t):</span><br><span class="line">    r&#39;\d+&#39;</span><br><span class="line">    t.value &#x3D; int(t.value)    </span><br><span class="line">    return t</span><br><span class="line"></span><br><span class="line"># Define a rule so we can track line numbers</span><br><span class="line">def t_newline(t):</span><br><span class="line">    r&#39;\n+&#39;</span><br><span class="line">    t.lexer.lineno +&#x3D; len(t.value)</span><br><span class="line"></span><br><span class="line"># A string containing ignored characters (spaces and tabs)</span><br><span class="line">t_ignore  &#x3D; &#39; \t&#39;</span><br><span class="line"></span><br><span class="line"># Error handling rule</span><br><span class="line">def t_error(t):</span><br><span class="line">    print &quot;Illegal character &#39;%s&#39;&quot; % t.value[0]</span><br></pre></td></tr></table></figure>
<pre><code>t.lexer.skip(1)
</code></pre><p>现在，如果你想要从不同的模块中构建分析器，应该这样：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import tokrules</span><br><span class="line">import ply.lex as lex</span><br><span class="line"></span><br><span class="line">lexer &#x3D; lex.lex(module&#x3D;tokrules)</span><br><span class="line">lexer.input(&quot;3 + 4&quot;)</span><br><span class="line">for tok in lexer:</span><br><span class="line">    if not tok: break      # No more input</span><br><span class="line">    print(tok.type, tok.value, tok.lineno, tok.lexpos)</span><br></pre></td></tr></table></figure><br>module选项也可以指定类型的实例，例如：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import ply.lex as lex</span><br><span class="line"></span><br><span class="line">class MyLexer:</span><br><span class="line">    # List of token names.   This is always required</span><br><span class="line">    tokens &#x3D; (</span><br><span class="line">       &#39;NUMBER&#39;,</span><br><span class="line">       &#39;PLUS&#39;,</span><br><span class="line">       &#39;MINUS&#39;,</span><br><span class="line">       &#39;TIMES&#39;,</span><br><span class="line">       &#39;DIVIDE&#39;,</span><br><span class="line">       &#39;LPAREN&#39;,</span><br><span class="line">       &#39;RPAREN&#39;,</span><br><span class="line">    )</span><br><span class="line">    # Regular expression rules for simple tokens</span><br><span class="line">    t_PLUS    &#x3D; r&#39;\+&#39;</span><br><span class="line">    t_MINUS   &#x3D; r&#39;-&#39;</span><br><span class="line">    t_TIMES   &#x3D; r&#39;\*&#39;</span><br><span class="line">    t_DIVIDE  &#x3D; r&#39;&#x2F;&#39;</span><br><span class="line">    t_LPAREN  &#x3D; r&#39;\(&#39;</span><br><span class="line">    t_RPAREN  &#x3D; r&#39;\)&#39;</span><br><span class="line">    # A regular expression rule with some action code</span><br><span class="line">    # Note addition of self parameter since we&#39;re in a class</span><br><span class="line">    def t_NUMBER(self,t):</span><br><span class="line">        r&#39;\d+&#39;</span><br><span class="line">        t.value &#x3D; int(t.value)    </span><br><span class="line">        return t</span><br><span class="line">    # Define a rule so we can track line numbers</span><br><span class="line">    def t_newline(self,t):</span><br><span class="line">        r&#39;\n+&#39;</span><br><span class="line">        t.lexer.lineno +&#x3D; len(t.value)</span><br><span class="line">    # A string containing ignored characters (spaces and tabs)</span><br><span class="line">    t_ignore  &#x3D; &#39; \t&#39;</span><br><span class="line">    # Error handling rule</span><br><span class="line">    def t_error(self,t):</span><br><span class="line">        print(&quot;Illegal character &#39;%s&#39;&quot; % t.value[0])</span><br><span class="line">        t.lexer.skip(1)</span><br><span class="line">    # Build the lexer</span><br><span class="line">    def build(self,**kwargs):</span><br><span class="line">        self.lexer &#x3D; lex.lex(module&#x3D;self, **kwargs)</span><br><span class="line">    </span><br><span class="line">    # Test it output</span><br><span class="line">    def test(self,data):</span><br><span class="line">        self.lexer.input(data)</span><br><span class="line">        while True:</span><br><span class="line">             tok &#x3D; self.lexer.token()</span><br><span class="line">             if not tok: break</span><br><span class="line">             print(tok)</span><br><span class="line"># Build the lexer and try it out</span><br><span class="line">m &#x3D; MyLexer()</span><br><span class="line">m.build()           # Build the lexer</span><br><span class="line">m.test(&quot;3 + 4&quot;)     # Test it</span><br></pre></td></tr></table></figure><br>当从类中定义 lexer，你需要创建类的实例，而不是类本身。这是因为，lexer 的方法只有被绑定（bound-methods）对象后才能使 PLY 正常工作。<br>当给 lex() 方法使用 module 选项时，PLY 使用dir()方法，从对象中获取符号信息，因为不能直接访问对象的<strong>dict</strong>属性。（译者注：可能是因为兼容性原因，<strong>dict</strong>这个方法可能不存在）</p>
<p>最后，如果你希望保持较好的封装性，但不希望什么东西都写在类里面，lexers 可以在闭包中定义，例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import ply.lex as lex</span><br><span class="line"></span><br><span class="line"># List of token names.   This is always required</span><br><span class="line">tokens &#x3D; (</span><br><span class="line">  &#39;NUMBER&#39;,</span><br><span class="line">  &#39;PLUS&#39;,</span><br><span class="line">  &#39;MINUS&#39;,</span><br><span class="line">  &#39;TIMES&#39;,</span><br><span class="line">  &#39;DIVIDE&#39;,</span><br><span class="line">  &#39;LPAREN&#39;,</span><br><span class="line">  &#39;RPAREN&#39;,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">def MyLexer():</span><br><span class="line">    # Regular expression rules for simple tokens</span><br><span class="line">    t_PLUS    &#x3D; r&#39;\+&#39;</span><br><span class="line">    t_MINUS   &#x3D; r&#39;-&#39;</span><br><span class="line">    t_TIMES   &#x3D; r&#39;\*&#39;</span><br><span class="line">    t_DIVIDE  &#x3D; r&#39;&#x2F;&#39;</span><br><span class="line">    t_LPAREN  &#x3D; r&#39;\(&#39;</span><br><span class="line">    t_RPAREN  &#x3D; r&#39;\)&#39;</span><br><span class="line"></span><br><span class="line">    # A regular expression rule with some action code</span><br><span class="line">    def t_NUMBER(t):</span><br><span class="line">        r&#39;\d+&#39;</span><br><span class="line">        t.value &#x3D; int(t.value)    </span><br><span class="line">        return t</span><br><span class="line"></span><br><span class="line">    # Define a rule so we can track line numbers</span><br><span class="line">    def t_newline(t):</span><br><span class="line">        r&#39;\n+&#39;</span><br><span class="line">        t.lexer.lineno +&#x3D; len(t.value)</span><br><span class="line"></span><br><span class="line">    # A string containing ignored characters (spaces and tabs)</span><br><span class="line">    t_ignore  &#x3D; &#39; \t&#39;</span><br><span class="line"></span><br><span class="line">    # Error handling rule</span><br><span class="line">    def t_error(t):</span><br><span class="line">        print &quot;Illegal character &#39;%s&#39;&quot; % t.value[0]</span><br><span class="line">        t.lexer.skip(1)</span><br><span class="line"></span><br><span class="line">    # Build the lexer from my environment and return it    </span><br><span class="line">    return lex.lex()</span><br></pre></td></tr></table></figure>
<h3 id="额外状态维护"><a href="#额外状态维护" class="headerlink" title="额外状态维护"></a>额外状态维护</h3><p>在你的词法分析器中，你可能想要维护一些状态。这可能包括模式设置，符号表和其他细节。例如，假设你想要跟踪NUMBER标记的出现个数。</p>
<p>一种方法是维护一个全局变量：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">num_count &#x3D; 0</span><br><span class="line">def t_NUMBER(t):</span><br><span class="line">    r&#39;\d+&#39;</span><br><span class="line">    global num_count</span><br><span class="line">    num_count +&#x3D; 1</span><br><span class="line">    t.value &#x3D; int(t.value)    </span><br><span class="line">    return t</span><br></pre></td></tr></table></figure>
<p>如果你不喜欢全局变量，另一个记录信息的地方是 lexer 对象内部。可以通过当前标记的 lexer 属性访问：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def t_NUMBER(t):</span><br><span class="line">    r&#39;\d+&#39;</span><br><span class="line">    t.lexer.num_count +&#x3D; 1     # Note use of lexer attribute</span><br><span class="line">    t.value &#x3D; int(t.value)    </span><br><span class="line">    return t</span><br><span class="line"></span><br><span class="line">lexer &#x3D; lex.lex()</span><br><span class="line">lexer.num_count &#x3D; 0            # Set the initial count</span><br></pre></td></tr></table></figure><br>上面这样做的优点是当同时存在多个 lexer 实例的情况下，简单易行。不过这看上去似乎是严重违反了面向对象的封装原则。lexer 的内部属性（除了 lineno ）都是以 lex 开头命名的（lexdata、lexpos）。因此，只要不以 lex 开头来命名属性就很安全的。<br>如果你不喜欢给 lexer 对象赋值，你可以自定义你的 lexer 类型，就像前面看到的那样：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class MyLexer:</span><br><span class="line">    ...</span><br><span class="line">    def t_NUMBER(self,t):</span><br><span class="line">        r&#39;\d+&#39;</span><br><span class="line">        self.num_count +&#x3D; 1</span><br><span class="line">        t.value &#x3D; int(t.value)    </span><br><span class="line">        return t</span><br><span class="line"></span><br><span class="line">    def build(self, **kwargs):</span><br><span class="line">        self.lexer &#x3D; lex.lex(object&#x3D;self,**kwargs)</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.num_count &#x3D; 0</span><br></pre></td></tr></table></figure>
<p>如果你的应用会创建很多 lexer 的实例，并且需要维护很多状态，上面的类可能是最容易管理的。<br>状态也可以用闭包来管理，比如，在 Python3 中：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def MyLexer():</span><br><span class="line">    num_count &#x3D; 0</span><br><span class="line">    ...</span><br><span class="line">    def t_NUMBER(t):</span><br><span class="line">        r&#39;\d+&#39;</span><br><span class="line">        nonlocal num_count</span><br><span class="line">        num_count +&#x3D; 1</span><br><span class="line">        t.value &#x3D; int(t.value)    </span><br><span class="line">        return t</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<h3 id="Lexer-克隆"><a href="#Lexer-克隆" class="headerlink" title="Lexer 克隆"></a>Lexer 克隆</h3><p>如果有必要的话，lexer 对象可以通过clone()方法来复制：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lexer &#x3D; lex.lex()</span><br><span class="line">...</span><br><span class="line">newlexer &#x3D; lexer.clone()</span><br></pre></td></tr></table></figure>
<p>当 lexer 被克隆后，复制品能够精确的保留输入串和内部状态，不过，新的 lexer 可以接受一个不同的输出字串，并独立运作起来。这在几种情况下也许有用：当你在编写的解析器或编译器涉及到递归或者回退处理时，你需要扫描先前的部分，你可以clone并使用复制品，或者你在实现某种预编译处理，可以 clone 一些 lexer 来处理不同的输入文件。<br>创建克隆跟重新调用 lex.lex() 的不同点在于，PLY 不会重新构建任何的内部分析表或者正则式。当 lexer 是用类或者闭包创建的，需要注意类或闭包本身的的状态。换句话说你要注意新创建的 lexer 会共享原始 lexer 的这些状态，比如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">m &#x3D; MyLexer()</span><br><span class="line">a &#x3D; lex.lex(object&#x3D;m)      # Create a lexer</span><br><span class="line"></span><br><span class="line">b &#x3D; a.clone()              # Clone the lexer</span><br></pre></td></tr></table></figure>
<h3 id="Lexer-的内部状态"><a href="#Lexer-的内部状态" class="headerlink" title="Lexer 的内部状态"></a>Lexer 的内部状态</h3><p>lexer 有一些内部属性在特定情况下有用：</p>
<ul>
<li>lexer.lexpos。这是一个表示当前分析点的位置的整型值。如果你修改这个值的话，这会改变下一个 token() 的调用行为。在标记的规则方法里面，这个值表示紧跟匹配字串后面的第一个字符的位置，如果这个值在规则中修改，下一个返回的标记将从新的位置开始匹配</li>
<li>lexer.lineno。表示当前行号。PLY 只是声明这个属性的存在，却永远不更新这个值。如果你想要跟踪行号的话，你需要自己添加代码（ 4.6 行号和位置信息）</li>
<li>lexer.lexdata。当前 lexer 的输入字串，这个字符串就是 input() 方法的输入字串，更改它可能是个糟糕的做法，除非你知道自己在干什么。</li>
<li>lexer.lexmatch。PLY 内部调用 Python 的 re.match() 方法得到的当前标记的原始的 Match 对象，该对象被保存在这个属性中。如果你的正则式中包含分组的话，你可以通过这个对象获得这些分组的值。注意：这个属性只在有标记规则定义的方法中才有效。<h3 id="基于条件的扫描和启动条件"><a href="#基于条件的扫描和启动条件" class="headerlink" title="基于条件的扫描和启动条件"></a>基于条件的扫描和启动条件</h3>在高级的分析器应用程序中，使用状态化的词法扫描是很有用的。比如，你想在出现特定标记或句子结构的时候触发开始一个不同的词法分析逻辑。PLY 允许 lexer 在不同的状态之间转换。每个状态可以包含一些自己独特的标记和规则等。这是基于 GNU flex 的“启动条件”来实现的，关于 flex 详见 <a href="http://flex.sourceforge.net/manual/Start-Conditions.html#Start-Conditions" target="_blank" rel="noopener">http://flex.sourceforge.net/manual/Start-Conditions.html#Start-Conditions</a></li>
</ul>
<p>要使用 lex 的状态，你必须首先声明。通过在 lex 模块中声明”states”来做到：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">states &#x3D; (</span><br><span class="line">   (&#39;foo&#39;,&#39;exclusive&#39;),</span><br><span class="line">   (&#39;bar&#39;,&#39;inclusive&#39;),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>这个声明中包含有两个状态：’foo’和’bar’。状态可以有两种类型：’排他型’和’包容型’。排他型的状态会使得 lexer 的行为发生完全的改变：只有能够匹配在这个状态下定义的规则的标记才会返回；包容型状态会将定义在这个状态下的规则添加到默认的规则集中，进而，只要能匹配这个规则集的标记都会返回。<br>一旦声明好之后，标记规则的命名需要包含状态名：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">t_foo_NUMBER &#x3D; r&#39;\d+&#39;                      # Token &#39;NUMBER&#39; in state &#39;foo&#39;        </span><br><span class="line">t_bar_ID     &#x3D; r&#39;[a-zA-Z_][a-zA-Z0-9_]*&#39;   # Token &#39;ID&#39; in state &#39;bar&#39;</span><br><span class="line"></span><br><span class="line">def t_foo_newline(t):</span><br><span class="line">    r&#39;\n&#39;</span><br><span class="line">    t.lexer.lineno +&#x3D; 1</span><br></pre></td></tr></table></figure>
<p>一个标记可以用在多个状态中，只要将多个状态名包含在声明中：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">t_foo_bar_NUMBER &#x3D; r&#39;\d+&#39;         # Defines token &#39;NUMBER&#39; in both state &#39;foo&#39; and &#39;bar&#39;</span><br></pre></td></tr></table></figure><br>同样的，在任何状态下都生效的声明可以在命名中使用ANY：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">t_ANY_NUMBER &#x3D; r&#39;\d+&#39;         # Defines a token &#39;NUMBER&#39; in all states</span><br></pre></td></tr></table></figure><br>不包含状态名的情况下，标记被关联到一个特殊的状态INITIAL，比如，下面两个声明是等价的：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">t_NUMBER &#x3D; r&#39;\d+&#39;</span><br><span class="line">t_INITIAL_NUMBER &#x3D; r&#39;\d+&#39;</span><br></pre></td></tr></table></figure><br>特殊的t_ignore()和t_error()也可以用状态关联：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">t_foo_ignore &#x3D; &quot; \t\n&quot;       # Ignored characters for state &#39;foo&#39;</span><br><span class="line">def t_bar_error(t):          # Special error handler for state &#39;bar&#39;</span><br><span class="line">    pass</span><br></pre></td></tr></table></figure><br>词法分析默认在INITIAL状态下工作，这个状态下包含了所有默认的标记规则定义。对于不希望使用“状态”的用户来说，这是完全透明的。在分析过程中，如果你想要改变词法分析器的这种的状态，使用begin()方法：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def t_begin_foo(t):</span><br><span class="line">    r&#39;start_foo&#39;</span><br><span class="line">    t.lexer.begin(&#39;foo&#39;)             # Starts &#39;foo&#39; state</span><br></pre></td></tr></table></figure><br>使用 begin() 切换回初始状态：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def t_foo_end(t):</span><br><span class="line">    r&#39;end_foo&#39;</span><br><span class="line">    t.lexer.begin(&#39;INITIAL&#39;)        # Back to the initial state</span><br></pre></td></tr></table></figure><br>状态的切换可以使用栈：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def t_begin_foo(t):</span><br><span class="line">    r&#39;start_foo&#39;</span><br><span class="line">    t.lexer.push_state(&#39;foo&#39;)             # Starts &#39;foo&#39; state</span><br><span class="line"></span><br><span class="line">def t_foo_end(t):</span><br><span class="line">    r&#39;end_foo&#39;</span><br><span class="line">    t.lexer.pop_state()                   # Back to the previous state</span><br></pre></td></tr></table></figure><br>当你在面临很多状态可以选择进入，而又仅仅想要回到之前的状态时，状态栈比较有用。<br>举个例子会更清晰。假设你在写一个分析器想要从一堆 C 代码中获取任意匹配的闭合的大括号里面的部分：这意味着，当遇到起始括号’{‘，你需要读取与之匹配的’}’以上的所有部分。并返回字符串。使用通常的正则表达式几乎不可能，这是因为大括号可以嵌套，而且可以有注释，字符串等干扰。因此，试图简单的匹配第一个出现的’}’是不行的。这里你可以用lex的状态来做到：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Declare the state</span><br><span class="line">states &#x3D; (</span><br><span class="line">  (&#39;ccode&#39;,&#39;exclusive&#39;),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># Match the first &#123;. Enter ccode state.</span><br><span class="line">def t_ccode(t):</span><br><span class="line">    r&#39;\&#123;&#39;</span><br><span class="line">    t.lexer.code_start &#x3D; t.lexer.lexpos        # Record the starting position</span><br><span class="line">    t.lexer.level &#x3D; 1                          # Initial brace level</span><br><span class="line">    t.lexer.begin(&#39;ccode&#39;)                     # Enter &#39;ccode&#39; state</span><br><span class="line"></span><br><span class="line"># Rules for the ccode state</span><br><span class="line">def t_ccode_lbrace(t):     </span><br><span class="line">    r&#39;\&#123;&#39;</span><br><span class="line">    t.lexer.level +&#x3D;1                </span><br><span class="line"></span><br><span class="line">def t_ccode_rbrace(t):</span><br><span class="line">    r&#39;\&#125;&#39;</span><br><span class="line">    t.lexer.level -&#x3D;1</span><br><span class="line"></span><br><span class="line">    # If closing brace, return the code fragment</span><br><span class="line">    if t.lexer.level &#x3D;&#x3D; 0:</span><br><span class="line">         t.value &#x3D; t.lexer.lexdata[t.lexer.code_start:t.lexer.lexpos+1]</span><br><span class="line">         t.type &#x3D; &quot;CCODE&quot;</span><br><span class="line">         t.lexer.lineno +&#x3D; t.value.count(&#39;\n&#39;)</span><br><span class="line">         t.lexer.begin(&#39;INITIAL&#39;)           </span><br><span class="line">         return t</span><br><span class="line"></span><br><span class="line"># C or C++ comment (ignore)    </span><br><span class="line">def t_ccode_comment(t):</span><br><span class="line">    r&#39;(&#x2F;\*(.|\n)*?*&#x2F;)|(&#x2F;&#x2F;.*)&#39;</span><br><span class="line">    pass</span><br><span class="line"></span><br><span class="line"># C string</span><br><span class="line">def t_ccode_string(t):</span><br><span class="line">   r&#39;\&quot;([^\\\n]|(\\.))*?\&quot;&#39;</span><br><span class="line"></span><br><span class="line"># C character literal</span><br><span class="line">def t_ccode_char(t):</span><br><span class="line">   r&#39;\&#39;([^\\\n]|(\\.))*?\&#39;&#39;</span><br><span class="line"></span><br><span class="line"># Any sequence of non-whitespace characters (not braces, strings)</span><br><span class="line">def t_ccode_nonspace(t):</span><br><span class="line">   r&#39;[^\s\&#123;\&#125;\&#39;\&quot;]+&#39;</span><br><span class="line"></span><br><span class="line"># Ignored characters (whitespace)</span><br><span class="line">t_ccode_ignore &#x3D; &quot; \t\n&quot;</span><br><span class="line"></span><br><span class="line"># For bad characters, we just skip over it</span><br><span class="line">def t_ccode_error(t):</span><br></pre></td></tr></table></figure>
<pre><code>t.lexer.skip(1)
</code></pre><p>这个例子中，第一个’{‘使得 lexer 记录了起始位置，并且进入新的状态’ccode’。一系列规则用来匹配接下来的输入，这些规则只是丢弃掉标记（不返回值），如果遇到闭合右括号，t_ccode_rbrace 规则收集其中所有的代码（利用先前记录的开始位置），并保存，返回的标记类型为’CCODE’，与此同时，词法分析的状态退回到初始状态。</p>
<h3 id="其他问题"><a href="#其他问题" class="headerlink" title="其他问题"></a>其他问题</h3><ul>
<li>lexer 需要输入的是一个字符串。好在大多数机器都有足够的内存，这很少导致性能的问题。这意味着，lexer 现在还不能用来处理文件流或者 socket 流。这主要是受到 re 模块的限制。</li>
<li>lexer 支持用 Unicode 字符描述标记的匹配规则，也支持输入字串包含 Unicode</li>
<li>如果你想要向re.compile()方法提供 flag，使用 reflags 选项：lex.lex(reflags=re.UNICODE)</li>
<li>由于 lexer 是全部用 Python 写的，性能很大程度上取决于 Python 的 re 模块，即使已经尽可能的高效了。当接收极其大量的输入文件时表现并不尽人意。如果担忧性能，你可以升级到最新的 Python，或者手工创建分析器，或者用 C 语言写 lexer 并做成扩展模块。</li>
</ul>
<p>如果你要创建一个手写的词法分析器并计划用在 yacc.py 中，只需要满足下面的要求：</p>
<ul>
<li>需要提供一个 token() 方法来返回下一个标记，如果没有可用的标记了，则返回 None。</li>
<li>token() 方法必须返回一个 tok 对象，具有 type 和 valu e属性。如果行号需要跟踪的话，标记还需要定义 lineno 属性。<h2 id="语法分析基础"><a href="#语法分析基础" class="headerlink" title="语法分析基础"></a>语法分析基础</h2>‘语法’通常用 BNF 范式来表达。例如，如果想要分析简单的算术表达式，你应该首先写下无二义的文法：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">expression : expression + term</span><br><span class="line">           | expression - term</span><br><span class="line">           | term</span><br><span class="line"></span><br><span class="line">term       : term * factor</span><br><span class="line">           | term &#x2F; factor</span><br><span class="line">           | factor</span><br><span class="line"></span><br><span class="line">factor     : NUMBER</span><br><span class="line">           | ( expression )</span><br></pre></td></tr></table></figure>
<p>在这个文法中，像NUMBER,+,-,*,/的符号被称为终结符，对应原始的输入。类似term，factor等称为非终结符，它们由一系列终结符或其他规则的符号组成，用来指代语法规则。<br>通常使用一种叫语法制导翻译的技术来指定某种语言的语义。在语法制导翻译中，符号及其属性出现在每个语法规则后面的动作中。每当一个语法被识别，动作就能够描述需要做什么。比如，对于上面给定的文法，想要实现一个简单的计算器，应该写成下面这样：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Grammar                             Action</span><br><span class="line">--------------------------------    -------------------------------------------- </span><br><span class="line">expression0 : expression1 + term    expression0.val &#x3D; expression1.val + term.val</span><br><span class="line">            | expression1 - term    expression0.val &#x3D; expression1.val - term.val</span><br><span class="line">            | term                  expression0.val &#x3D; term.val</span><br><span class="line"></span><br><span class="line">term0       : term1 * factor        term0.val &#x3D; term1.val * factor.val</span><br><span class="line">            | term1 &#x2F; factor        term0.val &#x3D; term1.val &#x2F; factor.val</span><br><span class="line">            | factor                term0.val &#x3D; factor.val</span><br><span class="line"></span><br><span class="line">factor      : NUMBER                factor.val &#x3D; int(NUMBER.lexval)</span><br></pre></td></tr></table></figure>
<pre><code>        | ( expression )        factor.val = expression.val
</code></pre><p>一种理解语法指导翻译的好方法是将符号看成对象。与符号相关的值代表了符号的“状态”（比如上面的 val 属性），语义行为用一组操作符号及符号值的函数或者方法来表达。<br>Yacc 用的分析技术是著名的 LR 分析法或者叫移进-归约分析法。LR 分析法是一种自下而上的技术：首先尝试识别右部的语法规则，每当右部得到满足，相应的行为代码将被触发执行，当前右边的语法符号将被替换为左边的语法符号。（归约）</p>
<p>LR 分析法一般这样实现：将下一个符号进栈，然后结合栈顶的符号和后继符号（译者注：下一个将要输入符号），与文法中的某种规则相比较。具体的算法可以在编译器的手册中查到，下面的例子展现了如果通过上面定义的文法，来分析 3 + 5 * ( 10 - 20 ) 这个表达式，$ 用来表示输入结束，action 里面的 Shift 就是进栈动作，简称移进；Reduce 是归约：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Step Symbol Stack           Input Tokens            Action</span><br><span class="line">---- ---------------------  ---------------------   -------------------------------</span><br><span class="line">1                           3 + 5 * ( 10 - 20 )$    Shift 3</span><br><span class="line">2    3                        + 5 * ( 10 - 20 )$    Reduce factor : NUMBER</span><br><span class="line">3    factor                   + 5 * ( 10 - 20 )$    Reduce term   : factor</span><br><span class="line">4    term                     + 5 * ( 10 - 20 )$    Reduce expr : term</span><br><span class="line">5    expr                     + 5 * ( 10 - 20 )$    Shift +</span><br><span class="line">6    expr +                     5 * ( 10 - 20 )$    Shift 5</span><br><span class="line">7    expr + 5                     * ( 10 - 20 )$    Reduce factor : NUMBER</span><br><span class="line">8    expr + factor                * ( 10 - 20 )$    Reduce term   : factor</span><br><span class="line">9    expr + term                  * ( 10 - 20 )$    Shift *</span><br><span class="line">10   expr + term *                  ( 10 - 20 )$    Shift (</span><br><span class="line">11   expr + term * (                  10 - 20 )$    Shift 10</span><br><span class="line">12   expr + term * ( 10                  - 20 )$    Reduce factor : NUMBER</span><br><span class="line">13   expr + term * ( factor              - 20 )$    Reduce term : factor</span><br><span class="line">14   expr + term * ( term                - 20 )$    Reduce expr : term</span><br><span class="line">15   expr + term * ( expr                - 20 )$    Shift -</span><br><span class="line">16   expr + term * ( expr -                20 )$    Shift 20</span><br><span class="line">17   expr + term * ( expr - 20                )$    Reduce factor : NUMBER</span><br><span class="line">18   expr + term * ( expr - factor            )$    Reduce term : factor</span><br><span class="line">19   expr + term * ( expr - term              )$    Reduce expr : expr - term</span><br><span class="line">20   expr + term * ( expr                     )$    Shift )</span><br><span class="line">21   expr + term * ( expr )                    $    Reduce factor : (expr)</span><br><span class="line">22   expr + term * factor                      $    Reduce term : term * factor</span><br><span class="line">23   expr + term                               $    Reduce expr : expr + term</span><br><span class="line">24   expr                                      $    Reduce expr</span><br></pre></td></tr></table></figure>
<p>25                                             $    Success!</p>
<p>在分析表达式的过程中，一个相关的自动状态机和后继符号决定了下一步应该做什么。如果下一个标记看起来是一个有效语法（产生式）的一部分（通过栈上的其他项判断这一点），那么这个标记应该进栈。如果栈顶的项可以组成一个完整的右部语法规则，一般就可以进行“归约”，用产生式左边的符号代替这一组符号。当归约发生时，相应的行为动作就会执行。如果输入标记既不能移进也不能归约的话，就会发生语法错误，分析器必须进行相应的错误恢复。分析器直到栈空并且没有另外的输入标记时，才算成功。 需要注意的是，这是基于一个有限自动机实现的，有限自动器被转化成分析表。分析表的构建比较复杂，超出了本文的讨论范围。不过，这构建过程的微妙细节能够解释为什么在上面的例子中，解析器选择在步骤 9 将标记转移到堆栈中，而不是按照规则 expr : expr + term 做归约。</p>
<h2 id="Yacc简介"><a href="#Yacc简介" class="headerlink" title="Yacc简介"></a>Yacc简介</h2><p>ply.yacc 模块实现了 PLY 的分析功能，‘yacc’是‘Yet Another Compiler Compiler’的缩写并保留了其作为 Unix 工具的名字。</p>
<h3 id="一个例子"><a href="#一个例子" class="headerlink" title="一个例子"></a>一个例子</h3><p>假设你希望实现上面的简单算术表达式的语法分析：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Yacc example</span><br><span class="line"></span><br><span class="line">import ply.yacc as yacc</span><br><span class="line"></span><br><span class="line"># Get the token map from the lexer.  This is required.</span><br><span class="line">from calclex import tokens</span><br><span class="line"></span><br><span class="line">def p_expression_plus(p):</span><br><span class="line">    &#39;expression : expression PLUS term&#39;</span><br><span class="line">    p[0] &#x3D; p[1] + p[3]</span><br><span class="line"></span><br><span class="line">def p_expression_minus(p):</span><br><span class="line">    &#39;expression : expression MINUS term&#39;</span><br><span class="line">    p[0] &#x3D; p[1] - p[3]</span><br><span class="line"></span><br><span class="line">def p_expression_term(p):</span><br><span class="line">    &#39;expression : term&#39;</span><br><span class="line">    p[0] &#x3D; p[1]</span><br><span class="line"></span><br><span class="line">def p_term_times(p):</span><br><span class="line">    &#39;term : term TIMES factor&#39;</span><br><span class="line">    p[0] &#x3D; p[1] * p[3]</span><br><span class="line"></span><br><span class="line">def p_term_div(p):</span><br><span class="line">    &#39;term : term DIVIDE factor&#39;</span><br><span class="line">    p[0] &#x3D; p[1] &#x2F; p[3]</span><br><span class="line"></span><br><span class="line">def p_term_factor(p):</span><br><span class="line">    &#39;term : factor&#39;</span><br><span class="line">    p[0] &#x3D; p[1]</span><br><span class="line"></span><br><span class="line">def p_factor_num(p):</span><br><span class="line">    &#39;factor : NUMBER&#39;</span><br><span class="line">    p[0] &#x3D; p[1]</span><br><span class="line"></span><br><span class="line">def p_factor_expr(p):</span><br><span class="line">    &#39;factor : LPAREN expression RPAREN&#39;</span><br><span class="line">    p[0] &#x3D; p[2]</span><br><span class="line"></span><br><span class="line"># Error rule for syntax errors</span><br><span class="line">def p_error(p):</span><br><span class="line">    print &quot;Syntax error in input!&quot;</span><br><span class="line"></span><br><span class="line"># Build the parser</span><br><span class="line">parser &#x3D; yacc.yacc()</span><br><span class="line"></span><br><span class="line">while True:</span><br><span class="line">   try:</span><br><span class="line">       s &#x3D; raw_input(&#39;calc &gt; &#39;)</span><br><span class="line">   except EOFError:</span><br><span class="line">       break</span><br><span class="line">   if not s: continue</span><br><span class="line">   result &#x3D; parser.parse(s)</span><br></pre></td></tr></table></figure>
<p>   print result</p>
<p>在这个例子中，每个语法规则被定义成一个 Python 的方法，方法的文档字符串描述了相应的上下文无关文法，方法的语句实现了对应规则的语义行为。每个方法接受一个单独的 p 参数，p 是一个包含有当前匹配语法的符号的序列，p[i] 与语法符号的对应关系如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_expression_plus(p):</span><br><span class="line">    &#39;expression : expression PLUS term&#39;</span><br><span class="line">    #   ^            ^        ^    ^</span><br><span class="line">    #  p[0]         p[1]     p[2] p[3]</span><br><span class="line"></span><br><span class="line">    p[0] &#x3D; p[1] + p[3]</span><br></pre></td></tr></table></figure><br>其中，p[i] 的值相当于词法分析模块中对 p.value 属性赋的值，对于非终结符的值，将在归约时由 p[0] 的赋值决定，这里的值可以是任何类型，当然，大多数情况下只是 Python 的简单类型、元组或者类的实例。在这个例子中，我们依赖这样一个事实：NUMBER 标记的值保存的是整型值，所有规则的行为都是得到这些整型值的算术运算结果，并传递结果。<br>在 yacc 中定义的第一个语法规则被默认为起始规则（这个例子中的第一个出现的 expression 规则）。一旦起始规则被分析器归约，而且再无其他输入，分析器终止，最后的值将返回（这个值将是起始规则的p[0]）。注意：也可以通过在 yacc() 中使用 start 关键字参数来指定起始规则。</p>
<p>p_error(p) 规则用于捕获语法错误。详见处理语法错误部分。</p>
<p>为了构建分析器，需要调用 yacc.yacc() 方法。这个方法查看整个当前模块，然后试图根据你提供的文法构建 LR 分析表。由于分析表的得出相对开销较大（尤其包含大量的语法的情况下），分析表被写入当前目录的一个叫 parsetab.py 的文件中。除此之外，会生成一个调试文件 parser.out。在接下来的执行中，yacc 直到发现文法发生变化，才会重新生成分析表和 parsetab.py 文件，否则 yacc 会从 parsetab.py 中加载分析表。注：如果有必要的话这里输出的文件名是可以改的。</p>
<p>如果在你的文法中有任何错误的话，yacc.py 会产生调试信息，而且可能抛出异常。一些可以被检测到的错误如下：</p>
<ul>
<li>方法重复定义（在语法文件中具有相同名字的方法）</li>
<li>二义文法产生的移进-归约和归约-归约冲突</li>
<li>指定了错误的文法</li>
<li>不可终止的递归（规则永远无法终结）</li>
<li>未使用的规则或标记</li>
<li>未定义的规则或标记</li>
</ul>
<p>这个例子的最后部分展示了如何执行由 yacc() 方法创建的分析器。你只需要简单的调用 parse()，并将输入字符串作为参数就能运行分析器。它将运行所有的语法规则，并返回整个分析的结果，这个结果就是在起始规则中赋给 p[0] 的值。</p>
<h3 id="将语法规则合并"><a href="#将语法规则合并" class="headerlink" title="将语法规则合并"></a>将语法规则合并</h3><p>如果语法规则类似的话，可以合并到一个方法中。例如，考虑前面例子中的两个规则：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_expression_plus(p):</span><br><span class="line">    &#39;expression : expression PLUS term&#39;</span><br><span class="line">    p[0] &#x3D; p[1] + p[3]</span><br><span class="line"></span><br><span class="line">def p_expression_minus(t):</span><br><span class="line">    &#39;expression : expression MINUS term&#39;</span><br><span class="line">    p[0] &#x3D; p[1] - p[3]</span><br></pre></td></tr></table></figure>
<p>比起写两个方法，你可以像下面这样写在一个方法里面：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_expression(p):</span><br><span class="line">    &#39;&#39;&#39;expression : expression PLUS term</span><br><span class="line">                  | expression MINUS term&#39;&#39;&#39;</span><br><span class="line">    if p[2] &#x3D;&#x3D; &#39;+&#39;:</span><br><span class="line">        p[0] &#x3D; p[1] + p[3]</span><br><span class="line">    elif p[2] &#x3D;&#x3D; &#39;-&#39;:</span><br><span class="line">        p[0] &#x3D; p[1] - p[3]</span><br></pre></td></tr></table></figure><br>总之，方法的文档字符串可以包含多个语法规则。所以，像这样写也是合法的（尽管可能会引起困惑）：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_binary_operators(p):</span><br><span class="line">    &#39;&#39;&#39;expression : expression PLUS term</span><br><span class="line">                  | expression MINUS term</span><br><span class="line">       term       : term TIMES factor</span><br><span class="line">                  | term DIVIDE factor&#39;&#39;&#39;</span><br><span class="line">    if p[2] &#x3D;&#x3D; &#39;+&#39;:</span><br><span class="line">        p[0] &#x3D; p[1] + p[3]</span><br><span class="line">    elif p[2] &#x3D;&#x3D; &#39;-&#39;:</span><br><span class="line">        p[0] &#x3D; p[1] - p[3]</span><br><span class="line">    elif p[2] &#x3D;&#x3D; &#39;*&#39;:</span><br><span class="line">        p[0] &#x3D; p[1] * p[3]</span><br><span class="line">    elif p[2] &#x3D;&#x3D; &#39;&#x2F;&#39;:</span><br><span class="line">        p[0] &#x3D; p[1] &#x2F; p[3]</span><br></pre></td></tr></table></figure><br>如果所有的规则都有相似的结构，那么将语法规则合并才是个不错的注意（比如，产生式的项数相同）。不然，语义动作可能会变得复杂。不过，简单情况下，可以使用len()方法区分，比如：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_expressions(p):</span><br><span class="line">    &#39;&#39;&#39;expression : expression MINUS expression</span><br><span class="line">                  | MINUS expression&#39;&#39;&#39;</span><br><span class="line">    if (len(p) &#x3D;&#x3D; 4):</span><br><span class="line">        p[0] &#x3D; p[1] - p[3]</span><br><span class="line">    elif (len(p) &#x3D;&#x3D; 3):</span><br><span class="line">        p[0] &#x3D; -p[2]</span><br></pre></td></tr></table></figure><br>如果考虑解析的性能，你应该避免像这些例子一样在一个语法规则里面用很多条件来处理。因为，每次检查当前究竟匹配的是哪个语法规则的时候，实际上重复做了分析器已经做过的事（分析器已经准确的知道哪个规则被匹配了）。为每个规则定义单独的方法，可以消除这点开销。</p>
<h3 id="字面字符-1"><a href="#字面字符-1" class="headerlink" title="字面字符"></a>字面字符</h3><p>如果愿意，可以在语法规则里面使用单个的字面字符，例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_binary_operators(p):</span><br><span class="line">    &#39;&#39;&#39;expression : expression &#39;+&#39; term</span><br><span class="line">                  | expression &#39;-&#39; term</span><br><span class="line">       term       : term &#39;*&#39; factor</span><br><span class="line">                  | term &#39;&#x2F;&#39; factor&#39;&#39;&#39;</span><br><span class="line">    if p[2] &#x3D;&#x3D; &#39;+&#39;:</span><br><span class="line">        p[0] &#x3D; p[1] + p[3]</span><br><span class="line">    elif p[2] &#x3D;&#x3D; &#39;-&#39;:</span><br><span class="line">        p[0] &#x3D; p[1] - p[3]</span><br><span class="line">    elif p[2] &#x3D;&#x3D; &#39;*&#39;:</span><br><span class="line">        p[0] &#x3D; p[1] * p[3]</span><br><span class="line">    elif p[2] &#x3D;&#x3D; &#39;&#x2F;&#39;:</span><br><span class="line">        p[0] &#x3D; p[1] &#x2F; p[3]</span><br></pre></td></tr></table></figure>
<p>字符必须像’+’那样使用单引号。除此之外，需要将用到的字符定义单独定义在 lex 文件的literals列表里：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Literals.  Should be placed in module given to lex()</span><br><span class="line">literals &#x3D; [&#39;+&#39;,&#39;-&#39;,&#39;*&#39;,&#39;&#x2F;&#39; ]</span><br></pre></td></tr></table></figure><br>字面的字符只能是单个字符。因此，像’&lt;=’或者’==’都是不合法的，只能使用一般的词法规则（例如 t_EQ = r’==’)。</p>
<h3 id="空产生式"><a href="#空产生式" class="headerlink" title="空产生式"></a>空产生式</h3><p>yacc.py 可以处理空产生式，像下面这样做：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_empty(p):</span><br><span class="line">    &#39;empty :&#39;</span><br><span class="line">    pass</span><br></pre></td></tr></table></figure>
<p>现在可以使用空匹配，只要将’empty’当成一个符号使用：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_optitem(p):</span><br><span class="line">    &#39;optitem : item&#39;</span><br><span class="line">    &#39;        | empty&#39;</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><br>注意：你可以将产生式保持’空’，来表示空匹配。然而，我发现用一个’empty’规则并用其来替代’空’，更容易表达意图，并有较好的可读性。</p>
<h3 id="改变起始符号"><a href="#改变起始符号" class="headerlink" title="改变起始符号"></a>改变起始符号</h3><p>默认情况下，在 yacc 中的第一条规则是起始语法规则（顶层规则）。可以用 start 标识来改变这种行为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">start &#x3D; &#39;foo&#39;</span><br><span class="line">def p_bar(p):</span><br><span class="line">    &#39;bar : A B&#39;</span><br><span class="line"></span><br><span class="line"># This is the starting rule due to the start specifier above</span><br><span class="line">def p_foo(p):</span><br><span class="line">    &#39;foo : bar X&#39;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>用 start 标识有助于在调试的时候将大型的语法规则分成小部分来分析。也可把 start 符号作为yacc的参数：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yacc.yacc(start&#x3D;&#39;foo&#39;)</span><br></pre></td></tr></table></figure></p>
<h3 id="处理二义文法"><a href="#处理二义文法" class="headerlink" title="处理二义文法"></a>处理二义文法</h3><p>上面例子中，对表达式的文法描述用一种特别的形式规避了二义文法。然而，在很多情况下，这样的特殊文法很难写，或者很别扭。一个更为自然和舒服的语法表达应该是这样的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">expression : expression PLUS expression</span><br><span class="line">           | expression MINUS expression</span><br><span class="line">           | expression TIMES expression</span><br><span class="line">           | expression DIVIDE expression</span><br><span class="line">           | LPAREN expression RPAREN</span><br><span class="line">           | NUMBER</span><br></pre></td></tr></table></figure>
<p>不幸的是，这样的文法是存在二义性的。举个例子，如果你要解析字符串”3 <em> 4 + 5”，操作符如何分组并没有指明，究竟是表示”(3 </em> 4) + 5”还是”3 <em> (4 + 5)”呢？<br>如果在 yacc.py 中存在二义文法，会输出”移进归约冲突”或者”归约归约冲突”。在分析器无法确定是将下一个符号移进栈还是将当前栈中的符号归约时会产生移进归约冲突。例如，对于”3 </em> 4 + 5”，分析器内部栈是这样工作的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Step Symbol Stack           Input Tokens            Action</span><br><span class="line">---- ---------------------  ---------------------   -------------------------------</span><br><span class="line">1    $                                3 * 4 + 5$    Shift 3</span><br><span class="line">2    $ 3                                * 4 + 5$    Reduce : expression : NUMBER</span><br><span class="line">3    $ expr                             * 4 + 5$    Shift *</span><br><span class="line">4    $ expr *                             4 + 5$    Shift 4</span><br><span class="line">5    $ expr * 4                             + 5$    Reduce: expression : NUMBER</span><br><span class="line">6    $ expr * expr                          + 5$    SHIFT&#x2F;REDUCE CONFLICT ????</span><br></pre></td></tr></table></figure>
<p>两种选择对于上面的上下文无关文法而言都是合法的。<br>默认情况下，所有的移进归约冲突会倾向于使用移进来处理。因此，对于上面的例子，分析器总是会将’+’进栈，而不是做归约。虽然在很多情况下，这个策略是合适的（像”if-then”和”if-then-else”），但这对于算术表达式是不够的。事实上，对于上面的例子，将’+’进栈是完全错误的，应当先将expr * expr归约，因为乘法的优先级要高于加法。</p>
<p>为了解决二义文法，尤其是对表达式文法，yacc.py 允许为标记单独指定优先级和结合性。需要像下面这样增加一个 precedence 变量：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">precedence &#x3D; (</span><br><span class="line">    (&#39;left&#39;, &#39;PLUS&#39;, &#39;MINUS&#39;),</span><br><span class="line">    (&#39;left&#39;, &#39;TIMES&#39;, &#39;DIVIDE&#39;),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>这样的定义说明 PLUS/MINUS 标记具有相同的优先级和左结合性，TIMES/DIVIDE 具有相同的优先级和左结合性。在 precedence 声明中，标记的优先级从低到高。因此，这个声明表明 TIMES/DIVIDE（他们较晚加入 precedence）的优先级高于 PLUS/MINUS。<br>由于为标记添加了数字表示的优先级和结合性的属性，所以，对于上面的例子，将会得到：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PLUS      : level &#x3D; 1,  assoc &#x3D; &#39;left&#39;</span><br><span class="line">MINUS     : level &#x3D; 1,  assoc &#x3D; &#39;left&#39;</span><br><span class="line">TIMES     : level &#x3D; 2,  assoc &#x3D; &#39;left&#39;</span><br><span class="line">DIVIDE    : level &#x3D; 2,  assoc &#x3D; &#39;left&#39;</span><br></pre></td></tr></table></figure>
<p>随后这些值被附加到语法规则的优先级和结合性属性上，这些值由最右边的终结符的优先级和结合性决定：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">expression : expression PLUS expression                 # level &#x3D; 1, left</span><br><span class="line">           | expression MINUS expression                # level &#x3D; 1, left</span><br><span class="line">           | expression TIMES expression                # level &#x3D; 2, left</span><br><span class="line">           | expression DIVIDE expression               # level &#x3D; 2, left</span><br><span class="line">           | LPAREN expression RPAREN                   # level &#x3D; None (not specified)</span><br></pre></td></tr></table></figure><br>           | NUMBER                                     # level = None (not specified)</p>
<p>当出现移进归约冲突时，分析器生成器根据下面的规则解决二义文法：</p>
<ul>
<li>如果当前的标记的优先级高于栈顶规则的优先级，移进当前标记</li>
<li>如果栈顶规则的优先级更高，进行归约</li>
<li>如果当前的标记与栈顶规则的优先级相同，如果标记是左结合的，则归约，否则，如果是右结合的则移进</li>
<li>如果没有优先级可以参考，默认对于移进归约冲突执行移进</li>
</ul>
<p>比如，当解析到”expression PLUS expression”这个语法时，下一个标记是 TIMES，此时将执行移进，因为 TIMES 具有比 PLUS 更高的优先级；当解析到”expression TIMES expression”，下一个标记是 PLUS，此时将执行归约，因为 PLUS 的优先级低于 TIMES。</p>
<p>如果在使用前三种技术解决已经归约冲突后，yacc.py 将不会报告语法中的冲突或者错误（不过，会在 parser.out 这个调试文件中输出一些信息）。</p>
<p>使用 precedence 指定优先级的技术会带来一个问题，有时运算符的优先级需要基于上下文。例如，考虑”3 + 4 * -5”中的一元的’-‘。数学上讲，一元运算符应当拥有较高的优先级。然而，在我们的 precedence 定义中，MINUS 的优先级却低于 TIMES。为了解决这个问题，precedene 规则中可以包含”虚拟标记”：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">precedence &#x3D; (</span><br><span class="line">    (&#39;left&#39;, &#39;PLUS&#39;, &#39;MINUS&#39;),</span><br><span class="line">    (&#39;left&#39;, &#39;TIMES&#39;, &#39;DIVIDE&#39;),</span><br><span class="line">    (&#39;right&#39;, &#39;UMINUS&#39;),            # Unary minus operator</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>在语法文件中，我们可以这么表示一元算符：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_expr_uminus(p):</span><br><span class="line">    &#39;expression : MINUS expression %prec UMINUS&#39;</span><br><span class="line">    p[0] &#x3D; -p[2]</span><br></pre></td></tr></table></figure><br>在这个例子中，%prec UMINUS 覆盖了默认的优先级（MINUS 的优先级），将 UMINUS 指代的优先级应用在该语法规则上。<br>起初，UMINUS 标记的例子会让人感到困惑。UMINUS 既不是输入的标记也不是语法规则，你应当将其看成 precedence 表中的特殊的占位符。当你使用 %prec 宏时，你是在告诉 yacc，你希望表达式使用这个占位符所表示的优先级，而不是正常的优先级。</p>
<p>还可以在 precedence 表中指定”非关联”。这表明你不希望链式运算符。比如，假如你希望支持比较运算符’&lt;’和’&gt;’，但是你不希望支持 a &lt; b &lt; c，只要简单指定规则如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">precedence &#x3D; (</span><br><span class="line">    (&#39;nonassoc&#39;, &#39;LESSTHAN&#39;, &#39;GREATERTHAN&#39;),  # Nonassociative operators</span><br><span class="line">    (&#39;left&#39;, &#39;PLUS&#39;, &#39;MINUS&#39;),</span><br><span class="line">    (&#39;left&#39;, &#39;TIMES&#39;, &#39;DIVIDE&#39;),</span><br><span class="line">    (&#39;right&#39;, &#39;UMINUS&#39;),            # Unary minus operator</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>此时，当输入形如 a &lt; b &lt; c 时，将产生语法错误，却不影响形如 a &lt; b 的表达式。<br>对于给定的符号集，存在多种语法规则可以匹配时会产生归约/归约冲突。这样的冲突往往很严重，而且总是通过匹配最早出现的语法规则来解决。归约/归约冲突几乎总是相同的符号集合具有不同的规则可以匹配，而在这一点上无法抉择，比如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">assignment :  ID EQUALS NUMBER</span><br><span class="line">           |  ID EQUALS expression</span><br><span class="line">           </span><br><span class="line">expression : expression PLUS expression</span><br><span class="line">           | expression MINUS expression</span><br><span class="line">           | expression TIMES expression</span><br><span class="line">           | expression DIVIDE expression</span><br><span class="line">           | LPAREN expression RPAREN</span><br><span class="line">           | NUMBER</span><br></pre></td></tr></table></figure>
<p>这个例子中，对于下面这两条规则将产生归约/归约冲突：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">assignment  : ID EQUALS NUMBER</span><br><span class="line">expression  : NUMBER</span><br></pre></td></tr></table></figure><br>比如，对于”a = 5”，分析器不知道应当按照 assignment : ID EQUALS NUMBER 归约，还是先将 5 归约成 expression，再归约成 assignment : ID EQUALS expression。<br>应当指出的是，只是简单的查看语法规则是很难减少归约/归约冲突。如果出现归约/归约冲突，yacc()会帮助打印出警告信息：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">WARNING: 1 reduce&#x2F;reduce conflict</span><br><span class="line">WARNING: reduce&#x2F;reduce conflict in state 15 resolved using rule (assignment -&gt; ID EQUALS NUMBER)</span><br><span class="line">WARNING: rejected rule (expression -&gt; NUMBER)</span><br></pre></td></tr></table></figure>
<p>上面的信息标识出了冲突的两条规则，但是，并无法指出究竟在什么情况下会出现这样的状态。想要发现问题，你可能需要结合语法规则和parser.out调试文件的内容。</p>
<h3 id="parser-out调试文件"><a href="#parser-out调试文件" class="headerlink" title="parser.out调试文件"></a>parser.out调试文件</h3><p>使用 LR 分析算法跟踪移进/归约冲突和归约/归约冲突是件乐在其中的事。为了辅助调试，yacc.py 在生成分析表时会创建出一个调试文件叫 parser.out：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Unused terminals:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Grammar</span><br><span class="line"></span><br><span class="line">Rule 1     expression -&gt; expression PLUS expression</span><br><span class="line">Rule 2     expression -&gt; expression MINUS expression</span><br><span class="line">Rule 3     expression -&gt; expression TIMES expression</span><br><span class="line">Rule 4     expression -&gt; expression DIVIDE expression</span><br><span class="line">Rule 5     expression -&gt; NUMBER</span><br><span class="line">Rule 6     expression -&gt; LPAREN expression RPAREN</span><br><span class="line"></span><br><span class="line">Terminals, with rules where they appear</span><br><span class="line"></span><br><span class="line">TIMES                : 3</span><br><span class="line">error                : </span><br><span class="line">MINUS                : 2</span><br><span class="line">RPAREN               : 6</span><br><span class="line">LPAREN               : 6</span><br><span class="line">DIVIDE               : 4</span><br><span class="line">PLUS                 : 1</span><br><span class="line">NUMBER               : 5</span><br><span class="line"></span><br><span class="line">Nonterminals, with rules where they appear</span><br><span class="line"></span><br><span class="line">expression           : 1 1 2 2 3 3 4 4 6 0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Parsing method: LALR</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">state 0</span><br><span class="line"></span><br><span class="line">    S&#39; -&gt; . expression</span><br><span class="line">    expression -&gt; . expression PLUS expression</span><br><span class="line">    expression -&gt; . expression MINUS expression</span><br><span class="line">    expression -&gt; . expression TIMES expression</span><br><span class="line">    expression -&gt; . expression DIVIDE expression</span><br><span class="line">    expression -&gt; . NUMBER</span><br><span class="line">    expression -&gt; . LPAREN expression RPAREN</span><br><span class="line"></span><br><span class="line">    NUMBER          shift and go to state 3</span><br><span class="line">    LPAREN          shift and go to state 2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">state 1</span><br><span class="line"></span><br><span class="line">    S&#39; -&gt; expression .</span><br><span class="line">    expression -&gt; expression . PLUS expression</span><br><span class="line">    expression -&gt; expression . MINUS expression</span><br><span class="line">    expression -&gt; expression . TIMES expression</span><br><span class="line">    expression -&gt; expression . DIVIDE expression</span><br><span class="line"></span><br><span class="line">    PLUS            shift and go to state 6</span><br><span class="line">    MINUS           shift and go to state 5</span><br><span class="line">    TIMES           shift and go to state 4</span><br><span class="line">    DIVIDE          shift and go to state 7</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">state 2</span><br><span class="line"></span><br><span class="line">    expression -&gt; LPAREN . expression RPAREN</span><br><span class="line">    expression -&gt; . expression PLUS expression</span><br><span class="line">    expression -&gt; . expression MINUS expression</span><br><span class="line">    expression -&gt; . expression TIMES expression</span><br><span class="line">    expression -&gt; . expression DIVIDE expression</span><br><span class="line">    expression -&gt; . NUMBER</span><br><span class="line">    expression -&gt; . LPAREN expression RPAREN</span><br><span class="line"></span><br><span class="line">    NUMBER          shift and go to state 3</span><br><span class="line">    LPAREN          shift and go to state 2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">state 3</span><br><span class="line"></span><br><span class="line">    expression -&gt; NUMBER .</span><br><span class="line"></span><br><span class="line">    $               reduce using rule 5</span><br><span class="line">    PLUS            reduce using rule 5</span><br><span class="line">    MINUS           reduce using rule 5</span><br><span class="line">    TIMES           reduce using rule 5</span><br><span class="line">    DIVIDE          reduce using rule 5</span><br><span class="line">    RPAREN          reduce using rule 5</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">state 4</span><br><span class="line"></span><br><span class="line">    expression -&gt; expression TIMES . expression</span><br><span class="line">    expression -&gt; . expression PLUS expression</span><br><span class="line">    expression -&gt; . expression MINUS expression</span><br><span class="line">    expression -&gt; . expression TIMES expression</span><br><span class="line">    expression -&gt; . expression DIVIDE expression</span><br><span class="line">    expression -&gt; . NUMBER</span><br><span class="line">    expression -&gt; . LPAREN expression RPAREN</span><br><span class="line"></span><br><span class="line">    NUMBER          shift and go to state 3</span><br><span class="line">    LPAREN          shift and go to state 2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">state 5</span><br><span class="line"></span><br><span class="line">    expression -&gt; expression MINUS . expression</span><br><span class="line">    expression -&gt; . expression PLUS expression</span><br><span class="line">    expression -&gt; . expression MINUS expression</span><br><span class="line">    expression -&gt; . expression TIMES expression</span><br><span class="line">    expression -&gt; . expression DIVIDE expression</span><br><span class="line">    expression -&gt; . NUMBER</span><br><span class="line">    expression -&gt; . LPAREN expression RPAREN</span><br><span class="line"></span><br><span class="line">    NUMBER          shift and go to state 3</span><br><span class="line">    LPAREN          shift and go to state 2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">state 6</span><br><span class="line"></span><br><span class="line">    expression -&gt; expression PLUS . expression</span><br><span class="line">    expression -&gt; . expression PLUS expression</span><br><span class="line">    expression -&gt; . expression MINUS expression</span><br><span class="line">    expression -&gt; . expression TIMES expression</span><br><span class="line">    expression -&gt; . expression DIVIDE expression</span><br><span class="line">    expression -&gt; . NUMBER</span><br><span class="line">    expression -&gt; . LPAREN expression RPAREN</span><br><span class="line"></span><br><span class="line">    NUMBER          shift and go to state 3</span><br><span class="line">    LPAREN          shift and go to state 2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">state 7</span><br><span class="line"></span><br><span class="line">    expression -&gt; expression DIVIDE . expression</span><br><span class="line">    expression -&gt; . expression PLUS expression</span><br><span class="line">    expression -&gt; . expression MINUS expression</span><br><span class="line">    expression -&gt; . expression TIMES expression</span><br><span class="line">    expression -&gt; . expression DIVIDE expression</span><br><span class="line">    expression -&gt; . NUMBER</span><br><span class="line">    expression -&gt; . LPAREN expression RPAREN</span><br><span class="line"></span><br><span class="line">    NUMBER          shift and go to state 3</span><br><span class="line">    LPAREN          shift and go to state 2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">state 8</span><br><span class="line"></span><br><span class="line">    expression -&gt; LPAREN expression . RPAREN</span><br><span class="line">    expression -&gt; expression . PLUS expression</span><br><span class="line">    expression -&gt; expression . MINUS expression</span><br><span class="line">    expression -&gt; expression . TIMES expression</span><br><span class="line">    expression -&gt; expression . DIVIDE expression</span><br><span class="line"></span><br><span class="line">    RPAREN          shift and go to state 13</span><br><span class="line">    PLUS            shift and go to state 6</span><br><span class="line">    MINUS           shift and go to state 5</span><br><span class="line">    TIMES           shift and go to state 4</span><br><span class="line">    DIVIDE          shift and go to state 7</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">state 9</span><br><span class="line"></span><br><span class="line">    expression -&gt; expression TIMES expression .</span><br><span class="line">    expression -&gt; expression . PLUS expression</span><br><span class="line">    expression -&gt; expression . MINUS expression</span><br><span class="line">    expression -&gt; expression . TIMES expression</span><br><span class="line">    expression -&gt; expression . DIVIDE expression</span><br><span class="line"></span><br><span class="line">    $               reduce using rule 3</span><br><span class="line">    PLUS            reduce using rule 3</span><br><span class="line">    MINUS           reduce using rule 3</span><br><span class="line">    TIMES           reduce using rule 3</span><br><span class="line">    DIVIDE          reduce using rule 3</span><br><span class="line">    RPAREN          reduce using rule 3</span><br><span class="line"></span><br><span class="line">  ! PLUS            [ shift and go to state 6 ]</span><br><span class="line">  ! MINUS           [ shift and go to state 5 ]</span><br><span class="line">  ! TIMES           [ shift and go to state 4 ]</span><br><span class="line">  ! DIVIDE          [ shift and go to state 7 ]</span><br><span class="line"></span><br><span class="line">state 10</span><br><span class="line"></span><br><span class="line">    expression -&gt; expression MINUS expression .</span><br><span class="line">    expression -&gt; expression . PLUS expression</span><br><span class="line">    expression -&gt; expression . MINUS expression</span><br><span class="line">    expression -&gt; expression . TIMES expression</span><br><span class="line">    expression -&gt; expression . DIVIDE expression</span><br><span class="line"></span><br><span class="line">    $               reduce using rule 2</span><br><span class="line">    PLUS            reduce using rule 2</span><br><span class="line">    MINUS           reduce using rule 2</span><br><span class="line">    RPAREN          reduce using rule 2</span><br><span class="line">    TIMES           shift and go to state 4</span><br><span class="line">    DIVIDE          shift and go to state 7</span><br><span class="line"></span><br><span class="line">  ! TIMES           [ reduce using rule 2 ]</span><br><span class="line">  ! DIVIDE          [ reduce using rule 2 ]</span><br><span class="line">  ! PLUS            [ shift and go to state 6 ]</span><br><span class="line">  ! MINUS           [ shift and go to state 5 ]</span><br><span class="line"></span><br><span class="line">state 11</span><br><span class="line"></span><br><span class="line">    expression -&gt; expression PLUS expression .</span><br><span class="line">    expression -&gt; expression . PLUS expression</span><br><span class="line">    expression -&gt; expression . MINUS expression</span><br><span class="line">    expression -&gt; expression . TIMES expression</span><br><span class="line">    expression -&gt; expression . DIVIDE expression</span><br><span class="line"></span><br><span class="line">    $               reduce using rule 1</span><br><span class="line">    PLUS            reduce using rule 1</span><br><span class="line">    MINUS           reduce using rule 1</span><br><span class="line">    RPAREN          reduce using rule 1</span><br><span class="line">    TIMES           shift and go to state 4</span><br><span class="line">    DIVIDE          shift and go to state 7</span><br><span class="line"></span><br><span class="line">  ! TIMES           [ reduce using rule 1 ]</span><br><span class="line">  ! DIVIDE          [ reduce using rule 1 ]</span><br><span class="line">  ! PLUS            [ shift and go to state 6 ]</span><br><span class="line">  ! MINUS           [ shift and go to state 5 ]</span><br><span class="line"></span><br><span class="line">state 12</span><br><span class="line"></span><br><span class="line">    expression -&gt; expression DIVIDE expression .</span><br><span class="line">    expression -&gt; expression . PLUS expression</span><br><span class="line">    expression -&gt; expression . MINUS expression</span><br><span class="line">    expression -&gt; expression . TIMES expression</span><br><span class="line">    expression -&gt; expression . DIVIDE expression</span><br><span class="line"></span><br><span class="line">    $               reduce using rule 4</span><br><span class="line">    PLUS            reduce using rule 4</span><br><span class="line">    MINUS           reduce using rule 4</span><br><span class="line">    TIMES           reduce using rule 4</span><br><span class="line">    DIVIDE          reduce using rule 4</span><br><span class="line">    RPAREN          reduce using rule 4</span><br><span class="line"></span><br><span class="line">  ! PLUS            [ shift and go to state 6 ]</span><br><span class="line">  ! MINUS           [ shift and go to state 5 ]</span><br><span class="line">  ! TIMES           [ shift and go to state 4 ]</span><br><span class="line">  ! DIVIDE          [ shift and go to state 7 ]</span><br><span class="line"></span><br><span class="line">state 13</span><br><span class="line"></span><br><span class="line">    expression -&gt; LPAREN expression RPAREN .</span><br><span class="line"></span><br><span class="line">    $               reduce using rule 6</span><br><span class="line">    PLUS            reduce using rule 6</span><br><span class="line">    MINUS           reduce using rule 6</span><br><span class="line">    TIMES           reduce using rule 6</span><br><span class="line">    DIVIDE          reduce using rule 6</span><br></pre></td></tr></table></figure>
<pre><code>RPAREN          reduce using rule 6
</code></pre><p>文件中出现的不同状态，代表了有效输入标记的所有可能的组合，这是依据文法规则得到的。当得到输入标记时，分析器将构造一个栈，并找到匹配的规则。每个状态跟踪了当前输入进行到语法规则中的哪个位置，在每个规则中，’.’表示当前分析到规则的哪个位置，而且，对于在当前状态下，输入的每个有效标记导致的动作也被罗列出来。当出现移进/归约或归约/归约冲突时，被忽略的规则前面会添加!，就像这样：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">! TIMES           [ reduce using rule 2 ]</span><br><span class="line">  ! DIVIDE          [ reduce using rule 2 ]</span><br><span class="line">  ! PLUS            [ shift and go to state 6 ]</span><br><span class="line">  ! MINUS           [ shift and go to state 5 ]</span><br></pre></td></tr></table></figure>
<p>通过查看这些规则并结合一些实例，通常能够找到大部分冲突的根源。应该强调的是，不是所有的移进归约冲突都是不好的，想要确定解决方法是否正确，唯一的办法就是查看 parser.out。</p>
<h3 id="处理语法错误"><a href="#处理语法错误" class="headerlink" title="处理语法错误"></a>处理语法错误</h3><p>如果你创建的分析器用于产品，处理语法错误是很重要的。一般而言，你不希望分析器在遇到错误的时候就抛出异常并终止，相反，你需要它报告错误，尽可能的恢复并继续分析，一次性的将输入中所有的错误报告给用户。这是一些已知语言编译器的标准行为，例如 C,C++,Java。在 PLY 中，在语法分析过程中出现错误，错误会被立即检测到（分析器不会继续读取源文件中错误点后面的标记）。然而，这时，分析器会进入恢复模式，这个模式能够用来尝试继续向下分析。LR 分析器的错误恢复是个理论与技巧兼备的问题，yacc.py 提供的错误机制与 Unix 下的 yacc 类似，所以你可以从诸如 O’Reilly 出版的《Lex and yacc》的书中找到更多的细节。</p>
<p>当错误发生时，yacc.py 按照如下步骤进行：</p>
<ul>
<li>第一次错误产生时，用户定义的 p_error()方法会被调用，出错的标记会作为参数传入；如果错误是因为到达文件结尾造成的，传入的参数将为 None。随后，分析器进入到“错误恢复”模式，该模式下不会在产生p_error()调用，直到它成功的移进 3 个标记，然后回归到正常模式。</li>
<li>如果在 p_error() 中没有指定恢复动作的话，这个导致错误的标记会被替换成一个特殊的 error 标记。</li>
<li>如果导致错误的标记已经是 error 的话，原先的栈顶的标记将被移除。</li>
<li>如果整个分析栈被放弃，分析器会进入重置状态，并从他的初始状态开始分析。</li>
<li>如果此时的语法规则接受 error 标记，error 标记会移进栈。</li>
<li>如果当前栈顶是 error 标记，之后的标记将被忽略，直到有标记能够导致 error 的归约。<ul>
<li>根据 error 规则恢复和再同步</li>
</ul>
</li>
</ul>
<p>最佳的处理语法错误的做法是在语法规则中包含 error 标记。例如，假设你的语言有一个关于 print 的语句的语法规则：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_statement_print(p):</span><br><span class="line">     &#39;statement : PRINT expr SEMI&#39;</span><br><span class="line">     ...</span><br></pre></td></tr></table></figure>
<p>为了处理可能的错误表达式，你可以添加一条额外的语法规则：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_statement_print_error(p):</span><br><span class="line">     &#39;statement : PRINT error SEMI&#39;</span><br><span class="line">     print &quot;Syntax error in print statement. Bad expression&quot;</span><br></pre></td></tr></table></figure><br>这样（expr 错误时），error 标记会匹配任意多个分号之前的标记（分号是SEMI指代的字符）。一旦找到分号，规则将被匹配，这样 error 标记就被归约了。<br>这种类型的恢复有时称为”分析器再同步”。error 标记扮演了表示所有错误标记的通配符的角色，而紧随其后的标记扮演了同步标记的角色。</p>
<p>重要的一个说明是，通常 error 不会作为语法规则的最后一个标记，像这样：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_statement_print_error(p):</span><br><span class="line">    &#39;statement : PRINT error&#39;</span><br><span class="line">    print &quot;Syntax error in print statement. Bad expression&quot;</span><br></pre></td></tr></table></figure>
<p>这是因为，第一个导致错误的标记会使得该规则立刻归约，进而使得在后面还有错误标记的情况下，恢复变得困难。</p>
<ul>
<li>悲观恢复模式</li>
</ul>
<p>另一个错误恢复方法是采用“悲观模式”：该模式下，开始放弃剩余的标记，直到能够达到一个合适的恢复机会。</p>
<p>悲观恢复模式都是在 p_error() 方法中做到的。例如，这个方法在开始丢弃标记后，直到找到闭合的’}’，才重置分析器到初始化状态：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_error(p):</span><br><span class="line">    print &quot;Whoa. You are seriously hosed.&quot;</span><br><span class="line">    # Read ahead looking for a closing &#39;&#125;&#39;</span><br><span class="line">    while 1:</span><br><span class="line">        tok &#x3D; yacc.token()             # Get the next token</span><br><span class="line">        if not tok or tok.type &#x3D;&#x3D; &#39;RBRACE&#39;: break</span><br><span class="line">    yacc.restart()</span><br></pre></td></tr></table></figure>
<p>下面这个方法简单的抛弃错误的标记，并告知分析器错误被接受了：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_error(p):</span><br><span class="line">    print &quot;Syntax error at token&quot;, p.type</span><br><span class="line">    # Just discard the token and tell the parser it&#39;s okay.</span><br><span class="line">    yacc.errok()</span><br></pre></td></tr></table></figure><br>在p_error()方法中，有三个可用的方法来控制分析器的行为：</p>
<ul>
<li>yacc.errok() 这个方法将分析器从恢复模式切换回正常模式。这会使得不会产生 error 标记，并重置内部的 error 计数器，而且下一个语法错误会再次产生 p_error() 调用</li>
<li>yacc.token() 这个方法用于得到下一个标记</li>
<li>yacc.restart() 这个方法抛弃当前整个分析栈，并重置分析器为起始状态</li>
</ul>
<p>注意：这三个方法只能在p_error()中使用，不能用在其他任何地方。</p>
<p>p_error()方法也可以返回标记，这样能够控制将哪个标记作为下一个标记返回给分析器。这对于需要同步一些特殊标记的时候有用，比如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_error(p):</span><br><span class="line">    # Read ahead looking for a terminating &quot;;&quot;</span><br><span class="line">    while 1:</span><br><span class="line">        tok &#x3D; yacc.token()             # Get the next token</span><br><span class="line">        if not tok or tok.type &#x3D;&#x3D; &#39;SEMI&#39;: break</span><br><span class="line">    yacc.errok()</span><br><span class="line"></span><br><span class="line">    # Return SEMI to the parser as the next lookahead token</span><br><span class="line">    return tok</span><br></pre></td></tr></table></figure>
<ul>
<li>从产生式中抛出错误</li>
</ul>
<p>如果有需要的话，产生式规则可以主动的使分析器进入恢复模式。这是通过抛出SyntaxError异常做到的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_production(p):</span><br><span class="line">    &#39;production : some production ...&#39;</span><br><span class="line">    raise SyntaxError</span><br></pre></td></tr></table></figure>
<p>raise SyntaxError 错误的效果就如同当前的标记是错误标记一样。因此，当你这么做的话，最后一个标记将被弹出栈，当前的下一个标记将是 error 标记，分析器进入恢复模式，试图归约满足 error 标记的规则。此后的步骤与检测到语法错误的情况是完全一样的，p_error() 也会被调用。<br>手动设置错误有个重要的方面，就是 p_error() 方法在这种情况下不会调用。如果你希望记录错误，确保在抛出 SyntaxError 错误的产生式中实现。</p>
<p>注：这个功能是为了模仿 yacc 中的YYERROR宏的行为</p>
<ul>
<li>错误恢复总结</li>
</ul>
<p>对于通常的语言，使用 error 规则和再同步标记可能是最合理的手段。这是因为你可以将语法设计成在一个相对容易恢复和继续分析的点捕获错误。悲观恢复模式只在一些十分特殊的应用中有用，这些应用往往需要丢弃掉大量输入，再寻找合理的同步点。</p>
<h3 id="行号和位置的跟踪"><a href="#行号和位置的跟踪" class="headerlink" title="行号和位置的跟踪"></a>行号和位置的跟踪</h3><p>位置跟踪通常是个设计编译器时的技巧性玩意儿。默认情况下，PLY 跟踪所有标记的行号和位置，这些信息可以这样得到：</p>
<ul>
<li>p.lineno(num) 返回第 num 个符号的行号</li>
<li>p.lexpos(num) 返回第 num 个符号的词法位置偏移<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_expression(p):</span><br><span class="line">    &#39;expression : expression PLUS expression&#39;</span><br><span class="line">    p.lineno(1)        # Line number of the left expression</span><br><span class="line">    p.lineno(2)        # line number of the PLUS operator</span><br><span class="line">    p.lineno(3)        # line number of the right expression</span><br><span class="line">    ...</span><br><span class="line">    start,end &#x3D; p.linespan(3)    # Start,end lines of the right expression</span><br><span class="line">    starti,endi &#x3D; p.lexspan(3)   # Start,end positions of right expression</span><br></pre></td></tr></table></figure>
注意：lexspan() 方法只会返回的结束位置是最后一个符号的起始位置。<br>虽然，PLY 对所有符号的行号和位置的跟踪很管用，但经常是不必要的。例如，你仅仅是在错误信息中使用行号，你通常可以仅仅使用关键标记的信息，比如：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_bad_func(p):</span><br><span class="line">    &#39;funccall : fname LPAREN error RPAREN&#39;</span><br><span class="line">    # Line number reported from LPAREN token</span><br><span class="line">    print &quot;Bad function call at line&quot;, p.lineno(2)</span><br></pre></td></tr></table></figure>
<p>类似的，为了改善性能，你可以有选择性的将行号信息在必要的时候进行传递，这是通过 p.set_lineno() 实现的，例如：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_fname(p):</span><br><span class="line">    &#39;fname : ID&#39;</span><br><span class="line">    p[0] &#x3D; p[1]</span><br><span class="line">    p.set_lineno(0,p.lineno(1))</span><br></pre></td></tr></table></figure><br>对于已经完成分析的规则，PLY 不会保留行号信息，如果你是在构建抽象语法树而且需要行号，你应该确保行号保留在树上。</p>
<h3 id="构造抽象语法树"><a href="#构造抽象语法树" class="headerlink" title="构造抽象语法树"></a>构造抽象语法树</h3><p>yacc.py 没有构造抽像语法树的特殊方法。不过，你可以自己很简单的构造出来。</p>
<p>一个最为简单的构造方法是为每个语法规则创建元组或者字典，并传递它们。有很多中可行的方案，下面是一个例子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_expression_binop(p):</span><br><span class="line">    &#39;&#39;&#39;expression : expression PLUS expression</span><br><span class="line">                  | expression MINUS expression</span><br><span class="line">                  | expression TIMES expression</span><br><span class="line">                  | expression DIVIDE expression&#39;&#39;&#39;</span><br><span class="line"></span><br><span class="line">    p[0] &#x3D; (&#39;binary-expression&#39;,p[2],p[1],p[3])</span><br><span class="line"></span><br><span class="line">def p_expression_group(p):</span><br><span class="line">    &#39;expression : LPAREN expression RPAREN&#39;</span><br><span class="line">    p[0] &#x3D; (&#39;group-expression&#39;,p[2])</span><br><span class="line"></span><br><span class="line">def p_expression_number(p):</span><br><span class="line">    &#39;expression : NUMBER&#39;</span><br><span class="line">    p[0] &#x3D; (&#39;number-expression&#39;,p[1])</span><br></pre></td></tr></table></figure>
<p>另一种方法可以是为不同的抽象树节点创建一系列的数据结构，并赋值给 p[0]：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Expr: pass</span><br><span class="line"></span><br><span class="line">class BinOp(Expr):</span><br><span class="line">    def __init__(self,left,op,right):</span><br><span class="line">        self.type &#x3D; &quot;binop&quot;</span><br><span class="line">        self.left &#x3D; left</span><br><span class="line">        self.right &#x3D; right</span><br><span class="line">        self.op &#x3D; op</span><br><span class="line"></span><br><span class="line">class Number(Expr):</span><br><span class="line">    def __init__(self,value):</span><br><span class="line">        self.type &#x3D; &quot;number&quot;</span><br><span class="line">        self.value &#x3D; value</span><br><span class="line"></span><br><span class="line">def p_expression_binop(p):</span><br><span class="line">    &#39;&#39;&#39;expression : expression PLUS expression</span><br><span class="line">                  | expression MINUS expression</span><br><span class="line">                  | expression TIMES expression</span><br><span class="line">                  | expression DIVIDE expression&#39;&#39;&#39;</span><br><span class="line"></span><br><span class="line">    p[0] &#x3D; BinOp(p[1],p[2],p[3])</span><br><span class="line"></span><br><span class="line">def p_expression_group(p):</span><br><span class="line">    &#39;expression : LPAREN expression RPAREN&#39;</span><br><span class="line">    p[0] &#x3D; p[2]</span><br><span class="line"></span><br><span class="line">def p_expression_number(p):</span><br><span class="line">    &#39;expression : NUMBER&#39;</span><br><span class="line">    p[0] &#x3D; Number(p[1])</span><br></pre></td></tr></table></figure><br>这种方式的好处是在处理复杂语义时比较简单：类型检查、代码生成、以及其他针对树节点的功能。<br>为了简化树的遍历，可以创建一个通用的树节点结构，例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Node:</span><br><span class="line">    def __init__(self,type,children&#x3D;None,leaf&#x3D;None):</span><br><span class="line">         self.type &#x3D; type</span><br><span class="line">         if children:</span><br><span class="line">              self.children &#x3D; children</span><br><span class="line">         else:</span><br><span class="line">              self.children &#x3D; [ ]</span><br><span class="line">         self.leaf &#x3D; leaf</span><br><span class="line"> </span><br><span class="line">def p_expression_binop(p):</span><br><span class="line">    &#39;&#39;&#39;expression : expression PLUS expression</span><br><span class="line">                  | expression MINUS expression</span><br><span class="line">                  | expression TIMES expression</span><br><span class="line">                  | expression DIVIDE expression&#39;&#39;&#39;</span><br><span class="line">    p[0] &#x3D; Node(&quot;binop&quot;, [p[1],p[3]], p[2])</span><br></pre></td></tr></table></figure>
<h3 id="嵌入式动作"><a href="#嵌入式动作" class="headerlink" title="嵌入式动作"></a>嵌入式动作</h3><p>yacc 使用的分析技术只允许在规则规约后执行动作。假设有如下规则：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_foo(p):</span><br><span class="line">    &quot;foo : A B C D&quot;</span><br><span class="line">    print &quot;Parsed a foo&quot;, p[1],p[2],p[3],p[4]</span><br></pre></td></tr></table></figure>
<p>方法只会在符号 A,B,C和D 都完成后才能执行。可是有的时候，在中间阶段执行一小段代码是有用的。假如，你想在 A 完成后立即执行一些动作，像下面这样用空规则：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_foo(p):</span><br><span class="line">    &quot;foo : A seen_A B C D&quot;</span><br><span class="line">    print &quot;Parsed a foo&quot;, p[1],p[3],p[4],p[5]</span><br><span class="line">    print &quot;seen_A returned&quot;, p[2]</span><br><span class="line">def p_seen_A(p):</span><br><span class="line">    &quot;seen_A :&quot;</span><br><span class="line">    print &quot;Saw an A &#x3D; &quot;, p[-1]   # Access grammar symbol to left</span><br><span class="line">    p[0] &#x3D; some_value            # Assign value to seen_A</span><br></pre></td></tr></table></figure><br>在这个例子中，空规则 seen_A 将在 A 移进分析栈后立即执行。p[-1] 指代的是在分析栈上紧跟在 seen_A 左侧的符号。在这个例子中，是 A 符号。像其他普通的规则一样，在嵌入式行为中也可以通过为 p[0] 赋值来返回某些值。<br>使用嵌入式动作可能会导致移进归约冲突，比如，下面的语法是没有冲突的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_foo(p):</span><br><span class="line">    &quot;&quot;&quot;foo : abcd</span><br><span class="line">           | abcx&quot;&quot;&quot;</span><br><span class="line">def p_abcd(p):</span><br><span class="line">    &quot;abcd : A B C D&quot;</span><br><span class="line">def p_abcx(p):</span><br><span class="line">    &quot;abcx : A B C X&quot;</span><br></pre></td></tr></table></figure>
<p>可是，如果像这样插入一个嵌入式动作：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_foo(p):</span><br><span class="line">    &quot;&quot;&quot;foo : abcd</span><br><span class="line">           | abcx&quot;&quot;&quot;</span><br><span class="line">def p_abcd(p):</span><br><span class="line">    &quot;abcd : A B C D&quot;</span><br><span class="line">def p_abcx(p):</span><br><span class="line">    &quot;abcx : A B seen_AB C X&quot;</span><br><span class="line">def p_seen_AB(p):</span><br><span class="line">    &quot;seen_AB :&quot;</span><br></pre></td></tr></table></figure><br>会产生移进归约冲，只是由于对于两个规则 abcd 和 abcx 中的 C，分析器既可以根据 abcd 规则移进，也可以根据 abcx 规则先将空的 seen_AB 归约。<br>嵌入动作的一般用于分析以外的控制，比如为本地变量定义作用于。对于 C 语言：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_statements_block(p):</span><br><span class="line">    &quot;statements: LBRACE new_scope statements RBRACE&quot;&quot;&quot;</span><br><span class="line">    # Action code</span><br><span class="line">    ...</span><br><span class="line">    pop_scope()        # Return to previous scope</span><br><span class="line"></span><br><span class="line">def p_new_scope(p):</span><br><span class="line">    &quot;new_scope :&quot;</span><br><span class="line">    # Create a new scope for local variables</span><br><span class="line">    s &#x3D; new_scope()</span><br><span class="line">    push_scope(s)</span><br></pre></td></tr></table></figure>
<p>在这个例子中，new_scope 作为嵌入式行为，在左大括号{之后立即执行。可以是调正内部符号表或者其他方面。statements_block 一完成，代码可能会撤销在嵌入动作时的操作（比如，pop_scope())</p>
<h3 id="Yacc-的其他"><a href="#Yacc-的其他" class="headerlink" title="Yacc 的其他"></a>Yacc 的其他</h3><ul>
<li>默认的分析方法是 LALR，使用 SLR 请像这样运行 yacc()：yacc.yacc(method=”SLR”) 注意：LRLR 生成的分析表大约要比 SLR 的大两倍。解析的性能没有本质的区别，因为代码是一样的。由于 LALR 能力稍强，所以更多的用于复杂的语法。</li>
<li>默认情况下，yacc.py 依赖 lex.py 产生的标记。不过，可以用一个等价的词法标记生成器代替： yacc.parse(lexer=x) 这个例子中，x 必须是一个 Lexer 对象，至少拥有 x.token() 方法用来获取标记。如果将输入字串提供给 yacc.parse()，lexer 还必须具有 x.input() 方法。</li>
<li>默认情况下，yacc 在调试模式下生成分析表（会生成 parser.out 文件和其他东西），使用 yacc.yacc(debug=0) 禁用调试模式。</li>
<li>改变 parsetab.py 的文件名：yacc.yacc(tabmodule=”foo”)</li>
<li>改变 parsetab.py 的生成目录：yacc.yacc(tabmodule=”foo”,outputdir=”somedirectory”)</li>
<li>不生成分析表：yacc.yacc(write_tables=0)。注意：如果禁用分析表生成，yacc()将在每次运行的时候重新构建分析表（这里耗费的时候取决于语法文件的规模）</li>
<li>想在分析过程中输出丰富的调试信息，使用：yacc.parse(debug=1)</li>
<li>yacc.yacc()方法会返回分析器对象，如果你想在一个程序中支持多个分析器：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">p &#x3D; yacc.yacc()</span><br><span class="line">...</span><br><span class="line">p.parse()</span><br></pre></td></tr></table></figure>
注意：yacc.parse() 方法只绑定到最新创建的分析器对象上。</li>
<li>由于生成生成 LALR 分析表相对开销较大，先前生成的分析表会被缓存和重用。判断是否重新生成的依据是对所有的语法规则和优先级规则进行 MD5 校验，只有不匹配时才会重新生成。生成分析表是合理有效的办法，即使是面对上百个规则和状态的语法。对于复杂的编程语言，像 C 语言，在一些慢的机器上生成分析表可能要花费 30-60 秒，请耐心。</li>
<li>由于 LR 分析过程是基于分析表的，分析器的性能很大程度上取决于语法的规模。最大的瓶颈可能是词法分析器和语法规则的复杂度。<h2 id="多个语法和词法分析器"><a href="#多个语法和词法分析器" class="headerlink" title="多个语法和词法分析器"></a>多个语法和词法分析器</h2>在高级的分析器程序中，你可能同时需要多个语法和词法分析器。依照规则行事不会有问题。不过，你需要小心确定所有东西都正确的绑定(hooked up)了。首先，保证将 lex() 和 yacc() 返回的对象保存起来：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lexer  &#x3D; lex.lex()       # Return lexer object</span><br><span class="line">parser &#x3D; yacc.yacc()     # Return parser object</span><br></pre></td></tr></table></figure>
<p>接着，在解析时，确保给 parse() 方法一个正确的 lexer 引用：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">parser.parse(text,lexer&#x3D;lexer)</span><br></pre></td></tr></table></figure><br>如果遗漏这一步，分析器会使用最新创建的 lexer 对象，这可能不是你希望的。<br>词法器和语法器的方法中也可以访问这些对象。在词法器中，标记的 lexer 属性指代的是当前触发规则的词法器对象：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def t_NUMBER(t):</span><br><span class="line">   r&#39;\d+&#39;</span><br><span class="line">   ...</span><br><span class="line">   print t.lexer           # Show lexer object</span><br></pre></td></tr></table></figure>
<p>在语法器中，lexer 和 parser 属性指代的是对应的词法器对象和语法器对象<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def p_expr_plus(p):</span><br><span class="line">   &#39;expr : expr PLUS expr&#39;</span><br><span class="line">   ...</span><br><span class="line">   print p.parser          # Show parser object</span><br><span class="line">   print p.lexer           # Show lexer object</span><br></pre></td></tr></table></figure><br>如果有必要，lexe r对象和 parser 对象都可以附加其他属性。例如，你想要有不同的解析器状态，可以为 parser 对象附加更多的属性，并在后面用到它们。</p>
<h2 id="使用Python的优化模式"><a href="#使用Python的优化模式" class="headerlink" title="使用Python的优化模式"></a>使用Python的优化模式</h2><p>由于 PLY 从文档字串中获取信息，语法解析和词法分析信息必须通过正常模式下的 Python 解释器得到（不带 有-O 或者 -OO 选项）。不过，如果你像这样指定 optimize 模式：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lex.lex(optimize&#x3D;1)</span><br><span class="line">yacc.yacc(optimize&#x3D;1)</span><br></pre></td></tr></table></figure>
<p>PLY 可以在下次执行，在 Python 的优化模式下执行。但你必须确保第一次执行是在 Python 的正常模式下进行，一旦词法分析表和语法分析表生成一次后，在 Python 优化模式下执行，PLY 会使用生成好的分析表而不再需要文档字串。</p>
<h2 id="高级调试"><a href="#高级调试" class="headerlink" title="高级调试"></a>高级调试</h2><p>调试一个编译器不是件容易的事情。PLY 提供了一些高级的调试能力，这是通过 Python 的l ogging 模块实现的，下面两节介绍这一主题：</p>
<h3 id="调试-lex-和-yacc-命令"><a href="#调试-lex-和-yacc-命令" class="headerlink" title="调试 lex() 和 yacc() 命令"></a>调试 lex() 和 yacc() 命令</h3><p>lex() 和 yacc() 命令都有调试模式，可以通过 debug 标识实现：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lex.lex(debug&#x3D;True)</span><br><span class="line">yacc.yacc(debug&#x3D;True)</span><br></pre></td></tr></table></figure>
<p>正常情况下，调试不仅输出标准错误，对于 yacc()，还会给出 parser.out 文件。这些输出可以通过提供 logging 对象来精细的控制。下面这个例子增加了对调试信息来源的输出：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Set up a logging object</span><br><span class="line">import logging</span><br><span class="line">logging.basicConfig(</span><br><span class="line">    level &#x3D; logging.DEBUG,</span><br><span class="line">    filename &#x3D; &quot;parselog.txt&quot;,</span><br><span class="line">    filemode &#x3D; &quot;w&quot;,</span><br><span class="line">    format &#x3D; &quot;%(filename)10s:%(lineno)4d:%(message)s&quot;</span><br><span class="line">)</span><br><span class="line">log &#x3D; logging.getLogger()</span><br><span class="line"></span><br><span class="line">lex.lex(debug&#x3D;True,debuglog&#x3D;log)</span><br><span class="line">yacc.yacc(debug&#x3D;True,debuglog&#x3D;log)</span><br></pre></td></tr></table></figure><br>如果你提供一个自定义的 logger，大量的调试信息可以通过分级来控制。典型的是将调试信息分为 DEBUG,INFO,或者 WARNING 三个级别。<br>PLY 的错误和警告信息通过日志接口提供，可以从 errorlog 参数中传入日志对象</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lex.lex(errorlog&#x3D;log)</span><br><span class="line">yacc.yacc(errorlog&#x3D;log)</span><br></pre></td></tr></table></figure>
<p>如果想完全过滤掉警告信息，你除了可以使用带级别过滤功能的日志对象，也可以使用 lex 和 yacc 模块都内建的 Nulllogger 对象。例如：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yacc.yacc(errorlog&#x3D;yacc.NullLogger())</span><br></pre></td></tr></table></figure></p>
<h3 id="运行时调试"><a href="#运行时调试" class="headerlink" title="运行时调试"></a>运行时调试</h3><p>为分析器指定 debug 选项，可以激活语法分析器的运行时调试功能。这个选项可以是整数（表示对调试功能是开还是关），也可以是 logger 对象。例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">log &#x3D; logging.getLogger()</span><br><span class="line">parser.parse(input,debug&#x3D;log)</span><br></pre></td></tr></table></figure>
<p>如果传入日志对象的话，你可以使用其级别过滤功能来控制内容的输出。INFO 级别用来产生归约信息；DEBUG 级别会显示分析栈的信息、移进的标记和其他详细信息。ERROR 级别显示分析过程中的错误相关信息。<br>对于每个复杂的问题，你应该用日志对象，以便输出重定向到文件中，进而方便在执行结束后检查。</p>
<h1 id="写一个计算器"><a href="#写一个计算器" class="headerlink" title="写一个计算器"></a>写一个计算器</h1><h3 id="Lex文件"><a href="#Lex文件" class="headerlink" title="Lex文件"></a>Lex文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from ply import lex</span><br><span class="line"></span><br><span class="line"># Define &#96;tokens&#96;, a list of token names.</span><br><span class="line">tokens &#x3D; ( &#39;PLUS&#39;, &#39;MINUS&#39;, &#39;MULT&#39;, &#39;DIV&#39;, &#39;EXPONENT&#39;, \</span><br><span class="line">        &#39;LPAREN&#39;, &#39;RPAREN&#39;, &#39;AB&#39;, &#39;NUMBER&#39;, \</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"># Define &#96;t_ignore&#96; to ignore unnecessary characters between tokens, such as whitespaces.</span><br><span class="line">t_ignore &#x3D; &quot; \t&quot;</span><br><span class="line"></span><br><span class="line"># Define functions representing regular expression rules for each token.</span><br><span class="line"># The name of functions must be like &#96;t_&lt;token_name&gt;&#96;.</span><br><span class="line"># Functions accept one argument, which is a parsed token.</span><br><span class="line">#    t.type  : name of token</span><br><span class="line">#    t.value : string of parsed token </span><br><span class="line">#    t.lineno: line number of token</span><br><span class="line">#    t.lexpos: position of token from the beginning of input string</span><br><span class="line"></span><br><span class="line">def t_PLUS(t):</span><br><span class="line">    r&#39;\+&#39; # regular expression for the token</span><br><span class="line">    return t</span><br><span class="line"></span><br><span class="line">def t_MINUS(t):</span><br><span class="line">    r&#39;\-&#39;</span><br><span class="line">    return t</span><br><span class="line"></span><br><span class="line"># The order of declaration is also the order of rules the lexer uses.</span><br><span class="line"># That is why &#96;t_EXPONENT&#96; must be before &#96;t_MULT&#96;.</span><br><span class="line">def t_EXPONENT(t):</span><br><span class="line">    r&#39;\*\*&#39;</span><br><span class="line">    return t</span><br><span class="line"></span><br><span class="line">def t_MULT(t):</span><br><span class="line">    r&#39;\*&#39;</span><br><span class="line">    return t</span><br><span class="line"></span><br><span class="line">def t_DIV(t):</span><br><span class="line">    r&#39;&#x2F;&#39;</span><br><span class="line">    return t</span><br><span class="line"></span><br><span class="line">def t_LPAREN(t):</span><br><span class="line">    r&#39;\(&#39;</span><br><span class="line">    return t</span><br><span class="line"></span><br><span class="line">def t_RPAREN(t):</span><br><span class="line">    r&#39;\)&#39;</span><br><span class="line">    return t</span><br><span class="line"></span><br><span class="line">def t_AB(t):</span><br><span class="line">    r&#39;ab&#39;</span><br><span class="line">    return t</span><br><span class="line"></span><br><span class="line">def t_NUMBER(t):</span><br><span class="line">    r&#39;[0-9]+&#39;</span><br><span class="line">    t.value &#x3D; int(t.value)</span><br><span class="line">    return t</span><br><span class="line"></span><br><span class="line"># To count correct line number</span><br><span class="line">def t_newline(t):</span><br><span class="line">    r&#39;\n+&#39;</span><br><span class="line">    t.lexer.lineno +&#x3D; t.value.count(&quot;\n&quot;)</span><br><span class="line">    # return None, so this newlines will not be in the parsed token list.</span><br><span class="line"></span><br><span class="line"># Special function for error handling</span><br><span class="line">def t_error(t):</span><br><span class="line">    print(&quot;illegal character &#39;%s&#39;&quot; % (t.value[0]))</span><br><span class="line">    t.lexer.skip(1)</span><br><span class="line"></span><br><span class="line"># Generate a lexer by &#96;lex.lex()&#96;</span><br><span class="line">lexer &#x3D; lex.lex()</span><br><span class="line"></span><br><span class="line">def test_lexer(input_string):</span><br><span class="line">    lexer.input(input_string)</span><br><span class="line">    result &#x3D; []</span><br><span class="line">    while True:</span><br><span class="line">        tok &#x3D; lexer.token()</span><br><span class="line">        if not tok:</span><br><span class="line">            break</span><br><span class="line">        result &#x3D; result + [(tok.type, tok.value)]</span><br><span class="line">    return result</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    print(test_lexer(&#39;1 + 2&#39;))</span><br><span class="line">    print(test_lexer(&#39;1 + 20 * 3 - 10 &#x2F; -2 * (1 + 3)&#39;))</span><br><span class="line">    print(test_lexer(&#39;1 ** 2&#39;))</span><br><span class="line">    print(test_lexer(&#39;ab 5 + ab -2 * ab (1 - 2)&#39;))</span><br></pre></td></tr></table></figure>
<h3 id="Yacc文件"><a href="#Yacc文件" class="headerlink" title="Yacc文件"></a>Yacc文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from ply import yacc</span><br><span class="line">from calclexer import tokens, lexer</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">Grammars:</span><br><span class="line">S -&gt; E</span><br><span class="line">E -&gt; E + E</span><br><span class="line">E -&gt; E - E</span><br><span class="line">E -&gt; E * E</span><br><span class="line">E -&gt; E &#x2F; E</span><br><span class="line">E -&gt; E ** E</span><br><span class="line">E -&gt; N</span><br><span class="line">E -&gt; +N</span><br><span class="line">E -&gt; -N</span><br><span class="line">E -&gt; ab E</span><br><span class="line">E -&gt; (E)</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"># (optional) Define precedence and associativity of operators.</span><br><span class="line"># The format is ( (&#39;left&#39; or &#39;right&#39;, &lt;token name&gt;, ...), (...) ).</span><br><span class="line"># &lt;token name&gt; is expected to be defined in the lexer definition.</span><br><span class="line"># The latter has the the higher precedence (e.g. &#39;MULT&#39; and &#39;DIV&#39; have the higher precedence than &#39;PLUS&#39; and &#39;MINUS&#39;).</span><br><span class="line"># &#39;UPLUS&#39; and &#39;UMINUS&#39; are defined as aliases to override precedence (see &#96;p_expr_um_num&#96;)</span><br><span class="line">precedence &#x3D; ( \</span><br><span class="line">        (&#39;left&#39;, &#39;PLUS&#39;, &#39;MINUS&#39;), \</span><br><span class="line">        (&#39;left&#39;, &#39;MULT&#39;, &#39;DIV&#39;), \</span><br><span class="line">        (&#39;right&#39;, &#39;EXPONENT&#39;), \</span><br><span class="line">        (&#39;right&#39;, &#39;UPLUS&#39;, &#39;UMINUS&#39;, &#39;AB&#39;), \</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"># Parsing rules</span><br><span class="line"># Functions should be start with &#96;p_&#96;.</span><br><span class="line"># The first rule will be the starting rule of parsing (?).</span><br><span class="line"></span><br><span class="line"># S -&gt; E</span><br><span class="line">def p_statement(p):</span><br><span class="line">    &#39;statement : expr&#39;</span><br><span class="line">    p[0] &#x3D; p[1]</span><br><span class="line"></span><br><span class="line"># E -&gt; E + E</span><br><span class="line">def p_expr_plus(p):</span><br><span class="line">    &#39;expr : expr PLUS expr&#39;</span><br><span class="line">    p[0] &#x3D; p[1] + p[3]</span><br><span class="line"></span><br><span class="line"># E -&gt; E - E</span><br><span class="line">def p_expr_minus(p):</span><br><span class="line">    &#39;expr : expr MINUS expr&#39;</span><br><span class="line">    p[0] &#x3D; p[1] - p[3]</span><br><span class="line"></span><br><span class="line"># E -&gt; E * E</span><br><span class="line">def p_expr_mult(p):</span><br><span class="line">    &#39;expr : expr MULT expr&#39;</span><br><span class="line">    p[0] &#x3D; p[1] * p[3]</span><br><span class="line"></span><br><span class="line"># E -&gt; E &#x2F; E</span><br><span class="line">def p_expr_div(p):</span><br><span class="line">    &#39;expr : expr DIV expr&#39;</span><br><span class="line">    p[0] &#x3D; p[1] &#x2F; p[3]</span><br><span class="line"></span><br><span class="line"># E -&gt; E ** E</span><br><span class="line">def p_expr_exponent(p):</span><br><span class="line">    &#39;expr : expr EXPONENT expr&#39;</span><br><span class="line">    p[0] &#x3D; p[1] ** p[3]</span><br><span class="line"></span><br><span class="line"># E -&gt; N</span><br><span class="line">def p_expr_num(p):</span><br><span class="line">    &#39;expr : NUMBER&#39;</span><br><span class="line">    p[0] &#x3D; p[1]</span><br><span class="line"></span><br><span class="line"># E -&gt; +N</span><br><span class="line">def p_expr_up_num(p):</span><br><span class="line">    &#39;expr : PLUS NUMBER %prec UPLUS&#39; # override precedence of PLUS by &#96;%prec UPLUS&#96;</span><br><span class="line">    p[0] &#x3D; p[2]</span><br><span class="line"></span><br><span class="line"># E -&gt; -N</span><br><span class="line">def p_expr_um_num(p):</span><br><span class="line">    &#39;expr : MINUS NUMBER %prec UMINUS&#39; # override precedence of MINUS by &#96;%prec UMINUS&#96;</span><br><span class="line">    p[0] &#x3D; -p[2]</span><br><span class="line"></span><br><span class="line"># E -&gt; ab E</span><br><span class="line">def p_expr_ab(p):</span><br><span class="line">    &#39;expr : AB expr&#39;</span><br><span class="line">    p[0] &#x3D; abs(p[2])</span><br><span class="line"></span><br><span class="line"># E -&gt; ( E )</span><br><span class="line">def p_expr_paren(p):</span><br><span class="line">    &#39;expr : LPAREN expr RPAREN&#39;</span><br><span class="line">    p[0] &#x3D; p[2]</span><br><span class="line"></span><br><span class="line"># Rule for error handling</span><br><span class="line">def p_error(t):</span><br><span class="line">    print(&quot;syntax error at &#39;%s&#39;&quot; % (t.value))</span><br><span class="line"></span><br><span class="line"># Generate a LALR parser</span><br><span class="line">parser &#x3D; yacc.yacc()</span><br><span class="line"></span><br><span class="line">def parse(input_string):</span><br><span class="line">    lexer.input(input_string)</span><br><span class="line">    parse_tree &#x3D; parser.parse(input_string, lexer&#x3D;lexer)</span><br><span class="line">    return parse_tree</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    assert parse(&#39;1 + 2 + 3&#39;) &#x3D;&#x3D; 6</span><br><span class="line">    assert parse(&#39;1 + 2 * 3 * 4&#39;) &#x3D;&#x3D; 25</span><br><span class="line">    assert parse(&#39;3 * 4 - 10 &#x2F; 2 + 5&#39;) &#x3D;&#x3D; 12</span><br><span class="line">    assert parse(&#39;-3 * (+4 - 10) &#x2F; -2 + 5&#39;) &#x3D;&#x3D; -4</span><br><span class="line">    assert parse(&#39;1 + 2 ** 3 ** 2&#39;) &#x3D;&#x3D; 513</span><br><span class="line">    assert parse(&#39;ab (1 - 2  -3)&#39;) &#x3D;&#x3D; 4</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>工具使用</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark入门</title>
    <url>/2020/02/26/Spark%E5%85%A5%E9%97%A8/</url>
    <content><![CDATA[<p>最近需要用spark比较多，重新学习一下。今天先学习一些基础。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://classroom.udacity.com/courses/ud2002" target="_blank" rel="noopener">https://classroom.udacity.com/courses/ud2002</a></p>
</blockquote>
<h1 id="Spark处理数据"><a href="#Spark处理数据" class="headerlink" title="Spark处理数据"></a>Spark处理数据</h1><h2 id="函数式编程"><a href="#函数式编程" class="headerlink" title="函数式编程"></a>函数式编程</h2><p>首先用下图来看一下，函数式编程和过程式编程的区别。<img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/maj1.png" alt="图片"></p>
<p>函数式编程非常适合分布式系统。Python并不是函数编程语言，但使用PySparkAPI 可以让你编写Spark程序，并确保你的代码使用了函数式编程。在底层，Python 代码使用 py4j 来调用 Java 虚拟机(JVM)。</p>
<p>假设有下面一段代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">log_of_songs &#x3D; [</span><br><span class="line">        &quot;Despacito&quot;,</span><br><span class="line">        &quot;Nice for what&quot;,</span><br><span class="line">        &quot;No tears left to cry&quot;,</span><br><span class="line">        &quot;Despacito&quot;,</span><br><span class="line">        &quot;Havana&quot;,</span><br><span class="line">        &quot;In my feelings&quot;,</span><br><span class="line">        &quot;Nice for what&quot;,</span><br><span class="line">        &quot;Despacito&quot;,</span><br><span class="line">        &quot;All the stars&quot;</span><br><span class="line">]</span><br><span class="line">play_count &#x3D; 0</span><br><span class="line">def count_plays(song_title):</span><br><span class="line">    global play_count</span><br><span class="line">    for song in log_of_songs:</span><br><span class="line">        if song &#x3D;&#x3D; song_title:</span><br><span class="line">            play_count &#x3D; play_count + 1</span><br><span class="line">    return play_count</span><br></pre></td></tr></table></figure>
<p>调用两次count_plays(“Despacito”)会得到不同的结果，这是因为play_count是作为全局变量，在函数内部进行了修改。解决这个问题可以采用如下代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def count_plays(song_title, play_count):</span><br><span class="line">    for song in log_of_songs:</span><br><span class="line">        if song &#x3D;&#x3D; song_title:</span><br><span class="line">            play_count &#x3D; play_count + 1</span><br><span class="line">    return play_count</span><br></pre></td></tr></table></figure>
<p>这就是Spark解决问题的方式。<br>在Spark中我们使用Pure Function（纯函数），就像面包制造厂，不同的面包机器之间是互不干扰的，且不会损坏原材料。Spark会在函数执行前，将数据复制多分，以输入到不同函数中。为了防止内存溢出，Spark会在代码中建立一个数据的有向无环图，在运行前检查是否有必要对某一分数据进行复制。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/maj2.png" alt="图片"></p>
<h2 id="运行时参数设置"><a href="#运行时参数设置" class="headerlink" title="运行时参数设置"></a>运行时参数设置</h2><blockquote>
<p>参考：<br><a href="https://tech.meituan.com/2016/04/29/spark-tuning-basic.html" target="_blank" rel="noopener">https://tech.meituan.com/2016/04/29/spark-tuning-basic.html</a><br><a href="https://spark.apache.org/docs/1.6.1/running-on-yarn.html" target="_blank" rel="noopener">https://spark.apache.org/docs/1.6.1/running-on-yarn.html</a></p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line">    --master yarn \</span><br><span class="line">    --deploy-mode cluster  \</span><br><span class="line">    --num-executors 100 \</span><br><span class="line">    --driver-memory 2g \</span><br><span class="line">    --executor-memory 14g \</span><br><span class="line">    --executor-cores 6 \</span><br><span class="line">    --conf spark.default.parallelism&#x3D;1000 \</span><br><span class="line">    --conf spark.storage.memoryFraction&#x3D;0.2 \</span><br><span class="line">    --conf spark.shuffle.memoryFraction&#x3D;0.6 \</span><br><span class="line">    --conf spark.executor.extraJavaOptions&#x3D;&#39;-Dlog4j.configuration&#x3D;log4j.properties&#39; \</span><br><span class="line">    --driver-java-options -Dlog4j.configuration&#x3D;log4j.properties \</span><br><span class="line">    python文件  \</span><br></pre></td></tr></table></figure>
<ul>
<li>spark-submit: which spark-submit 查看该命令是 spark 系统的还是 pyspark 包自带的，应该使用 spark 系统的<ul>
<li>master:</li>
<li>standaloone: spark 自带的集群资源管理器</li>
<li>yarn</li>
<li>local: 本地运行</li>
</ul>
</li>
<li>deploy-mode：<ul>
<li>client: driver 在本机上，能够直接使用本机文件系统</li>
<li>cluster: driver 指不定在哪台机器上，不能读取本机文件系统</li>
</ul>
</li>
<li>spark 运行时配置：主要的有运行内存和节点数量:<ul>
<li>num_executors</li>
<li>spark_driver_memory</li>
<li>spark_executor_memory</li>
</ul>
</li>
<li>addFiles 与 —files（将需要使用的文件分发到每台机器上）：<ul>
<li>addFiles()：能够分发到每台机器上,包括 driver 上</li>
<li>—files: 只能分发到 executor 上</li>
</ul>
</li>
<li>引用其他模块的问题：<ul>
<li>第三方库：需要将第三方库打包上传供使用</li>
<li>自己的模块：也需要打包上传,以供使用</li>
</ul>
</li>
<li>运行下面前请确认<ul>
<li>export SPARK_HOME=…../spark-1.6.2-bin-hadoop2.6</li>
<li>export PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH</li>
<li>export JAVA_HOME=…/jdk1.8.0_60</li>
<li>PYSPARK_PYTHON=./NLTK/conda-env/bin/python spark-submit —conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./NLTK/conda-env/bin/python —master yarn-cluster —archives conda-env.zip#NLTK clean_step_two.py</li>
</ul>
</li>
</ul>
<h2 id="Maps和Lambda"><a href="#Maps和Lambda" class="headerlink" title="Maps和Lambda"></a>Maps和Lambda</h2><blockquote>
<p>lambda函数起源：<br><a href="http://palmstroem.blogspot.com/2012/05/lambda-calculus-for-absolute-dummies.html" target="_blank" rel="noopener">http://palmstroem.blogspot.com/2012/05/lambda-calculus-for-absolute-dummies.html</a></p>
</blockquote>
<p>Maps会复制原始数据，并把副本数据按照Maps中的函数进行转换。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from pyspark import SparkContext</span><br><span class="line">log_of_songs &#x3D; [</span><br><span class="line">    &quot;Despacito&quot;,</span><br><span class="line">    &quot;Nice for what&quot;,</span><br><span class="line">    &quot;No tears left to cry&quot;,</span><br><span class="line">    &quot;Despacito&quot;,</span><br><span class="line">    &quot;Havana&quot;,</span><br><span class="line">    &quot;In my feelings&quot;,</span><br><span class="line">    &quot;Nice for what&quot;,</span><br><span class="line">    &quot;Despacito&quot;,</span><br><span class="line">    &quot;All the stars&quot;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">def convert_song_to_lowercase(song):</span><br><span class="line">    return song.lower()</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">    conf &#x3D; SparkConf()</span><br><span class="line">    conf.setAppName(&quot;Testing&quot;).setMaster(&quot;local&quot;)</span><br><span class="line">    sc &#x3D; SparkContext(conf&#x3D;conf)</span><br><span class="line">    sc.setLogLevel(&quot;WARN&quot;)</span><br><span class="line"></span><br><span class="line">    # parallelize将对象分配到不同节点上</span><br><span class="line">    distributed_song_log &#x3D; sc.parallelize(log_of_songs)</span><br><span class="line">    # 定义不同节点的所有数据执行convert_song_to_lowercase的操作</span><br><span class="line">    # 但此时spark还未执行，它在等待所有定义结束后，看是否可以优化某些操作</span><br><span class="line">    distributed_song_log.map(convert_song_to_lowercase)</span><br><span class="line">    # 如果想强制spark执行，则可以使用collect，则会将所有数据汇总</span><br><span class="line">    # 注意此时spark并没有改变原始数据的大小写，它将原始数据进行了拷贝，再做的处理</span><br><span class="line">    distributed_song_log.collect()</span><br><span class="line">    # 也可以使用python的匿名函数进行map</span><br><span class="line">    distributed_song_log.map(lambda song: song.lower()).collect()</span><br></pre></td></tr></table></figure>
<h2 id="Data-Frame"><a href="#Data-Frame" class="headerlink" title="Data Frame"></a>Data Frame</h2><p>数据处理有两种方式，一种使用Data Frame和Python进行命令式编程，另一种使用SQL进行声明式编程。命令式编程关注的是”How”，声明式编程关注的是”What”。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/maj3.png" alt="图片"></p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/maj4.png" alt="图片"></p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/maj5.png" alt="图片"></p>
<h3 id="Data-Frame的读取和写入"><a href="#Data-Frame的读取和写入" class="headerlink" title="Data Frame的读取和写入"></a>Data Frame的读取和写入</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import pyspark</span><br><span class="line">from pyspark import SparkConf</span><br><span class="line">from pyspark.sql import SparkSession</span><br><span class="line"></span><br><span class="line">spark &#x3D; SparkSession \</span><br><span class="line">    .builder \</span><br><span class="line">    .appName(&quot;Our first Python Spark SQL example&quot;) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"># 检查一下是否生效了。</span><br><span class="line">spark.sparkContext.getConf().getAll()</span><br><span class="line">path &#x3D; &quot;data&#x2F;sparkify_log_small.json&quot;</span><br><span class="line">user_log &#x3D; spark.read.json(path)</span><br><span class="line">user_log.printSchema()</span><br><span class="line">user_log.describe()</span><br><span class="line">user_log.show(n&#x3D;1)</span><br><span class="line"># 取数据的前5条</span><br><span class="line">user_log.take(5)</span><br><span class="line">out_path &#x3D; &quot;data&#x2F;sparkify_log_small.csv&quot;</span><br><span class="line">user_log.write.save(out_path, format&#x3D;&quot;csv&quot;, header&#x3D;True)</span><br><span class="line"># 读取另一个daraframe</span><br><span class="line">user_log_2 &#x3D; spark.read.csv(out_path, header&#x3D;True)</span><br><span class="line">user_log_2.printSchema()</span><br><span class="line">user_log_2.take(2)</span><br><span class="line">user_log_2.select(&quot;userID&quot;).show()</span><br></pre></td></tr></table></figure>
<h3 id="Data-Frame数据处理"><a href="#Data-Frame数据处理" class="headerlink" title="Data Frame数据处理"></a>Data Frame数据处理</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from pyspark.sql import SparkSession</span><br><span class="line">from pyspark.sql.functions import udf</span><br><span class="line">from pyspark.sql.types import StringType</span><br><span class="line">from pyspark.sql.types import IntegerType</span><br><span class="line">from pyspark.sql.functions import desc</span><br><span class="line">from pyspark.sql.functions import asc</span><br><span class="line">from pyspark.sql.functions import sum as Fsum</span><br><span class="line"></span><br><span class="line">import datetime</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">%matplotlib inline</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">spark &#x3D; SparkSession \</span><br><span class="line">    .builder \</span><br><span class="line">    .appName(&quot;Wrangling Data&quot;) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line">path &#x3D; &quot;data&#x2F;sparkify_log_small.json&quot;</span><br><span class="line">user_log &#x3D; spark.read.json(path)</span><br><span class="line"># 数据搜索</span><br><span class="line">user_log.take(5)</span><br><span class="line">user_log.printSchema()</span><br><span class="line">user_log.describe().show()</span><br><span class="line">user_log.describe(&quot;artist&quot;).show()</span><br><span class="line">user_log.describe(&quot;sessionId&quot;).show()</span><br><span class="line">user_log.count()</span><br><span class="line">user_log.select(&quot;page&quot;).dropDuplicates().sort(&quot;page&quot;).show()</span><br><span class="line">user_log.select([&quot;userId&quot;, &quot;firstname&quot;, &quot;page&quot;, &quot;song&quot;]).where(user_log.userId &#x3D;&#x3D; &quot;1046&quot;).collect()</span><br><span class="line"># 按小时统计数据</span><br><span class="line">get_hour &#x3D; udf(lambda x: datetime.datetime.fromtimestamp(x &#x2F; 1000.0). hour)</span><br><span class="line">user_log &#x3D; user_log.withColumn(&quot;hour&quot;, get_hour(user_log.ts))</span><br><span class="line">user_log.head()</span><br><span class="line">songs_in_hour &#x3D; user_log.filter(user_log.page &#x3D;&#x3D; &quot;NextSong&quot;).groupby(user_log.hour).count().orderBy(user_log.hour.cast(&quot;float&quot;))</span><br><span class="line">songs_in_hour.show()</span><br><span class="line">songs_in_hour_pd &#x3D; songs_in_hour.toPandas()</span><br><span class="line">songs_in_hour_pd.hour &#x3D; pd.to_numeric(songs_in_hour_pd.hour)</span><br><span class="line">plt.scatter(songs_in_hour_pd[&quot;hour&quot;], songs_in_hour_pd[&quot;count&quot;])</span><br><span class="line">plt.xlim(-1, 24);</span><br><span class="line">plt.ylim(0, 1.2 * max(songs_in_hour_pd[&quot;count&quot;]))</span><br><span class="line">plt.xlabel(&quot;Hour&quot;)</span><br><span class="line">plt.ylabel(&quot;Songs played&quot;);</span><br><span class="line"></span><br><span class="line"># 删除空值的行</span><br><span class="line">user_log_valid &#x3D; user_log.dropna(how &#x3D; &quot;any&quot;, subset &#x3D; [&quot;userId&quot;, &quot;sessionId&quot;])</span><br><span class="line">user_log_valid.count()</span><br><span class="line">user_log.select(&quot;userId&quot;).dropDuplicates().sort(&quot;userId&quot;).show()</span><br><span class="line">user_log_valid &#x3D; user_log_valid.filter(user_log_valid[&quot;userId&quot;] !&#x3D; &quot;&quot;)</span><br><span class="line">user_log_valid.count()</span><br><span class="line"># 降级服务的用户</span><br><span class="line">user_log_valid.filter(&quot;page &#x3D; &#39;Submit Downgrade&#39;&quot;).show()</span><br><span class="line">user_log.select([&quot;userId&quot;, &quot;firstname&quot;, &quot;page&quot;, &quot;level&quot;, &quot;song&quot;]).where(user_log.userId &#x3D;&#x3D; &quot;1138&quot;).collect()</span><br><span class="line">flag_downgrade_event &#x3D; udf(lambda x: 1 if x &#x3D;&#x3D; &quot;Submit Downgrade&quot; else 0, IntegerType())</span><br><span class="line">user_log_valid &#x3D; user_log_valid.withColumn(&quot;downgraded&quot;, flag_downgrade_event(&quot;page&quot;))</span><br><span class="line">user_log_valid.head()</span><br><span class="line">from pyspark.sql import Window</span><br><span class="line">windowval &#x3D; Window.partitionBy(&quot;userId&quot;).orderBy(desc(&quot;ts&quot;)).rangeBetween(Window.unboundedPreceding, 0)</span><br><span class="line">user_log_valid &#x3D; user_log_valid.withColumn(&quot;phase&quot;, Fsum(&quot;downgraded&quot;).over(windowval))</span><br><span class="line">user_log_valid.select([&quot;userId&quot;, &quot;firstname&quot;, &quot;ts&quot;, &quot;page&quot;, &quot;level&quot;, &quot;phase&quot;]).where(user_log.userId &#x3D;&#x3D; &quot;1138&quot;).sort(&quot;ts&quot;).collect()</span><br></pre></td></tr></table></figure>
<h2 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from pyspark.sql import SparkSession</span><br><span class="line">from pyspark.sql.functions import udf</span><br><span class="line">from pyspark.sql.types import StringType</span><br><span class="line">from pyspark.sql.types import IntegerType</span><br><span class="line">from pyspark.sql.functions import desc</span><br><span class="line">from pyspark.sql.functions import asc</span><br><span class="line">from pyspark.sql.functions import sum as Fsum</span><br><span class="line"></span><br><span class="line">import datetime</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">%matplotlib inline</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">spark &#x3D; SparkSession \</span><br><span class="line">    .builder \</span><br><span class="line">    .appName(&quot;Data wrangling with Spark SQL&quot;) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line">path &#x3D; &quot;data&#x2F;sparkify_log_small.json&quot;</span><br><span class="line">user_log &#x3D; spark.read.json(path)</span><br><span class="line"></span><br><span class="line">user_log.take(1)</span><br><span class="line"># 下面的代码创建了一个临时视图，你可以使用该视图运行 SQL 查询</span><br><span class="line">user_log.createOrReplaceTempView(&quot;user_log_table&quot;)</span><br><span class="line">spark.sql(&quot;SELECT * FROM user_log_table LIMIT 2&quot;).show()</span><br><span class="line">spark.sql(&#39;&#39;&#39;</span><br><span class="line">          SELECT * </span><br><span class="line">          FROM user_log_table </span><br><span class="line">          LIMIT 2</span><br><span class="line">          &#39;&#39;&#39;</span><br><span class="line">          ).show()</span><br><span class="line">spark.sql(&#39;&#39;&#39;</span><br><span class="line">          SELECT COUNT(*) </span><br><span class="line">          FROM user_log_table </span><br><span class="line">          &#39;&#39;&#39;</span><br><span class="line">          ).show()</span><br><span class="line">spark.sql(&#39;&#39;&#39;</span><br><span class="line">          SELECT userID, firstname, page, song</span><br><span class="line">          FROM user_log_table </span><br><span class="line">          WHERE userID &#x3D;&#x3D; &#39;1046&#39;</span><br><span class="line">          &#39;&#39;&#39;</span><br><span class="line">          ).collect()</span><br><span class="line">spark.sql(&#39;&#39;&#39;</span><br><span class="line">          SELECT DISTINCT page</span><br><span class="line">          FROM user_log_table </span><br><span class="line">          ORDER BY page ASC</span><br><span class="line">          &#39;&#39;&#39;</span><br><span class="line">          ).show()</span><br><span class="line"></span><br><span class="line"># 自定义函数</span><br><span class="line">spark.udf.register(&quot;get_hour&quot;, lambda x: int(datetime.datetime.fromtimestamp(x &#x2F; 1000.0).hour))</span><br><span class="line">spark.sql(&#39;&#39;&#39;</span><br><span class="line">          SELECT *, get_hour(ts) AS hour</span><br><span class="line">          FROM user_log_table </span><br><span class="line">          LIMIT 1</span><br><span class="line">          &#39;&#39;&#39;</span><br><span class="line">          ).collect()</span><br><span class="line">songs_in_hour &#x3D; spark.sql(&#39;&#39;&#39;</span><br><span class="line">          SELECT get_hour(ts) AS hour, COUNT(*) as plays_per_hour</span><br><span class="line">          FROM user_log_table</span><br><span class="line">          WHERE page &#x3D; &quot;NextSong&quot;</span><br><span class="line">          GROUP BY hour</span><br><span class="line">          ORDER BY cast(hour as int) ASC</span><br><span class="line">          &#39;&#39;&#39;</span><br><span class="line">          )</span><br><span class="line">songs_in_hour.show()</span><br><span class="line"># 用 Pandas 转换数据</span><br><span class="line">songs_in_hour_pd &#x3D; songs_in_hour.toPandas()</span><br><span class="line">print(songs_in_hour_pd)</span><br></pre></td></tr></table></figure>
<h2 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h2><blockquote>
<p>参考：<br><a href="https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html" target="_blank" rel="noopener">https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html</a><br><a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/rdd-programming-guide.html</a></p>
</blockquote>
<p>不管使用Pyspark还是其他语言，Spark的底层都会通过Catalyst转成执行DAG序列：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/spark1.png" alt="图片"></p>
<p>DAG在底层使用RDD对象进行操作。</p>
<h1 id="Spark中的机器学习"><a href="#Spark中的机器学习" class="headerlink" title="Spark中的机器学习"></a>Spark中的机器学习</h1><h2 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from pyspark.sql import SparkSession</span><br><span class="line">from pyspark.ml.feature import RegexTokenizer, VectorAssembler, Normalizer, StandardScaler</span><br><span class="line">from pyspark.sql.functions import udf</span><br><span class="line">from pyspark.sql.types import IntegerType</span><br><span class="line"></span><br><span class="line">import re</span><br><span class="line"># create a SparkSession: note this step was left out of the screencast</span><br><span class="line">spark &#x3D; SparkSession.builder \</span><br><span class="line">    .master(&quot;local&quot;) \</span><br><span class="line">    .appName(&quot;Word Count&quot;) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line"># 如何读取数据集</span><br><span class="line">stack_overflow_data &#x3D; &#39;Train_onetag_small.json&#39;</span><br><span class="line">df &#x3D; spark.read.json(stack_overflow_data)</span><br><span class="line">df.head()</span><br><span class="line"># 把字符串分为单独的单词。Spark有一个[Tokenizer]（https:&#x2F;&#x2F;spark.apache.org&#x2F;docs&#x2F;latest&#x2F;ml-features.html#tokenizer） 类以及RegexTokenizer。 后者在分词时有更大的自由度。</span><br><span class="line"># split the body text into separate words</span><br><span class="line">regexTokenizer &#x3D; RegexTokenizer(inputCol&#x3D;&quot;Body&quot;, outputCol&#x3D;&quot;words&quot;, pattern&#x3D;&quot;\\W&quot;)</span><br><span class="line">df &#x3D; regexTokenizer.transform(df)</span><br><span class="line">df.head()</span><br><span class="line"># count the number of words in each body tag</span><br><span class="line">body_length &#x3D; udf(lambda x: len(x), IntegerType())</span><br><span class="line">df &#x3D; df.withColumn(&quot;BodyLength&quot;, body_length(df.words))</span><br><span class="line"># count the number of paragraphs and links in each body tag</span><br><span class="line">number_of_paragraphs &#x3D; udf(lambda x: len(re.findall(&quot;&lt;&#x2F;p&gt;&quot;, x)), IntegerType())</span><br><span class="line">number_of_links &#x3D; udf(lambda x: len(re.findall(&quot;&lt;&#x2F;a&gt;&quot;, x)), IntegerType())</span><br><span class="line">df &#x3D; df.withColumn(&quot;NumParagraphs&quot;, number_of_paragraphs(df.Body))</span><br><span class="line">df &#x3D; df.withColumn(&quot;NumLinks&quot;, number_of_links(df.Body))</span><br><span class="line">df.head(2)</span><br><span class="line"># 将内容长度，段落数和内容中的链接数合并为一个向量</span><br><span class="line">assembler &#x3D; VectorAssembler(inputCols&#x3D;[&quot;BodyLength&quot;, &quot;NumParagraphs&quot;, &quot;NumLinks&quot;], outputCol&#x3D;&quot;NumFeatures&quot;)</span><br><span class="line">df &#x3D; assembler.transform(df)</span><br><span class="line">df.head()</span><br><span class="line"># 归一化向量</span><br><span class="line">scaler &#x3D; Normalizer(inputCol&#x3D;&quot;NumFeatures&quot;, outputCol&#x3D;&quot;ScaledNumFeatures&quot;)</span><br><span class="line">df &#x3D; scaler.transform(df)</span><br><span class="line">df.head(2)</span><br><span class="line"># 缩放向量</span><br><span class="line">scaler2 &#x3D; StandardScaler(inputCol&#x3D;&quot;NumFeatures&quot;, outputCol&#x3D;&quot;ScaledNumFeatures2&quot;, withStd&#x3D;True)</span><br><span class="line">scalerModel &#x3D; scaler2.fit(df)</span><br><span class="line">df &#x3D; scalerModel.transform(df)</span><br><span class="line">df.head(2)</span><br></pre></td></tr></table></figure>
<h2 id="文本特征"><a href="#文本特征" class="headerlink" title="文本特征"></a>文本特征</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from pyspark.sql import SparkSession</span><br><span class="line">from pyspark.ml.feature import RegexTokenizer, CountVectorizer, \</span><br><span class="line">    IDF, StringIndexer</span><br><span class="line">from pyspark.sql.functions import udf</span><br><span class="line">from pyspark.sql.types import IntegerType</span><br><span class="line"></span><br><span class="line">import re</span><br><span class="line"># create a SparkSession: note this step was left out of the screencast</span><br><span class="line">spark &#x3D; SparkSession.builder \</span><br><span class="line">    .master(&quot;local&quot;) \</span><br><span class="line">    .appName(&quot;Word Count&quot;) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line"># 如何读取数据集</span><br><span class="line">stack_overflow_data &#x3D; &#39;Train_onetag_small.json&#39;</span><br><span class="line">df &#x3D; spark.read.json(stack_overflow_data)</span><br><span class="line">df.head()</span><br><span class="line"></span><br><span class="line"># 分词将字符串拆分为单独的单词。Spark 有一个[Tokenizer] （https:&#x2F;&#x2F;spark.apache.org&#x2F;docs&#x2F;latest&#x2F;ml-features.html#tokenizer） 类和RegexTokenizer。后者在分词时有更大的自由度 。</span><br><span class="line"># split the body text into separate words</span><br><span class="line">regexTokenizer &#x3D; RegexTokenizer(inputCol&#x3D;&quot;Body&quot;, outputCol&#x3D;&quot;words&quot;, pattern&#x3D;&quot;\\W&quot;)</span><br><span class="line">df &#x3D; regexTokenizer.transform(df)</span><br><span class="line">df.head()</span><br><span class="line"></span><br><span class="line"># CountVectorizer</span><br><span class="line"># find the term frequencies of the words</span><br><span class="line">cv &#x3D; CountVectorizer(inputCol&#x3D;&quot;words&quot;, outputCol&#x3D;&quot;TF&quot;, vocabSize&#x3D;1000)</span><br><span class="line">cvmodel &#x3D; cv.fit(df)</span><br><span class="line">df &#x3D; cvmodel.transform(df)</span><br><span class="line">df.take(1)</span><br><span class="line"># show the vocabulary in order of </span><br><span class="line">cvmodel.vocabulary</span><br><span class="line"># show the last 10 terms in the vocabulary</span><br><span class="line">cvmodel.vocabulary[-10:]</span><br><span class="line"></span><br><span class="line"># 逆文本频率指数（Inter-document Frequency ）</span><br><span class="line">idf &#x3D; IDF(inputCol&#x3D;&quot;TF&quot;, outputCol&#x3D;&quot;TFIDF&quot;)</span><br><span class="line">idfModel &#x3D; idf.fit(df)</span><br><span class="line">df &#x3D; idfModel.transform(df)</span><br><span class="line">df.head()</span><br><span class="line"></span><br><span class="line"># StringIndexer</span><br><span class="line">indexer &#x3D; StringIndexer(inputCol&#x3D;&quot;oneTag&quot;, outputCol&#x3D;&quot;label&quot;)</span><br><span class="line">df &#x3D; indexer.fit(df).transform(df)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>一首小诗：做最好的自己</title>
    <url>/2020/02/16/%E9%9A%8F%E6%84%9F%E4%B8%80%E7%AF%87/</url>
    <content><![CDATA[<p>今天看一个纪录片《人生第一次》时听到的小诗，来自美国诗人、短片小说作家——道格拉斯·马拉赫。</p>
<a id="more"></a>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=475218187&auto=1&height=66"></iframe>

<h4 id="如果你不能成为山顶上的高松，"><a href="#如果你不能成为山顶上的高松，" class="headerlink" title="如果你不能成为山顶上的高松，"></a>如果你不能成为山顶上的高松，</h4><h4 id="那就当棵山谷里的小树吧，"><a href="#那就当棵山谷里的小树吧，" class="headerlink" title="那就当棵山谷里的小树吧，"></a>那就当棵山谷里的小树吧，</h4><h4 id="但要当棵溪边最好的小树。"><a href="#但要当棵溪边最好的小树。" class="headerlink" title="但要当棵溪边最好的小树。"></a>但要当棵溪边最好的小树。</h4><p><br/></p>
<h4 id="如果你不能成为一棵大树，"><a href="#如果你不能成为一棵大树，" class="headerlink" title="如果你不能成为一棵大树，"></a>如果你不能成为一棵大树，</h4><h4 id="那就当丛小灌木；"><a href="#那就当丛小灌木；" class="headerlink" title="那就当丛小灌木；"></a>那就当丛小灌木；</h4><h4 id="如果你不能成为一丛小灌木，"><a href="#如果你不能成为一丛小灌木，" class="headerlink" title="如果你不能成为一丛小灌木，"></a>如果你不能成为一丛小灌木，</h4><h4 id="那就当一片小草地。"><a href="#那就当一片小草地。" class="headerlink" title="那就当一片小草地。"></a>那就当一片小草地。</h4><p><br/></p>
<h4 id="如果你不能是一只香獐，"><a href="#如果你不能是一只香獐，" class="headerlink" title="如果你不能是一只香獐，"></a>如果你不能是一只香獐，</h4><h4 id="那就当尾小鲈鱼，"><a href="#那就当尾小鲈鱼，" class="headerlink" title="那就当尾小鲈鱼，"></a>那就当尾小鲈鱼，</h4><h4 id="但要当湖里最活泼的小鲈鱼。"><a href="#但要当湖里最活泼的小鲈鱼。" class="headerlink" title="但要当湖里最活泼的小鲈鱼。"></a>但要当湖里最活泼的小鲈鱼。</h4><p><br/></p>
<h4 id="我们不能全是船长，"><a href="#我们不能全是船长，" class="headerlink" title="我们不能全是船长，"></a>我们不能全是船长，</h4><h4 id="必须有人也是水手。"><a href="#必须有人也是水手。" class="headerlink" title="必须有人也是水手。"></a>必须有人也是水手。</h4><p><br/></p>
<h4 id="这里有许多事让我们去做，"><a href="#这里有许多事让我们去做，" class="headerlink" title="这里有许多事让我们去做，"></a>这里有许多事让我们去做，</h4><h4 id="有大事，有小事，"><a href="#有大事，有小事，" class="headerlink" title="有大事，有小事，"></a>有大事，有小事，</h4><h4 id="但最重要的是我们身旁的事。"><a href="#但最重要的是我们身旁的事。" class="headerlink" title="但最重要的是我们身旁的事。"></a>但最重要的是我们身旁的事。</h4><p><br/></p>
<h4 id="如果你不能成为大道，"><a href="#如果你不能成为大道，" class="headerlink" title="如果你不能成为大道，"></a>如果你不能成为大道，</h4><h4 id="那就当一条小路，"><a href="#那就当一条小路，" class="headerlink" title="那就当一条小路，"></a>那就当一条小路，</h4><h4 id="如果你不能成为太阳，"><a href="#如果你不能成为太阳，" class="headerlink" title="如果你不能成为太阳，"></a>如果你不能成为太阳，</h4><h4 id="那就当一颗星星。"><a href="#那就当一颗星星。" class="headerlink" title="那就当一颗星星。"></a>那就当一颗星星。</h4><p><br/></p>
<h4 id="决定成败的不是你的身材"><a href="#决定成败的不是你的身材" class="headerlink" title="决定成败的不是你的身材"></a>决定成败的不是你的身材</h4><h4 id="而是做一个最好的你。"><a href="#而是做一个最好的你。" class="headerlink" title="而是做一个最好的你。"></a>而是做一个最好的你。</h4><p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/suigan1.png" alt="图片"></p>
]]></content>
      <tags>
        <tag>小诗</tag>
      </tags>
  </entry>
  <entry>
    <title>机器翻译检测</title>
    <url>/2020/02/13/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A3%80%E6%B5%8B/</url>
    <content><![CDATA[<p>因为本周要做一个机器翻译检测的任务，因此搜到了几篇论文，看一下大概有哪些思路。论文基本上只简单扫了一眼，简单介绍一下其中的3篇。</p>
<a id="more"></a>
<blockquote>
<p>参考：</p>
<p>Machine Translation Detection from Monolingual Web-Text</p>
<p>Automatic Detection of Machine Translated Text and Translation Quality</p>
<p>Detecting Machine-Translated Paragraphs by Matching Similar Words</p>
<p>Automatic Detection of Translated Text and its Impact on Machine Translation</p>
<p>BLEU: a Method for Automatic Evaluation of Machine Translation</p>
<p>Building a Web-based parallel corpus and filtering out machinetranslated text</p>
<p>Machine Translation Detection from MonolingualWeb-Text</p>
<p>Machine Translationness: a Concept for Machine Translation Evaluation and Detection</p>
<p>MT Detection in Web-Scraped Parallel Corpora</p>
<p>On the Features of Translationese</p>
<p>Translationese and Its Dialects</p>
</blockquote>
<h2 id="Machine-Translation-Detection-from-Monolingual-Web-Text"><a href="#Machine-Translation-Detection-from-Monolingual-Web-Text" class="headerlink" title="Machine Translation Detection from Monolingual Web-Text"></a>Machine Translation Detection from Monolingual Web-Text</h2><p>首先强调的是，这篇论文检测的是SMT机器翻译。看到论文摘要时我想到，针对不同的机器翻译模型，检测的机制也是不一样的，要有这点意识。这篇论文关注到的是SMT系统中“phrase salad”现象，并使用单语语料就可以达到95.8%的准确率。</p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul>
<li>SMT翻译中的‘phrase salad’现象，指的是翻译结果的每个短语单独拿出来是对的，但组合到一起是错的</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mtcheck1.png" alt="图片"></p>
<ul>
<li>比如上面这个例子，not only后面应该有but also，但这个短语在SMT翻译系统里只有一半被翻译了</li>
<li>使用了一个分类器对句子是否是‘phrase salad’进行检测，使用到的特征包括两个，一个是语言模型，另外是一些人们常用但对SMT来说难以生成的短语</li>
</ul>
<h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><p>基于SMT翻译的特点，在特征选择时主要考虑3点：句子流畅度、语法正确度、短语完整度。从人工翻译提取到的特征表达了它和人工产生句子的相似性，从机器翻译中提取到的特征表达了它和机器翻译句子的相似性。</p>
<h3 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h3><ul>
<li>Fluency Feature<ul>
<li>使用两个语言模型，f(w,H)和f(w,MT)，前者表示人工翻译的语言模型，后者是机器翻译的语言模型</li>
</ul>
</li>
<li>Grammaticality Feature<ul>
<li>使用POS语言模型，f(pos,H)和f(pos, MT)，前者表示人工翻译的POS序列的语言模型，后者是机器翻译的</li>
<li>对提取出的function word的POS语言模型，f(fw, H)和f(fw, MT)<ul>
<li>function word: <ul>
<li>Prepositions: of, at, in, without, between</li>
<li>Pro<a href="https://www.baidu.com/s?wd=no&amp;tn=SE_PcZhidaonwhc_ngpagmjz&amp;rsv_dl=gh_pc_zhidao" target="_blank" rel="noopener">no</a>uns: he, they, anybody, it, one</li>
<li>Determiners: the, a, that, my, more, much, either,neither</li>
<li>Conjunctions: and, that, when, while, although, or</li>
<li>Auxiliary verbs: be (is, am, are), have, got, do</li>
<li>Particles: <a href="https://www.baidu.com/s?wd=no&amp;tn=SE_PcZhidaonwhc_ngpagmjz&amp;rsv_dl=gh_pc_zhidao" target="_blank" rel="noopener">no</a>, <a href="https://www.baidu.com/s?wd=no&amp;tn=SE_PcZhidaonwhc_ngpagmjz&amp;rsv_dl=gh_pc_zhidao" target="_blank" rel="noopener">no</a>t, nor, as</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Gappy-Phrase Feature<ul>
<li>中间有间隔的短语：如not only * but also</li>
<li>使用character级别的LM衡量</li>
<li>Sequential Pattern Mining<ul>
<li>使用sequential pattern挖掘的方法找到所有Gappy-Phrase</li>
</ul>
</li>
<li>使用的信息增益进行的短语选择，但是没看懂是如何计算特征的</li>
</ul>
</li>
<li>最后使用SVM进行分类</li>
<li>其他可考虑的feature：<ul>
<li>Translationese and its dialects论文</li>
<li>On the features of translationese论文（比较学术）</li>
<li>average token length</li>
<li>type-token ratio</li>
</ul>
</li>
</ul>
<h2 id="Automatic-Detection-of-Machine-Translated-Text-and-Translation-Quality"><a href="#Automatic-Detection-of-Machine-Translated-Text-and-Translation-Quality" class="headerlink" title="Automatic Detection of Machine Translated Text and Translation Quality"></a>Automatic Detection of Machine Translated Text and Translation Quality</h2><p>这篇论文也是使用的单语语料训练的分类器，用来进行机器翻译的检测和翻译质量的评估。这篇文章的重点强调的是，通过分类器检测的方法，只能对质量特别差的翻译系统起作用，如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mtcheck2.png" alt="图片"></p>
<p>可以看出来，翻译系统越好，检测的准确率就越低。</p>
<h3 id="特征选择-1"><a href="#特征选择-1" class="headerlink" title="特征选择"></a>特征选择</h3><ul>
<li>本文使用的特征都是二元特征，即是否存在POS ngram和467个function word。</li>
<li>从4个方面构建特征：word、lemma、pos、mixed</li>
</ul>
<h2 id="Detecting-Machine-Translated-Paragraphs-by-Matching-Similar-Words"><a href="#Detecting-Machine-Translated-Paragraphs-by-Matching-Similar-Words" class="headerlink" title="Detecting Machine-Translated Paragraphs by Matching Similar Words"></a>Detecting Machine-Translated Paragraphs by Matching Similar Words</h2><p>这篇文章主要是检查段落级别的机器翻译，方法是通过计算word的match情况，和段落的coherence来检测机器翻译，整体流程如下：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mtcheck3.png" alt="图片"></p>
<h3 id="计算相似词"><a href="#计算相似词" class="headerlink" title="计算相似词"></a>计算相似词</h3><p>先把段落内所有词打上POS标签，依次计算和其他词的相似度（如果POS相同则保留）。能够看出人工翻译的整体相似度比较低，机器翻译的相似度高一些，如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mtcheck4.png" alt="图片"></p>
<h3 id="计算Coherence"><a href="#计算Coherence" class="headerlink" title="计算Coherence"></a>计算Coherence</h3><p>基于POS对统计根据的均值和方差。如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mtcheck5.png" alt="图片"></p>
<h3 id="进行分类"><a href="#进行分类" class="headerlink" title="进行分类"></a>进行分类</h3><p>使用SVM分类。</p>
]]></content>
      <tags>
        <tag>机器翻译</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：《Solving and Generating Chinese Character Riddles》</title>
    <url>/2020/02/11/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E3%80%8ASolving-and-Generating-Chinese-Character-Riddles%E3%80%8B/</url>
    <content><![CDATA[<p>之前想给语音助手增加一个猜字谜的功能，这两天不忙就读了一下这篇机器解谜语的论文。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://www.aclweb.org/anthology/D16-1081.pdf" target="_blank" rel="noopener">https://www.aclweb.org/anthology/D16-1081.pdf</a></p>
</blockquote>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>解谜语类似于下面的过程：</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/solgene1.png" alt="图片"></p>
<ul>
<li>解谜的pipeline<ul>
<li>解题过程<ul>
<li>学习谜语中的短语和部首的对齐关系</li>
<li>学习谜语和rule的关系</li>
<li>使用上面两个关系，用算法得到候选答案</li>
<li>使用Ranking-SVM来对candidate进行排序</li>
</ul>
</li>
<li>生成谜语过程<ul>
<li>使用模版方法</li>
<li>使用替代的方法</li>
<li>使用Ranking-SVM来对candidate进行排序</li>
</ul>
</li>
<li>整体过程如下图：</li>
</ul>
</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/solgene2.png" alt="图片"></p>
<h2 id="Phrase-Radical-Alignments-and-Rules"><a href="#Phrase-Radical-Alignments-and-Rules" class="headerlink" title="Phrase-Radical Alignments and Rules"></a>Phrase-Radical Alignments and Rules</h2><h3 id="Alignment"><a href="#Alignment" class="headerlink" title="Alignment"></a>Alignment</h3><ul>
<li>希望将“千里”和“马”进行对齐</li>
<li>方法一<ul>
<li>将谜语分词$\left(w_{1}, w_{2}, \ldots, w_{n}\right)$</li>
<li>将答案分成不同部首$\left(r_{1}, r_{2}, \ldots, r_{m}\right)$</li>
<li>统计对齐$\left(\left[w_{i}, w_{j}\right], r_{k}\right)(i, j \in[1, n], k \in[1, m])$</li>
</ul>
</li>
<li>方法二<ul>
<li>谜语中两个连续字符$\left(w_{1}, w_{2}\right)$，如果w1是w2的部首，且w2的其余部分r出现在答案q中，则$\left(\left(w_{1}, w_{2}\right), r\right)$是一个对齐</li>
</ul>
</li>
<li>统计所有的对齐，并过滤掉出现频次小于3的</li>
<li>特别常见的对齐如下图：</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/solgne3.png" alt="图片"></p>
<h3 id="Rules"><a href="#Rules" class="headerlink" title="Rules"></a>Rules</h3><ul>
<li>总结了6类规则</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/solgne4.png" alt="图片"></p>
<ul>
<li>对于$\left(\left[w_{1}, w_{n}\right], r\right)$，如果r是wi的部首，则$\left(w_{1}, \dots, w_{i-1},(.), w_{i+1}, \dots, w_{n}\right)$就是一个潜在的规则，我们从数据中最终总结193条规则，归纳为上面6类</li>
<li>1000个汉字有至少1个alignment，27个汉字有至少100个alignment</li>
</ul>
<h2 id="Riddle-Solving-and-Generation"><a href="#Riddle-Solving-and-Generation" class="headerlink" title="Riddle Solving and Generation"></a>Riddle Solving and Generation</h2><h3 id="Solving-Chinese-Character-Riddles"><a href="#Solving-Chinese-Character-Riddles" class="headerlink" title="Solving Chinese Character Riddles"></a>Solving Chinese Character Riddles</h3><ul>
<li>解谜算法的伪代码如下</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/solgne5.png" alt="图片"></p>
<ul>
<li>以“上岗必戴安全帽”为例，“上岗”通过规则“上(up) (.)”和山对齐，“必” 和 “戴”跟自己对齐，“安全帽”因为analogical shape和“宀”对齐，最终得到结果“密”</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/solgene6.png" alt="图片"></p>
<ul>
<li>对答案进行排序，排序时使用的特征如下：</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/solgene7.png" alt="图片"></p>
<h3 id="Generating-Chinese-Character-Riddles"><a href="#Generating-Chinese-Character-Riddles" class="headerlink" title="Generating Chinese Character Riddles"></a>Generating Chinese Character Riddles</h3><ul>
<li>基于模板的方法</li>
<li>基于替换的方法</li>
<li>对候选的description进行排序，排序使用的特征如下：</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/solgene8.png" alt="图片"></p>
<h3 id="Ranking-Model"><a href="#Ranking-Model" class="headerlink" title="Ranking Model"></a>Ranking Model</h3><ul>
<li>score的计算：$\text { Score }(c)=\sum_{i=1}^{m} \lambda_{i} * g_{i}(c)$，其中c表示一个候选，gi(c)表示c的第i个特征，m是特征的总数，$\lambda_{i}​$表示特征的权重</li>
<li>使用Ranking SVM算法求解特征权重参数</li>
</ul>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><ul>
<li>数据：从网络上爬取的7w+谜语，3k+的笔画，古代诗词和对联等用于训练语言模型</li>
<li>使用准确率评价解谜的效果，使用人工评测来评价生成谜题</li>
</ul>
]]></content>
      <tags>
        <tag>论文阅读</tag>
        <tag>机器猜字谜</tag>
      </tags>
  </entry>
  <entry>
    <title>论文粗读:《Generating Sentences by Editing Prototypes》</title>
    <url>/2020/02/11/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94%E3%80%8AGenerating-Sentences-by-Editing-Prototypes%E3%80%8B/</url>
    <content><![CDATA[<p>因为想做一个根据不同年龄段的人生成不同故事内容的demo，所以阅读了这篇文本风格转换的论文。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://arxiv.org/abs/1709.08878" target="_blank" rel="noopener">https://arxiv.org/abs/1709.08878</a><br><a href="https://github.com/kelvinguu/neural-editor" target="_blank" rel="noopener">https://github.com/kelvinguu/neural-editor</a></p>
</blockquote>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h3><ul>
<li>通过从训练集中挑选一个prototype sentence，产生一个能捕捉到句子相似度等句子级别信息的latent edit vector，并通过这个向量生成句子</li>
<li>模型的整体框架如下图，这个模型的由来是基于人们的一个经验，通常人们写一个复杂的句子时，都是根据一个简单的句子，逐步修改而来的</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/gsep1.png" alt="图片"></p>
<ul>
<li>目标函数：最大化生成模型的log likelihood</li>
<li>使用locality sensitive hashing寻找相似的句子</li>
</ul>
<h2 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h2><h3 id="解决问题分两步"><a href="#解决问题分两步" class="headerlink" title="解决问题分两步"></a>解决问题分两步</h3><ul>
<li>从语料库里选择一句话<ul>
<li>prototype distribution: p(x’) （uniform over X）</li>
<li>以p(x’)的概率从语料库中随机选择一个prototype sentence</li>
</ul>
</li>
<li>把这句话进行修改<ul>
<li>以edit prior: p(z)的概率sample出一个edit vector: z （实际是对edit type进行编码）</li>
<li>将z和x’送入到$p_{\text {edit }}\left(x | x^{\prime}, z\right)$的神经网络中，产生新的句子x</li>
</ul>
</li>
</ul>
<p>整体公式如下：</p>
<p>$p(x)=\sum_{x^{\prime} \in \mathcal{X}} p\left(x | x^{\prime}\right) p\left(x^{\prime}\right)$</p>
<p>$p\left(x | x^{\prime}\right)=\mathbb{E}_{z \sim p(z)}\left[p_{\mathrm{edit}}\left(x | x^{\prime}, z\right)\right]$</p>
<h3 id="模型满足两个条件"><a href="#模型满足两个条件" class="headerlink" title="模型满足两个条件"></a>模型满足两个条件</h3><ul>
<li>Semantic smoothness：一次edit只能对文本进行小改动；多次edit可以产生大的改动</li>
<li>Consistent edit behavior：对edit有类型的控制，对不同句子同一类型的edit应该产生相似的效果</li>
</ul>
<h2 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h2><h3 id="对p-x-进行近似"><a href="#对p-x-进行近似" class="headerlink" title="对p(x)进行近似"></a>对p(x)进行近似</h3><ul>
<li>大多数的prototype x都是不相关的，即p(x|x’)非常小；因此我们只考虑和x有非常高的lexical overlap的prototype x’</li>
<li>定义一个lexical similarity neighborhor $\mathcal{N}(x) \stackrel{\text { def }}{=}\left\{x^{\prime} \in \mathcal{X}: d_{J}\left(x, x^{\prime}\right)&lt;0.5\right\}$，其中dj是x和x’的Jaccard距离</li>
<li>利用neighborhood prototypes和Jensen不等式求解</li>
</ul>
<p>$\begin{aligned} \log p(x) &amp;=\log \left[\sum_{x^{\prime} \in \mathcal{X}} p\left(x | x^{\prime}\right) p\left(x^{\prime}\right)\right] \ &amp; \geq \log \left[\sum_{x^{\prime} \in \mathcal{N}(x)} p\left(x | x^{\prime}\right) p\left(x^{\prime}\right)\right] \end{aligned}$</p>
<p>$\begin{array}{l}{=\log \left[|\mathcal{N}(x)|^{-1} \sum_{x^{\prime} \in \mathcal{N}(x)} p\left(x | x^{\prime}\right)\right]+R(x)} \ {\geq|\mathcal{N}(x)|^{-1} \sum_{x^{\prime} \in \mathcal{N}(x)} \log p\left(x | x^{\prime}\right)+R(x)} \ {\underbrace{x^{\prime} \in \mathcal{N}(x)}_{\text {def }_{\mathrm{LEX}}(x)}+R(x)}\end{array}$</p>
<ul>
<li>$\begin{array}{l}{p\left(x^{\prime}\right)=1 /|\mathcal{X}|} \ {\mathrm{R}(\mathrm{x})=\log (|\mathcal{N}(x)| /|\mathcal{X}|)}\end{array}$</li>
<li>$|\mathcal{N}(x)|​$是跟x相关的常数，x的邻居使用locality sensitive hashing (LSH) and minhashing进行预先的计算</li>
</ul>
<h3 id="对log-p-x-x’-进行近似"><a href="#对log-p-x-x’-进行近似" class="headerlink" title="对log p(x|x’)进行近似"></a>对log p(x|x’)进行近似</h3><ul>
<li><p>使用蒙特卡洛对$z \sim p(z)​$进行采样时可能会产生比较高的方差，因为$p_{\text {edit }}\left(x | x^{\prime}, z\right)​$对于大部分从p(z) sample出来的z来说都输出0，只对一部分不常见的值输出较大的值</p>
</li>
<li><p>使用inverse neural editor：$q\left(z | x^{\prime}, x\right)$</p>
<ul>
<li>对prototype x’和修正后的句子x，生成一个x’到x的转换的edit vector z，这个z在重要的值上会有较大的概率</li>
</ul>
</li>
<li><p>使用evidence lower bound（ELBO）来计算log p(x|x’)</p>
<p>$\begin{aligned} \log p\left(x | x^{\prime}\right) \geq &amp; \underbrace{\mathbb{E}_{z \sim q\left(z | x^{\prime}, x\right)}\left[\log p_{\text {edit }}\left(x | x^{\prime}, z\right)\right]}_{\mathcal{L}_{\text {gen }}} \ &amp;-\underbrace{\operatorname{KL}\left(q\left(z | x^{\prime}, x\right) | p(z)\right)}_{\mathcal{E}_{\mathrm{KL}}^{L}} \ \stackrel{\text { def }}{=} \operatorname{ELBO}\left(x, x^{\prime}\right) \end{aligned}$</p>
</li>
<li><p>q(z|x’,x)可以看成是VAE的encoder，pedit(x|x’,z)可以看成是VAE的decoder</p>
</li>
</ul>
<h3 id="目标函数-sum-x-prime-in-mathcal-N-x-operatorname-ELBO-left-x-x-prime-right"><a href="#目标函数-sum-x-prime-in-mathcal-N-x-operatorname-ELBO-left-x-x-prime-right" class="headerlink" title="目标函数 $\sum_{x^{\prime} \in \mathcal{N}(x)} \operatorname{ELBO}\left(x, x^{\prime}\right)$"></a>目标函数 $\sum_{x^{\prime} \in \mathcal{N}(x)} \operatorname{ELBO}\left(x, x^{\prime}\right)$</h3><ul>
<li>参数：$\Theta=\left(\Theta_{p}, \Theta_{q}\right)$，包含neural editor的参数和inverse neural editor的参数</li>
</ul>
<h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><h3 id="Neural-editor-p-text-edit-left-x-x-prime-z-right"><a href="#Neural-editor-p-text-edit-left-x-x-prime-z-right" class="headerlink" title="Neural editor $p_{\text {edit }}\left(x | x^{\prime}, z\right)$"></a>Neural editor $p_{\text {edit }}\left(x | x^{\prime}, z\right)$</h3><ul>
<li>input: prototype x’</li>
<li>output: revised sentence x</li>
<li>seq2seq<ul>
<li>encoder: 3层双向LSTM，使用Glove词向量初始化</li>
<li>decoder: 3层包含attention的LSTM<ul>
<li>最上面一层的hidden state用来和encoder输出的hidden state一起算attention</li>
<li>将attention向量和z向量concate一起，再送入softmax中</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Edit-prior-p-z"><a href="#Edit-prior-p-z" class="headerlink" title="Edit prior $p(z)$"></a>Edit prior $p(z)$</h3><ul>
<li>edit vector z的sample方法<ul>
<li>先采样其scalar length：$z_{\text {norm }} \sim  \operatorname{Unif}(0,10)​$</li>
<li>再采样其direction:  在uniform distribution中采样一个zdir向量</li>
<li>$z=z_{\text {norm }} \cdot z_{\text {dir }}$</li>
<li>这样做是为了方便计算KL散度</li>
</ul>
</li>
</ul>
<h3 id="Inverse-neural-editor-q-left-z-x-prime-x-right"><a href="#Inverse-neural-editor-q-left-z-x-prime-x-right" class="headerlink" title="Inverse neural editor $q\left(z | x^{\prime}, x\right)$"></a>Inverse neural editor $q\left(z | x^{\prime}, x\right)$</h3><ul>
<li>假设x’和x只差了一个word，那么edit vector z跟word的词向量应该是一样的，那么多个word的插入就相当于多个word的词向量的和，删除同理</li>
<li>加入到x’的词的集合：$I=x \backslash x^{\prime}​$</li>
<li>从x’中删除的词的集合：$D=x^{\prime}\backslash  x$</li>
<li>x’和x的差异：$f\left(x, x^{\prime}\right)=\sum_{w \in I} \Phi(w) \oplus \sum_{w \in D} \Phi(w)$<ul>
<li>$\Phi(w)$表示w的词向量，它同时也是inverse neural editor q的参数，使用300维的Glove向量初始化</li>
<li>$\oplus$表示concate操作</li>
</ul>
</li>
<li>认为q是在f的基础上加入噪声获得的（先旋转，在rescale）：</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/gsep2.png" alt="图片"></p>
<ul>
<li>$\begin{array}{l}{f_{\text {norm }}=|f|} \ {f_{\text {dir }}=f / f_{\text {norm }}}\end{array}$</li>
<li>$\operatorname{vMF}(v ; \mu, \kappa)​$表示点v的unit空间中的vMF分布，参数包含mean vector$\mu​$和concentration parameter $ \kappa​$</li>
<li><p>因此可得：$\begin{aligned} q\left(z_{\text {dir }} | x^{\prime}, x\right) &amp;=\operatorname{vMF}\left(z_{\text {dir }} ; f_{\text {dir }}, \kappa\right) \ q\left(z_{\text {norm }} | x^{\prime}, x\right) &amp;=\text { Unif }\left(z_{\text {norn }} ;\left[\tilde{f}_{\text {norn }}, \tilde{f}_{\text {nom }}+\epsilon\right]\right) \end{aligned}$</p>
</li>
<li><p>其中 $\tilde{f}_{\text {norm }}=\min \left(f_{\text {norm }}, 10-\epsilon\right)$</p>
</li>
<li>最终 $z=z_{\mathrm{dir}} \cdot z_{\mathrm{norm}}$</li>
</ul>
<h2 id="Details"><a href="#Details" class="headerlink" title="Details"></a>Details</h2><h3 id="计算目标函数的梯度-nabla-Theta-q-operatorname-ELBO-left-x-x-prime-right-nabla-Theta-q-mathcal-L-mathrm-gen-nabla-Theta-q-mathcal-L-mathrm-KL"><a href="#计算目标函数的梯度-nabla-Theta-q-operatorname-ELBO-left-x-x-prime-right-nabla-Theta-q-mathcal-L-mathrm-gen-nabla-Theta-q-mathcal-L-mathrm-KL" class="headerlink" title="计算目标函数的梯度$\nabla_{\Theta_{q}} \operatorname{ELBO}\left(x, x^{\prime}\right)=\nabla_{\Theta_{q}} \mathcal{L}_{\mathrm{gen}}-\nabla_{\Theta_{q}} \mathcal{L}_{\mathrm{KL}}$"></a>计算目标函数的梯度$\nabla_{\Theta_{q}} \operatorname{ELBO}\left(x, x^{\prime}\right)=\nabla_{\Theta_{q}} \mathcal{L}_{\mathrm{gen}}-\nabla_{\Theta_{q}} \mathcal{L}_{\mathrm{KL}}$</h3><ul>
<li>使用重参数计算：$\nabla_{\Theta_{q}} \mathcal{L}_{\mathrm{gen}}$<ul>
<li>将z~q(z|x’,x)重写为$z=h(\alpha)$</li>
<li>$\begin{aligned} \nabla \Theta_{q} \mathcal{L}_{\mathrm{gen}} &amp;=\nabla \Theta_{q} \mathbb{E}_{z \sim q\left(z | x^{\prime}, x\right)}\left[\log p_{\text {edit }}\left(x | x^{\prime}, z\right)\right] \ &amp;=\mathbb{E}_{\alpha \sim p(\alpha)}\left[\nabla \Theta_{q} \log p_{\text {edit }}\left(x | x^{\prime}, h(\alpha)\right)\right] \end{aligned}$</li>
</ul>
</li>
<li>计算$\nabla_{\Theta_{q}} \mathcal{L}_{\mathrm{KL}}$<ul>
<li>$\begin{aligned} \mathcal{L}_{\mathrm{KL}} &amp;=\mathrm{KL}\left(q\left(z_{\text {norm }} | x^{\prime}, x\right) | p\left(z_{\text {norm }}\right)\right) \ &amp;+\mathrm{KL}\left(q\left(z_{\text {dir }} | x^{\prime}, x\right) | p\left(z_{\text {dir }}\right)\right) \end{aligned}$</li>
<li>$\begin{array}{l}{\operatorname{KL}(\operatorname{vMF}(\mu, \kappa) | \operatorname{vMF}(\mu, 0))=\kappa \frac{I_{d / 2}(\kappa)+I_{d / 2-1}(\kappa) \frac{d-2}{2 \kappa}}{I_{d / 2-1}(\kappa)-\frac{d-2}{2 \kappa}}} \ {-\log \left(I_{d / 2-1}(\kappa)\right)-\log (\Gamma(d / 2))} \ {+\log (\kappa)(d / 2-1)-(d-2) \log (2) / 2}\end{array}$</li>
</ul>
</li>
</ul>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="Datsets"><a href="#Datsets" class="headerlink" title="Datsets"></a>Datsets</h3><ul>
<li>Yelp</li>
<li>One BillionWord Language Model Benchmark</li>
<li>使用Spacy将NER的词替换成其NER category</li>
<li>将出现频次小于10000的词用UNK替换</li>
</ul>
<h3 id="Generative-Modeling"><a href="#Generative-Modeling" class="headerlink" title="Generative Modeling"></a>Generative Modeling</h3><ul>
<li>对比几个生成模型的效果（KENLM语言模型、自回归语言模型）</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/gsep3.png" alt="图片"></p>
<ul>
<li>使用neural editor可以生成跟prototype很不一样的句子</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/gsep4.png" alt="图片"></p>
<h3 id="Human-Evaluation"><a href="#Human-Evaluation" class="headerlink" title="Human Evaluation"></a>Human Evaluation</h3><ul>
<li>降低softmax temperture有助于产生更符合语法的句子，但也会产生short and generic sentence<ul>
<li>从corpus中sample出prototype sentence可以增加生成句子的多样性，因此即便temperature设置为0，也不会影响句子多样子，这是比传统的NLM强的地方</li>
</ul>
</li>
<li>从grammaticality 和 plausibility两方面进行评测</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/gsep5.png" alt="图片"></p>
<h3 id="Semantics-of-NeuralEditor"><a href="#Semantics-of-NeuralEditor" class="headerlink" title="Semantics of NeuralEditor"></a>Semantics of NeuralEditor</h3><ul>
<li>跟sentence variational autoencoder (SVAE)模型对比，SVAE将句子映射到semantic空间向量，再从向量还原句子</li>
<li>semantic smoothness<ul>
<li>smoothness表示每一个小的edit只能对句子有一点点改变</li>
<li>我们先从corpus中选择一个prototype sentence，然后不断对它使用neural editor，并让人工对semantic的变化进行打分。</li>
<li>对比实验有两个，一个是SVAE，一个是从corpus中根据cosine相似度选择出的句子</li>
</ul>
</li>
<li>consistent edit behavior</li>
</ul>
]]></content>
      <tags>
        <tag>论文阅读</tag>
        <tag>文本风格迁移</tag>
      </tags>
  </entry>
  <entry>
    <title>Separable Convolution</title>
    <url>/2020/02/06/Separable-Convolution/</url>
    <content><![CDATA[<p>阅读论文《The Evolved Transformer》时遇到了separable convolution的概念，因此找了相关资料学习了一下。</p>
<blockquote>
<p>参考：<br><a href="https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-al" target="_blank" rel="noopener">https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-al</a></p>
</blockquote>
<a id="more"></a>
<p>在讲Separable Convolution前先了解下常用的卷积网络的定义。</p>
<h2 id="Convolutional-Neural-Networks"><a href="#Convolutional-Neural-Networks" class="headerlink" title="Convolutional Neural Networks"></a>Convolutional Neural Networks</h2><blockquote>
<p>参考：<br><a href="https://towardsdatascience.com/what-is-a-neural-network-6010edabde2b" target="_blank" rel="noopener">https://towardsdatascience.com/what-is-a-neural-network-6010edabde2b</a></p>
</blockquote>
<p>卷积网络中最重要的是卷积核，通过卷积核在图像每个区域的运算，得到图像不同的特征，如下图（可以在<a href="http://setosa.io/ev/image-kernels/" target="_blank" rel="noopener">http://setosa.io/ev/image-kernels/</a>中更好得体验）：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn1.png" alt="图片"></p>
<p>上面这张图使用outliine卷积核，实际中可以使用sharp等不同功能的卷积核以达到不同效果。</p>
<p>对于神经网络的每一层而言，可以使用多个卷积和得到不同的特征图，并将这些特征图一起输入到下一层网络。最终这些特征供给最后一层的分类器进行匹配，得到分类结果。下面的动画展示了这个过程：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn1.gif" alt="图片"></p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn2.gif" alt="图片"></p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn3.gif" alt="图片"></p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn4.gif" alt="图片"></p>
<p>Separable Convolution可以分成spatial separable convolution和depthwise separable convolution。</p>
<p>对于12x12x3的图像，5x5x3的卷积核，能产生8x8x1的输出：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn2.png" alt="图片"></p>
<p>假设我们想要8x8x256的输出，则需要使用256个卷积核来创造256个8x8x1的图像，把他们叠加在一起产生8x8x256的输出：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn3.png" alt="图片"></p>
<p>即12x12x3 — (5x5x3x256) — &gt;12x12x256</p>
<h2 id="Spatial-Separable-Convolutions"><a href="#Spatial-Separable-Convolutions" class="headerlink" title="Spatial Separable Convolutions"></a>Spatial Separable Convolutions</h2><p>Spatial separable convolution将卷积分成两部分，最常见的是把3x3的kernel分解成3x1和1x3的kernel，如：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn4.png" alt="图片"></p>
<p>通过这种方式，原本一次卷积要算9次乘法，现在只需要6次。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn5.png" alt="图片"></p>
<p>还有一个Sobel kernel（用来检测边）也是用的这种方法：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn6.png" alt="图片"></p>
<p>但spatial separable convolution存在的问题是，不是所有kernel都能转换成2个小的kernel。</p>
<h2 id="Depthwise-Separable-Convolutions"><a href="#Depthwise-Separable-Convolutions" class="headerlink" title="Depthwise Separable Convolutions"></a>Depthwise Separable Convolutions</h2><p>由于卷积并不使用矩阵相乘，为了减少计算量，可以将卷积的过程分成两部分：a depthwise convolution and a pointwise convolution. </p>
<h3 id="depthwise-convolution"><a href="#depthwise-convolution" class="headerlink" title="depthwise convolution"></a>depthwise convolution</h3><p>首先，我们使用3个5x5x1的卷积核产生8x8x3的图像：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn7.png" alt="图片"></p>
<h3 id="pointwise-convolution"><a href="#pointwise-convolution" class="headerlink" title="pointwise convolution"></a>pointwise convolution</h3><p>其次，使用1x1x3 的卷积核对每个像素计算，得到8x8x1 的图像：</p>
<p><img src="http://q503tsu73.bkt.clouddn.com/sc8.png?e=1580956603&amp;token=05Ii263bPN3Z-CT3JPRaRfWi5sXIj8pwX6V1bN2j:BxS4Xqtt2YsqJ5GJVFVlnK9D3xw=&amp;attname=" alt="图片"></p>
<p>使用256个1x1x3 的卷积核，则恶意产生8x8x256的图像：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn9.png" alt="图片"></p>
<p>可以看到，整个过程由原来的12x12x3 — (5x5x3x256) →12x12x256，变成12x12x3 — (5x5x1x1) — &gt; (1x1x3x256) — &gt;12x12x256</p>
<h2 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h2><p>主要就是减少了计算量，原先是256个5x5x3的卷积核移动8x8次，即需要256x3x5x5x8x8=1,228,800次乘法计算。使用depthwise convolution，有3个5x5x1的卷积核移动8x8次，需要3x5x5x8x8 = 4,800次乘法计算。使用pointwise convolution，有256个1x1x3的卷积核移动8x8次，需要256x1x1x3x8x8=49,152次乘法计算，加起来共有53,952次计算。</p>
<h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><ul>
<li>keras doc：<a href="https://keras.io/layers/convolutional/" target="_blank" rel="noopener">https://keras.io/layers/convolutional/</a></li>
<li><a href="https://github.com/alexandrosstergiou/keras-DepthwiseConv3D" target="_blank" rel="noopener">https://github.com/alexandrosstergiou/keras-DepthwiseConv3D</a></li>
<li>[<a href="https://github.com/sara-kassani/Depthwise-Separable-Convolutional-Neural-Network-for-Skin-Lesion-Classification](" target="_blank" rel="noopener">https://github.com/sara-kassani/Depthwise-Separable-Convolutional-Neural-Network-for-Skin-Lesion-Classification](</a></li>
</ul>
]]></content>
      <tags>
        <tag>Convolution</tag>
        <tag>卷积网络</tag>
      </tags>
  </entry>
  <entry>
    <title>论文粗读:《The Evolved Transformer》</title>
    <url>/2020/02/05/%E8%AE%BA%E6%96%87%E7%B2%97%E8%AF%BB%E2%80%94%E2%80%94%E3%80%8AThe-Evolved-Transformer%E3%80%8B/</url>
    <content><![CDATA[<p>今天阅读了论文《The Evolved Transformer》，该论文使用了神经架构搜索方法找到了一个更优的transformer结构。下面是阅读过程中的笔记。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://arxiv.org/pdf/1901.11117.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1901.11117.pdf</a><br><a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/evolved_transformer.py" target="_blank" rel="noopener">https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/evolved_transformer.py</a><br><a href="https://blog.csdn.net/jasonzhoujx/article/details/88875469" target="_blank" rel="noopener">https://blog.csdn.net/jasonzhoujx/article/details/88875469</a></p>
</blockquote>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h3><ul>
<li>神经架构搜索<ul>
<li>tournament selection architecture search </li>
<li>warm start</li>
<li>Progressive Dynamic Hurdles（PDH）</li>
</ul>
</li>
<li>搜索出了一个新的transformer架构：Evolved Transformer</li>
</ul>
<h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><h3 id="搜索空间"><a href="#搜索空间" class="headerlink" title="搜索空间"></a>搜索空间</h3><ul>
<li>encoder stackable cell<ul>
<li>6个NASNet-style block<ul>
<li>左右两个block将输入的hidden state转成左右两个hidden state再归并成为一个新的hidden state，作为self-attention的输入</li>
</ul>
</li>
</ul>
</li>
<li>decoder stackable cell<ul>
<li>8个NASNet-style block</li>
</ul>
</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/et1.png" alt="图片"></p>
<ul>
<li>搜索空间branch<ul>
<li>Input：分支可以从输入池中选择一个隐藏状态作为当前block的输入。单元中的第i个block可以从[0, i]个隐藏状态中进行选择，其中第j个隐藏状态表示该cell中第j个block的输出，第0个候选项为单元的输入。</li>
<li>Normalization：归一化项提供了两个选项， [LAYER NORMALIZATION (Ba et al., 2016), NONE]</li>
<li>Layer：构造一个神经网络层，提供的选项包括：<ul>
<li>标准卷积</li>
<li>深度可分离卷积</li>
<li>LIGHTWEIGHT 卷积</li>
<li>n头注意力层</li>
<li>GATED LINEAR UNIT</li>
<li>ATTEND TO ENCODER（decoder专用）</li>
<li>全等无操作</li>
<li>Dead Branch，切断输出</li>
</ul>
</li>
<li>Relative Output Dimension：决定神经网络层输出的维度。</li>
<li>Activation：搜索中激活函数的选项有[SWISH, RELU, LEAKY RELU, NON]</li>
<li>Combiner Function：表征的是左枝和右枝的结合方式，包括{ADDITION、CONCATENATION、MULTIPLICATION}。如果左右枝最终输出形状不同，则需要使用padding进行填充。短的向量向长的向量对齐，当使用加法进行结合时使用0填充，当使用乘法进行结合时使用1填充。</li>
<li>Number of cells：纵向叠加的cell的数量，搜索范围是[1,6]</li>
</ul>
</li>
</ul>
<h3 id="演进过程"><a href="#演进过程" class="headerlink" title="演进过程"></a>演进过程</h3><ul>
<li>锦标赛选择（Tournament Selection）：<ul>
<li>tournament selection算法是一种遗传算法，首先随机生成一批个体, 这些个体是一个个由不同组件组成的完整的模型，我们在目标任务上训练这些个体并在验证集上面计算他们的表现。</li>
<li>首先在初始种群中进行采样产生子种群，从子种群中选出适应性（fitness）最高的个体作为亲本（parent）。被选中的亲本进行突变——也就是将网络模型中的一些组件改变为其他的组件——以产生子模型，然后在对这些子模型分配适应度（fitness），在训练集和测试集上进行训练和验证。</li>
<li>对种群重新进行采样，用通过评估的子模型代替子种群中的fitness的个体以生成新的种群。</li>
<li>重复上面的步骤，直到种群中出现超过给定指标的模型。</li>
</ul>
</li>
<li>渐进式动态障碍（Progressive Dynamic Hurdle）：<ul>
<li>实验使用的训练集是WMT14英语到德语的机器翻译数据集，完整的训练和验证过程需要很长的时间，如果在所有的子模型上进行完整的训练和验证过程将会耗费很大的计算资源。因此论文中使用渐进式动态障碍的方法来提前停止一些没有前景的模型的训练，转而将更多的计算资源分配那些当前表现更好的子模型。具体来说就是让当前表现最好的一些模型多训练一些step。</li>
<li>假设当前种群经过一次锦标赛选择，生成了m个子模型并且加入到了种群中，这时候计算整个种群fitness的平均值h0，下一次锦标赛选择将会以h0作为对照，生成的另外m个fitness超过h0的子模型可以继续训练s1个step，接着进行种群中的所有的其他个体会继续训练s1个step，然后在新的种群中生成h1，以此类推知道种群中所有的个体的训练step都达到一个指定值。</li>
<li>如果一个子模型是由第iii次锦标赛选择之后的亲本生成的，那么验证的过程将会进行iii次。第一次为该模型分配s0次的训练step并且在验证集上进行验证，若验证的fitness大于h0则再分配s1次训练step，再验证，再与h1比较，只有子样本通过h0,h1,…,hi次比较才能作为新的个体加入到新的种群中。</li>
</ul>
</li>
</ul>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><ul>
<li><p>机器翻译</p>
<ul>
<li>在初始的10K step使用0.01的learning rate</li>
<li><p>Transformer</p>
<ul>
<li>inverse-square-root decay to 0 at 300K steps：$l r=s t e p^{-0.00303926^{\circ}}-.962392$</li>
</ul>
</li>
<li><p>Evolved Transformer</p>
<ul>
<li>single-cycle cosine decay</li>
</ul>
</li>
<li>every decay was paired with the same constant 0.01 warmup.</li>
<li>大模型使用高一点的dropout（0.3），小模型使用0.2 dropout</li>
<li>beam-size=6, lenth-penalty=0.6, max-output=50</li>
</ul>
</li>
<li>语言模型<ul>
<li>跟机器翻译差不多，去掉了label smooth, intra-attention dropout=0.0</li>
</ul>
</li>
<li>Search Configuration<ul>
<li>populatino=100</li>
<li>mutation=2.5%</li>
<li>fitness: negative log perplexity</li>
</ul>
</li>
</ul>
<h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><ul>
<li>最终搜索出来的模型结构</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/et2.png" alt="图片"></p>
<ul>
<li>embedding_size=768, 6 encoder, 6 decoder</li>
<li>attention_head=16</li>
<li>ET比Transformer可以在更小的模型上达到更好的效果，当模型增大时两者的差距就不大了（可能因为模型越大越容易过拟合，而且单独增加embedding_size可能不起作用，需要和depth共同增加）</li>
</ul>
]]></content>
      <tags>
        <tag>Transformer</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读:《Towards a Human-like Open-Domain Chatbot》</title>
    <url>/2020/02/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94%E3%80%8ATowards-a-Human-like-Open-Domain-Chatbot%E3%80%8B/</url>
    <content><![CDATA[<p>今天阅读了谷歌最新出的一篇论文，《Towards a Human-like Open-Domain Chatbot》，主要提出了端到端对话机器人的一种评测方法和模型框架。下面是阅读过程中的笔记。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html" target="_blank" rel="noopener">https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html</a><br><a href="https://arxiv.org/pdf/2001.09977.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2001.09977.pdf</a><br><a href="https://github.com/google-research/google-research/tree/master/meena" target="_blank" rel="noopener">https://github.com/google-research/google-research/tree/master/meena</a></p>
</blockquote>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="开放的chatbot-API总结"><a href="#开放的chatbot-API总结" class="headerlink" title="开放的chatbot API总结"></a>开放的chatbot API总结</h3><ul>
<li>cleverbot API: <a href="https://www.cleverbot.com/api/" target="_blank" rel="noopener">https://www.cleverbot.com/api/</a><ul>
<li><a href="https://github.com/plasticuproject/cleverbotfree" target="_blank" rel="noopener">https://github.com/plasticuproject/cleverbotfree</a></li>
</ul>
</li>
<li>xiaobing: <a href="https://www.msxiaobing.com/" target="_blank" rel="noopener">https://www.msxiaobing.com/</a></li>
<li>mitsuku: <a href="https://www.pandorabots.com/mitsuku/" target="_blank" rel="noopener">https://www.pandorabots.com/mitsuku/</a><ul>
<li><a href="https://github.com/hanwenzhu/mitsuku-api" target="_blank" rel="noopener">https://github.com/hanwenzhu/mitsuku-api</a></li>
</ul>
</li>
</ul>
<h3 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h3><ul>
<li>模型架构：Evolved Transformer<ul>
<li>模型输入：多轮对话（最多7轮）</li>
<li>模型输出：回复</li>
<li>最佳模型：2.6B参数，10.2PPL，8K BPE subword vocabulary, 训练数据40B words</li>
</ul>
</li>
<li>评测指标<ul>
<li>PPL</li>
<li>SSA（Sensibleness and Specificity Average）用来评估<ul>
<li>whether make sense</li>
<li>whether specific</li>
</ul>
</li>
<li>人工评测使用static（1477个多轮对话）和interactive（想说啥就说啥）两种数据集，发现SSA和PPL在这两个数据集上高度相关</li>
<li>模型在评测集的表现：<ul>
<li>0.72的SSA</li>
<li>经过filtering mechanism 和 tuned decoding后有0.79的SSA，相比于人提供的0.86SSA的回复已经很接近了</li>
</ul>
</li>
</ul>
</li>
<li>方法的局限性<ul>
<li>评测数据集的局限性，不能解决所有领域的问题</li>
</ul>
</li>
</ul>
<h2 id="对话机器人的评价"><a href="#对话机器人的评价" class="headerlink" title="对话机器人的评价"></a>对话机器人的评价</h2><h3 id="人工进行评测时的参考标准"><a href="#人工进行评测时的参考标准" class="headerlink" title="人工进行评测时的参考标准"></a>人工进行评测时的参考标准</h3><ul>
<li>Sensibleness<ul>
<li>common sense</li>
<li>logical coherence</li>
<li>consistency</li>
<li>人工评测时对于可打的标签：confusing, illogical, out of context, factually wrong, make sense</li>
<li>缺陷：对于安全的回答，如I don’t know，无法区分</li>
</ul>
</li>
<li>Specificity<ul>
<li>A: I love tennis.   B: That’s nice 应该被标记为not specific，如果 B：Me too, I can’t get enough of Roger Federer!则被标记为specific</li>
<li>已经被标记为not sensible的直接标记为not specific</li>
</ul>
</li>
<li>SSA<ul>
<li>可以使用Sensibleness和Specificity标记在所有responses的比例来作为参考标准</li>
<li>使用SSA将Sensibleness和Specificity的比例进行了结合</li>
</ul>
</li>
</ul>
<h3 id="可进行对比的几个开源chatbot框架"><a href="#可进行对比的几个开源chatbot框架" class="headerlink" title="可进行对比的几个开源chatbot框架"></a>可进行对比的几个开源chatbot框架</h3><ul>
<li>基于RNN：<a href="https://github.com/lukalabs/cakechat" target="_blank" rel="noopener">https://github.com/lukalabs/cakechat</a></li>
<li>基于Transformer: <a href="https://github.com/microsoft/DialoGPT" target="_blank" rel="noopener">https://github.com/microsoft/DialoGPT</a><ul>
<li>762M参数的模型效果更好一些</li>
<li>dialogpt没有公开其解码和MMI-reranking的过程，gpt2bot实现了解码：<a href="https://github.com/polakowo/gpt2bot" target="_blank" rel="noopener">https://github.com/polakowo/gpt2bot</a></li>
<li>附加一个中文的基于DialoGPT开发的闲聊模型<ul>
<li><a href="https://github.com/yangjianxin1/GPT2-chitchat" target="_blank" rel="noopener">https://github.com/yangjianxin1/GPT2-chitchat</a></li>
<li><a href="https://blog.csdn.net/kingsonyoung/article/details/103803067" target="_blank" rel="noopener">https://blog.csdn.net/kingsonyoung/article/details/103803067</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="构建静态评测集"><a href="#构建静态评测集" class="headerlink" title="构建静态评测集"></a>构建静态评测集</h3><ul>
<li>从单轮开始：<a href="http://ai.stanford.edu/~quocle/QAresults.pdf" target="_blank" rel="noopener">http://ai.stanford.edu/~quocle/QAresults.pdf</a></li>
<li>增加一些个性化问题，如：Do you like cats?<ul>
<li>A: Do you like movies?; B: Yeah. I like sci-fi mostly; A: Really? Which is your favorite?期待I love Back to the Future这样的回答，对于I don’t like movies这样的回复应标记为not sensible</li>
</ul>
</li>
</ul>
<h3 id="进行动态评测"><a href="#进行动态评测" class="headerlink" title="进行动态评测"></a>进行动态评测</h3><ul>
<li>机器人以Hi开始，评测人员自由与bot对话，并对每一个bot的回复进行评测。每一个对话至少14轮，至多28轮。</li>
</ul>
<h2 id="Meena-Chatbot"><a href="#Meena-Chatbot" class="headerlink" title="Meena Chatbot"></a>Meena Chatbot</h2><h3 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h3><ul>
<li>来源于public social media</li>
<li>清洗流程<ul>
<li>去掉 subword 数目&lt;=2 或 subword 数目 &gt;= 128</li>
<li>去掉 字母比例&lt;0.7</li>
<li>去掉 包含URL</li>
<li>去掉 作者名字bot</li>
<li>去掉 出现100次以上</li>
<li>去掉 跟上文n-gram重复比例过高</li>
<li>去掉 敏感句子</li>
<li>去掉 括号中内容</li>
<li>当一个句子被删除时，则上文全部被删除</li>
</ul>
</li>
<li>共清洗出867M的(context, response)对</li>
<li>使用sentence piece进行BPE分词，得到8K的BPE vocab</li>
<li>最终语料包含341GB的语料(40B word)</li>
</ul>
<h3 id="模型框架"><a href="#模型框架" class="headerlink" title="模型框架"></a>模型框架</h3><ul>
<li>Evolved Transformer<ul>
<li>2.6B parameter</li>
<li>1 ET encoder + 13 ET decoder</li>
</ul>
</li>
<li>最大的模型可达到10.2的PPL</li>
<li>最大的传统Transformer模型（32层decoder）可达到10.7的PPL</li>
<li>hidden size: 2560</li>
<li>attention head: 32</li>
<li>共享编码、解码、softmax的embedding</li>
<li>编码、解码最长是128</li>
</ul>
<h3 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h3><ul>
<li>使用Adafactor optimizer，初始学习率0.01，在前10k step保持不变，使用inverse square root of the number of steps进行衰减</li>
<li>使用<a href="https://github.com/tensorflow/" target="_blank" rel="noopener">https://github.com/tensorflow/</a>tensor2tensor代码进行训练</li>
</ul>
<h3 id="解码细节"><a href="#解码细节" class="headerlink" title="解码细节"></a>解码细节</h3><ul>
<li><p>为了避免产生乏味的回复，可以使用多种方法进行解码</p>
<ul>
<li>reranking</li>
<li>基于profiles, topics, and styles</li>
<li>强化学习</li>
<li>变分自编吗</li>
</ul>
</li>
<li><p>当PPL足够小时，可以使用sample-and-rank策略进行解码</p>
<ul>
<li><p>使用temperature T随机产生N个独立的候选</p>
<ul>
<li><p>$p_{i}=\frac{\exp \left(z_{i} / T\right)}{\sum_{j} \exp \left(z_{j} / T\right)}$</p>
</li>
<li><p>T=1产生不经过修正的分布</p>
</li>
<li><p>T越大，越容易产生不常见的词，如相关的实体名词，但可能产生错误的词</p>
</li>
<li><p>T越小，越容易产生常见的词，如冠词或介词，虽然安全但不specific</p>
</li>
<li><p>解释1</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">温度是神经网络的超参数，用于在应用softmax之前通过缩放对数来控制预测的随机性。 例如，在TensorFlow的LSTM中，温度代表在计算softmax之前将logit除以多少。</span><br><span class="line"></span><br><span class="line">当温度为1时，我们直接在logits（较早层的未缩放输出）上计算softmax，并使用温度为0.6的模型在logits&#x2F;0.6上计算softmax，从而得出较大的值。 在更大的值上执行softmax可使LSTM 更加自信 （需要较少的输入来激活输出层），但在其样本中也更加保守 （从不太可能的候选样本中进行抽样的可能性较小）。 使用较高的温度会在各个类上产生较软的概率分布，并使RNN更容易被样本“激发”，从而导致更多的多样性和更多的错误 。</span><br><span class="line"></span><br><span class="line">softmax函数通过确保网络输出在每个时间步长都在零到一之间，基于其指数值对候选网络的每次迭代进行归一化。</span><br><span class="line"></span><br><span class="line">因此，温度增加了对低概率候选者的敏感性。</span><br></pre></td></tr></table></figure>
</li>
<li><p>解释2</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">当T很大时，即趋于正无穷时，所有的激活值对应的激活概率趋近于相同（激活概率差异性较小）；而当T很低时，即趋于0时，不同的激活值对应的激活概率差异也就越大。</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
<li><p>发现使用beam-search解码会产生重复且无趣的回复，使用sample-and-rank产生的回复会丰富一些</p>
</li>
<li>使用N=20，T=0.88</li>
<li>response score的计算：logP/T，P是response的likelihood，T是token的个数</li>
<li><p>解码时增加detect cross turn repetitions</p>
<ul>
<li>当两个turn的n-gram重复超过一定比例时，则从候选中删除</li>
</ul>
</li>
<li>增加一个分类层，用来过滤掉敏感回复</li>
</ul>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><h3 id="SSA和PPL是相关的"><a href="#SSA和PPL是相关的" class="headerlink" title="SSA和PPL是相关的"></a>SSA和PPL是相关的</h3><ul>
<li>基本呈线性关系</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/tod1.png" alt="图片"></p>
<h3 id="效果的比较"><a href="#效果的比较" class="headerlink" title="效果的比较"></a>效果的比较</h3><ul>
<li>小冰：呈现出个性化的回复，但有时也会无意义，且经常回复得太平常。小冰另一个特点就是具有同情心，可以在以后的评价指标中考虑这一点。小冰有near-human-level engagingness但not very close to human-level humanness，因此在我们的评测指标上SSA不高。</li>
<li>mitsuku：56%SSA（72%sensibility 40%specifity）, 网站上的对话并不是它参加图灵测试的版本</li>
<li>DialoGPT：48%SSA（57%sensibility 49%specifity）</li>
<li>CleverBot：在interactive评测表现比static上稍微好一些（56% interactive SSA，44% static SSA）。发现cleverbot更擅长将话题引入到它更擅长的领域中，缺少personality</li>
<li>Meena：base（72% SSA），full（79% SSA）</li>
</ul>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>论文阅读</tag>
        <tag>Chatbot</tag>
      </tags>
  </entry>
  <entry>
    <title>各类资源定期汇总</title>
    <url>/2020/02/01/%E5%90%84%E7%B1%BB%E8%B5%84%E6%BA%90%E5%AE%9A%E6%9C%9F%E6%B1%87%E6%80%BB/</url>
    <content><![CDATA[<p>一些学习等资源的总结，不定期更新。</p>
<a id="more"></a>
<h1 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h1><h2 id="视频类资源"><a href="#视频类资源" class="headerlink" title="视频类资源"></a>视频类资源</h2><h3 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h3><ul>
<li>贪心学院 人工智能Python变成特训营：<a href="https://www.greedyai.com/my/courses/learning" target="_blank" rel="noopener">https://www.greedyai.com/my/courses/learning</a></li>
<li>CSDN 全部课程：<a href="https://edu.csdn.net/course/index?is_member=1" target="_blank" rel="noopener">https://edu.csdn.net/course/index?is_member=1</a>优惠码：csdn2020 到3月31日</li>
</ul>
<h3 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h3><ul>
<li>吴恩达机器学习：<a href="https://study.163.com/course/courseMain.htm?courseId=1004570029" target="_blank" rel="noopener">https://study.163.com/course/courseMain.htm?courseId=1004570029</a></li>
<li>统计学习基础：链接:<a href="https://pan.baidu.com/s/1NWE6lEJrSgOFAEVVOW2TmQ" target="_blank" rel="noopener">https://pan.baidu.com/s/1NWE6lEJrSgOFAEVVOW2TmQ</a>提取码: g7km</li>
<li>林轩田机器学习：<a href="https://www.tinymind.cn/articles/168" target="_blank" rel="noopener">https://www.tinymind.cn/articles/168</a></li>
</ul>
<h3 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h3><ul>
<li>花书：<a href="https://www.bilibili.com/video/av69236102?from=search&amp;seid=4183604428283884229" target="_blank" rel="noopener">https://www.bilibili.com/video/av69236102?from=search&amp;seid=4183604428283884229</a></li>
<li>lecun深度学习：<a href="https://atcold.github.io/pytorch-Deep-Learning/" target="_blank" rel="noopener">https://atcold.github.io/pytorch-Deep-Learning/</a></li>
</ul>
<h3 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h3><ul>
<li>斯坦福自然语言处理：<a href="https://zhuanlan.zhihu.com/p/63199665" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/63199665</a><ul>
<li><a href="http://cs224d.stanford.edu/syllabus.html" target="_blank" rel="noopener">http://cs224d.stanford.edu/syllabus.html</a></li>
</ul>
</li>
<li>宗成庆统计自然语言处理：<ul>
<li><a href="https://space.bilibili.com/373951238" target="_blank" rel="noopener">https://space.bilibili.com/373951238</a></li>
<li><a href="https://space.bilibili.com/303667813/video" target="_blank" rel="noopener">https://space.bilibili.com/303667813/video</a></li>
</ul>
</li>
<li>牛津大学自然语言处理：<a href="https://machinelearningmastery.com/oxford-course-deep-learning-natural-language-processing/" target="_blank" rel="noopener">https://machinelearningmastery.com/oxford-course-deep-learning-natural-language-processing/</a></li>
<li>陈丹琦深度学习自然语言处理：<a href="https://www.yanxishe.com/resourceDetail/1389?from=leiphonecolumn_res0420" target="_blank" rel="noopener">https://www.yanxishe.com/resourceDetail/1389?from=leiphonecolumn_res0420</a><ul>
<li>课件：<a href="https://www.yanxishe.com/resourceDetail/1389" target="_blank" rel="noopener">https://www.yanxishe.com/resourceDetail/1389</a></li>
</ul>
</li>
<li>语音：CS236</li>
</ul>
<h3 id="CV"><a href="#CV" class="headerlink" title="CV"></a>CV</h3><ul>
<li>CS231n计算机视觉：<a href="https://www.bilibili.com/video/av77752864/" target="_blank" rel="noopener">https://www.bilibili.com/video/av77752864/</a></li>
</ul>
<h3 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h3><ul>
<li>David Silver强化学习：<a href="https://space.bilibili.com/74997410?spm_id_from=333.788.b_765f7570696e666f.2" target="_blank" rel="noopener">https://space.bilibili.com/74997410?spm_id_from=333.788.b_765f7570696e666f.2</a></li>
</ul>
<h3 id="数学"><a href="#数学" class="headerlink" title="数学"></a>数学</h3><ul>
<li>面向机器学习的数学：<a href="http://coursegraph.com/coursera-specializations-mathematics-machine-learning" target="_blank" rel="noopener">http://coursegraph.com/coursera-specializations-mathematics-machine-learning</a><ul>
<li><a href="https://github.com/mml-book/mml-book.github.io" target="_blank" rel="noopener">https://github.com/mml-book/mml-book.github.io</a></li>
</ul>
</li>
</ul>
<h2 id="博客类资源"><a href="#博客类资源" class="headerlink" title="博客类资源"></a>博客类资源</h2><ul>
<li>科学空间：<a href="https://kexue.fm/" target="_blank" rel="noopener">https://kexue.fm/</a></li>
<li><a href="https://lonepatient.top/" target="_blank" rel="noopener">https://lonepatient.top/</a></li>
<li><a href="https://www.cnblogs.com/shona" target="_blank" rel="noopener">https://www.cnblogs.com/shona</a></li>
<li><a href="https://towardsdatascience.com/" target="_blank" rel="noopener">https://towardsdatascience.com/</a></li>
<li><a href="http://www.52caml.com/" target="_blank" rel="noopener">http://www.52caml.com/</a></li>
</ul>
<h2 id="学习笔记类资源"><a href="#学习笔记类资源" class="headerlink" title="学习笔记类资源"></a>学习笔记类资源</h2><h3 id="机器学习-1"><a href="#机器学习-1" class="headerlink" title="机器学习"></a>机器学习</h3><ul>
<li>吴恩达机器学习笔记：<a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes" target="_blank" rel="noopener">https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes</a></li>
<li>统计学习基础笔记：<a href="https://github.com/SmirkCao/Lihang" target="_blank" rel="noopener">https://github.com/SmirkCao/Lihang</a></li>
<li>百面机器学习：<a href="https://github.com/Relph1119/QuestForMachineLearning-Camp" target="_blank" rel="noopener">https://github.com/Relph1119/QuestForMachineLearning-Camp</a></li>
</ul>
<h3 id="深度学习-1"><a href="#深度学习-1" class="headerlink" title="深度学习"></a>深度学习</h3><ul>
<li>花书笔记：<a href="https://discoverml.github.io/simplified-deeplearning/" target="_blank" rel="noopener">https://discoverml.github.io/simplified-deeplearning/</a><ul>
<li>中文版图书：<a href="https://github.com/exacity/deeplearningbook-chinese" target="_blank" rel="noopener">https://github.com/exacity/deeplearningbook-chinese</a></li>
</ul>
</li>
</ul>
<h3 id="NLP-1"><a href="#NLP-1" class="headerlink" title="NLP"></a>NLP</h3><ul>
<li>斯坦福自然语言处理：<a href="https://zhuanlan.zhihu.com/p/59011576" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/59011576</a></li>
<li>宗成庆统计自然语言处理：<ul>
<li><a href="https://github.com/aicourse/ZMC301-CAS-NLP-2019" target="_blank" rel="noopener">https://github.com/aicourse/ZMC301-CAS-NLP-2019</a></li>
<li><a href="http://www.nlpr.ia.ac.cn/cip/ZongReportandLecture/ReportandLectureIndex.htm" target="_blank" rel="noopener">http://www.nlpr.ia.ac.cn/cip/ZongReportandLecture/ReportandLectureIndex.htm</a></li>
</ul>
</li>
</ul>
<h3 id="CV-1"><a href="#CV-1" class="headerlink" title="CV"></a>CV</h3><ul>
<li>CS231n计算机视觉：<ul>
<li><a href="https://zhuanlan.zhihu.com/p/21930884" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21930884</a></li>
<li><a href="https://github.com/mbadry1/CS231n-2017-Summary" target="_blank" rel="noopener">https://github.com/mbadry1/CS231n-2017-Summary</a></li>
</ul>
</li>
</ul>
<h3 id="强化学习-1"><a href="#强化学习-1" class="headerlink" title="强化学习"></a>强化学习</h3><ul>
<li>David Silver强化学习笔记：<a href="https://zhuanlan.zhihu.com/reinforce" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/reinforce</a></li>
</ul>
<h3 id="tensorflow"><a href="#tensorflow" class="headerlink" title="tensorflow"></a>tensorflow</h3><ul>
<li><a href="https://github.com/lyhue1991/eat_tensorFlow2_in_30days" target="_blank" rel="noopener">https://github.com/lyhue1991/eat_tensorFlow2_in_30days</a></li>
<li><a href="https://github.com/ageron/handson-ml2" target="_blank" rel="noopener">https://github.com/ageron/handson-ml2</a></li>
</ul>
<h2 id="工具汇总"><a href="#工具汇总" class="headerlink" title="工具汇总"></a>工具汇总</h2><h3 id="NLP-2"><a href="#NLP-2" class="headerlink" title="NLP"></a>NLP</h3><ul>
<li>jialu:<a href="https://github.com/ownthink/Jiagu" target="_blank" rel="noopener">https://github.com/ownthink/Jiagu</a></li>
</ul>
<h3 id="办公"><a href="#办公" class="headerlink" title="办公"></a>办公</h3><ul>
<li>mac桌面整理：<a href="http://kitestack.com/desktopshelves/" target="_blank" rel="noopener">http://kitestack.com/desktopshelves/</a></li>
</ul>
<h3 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h3><ul>
<li>可视化机器学习模型：<a href="http://setosa.io/ev/" target="_blank" rel="noopener">http://setosa.io/ev/</a></li>
<li>mov转gif：<a href="https://ezgif.com/video-to-gif" target="_blank" rel="noopener">https://ezgif.com/video-to-gif</a></li>
</ul>
<h2 id="数据集汇总"><a href="#数据集汇总" class="headerlink" title="数据集汇总"></a>数据集汇总</h2><h3 id="NLP-3"><a href="#NLP-3" class="headerlink" title="NLP"></a>NLP</h3><ul>
<li>百度：<a href="http://ai.baidu.com/broad" target="_blank" rel="noopener">http://ai.baidu.com/broad</a></li>
</ul>
<h2 id="书目汇总"><a href="#书目汇总" class="headerlink" title="书目汇总"></a>书目汇总</h2><h3 id="机器学习-2"><a href="#机器学习-2" class="headerlink" title="机器学习"></a>机器学习</h3><ul>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649444696&amp;idx=1&amp;sn=a6ed7ef98dc972e93a9efe77cc203874&amp;chksm=82c0b8dcb5b731ca62a9dd77c465cfedf3f1aa077e7f1ba3c06bb2c85a9f56184567eddb9f9a&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649444696&amp;idx=1&amp;sn=a6ed7ef98dc972e93a9efe77cc203874&amp;chksm=82c0b8dcb5b731ca62a9dd77c465cfedf3f1aa077e7f1ba3c06bb2c85a9f56184567eddb9f9a&amp;scene=21#wechat_redirect</a></li>
<li>漫画：<a href="http://ct.vpan123.com/d/17400359.html" target="_blank" rel="noopener">http://ct.vpan123.com/d/17400359.html</a></li>
<li>机器学习实战：<a href="https://www.ituring.com.cn/book/1021" target="_blank" rel="noopener">https://www.ituring.com.cn/book/1021</a></li>
</ul>
<h3 id="NLP-4"><a href="#NLP-4" class="headerlink" title="NLP"></a>NLP</h3><ul>
<li>自然语言处理入门</li>
<li>Neural Network Methods in Natural Language Processing (Synthesis Lectures on Human Language Technologies)</li>
<li>Speech and Language Processing：<a href="https://web.stanford.edu/~jurafsky/slp3/" target="_blank" rel="noopener">https://web.stanford.edu/~jurafsky/slp3/</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649446010&amp;idx=1&amp;sn=253dfd8066e7b277adddfe177e3207ca&amp;chksm=82c0bffeb5b736e81886c17c63441e5734c86e321c1214550c0717082b481f785d27bee81561&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649446010&amp;idx=1&amp;sn=253dfd8066e7b277adddfe177e3207ca&amp;chksm=82c0bffeb5b736e81886c17c63441e5734c86e321c1214550c0717082b481f785d27bee81561&amp;scene=21#wechat_redirect</a></li>
</ul>
<h3 id="语音"><a href="#语音" class="headerlink" title="语音"></a>语音</h3><ul>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649446824&amp;idx=1&amp;sn=91b9043dbb0f746a76ab2da89c940757&amp;chksm=82c0b02cb5b7393a07694503c10c332b51b1914dac2daeb5ab8357de31591b3392300aaee3d9&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">ht</a><a href="https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649446824&amp;idx=1&amp;sn=91b9043dbb0f746a76ab2da89c940757&amp;chksm=82c0b02cb5b7393a07694503c10c332b51b1914dac2daeb5ab8357de31591b3392300aaee3d9&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">tps://mp.weixin.qq.com/s?_</a><a href="https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649446824&amp;idx=1&amp;sn=91b9043dbb0f746a76ab2da89c940757&amp;chksm=82c0b02cb5b7393a07694503c10c332b51b1914dac2daeb5ab8357de31591b3392300aaee3d9&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">_</a><a href="https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649446824&amp;idx=1&amp;sn=91b9043dbb0f746a76ab2da89c940757&amp;chksm=82c0b02cb5b7393a07694503c10c332b51b1914dac2daeb5ab8357de31591b3392300aaee3d9&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">biz</a><a href="https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649446824&amp;idx=1&amp;sn=91b9043dbb0f746a76ab2da89c940757&amp;chksm=82c0b02cb5b7393a07694503c10c332b51b1914dac2daeb5ab8357de31591b3392300aaee3d9&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">=MzA</a><a href="https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649446824&amp;idx=1&amp;sn=91b9043dbb0f746a76ab2da89c940757&amp;chksm=82c0b02cb5b7393a07694503c10c332b51b1914dac2daeb5ab8357de31591b3392300aaee3d9&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">wMTA3M</a><a href="https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649446824&amp;idx=1&amp;sn=91b9043dbb0f746a76ab2da89c940757&amp;chksm=82c0b02cb5b7393a07694503c10c332b51b1914dac2daeb5ab8357de31591b3392300aaee3d9&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">zM</a><a href="https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649446824&amp;idx=1&amp;sn=91b9043dbb0f746a76ab2da89c940757&amp;chksm=82c0b02cb5b7393a07694503c10c332b51b1914dac2daeb5ab8357de31591b3392300aaee3d9&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">4Nw==&amp;mid=26</a><a href="https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649446824&amp;idx=1&amp;sn=91b9043dbb0f746a76ab2da89c940757&amp;chksm=82c0b02cb5b7393a07694503c10c332b51b1914dac2daeb5ab8357de31591b3392300aaee3d9&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">4</a><a href="https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649446824&amp;idx=1&amp;sn=91b9043dbb0f746a76ab2da89c940757&amp;chksm=82c0b02cb5b7393a07694503c10c332b51b1914dac2daeb5ab8357de31591b3392300aaee3d9&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">9446</a><a href="https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649446824&amp;idx=1&amp;sn=91b9043dbb0f746a76ab2da89c940757&amp;chksm=82c0b02cb5b7393a07694503c10c332b51b1914dac2daeb5ab8357de31591b3392300aaee3d9&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">824&amp;i</a><a href="https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649446824&amp;idx=1&amp;sn=91b9043dbb0f746a76ab2da89c940757&amp;chksm=82c0b02cb5b7393a07694503c10c332b51b1914dac2daeb5ab8357de31591b3392300aaee3d9&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">dx=1</a><a href="https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649446824&amp;idx=1&amp;sn=91b9043dbb0f746a76ab2da89c940757&amp;chksm=82c0b02cb5b7393a07694503c10c332b51b1914dac2daeb5ab8357de31591b3392300aaee3d9&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">&amp;s</a><a href="https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649446824&amp;idx=1&amp;sn=91b9043dbb0f746a76ab2da89c940757&amp;chksm=82c0b02cb5b7393a07694503c10c332b51b1914dac2daeb5ab8357de31591b3392300aaee3d9&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">n=91b9043dbb0f746a7</a><a href="https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649446824&amp;idx=1&amp;sn=91b9043dbb0f746a76ab2da89c940757&amp;chksm=82c0b02cb5b7393a07694503c10c332b51b1914dac2daeb5ab8357de31591b3392300aaee3d9&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">6ab2</a><a href="https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649446824&amp;idx=1&amp;sn=91b9043dbb0f746a76ab2da89c940757&amp;chksm=82c0b02cb5b7393a07694503c10c332b51b1914dac2daeb5ab8357de31591b3392300aaee3d9&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">da</a><a href="https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649446824&amp;idx=1&amp;sn=91b9043dbb0f746a76ab2da89c940757&amp;chksm=82c0b02cb5b7393a07694503c10c332b51b1914dac2daeb5ab8357de31591b3392300aaee3d9&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">89c9</a><a href="https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649446824&amp;idx=1&amp;sn=91b9043dbb0f746a76ab2da89c940757&amp;chksm=82c0b02cb5b7393a07694503c10c332b51b1914dac2daeb5ab8357de31591b3392300aaee3d9&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">40757</a><a href="https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649446824&amp;idx=1&amp;sn=91b9043dbb0f746a76ab2da89c940757&amp;chksm=82c0b02cb5b7393a07694503c10c332b51b1914dac2daeb5ab8357de31591b3392300aaee3d9&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">&amp;chksm=82c0b02c</a><a href="https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649446824&amp;idx=1&amp;sn=91b9043dbb0f746a76ab2da89c940757&amp;chksm=82c0b02cb5b7393a07694503c10c332b51b1914dac2daeb5ab8357de31591b3392300aaee3d9&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">b5b73</a><a href="https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649446824&amp;idx=1&amp;sn=91b9043dbb0f746a76ab2da89c940757&amp;chksm=82c0b02cb5b7393a07694503c10c332b51b1914dac2daeb5ab8357de31591b3392300aaee3d9&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">93a07694503c</a><a href="https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649446824&amp;idx=1&amp;sn=91b9043dbb0f746a76ab2da89c940757&amp;chksm=82c0b02cb5b7393a07694503c10c332b51b1914dac2daeb5ab8357de31591b3392300aaee3d9&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">10c332b51b1914dac2daeb5ab83</a><a href="https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649446824&amp;idx=1&amp;sn=91b9043dbb0f746a76ab2da89c940757&amp;chksm=82c0b02cb5b7393a07694503c10c332b51b1914dac2daeb5ab8357de31591b3392300aaee3d9&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">57de</a><a href="https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649446824&amp;idx=1&amp;sn=91b9043dbb0f746a76ab2da89c940757&amp;chksm=82c0b02cb5b7393a07694503c10c332b51b1914dac2daeb5ab8357de31591b3392300aaee3d9&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">31591b3392300aaee3</a><a href="https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649446824&amp;idx=1&amp;sn=91b9043dbb0f746a76ab2da89c940757&amp;chksm=82c0b02cb5b7393a07694503c10c332b51b1914dac2daeb5ab8357de31591b3392300aaee3d9&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">d9&amp;scene=21#wech</a><a href="https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649446824&amp;idx=1&amp;sn=91b9043dbb0f746a76ab2da89c940757&amp;chksm=82c0b02cb5b7393a07694503c10c332b51b1914dac2daeb5ab8357de31591b3392300aaee3d9&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">at_redir</a><a href="https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649446824&amp;idx=1&amp;sn=91b9043dbb0f746a76ab2da89c940757&amp;chksm=82c0b02cb5b7393a07694503c10c332b51b1914dac2daeb5ab8357de31591b3392300aaee3d9&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">ec</a><a href="https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649446824&amp;idx=1&amp;sn=91b9043dbb0f746a76ab2da89c940757&amp;chksm=82c0b02cb5b7393a07694503c10c332b51b1914dac2daeb5ab8357de31591b3392300aaee3d9&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">t</a></li>
</ul>
<h3 id="图像"><a href="#图像" class="headerlink" title="图像"></a>图像</h3><ul>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649442925&amp;idx=1&amp;sn=aaeb7baba440d9a3d56c9367c38f0fd1&amp;chksm=82c0a3e9b5b72aff8d465d72e194d782c427c2d8cbe4f7cdeb9b5e5c960e833fa7a176c55ea4&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649442925&amp;idx=1&amp;sn=aaeb7baba440d9a3d56c9367c38f0fd1&amp;chksm=82c0a3e9b5b72aff8d465d72e194d782c427c2d8cbe4f7cdeb9b5e5c960e833fa7a176c55ea4&amp;scene=21#wechat_redirect</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649447181&amp;idx=1&amp;sn=cf784c0809738e76e91fa0e552dc7c23&amp;chksm=82c0b289b5b73b9f55b4f7284de23e8f8df9b011fa487fa8160a1546e8619f4a50b902fb8416&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649447181&amp;idx=1&amp;sn=cf784c0809738e76e91fa0e552dc7c23&amp;chksm=82c0b289b5b73b9f55b4f7284de23e8f8df9b011fa487fa8160a1546e8619f4a50b902fb8416&amp;scene=21#wechat_redirect</a></li>
</ul>
<h3 id="编程"><a href="#编程" class="headerlink" title="编程"></a>编程</h3><ul>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649448675&amp;idx=1&amp;sn=070b71e8be06a74486b13e921a6c826a&amp;chksm=82c08967b5b7007156a77e572018f712b016ae7bd9c0c247071c8b37edaeb10f0419fa72bad3&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&amp;mid=2649448675&amp;idx=1&amp;sn=070b71e8be06a74486b13e921a6c826a&amp;chksm=82c08967b5b7007156a77e572018f712b016ae7bd9c0c247071c8b37edaeb10f0419fa72bad3&amp;scene=21#wechat_redirect</a></li>
</ul>
<h3 id="综合"><a href="#综合" class="headerlink" title="综合"></a>综合</h3><ul>
<li>《分布式机器学习：算法、理论与实践》：理论、方法与实践的全面汇总</li>
</ul>
<h3 id="人文"><a href="#人文" class="headerlink" title="人文"></a>人文</h3><ul>
<li>《美国式婚姻》</li>
<li>《这些真相》</li>
<li>《准备好》</li>
<li>《为什么要睡觉》</li>
<li>《增长》</li>
</ul>
<h2 id="前沿追踪类资源"><a href="#前沿追踪类资源" class="headerlink" title="前沿追踪类资源"></a>前沿追踪类资源</h2><h3 id="NLP-5"><a href="#NLP-5" class="headerlink" title="NLP"></a>NLP</h3><ul>
<li><a href="https://github.com/sebastianruder/NLP-progress" target="_blank" rel="noopener">https://github.com/sebastianruder/NLP-progress</a></li>
</ul>
<h2 id="竞赛"><a href="#竞赛" class="headerlink" title="竞赛"></a>竞赛</h2><ul>
<li><a href="https://www.flyai.com/" target="_blank" rel="noopener">https://www.flyai.com/</a></li>
</ul>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ul>
<li>techflow资料分享：<a href="https://shimo.im/docs/JxvxtD6hcCRxXKpy" target="_blank" rel="noopener">https://shimo.im/docs/JxvxtD6hcCRxXKpy</a></li>
<li>C++学习：RAII SFINAE</li>
</ul>
<h1 id="画画"><a href="#画画" class="headerlink" title="画画"></a>画画</h1><ul>
<li><p>[<a href="https://mp.weixin.qq.com/s?__biz=MzA3MzA0MzIzMA==&amp;mid=2652074652&amp;idx=1&amp;sn=5efc72f5323ef6934016d9f4530069b2&amp;chksm=84f2ceedb38547fbae0c5ee3d3b53a2a4735504287981f2b8cb03f66dd409665f4e10c7819e5&amp;scene=0&amp;xtrack=1&amp;key=503aabc154a85e8386e52704d184a74c37628925dd7c230cde894040dc6ad84ccbd9bb4b9b4773798bee5d1ba56bfa31e87f08a881de313da452d1a888c36677e60cf6d8544e2f13ab3f50e6533b99ae&amp;ascene=0&amp;uin=NjI1NjAyODg0&amp;devicetype=iMac+MacBookPro12%2C1+OSX+OSX+10.12.6+build(16G2136)&amp;version=12020610&amp;nettype=WIFI&amp;lang=zh_CN&amp;fontScale=100&amp;exportkey=AXzqtd%2Fuy8f1J431fjIkjew%3D&amp;pass_ticket=dqWuD%2FfbuXrH7NuxVzcXiLbcebIgYSleEdJ%2BaeSxOW3oTBcjhs9oYfCg1nuYC4Z7](" target="_blank" rel="noopener">https://mp.weixin.qq.com/s?__biz=MzA3MzA0MzIzMA==&amp;mid=2652074652&amp;idx=1&amp;sn=5efc72f5323ef6934016d9f4530069b2&amp;chksm=84f2ceedb38547fbae0c5ee3d3b53a2a4735504287981f2b8cb03f66dd409665f4e10c7819e5&amp;scene=0&amp;xtrack=1&amp;key=503aabc154a85e8386e52704d184a74c37628925dd7c230cde894040dc6ad84ccbd9bb4b9b4773798bee5d1ba56bfa31e87f08a881de313da452d1a888c36677e60cf6d8544e2f13ab3f50e6533b99ae&amp;ascene=0&amp;uin=NjI1NjAyODg0&amp;devicetype=iMac+MacBookPro12%2C1+OSX+OSX+10.12.6+build(16G2136)&amp;version=12020610&amp;nettype=WIFI&amp;lang=zh_CN&amp;fontScale=100&amp;exportkey=AXzqtd%2Fuy8f1J431fjIkjew%3D&amp;pass_ticket=dqWuD%2FfbuXrH7NuxVzcXiLbcebIgYSleEdJ%2BaeSxOW3oTBcjhs9oYfCg1nuYC4Z7](</a></p>
</li>
<li><p><a href="https://github.com/sebastianruder/NLP-progress" target="_blank" rel="noopener">https://github.com/sebastianruder/NLP-progress</a>)</p>
</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>稀疏序列到序列模型</title>
    <url>/2019/11/15/%E7%A8%80%E7%96%8F%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p>ACL 2019论文 <a href="https://arxiv.org/pdf/1905.05702.pdf" target="_blank" rel="noopener">Sparse Sequence-to-Sequence Models</a>，代码实现 <a href="https://github.com/deep-spin/entmax" target="_blank" rel="noopener">entmax</a>。</p>
<a id="more"></a>
<h1 id="稀疏序列到序列模型"><a href="#稀疏序列到序列模型" class="headerlink" title="稀疏序列到序列模型"></a>稀疏序列到序列模型</h1><h2 id="论文概述"><a href="#论文概述" class="headerlink" title="论文概述"></a>论文概述</h2><p>（一）直观看论文的工作：修改了softmax计算方式，变成了稀疏的softmax，使得在某一阈值下的数值都变成了0。<br><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/sparse_seq2seq/1.png" alt="softmax"></p>
<p>（二）本文中稀疏的softmax用在了下面两个地方：</p>
<ul>
<li>seq2seq在解码输出层的softmax</li>
<li>attention中的softmax。</li>
</ul>
<p>（三）为什么要用稀疏softmax？</p>
<ul>
<li><p>从多标签分类角度：稀疏化Softmax的输出能更适合处理多标签问题。这里首先明确两个概念：多标签分类（multi-label classification）问题和多分类（multi-class classification）问题。多标签分类问题面对的实例可以属于多个标签，例如，一部电影可以是“韩国”、“动作”、“惊悚”。机器翻译的解码阶段，我也认为是个多标签分类问题，因为同一个单词在上下文语境的翻译也可以是多个。而多分类问题指的是一个实例有且仅属于多个分类中的一个。例如，手写数字识别，一张图片对应0～9中唯一一个数字。根据Softmax函数的形式化定义可知，Softmax函数更适合定义多分类问题。那么，为了转换Softmax函数的形式，使之适合多标签分类问题，就产生了sparsemax。</p>
</li>
<li><p>从Attention角度：Attention的关键是映射函数，它对输入元素的相对重要性进行编码，将数值映射为概率。而常用的关注机制Softmax会产生密集的注意力。因为softmax的输出都大于0，故其输入的所有元素对最终决策都有或多或少的影响。</p>
</li>
<li><p>实际工作中遇到的问题：两个语句的语义对齐，出现不该对齐的部分也对齐了的情况（参考上面图）</p>
</li>
</ul>
<h2 id="前人工作介绍"><a href="#前人工作介绍" class="headerlink" title="前人工作介绍"></a>前人工作介绍</h2><p>为了克服softmax这种缺点，Martins发表的论文 <a href="https://arxiv.org/pdf/1602.02068.pdf" target="_blank" rel="noopener">From Softmax to Sparsemax:A Sparse Model of Attention and Multi-Label Classification</a> 提出了能够输出稀疏概率的Sparsemax，这能为Attention提供更多的解释性。</p>
<h3 id="sparsemax的定义"><a href="#sparsemax的定义" class="headerlink" title="sparsemax的定义"></a>sparsemax的定义</h3><ul>
<li><p>假设 $\Delta d :=\{p\in \mathbb{R}^d: p\geqslant 0, ||p||$ 表示维度为d的概率向量，softmax可以看成是从$\mathbb{R}^d$到$\Delta d$的映射中的一个。则sparsemax可以定义成另一种形式的softmax，其特点是产生稀疏的概率分布：$sparsemax(z) := argmin_{p\in \Delta ^d} ||p-z||^2$。可以看到它的预测的分布满足：$p^* := sparsemax(z)$，很容易将低分数的元素置为0。</p>
<ul>
<li>对比softmax形式$softmax(z)=exp(z_i)/\sum _j(exp(z_j))$，首先Sparsemax的目标是直接逼近真实的多标签分类分布，另外Sparsemax不再由指数函数对输出z做smooth变换。采用直接将输出z投影到单纯形的方式能够起到输出的稀疏化效果</li>
</ul>
</li>
<li><p>sparsemax的物理意义等价于将输出z投影到单纯形上，投影的方式为通过欧式距离找到距离单纯形上最近的一点，其投影点为p。由于单纯形的限制，导致这个投影点会大概率（相对于d维空间单纯形很小）落在单纯形的边界上，从而导致了稀疏化的效果。例如，在下图的例子中，当2维空间中的投影点落在单纯形的边界上时即形成了稀疏化的输出结果。</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/sparse_seq2seq/2.png" alt="图片"></p>
</li>
</ul>
<h3 id="sparsemax的求解"><a href="#sparsemax的求解" class="headerlink" title="sparsemax的求解"></a>sparsemax的求解</h3><ul>
<li><p>Sparsemax的函数形式在真实分布未知的情况下并不能直接求解。通过拉格朗日法得到它的对偶形式：$L(z, \mu , \tau ) = \frac{1}{2}||p-z||^2-\mu^Tp+\tau (1^Tp-1)$，求解得：</p>
<ul>
<li><p>$Sparsemax(z) = [z_i- \tau(z)], s.t. \sum_j[z_j-\tau(z)]_+=1$</p>
</li>
<li><p>$[t]_+=max\{0,t\}$</p>
</li>
<li><p>$\tau(z)=(\sum_{j\leq k(z)}z(j)-1)/k(z)$</p>
</li>
<li><p>$k(z)=max\{k\in[K]|\sum_{j\leq k}z(j) &lt; 1+kz_{(k)}\}$</p>
</li>
<li><p>$z_{(1)} \geq z_{(2)} \geq …\geq z_{(k)}$</p>
<p>其中[K]表示K个标签对应的下标集合。</p>
</li>
</ul>
</li>
<li><p>用语言形式描述Sparsemax的求解过程：</p>
<ul>
<li>将输出z按值降序排列</li>
<li>按序比较条件$\sum_{j\leq k}z(j) &lt; 1+kz_{(k)}$，若不满足则退出，求得满足条件的输出数k(z)</li>
<li><p>计算$\tau(z)$并得到sparsemax</p>
<p>这里的$\tau(z)$也被称作关于z的阈值函数。通过在2维和3维空间上的示意图可以看到Sparsemax和Softmax的基本性质基本相同，在Sparsemax上基本保持了Softmax上的序关系，不同的是Sparsemax的输出不再平滑，头尾都被“截断”了。</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/sparse_seq2seq/3.png" alt="softmax_sparsemax"></p>
</li>
</ul>
</li>
</ul>
<h3 id="sparsemax的损失函数和梯度"><a href="#sparsemax的损失函数和梯度" class="headerlink" title="sparsemax的损失函数和梯度"></a>sparsemax的损失函数和梯度</h3><p>[注]：以下内容在Martins论文中和本篇论文中公式的形式不一样，下面的公式都是本篇论文中给出的。</p>
<ul>
<li><p>sparsemax的损失函数可以定义为：$L_{sparsemax}(y,z):=\frac{1}{2}(||e_y-z||^2-||p^*-z||^2)$，其中$e_y:=[0,…,1,0,…,0]$是真实标签。</p>
<ul>
<li>Sparsemax的损失函数是一个处处可导的凸函数。</li>
<li>对比Softmax，Softmax函数的负对数似然损失为$L_{softmax}=-\sum_{i=1}^My_ilog \frac{e^{zi}}{\sum _ke^{zk}}$</li>
<li>Sparsemax函数由于存在截断点，由于截断点的梯度不连续，有在截断点处的梯度不连续，被截断部分（值为0）的对数似然无法计算等问题，导致Sparsemax无法计算。因此文章提出了处处可导的凸函数。</li>
<li>$L_{sparsemax}(y,z)$在z上是连续可微，且存在性质：$L_{sparsemax}$ 为0，当且仅当对任意$y\neq y’$，有$z_y \geqslant z_{y’} +1$。（即有一个margin，可参考：<a href="https://zhuanlan.zhihu.com/p/52108088" target="_blank" rel="noopener">Softmax理解之margin</a>）</li>
</ul>
</li>
<li><p>训练模型使用sparsemax的loss时，其梯度为：$\bigtriangledown _zL_{sparsemax}(y,z)=-e_y+p^*$。</p>
</li>
<li><p>对这个梯度公式的直观理解：用到的部分回传梯度,没用到的部分梯度为0</p>
</li>
<li><p>将sparsemax运用于attention机制中时，可以发现sparsemax在任意处都是可导的（Martins论文中已证）：</p>
<p>  $\frac{\partial sparsemax(z)}{\partial z}=diag(s)-\frac{1}{||s||_1}ss^T$，</p>
<p>  其中当$0 &lt; p^*_j$时,$s_j=1$，否则$s_j=0$。</p>
</li>
</ul>
<h2 id="本文主要贡献"><a href="#本文主要贡献" class="headerlink" title="本文主要贡献"></a>本文主要贡献</h2><h3 id="从熵的角度定义了一簇sparsemax"><a href="#从熵的角度定义了一簇sparsemax" class="headerlink" title="从熵的角度定义了一簇sparsemax"></a>从熵的角度定义了一簇sparsemax</h3><h4 id="从“熵”角度解释sparsemax"><a href="#从“熵”角度解释sparsemax" class="headerlink" title="从“熵”角度解释sparsemax"></a>从“熵”角度解释sparsemax</h4><ul>
<li><p>为了理解sparsemax，我们可以从另一个角度理解softmax:<br>$softmax(z) = argmax_{p\in \Delta d}(p^Tz) + H^S(p)$</p>
<p>$H^S (p) :=\sum _jp_jlogp_j$</p>
<p>后者是著名的Gibbs-Boltzmann-Shannon entropy。</p>
</li>
<li><p>同样的去理解sparsemax:</p>
<p>$sparsemax(z) = argmax_{p\in \Delta d}(p^Tz) + H^G(p)$</p>
<p>$H^G (p) :=\frac{1}{2}\sum _jp_jlog(1-p_j$</p>
<p>后者是Gini entropy。</p>
</li>
</ul>
<p>因此，softmax和sparsemax只是在entropy的正则项上不同。</p>
<h4 id="通过定义的一系列的熵Tsallis"><a href="#通过定义的一系列的熵Tsallis" class="headerlink" title="通过定义的一系列的熵Tsallis"></a>通过<img src="http://latex.codecogs.com/gif.latex?\alpha"/>定义的一系列的熵Tsallis</h4><h5 id="Tsallis-alpha-entropies-："><a href="#Tsallis-alpha-entropies-：" class="headerlink" title="$Tsallis \alpha -entropies$："></a>$Tsallis \alpha -entropies$：</h5><p>$H^T_\alpha (p) := \left\{\begin{matrix}<br>\frac{1}{\alpha (\alpha -1)}\sum _j(p_j-p^\alpha _j) &amp; \alpha \neq 1\ <br>H^S(p) &amp;  \alpha= 1<br>\end{matrix}\right.$</p>
<p>这个熵家族是连续的：对于任意$p \in \Delta d$，有：</p>
<p>$lim_{\alpha \rightarrow 1}H^T_\alpha (p)=H^S(p)$，</p>
<p>$H^T_2\equiv H^G$</p>
<p>也就是说Tsallis熵在Shannon和Gini之间。</p>
<h4 id="由Tsallis熵定义entmax"><a href="#由Tsallis熵定义entmax" class="headerlink" title="由Tsallis熵定义entmax"></a>由Tsallis熵定义entmax</h4><ul>
<li><p>entmax定义：</p>
<p>$\alpha-entmax(z):=argmax_{p\in\Delta^d}p^Tz + H^T_\alpha (p)$</p>
<p>$p^* := \alpha-entmax(z)$</p>
</li>
<li><p>损失函数：</p>
<p>$L_\alpha (y,z):=(p^<em>-e_y)^Tz+H^T_\alpha (p^</em>)$</p>
</li>
<li><p>其梯度：</p>
</li>
</ul>
<p>$\bigtriangledown _zL_\alpha (y,z)=-e_y+p^*$</p>
<p>可以知道，$1-entmax \equiv  softmax$，$2-entmax \equiv  sparsemax$，$L_1$是负对数似然，$L_2$是sparsemax loss。对于任意$1 &lt; \alpha$ entmax可以产生稀疏的概率分布，如下图：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/sparse_seq2seq/4.png" alt="fig3"></p>
<ul>
<li>Tsallis熵损失函数是可导的，且有一个性质：当正确分类的得分比其他错误分类的得分多出$\frac{1}{\alpha -1}$以上时，损失变成是0。这对于seq2seq模型来说，解码器在非常容易进行解码的时间步上，会作出非常确定的预测，当有若干个可能的候选时，会有稀疏的不确定性。</li>
</ul>
<h3 id="计算-p-alpha-entmax-z"><a href="#计算-p-alpha-entmax-z" class="headerlink" title="计算 $p^* = \alpha -entmax(z)$"></a>计算 $p^* = \alpha -entmax(z)$</h3><ul>
<li><p>对于$\alpha=1$，有：$softmax(z)_j:=\frac{exp(z_j)}{\sum _iexp(z_i)}$。</p>
</li>
<li><p>对于$1&lt;\alpha&lt;2$，有：$\alpha -entmax(z)=[(\alpha -1)z-\tau 1]^{1/\alpha -1}_+$(对于$z_j\leq \tau / \alpha -1$的分数为0)</p>
</li>
<li><p>对于$\alpha=2$，则是在$\Delta ^d$上的欧几里得投影。</p>
<p>  可以使用两种算法计算<img src="http://latex.codecogs.com/gif.latex?\tau"/>：</p>
<ul>
<li><p>基于排序</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/sparse_seq2seq/5.png" alt="algo2"></p>
<p>可参考<a href="https://github.com/deep-spin/entmax/blob/master/entmax/root_finding.py" target="_blank" rel="noopener">root_finding</a>中第79～129行。</p>
<ul>
<li>基于分治法</li>
</ul>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/sparse_seq2seq/6.png" alt="algo1"></p>
<p>可参考<a href="https://github.com/deep-spin/entmax/blob/master/entmax/activations.py" target="_blank" rel="noopener">activations</a>中第214～244行</p>
</li>
</ul>
</li>
</ul>
<h2 id="实验结论"><a href="#实验结论" class="headerlink" title="实验结论"></a>实验结论</h2><h3 id="单词变形任务上的准确率（多语言）"><a href="#单词变形任务上的准确率（多语言）" class="headerlink" title="单词变形任务上的准确率（多语言）"></a>单词变形任务上的准确率（多语言）</h3><p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/sparse_seq2seq/7.png" alt="exp1"></p>
<h3 id="英德翻译任务上的BLEU"><a href="#英德翻译任务上的BLEU" class="headerlink" title="英德翻译任务上的BLEU"></a>英德翻译任务上的BLEU</h3><p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/sparse_seq2seq/8.png" alt="exp2"></p>
<h3 id="英德翻译任务上不同的产生的稀疏性"><a href="#英德翻译任务上不同的产生的稀疏性" class="headerlink" title="英德翻译任务上不同的产生的稀疏性"></a>英德翻译任务上不同的<img src="http://latex.codecogs.com/gif.latex?\alpha "/>产生的稀疏性</h3><p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/sparse_seq2seq/9.png" alt="exp3"></p>
<p>相比于softmax，1.5-entmax有更少的输出但有更高的BLEU，sparsemax虽然输出更稀疏，但BLEU指标不如1.5-entmax。<img src="http://latex.codecogs.com/gif.latex?\alpha "/>在attention和softmax输出中不同的取值，对于验证集准确率的影响可参考下图：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/sparse_seq2seq/10.png" alt="exp4"></p>
<p>在翻译任务上用1.5-entmax产生的attention可视化如下：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/sparse_seq2seq/11.png" alt="exp5"></p>
<p>在翻译任务上，softmax和1.5-entmax在训练时间上的比较：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/sparse_seq2seq/12.png" alt="exp6"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>目前attention受到关注，产生的很多稀疏的attention的计算方法，本文直接改变了softmax的计算方式，使其归一化的结果更为稀疏，对未来的研究有参考价值。</p>
]]></content>
      <tags>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>语言模型之XLNET</title>
    <url>/2019/09/14/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B9%8BXLNET/</url>
    <content><![CDATA[<p>ACL 2019论文 <a href="https://arxiv.org/pdf/1906.08237.pdf" target="_blank" rel="noopener">XLNet: Generalized Autoregressive Pretraining for Language Understanding</a>阅读笔记。</p>
<a id="more"></a>
<h1 id="语言模型之XLNET"><a href="#语言模型之XLNET" class="headerlink" title="语言模型之XLNET"></a>语言模型之XLNET</h1><h2 id="论文概述"><a href="#论文概述" class="headerlink" title="论文概述"></a>论文概述</h2><p>语言模型有两种：自回归语言模型（autoregressive）、自编码语言模型（autoencoding）。自回归语言模型的缺陷是只能是单向的（elmo实现了双向的编码，但只是在token之间建立了浅层的关联）。自编码语言模型（Bert）的缺陷是忽略了Mask的token之间的关联。本文提出的XLNET模型很好的结合了自回归和自编码语言模型，将文本的token按照随机出来的全排列顺序进行生成，在生成过程中使用了两种attention，很好的加入了预测单词的位置信息，弥补了Bert的缺陷。在XLNET模型中借鉴了Transformer-XL的相对位置编码和Memory机制，减少了计算量，提高了速度。</p>
<h2 id="前人工作介绍"><a href="#前人工作介绍" class="headerlink" title="前人工作介绍"></a>前人工作介绍</h2><h3 id="自回归语言模型"><a href="#自回归语言模型" class="headerlink" title="自回归语言模型"></a>自回归语言模型</h3><p>对于序列$x=(x_1…x_T)$，根据$(x_1…x_{i-1})$预测$x_i$，称为AR语言模型，比如GPT，ELMO等。AR的缺点在于序列要么从前往后，要么从后往前，无法将上文和下文信息完全结合起来。</p>
<h3 id="自编码语言模型"><a href="#自编码语言模型" class="headerlink" title="自编码语言模型"></a>自编码语言模型</h3><p>对于序列$x=(x_1…x_T)$，编码为$x=(y_1…y_T)$，成为AE语言模型，比如Bert。AE模型的缺点在于Pre-train阶段可能需要引入 [Mask] 标记（Bert模型），而 [Mask] 会带来一系列问题。</p>
<p>以序列[New, York, is, a, city]为例，Bert如果随机选择了[New]和[York]进行mask并预测，则mask后序列变为: [[Mask], [Mask], is, a, city]。比较好的优化函数应该优化的目标是：J=logp(New York | is a city)，但Bert实际优化的目标是J=logp(New | is a city) + logp(York | is a city)，此时p(New York | is a city)=p(New | is a city) * p(York | is a city)，即New和York相互独立。而实际上如果前面出现了“New”，那么后面出现“York”的概率理应大很多。</p>
<p>而且对于Bert而言，在在pretrain阶段时需要对语料进行Mask标记，但在finetune阶段没有Mask标记，数据分布不一致会影响finetune的效果 。</p>
<h2 id="XLNET模型"><a href="#XLNET模型" class="headerlink" title="XLNET模型"></a>XLNET模型</h2><h3 id="基于排序构造的语言模型"><a href="#基于排序构造的语言模型" class="headerlink" title="基于排序构造的语言模型"></a>基于排序构造的语言模型</h3><p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/xlnet/1.png" alt="fig1"></p>
<p>在训练过程中，XLNET对输入序列进行了排列组合，比如2-&gt;4-&gt;3-&gt;1的排序中，对位置3的预测，能够既考虑前文1，也考虑后文4。在计算时，XLNET使用了矩阵的Mask操作来实现排列组合。因此，XLNET能弥补Bert忽略Mask的词内部关联的问题。</p>
<p>XLNET在训练时，为了减少开销，对于序列1-&gt;2-&gt;3-&gt;4, 取位置c，只对$\boldsymbol{z}_{&gt;\boldsymbol{c}}$进行训练。c的取值通过一个超参数K设置，满足$|\boldsymbol{z}| /(|\boldsymbol{z}|-c) \approx K$。</p>
<h3 id="双流注意力"><a href="#双流注意力" class="headerlink" title="双流注意力"></a>双流注意力</h3><p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/xlnet/2.png" alt="fig2"></p>
<p>（a)表示内容attention(和普通attention一样)，(b)表示Query attention，把目标token的内容信息Mask掉了，(c)表示整个流程。</p>
<p>h和g的计算公式如下：</p>
<ul>
<li>$g_{z_{t}}^{(m)} \leftarrow \text { Attention }\left(\mathbf{Q}=g_{z_{t}}^{(m-1)}, \mathbf{K V}=\mathbf{h}_{\bar{z}_{&lt;t}}^{(m-1)} ; \theta\right)$</li>
<li>$h_{z_{t}}^{(m)} \leftarrow \text { Attention }\left(\mathrm{Q}=h_{z_{t}}^{(m-1)}, \mathrm{KV}={\mathbf{h}}_{z_{\leq}}^{(m-1)} ; \theta\right)$</li>
</ul>
<p>其中$h_{\theta}\left(\boldsymbol{X}_{\boldsymbol{Z}_{\langle t}}\right)$表示$\boldsymbol{X}_{\boldsymbol{Z}_{&lt;t}}$的hidden表示，$g_{\theta}$是为了修改softmax的计算方法：</p>
<p>$p_{\theta}\left(X_{z_{t}}=x \mid x_{z_{&lt;} t}\right)=\frac{\exp \left(e(x)^{\top} g_{\theta}\left(X_{Z_{&lt;t}}, z_{t}\right)\right)}{\sum_{x^{\prime}} \exp \left(e\left(x^{\prime}\right)^{\top} g_{\theta}\left(\mathbf{X}_{z_{&lt;t}}, z_{t}\right)\right)}$</p>
<p>可以看到相比于普通softmax，增加了zt，目的是为了区分不同位置的预测。例如有一个序列，[今天，北京，客流量，很大]，产生排列组合 [今天，北京，客流量] 和 [今天，北京，很大]，如果不增加zt，在预测未知3和4时，产生的概率分布是一致的，不符合逻辑。也正因为如此，XLNET使用了双流注意力，来避免这个问题。</p>
<h3 id="Memory缓存"><a href="#Memory缓存" class="headerlink" title="Memory缓存"></a>Memory缓存</h3><p>对于超长序列，XLNET借鉴了Transformer-XL的缓存机制，并采用相对位置编码。设长序列为[1…T…2T]，会拆分为$\tilde{\boldsymbol{z}}=[1 \ldots T]$和$\boldsymbol{z}=[T \ldots 2 T]$，h的计算公式为：</p>
<p>$h_{z_{t}}^{(m)} \leftarrow \text { Attention }\left(\mathrm{Q}=h_{z_{t}}^{(m-1)}, \mathrm{KV}=\left[\tilde{\mathrm{h}}^{(m-1)}, \mathrm{h}_{\mathrm{z} \leq t}^{(m-1)}\right] ; \theta\right)$</p>
<h2 id="实验结论"><a href="#实验结论" class="headerlink" title="实验结论"></a>实验结论</h2><p>XLNET在GLUE数据集上与其他模型的对比，可以看到有很大提升。</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/xlnet/3.png" alt="fig16"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>XLNet使用了排列组合、双流注意力机制很好地解决了BERT的一些天然缺陷。当然还有很多其他语言模型如ERNIE还有待学习。</p>
]]></content>
      <tags>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：《Uniﬁed Language Model Pre-training for Natural Language Understanding and Generation》</title>
    <url>/2019/07/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E3%80%8AUni%EF%AC%81ed-Language-Model-Pre-training-for-Natural-Language-Understanding-and-Generation%E3%80%8B/</url>
    <content><![CDATA[<p>最近阅读了ULM的论文，在此总结一下。</p>
<a id="more"></a>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>本文提出一个能够同时处理自然语言理解和生成任务UNIﬁed pre-trained Language Model (UNILM) 模型。UNILM模型的预训练是基于3个目标：单向LM(包括从左到右和从右到左)、双向LM和sequence-to-sequence LM。该模型采用一个共享参数的Transformer网络的同时还使用了特定的self-attention masks用以控制预测时候所用到的上下文信息。在下游任务微调时候，可以将UNILM模型视为单向的encoder、双向的encoder或者sequence-to-sequence模型，以适应不同的下游任务（自然语言理解和生成任务）。</p>
<p>在实验过程中，UNILM与BERT模型在在GLUE、SQuAD 2.0和CoQA数据集上进行了综合对比。本文模型在3项自然语言生成任务上刷新了记录，其中包括CNN/DailyMail 摘要生成（ROUGE-L为40.63，提升了2.16）、CoQA的问题生成（F1值为82.5，提升了37.1）、SQuAD的问题生成（BLEU-4为22.88，提升了6.5）。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>预训练语言模型在各个自然语言处理任务上的成果是有目共睹的，在这里就不多说了。预训练语言模型之所以有效是因为在海量语料数据集上学习到能够基于上下文的文本表征信息预测word tokens。在下游的任务中仅需要微调即可。</p>
<p>一般预训练的语言模型有以下4种预训练目标：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ulm/1.png" alt="图片"></p>
<h2 id="单-双向语言模型"><a href="#单-双向语言模型" class="headerlink" title="单/双向语言模型"></a>单/双向语言模型</h2><p>单向语言模型相当于把训练语料通过下述条件概率分布的方式“记住”了：$p\left(x_{1}, x_{2}, x_{3}, \ldots, x_{n}\right)=p\left(x_{1}\right) p\left(x_{2} \mid x_{1}\right) p\left(x_{3} \mid x_{1}, x_{2}\right) \ldots p\left(x_{n} \mid x_{1}, \ldots, x_{n-1}\right)$</p>
<p>我们一般说的“语言模型”，就是指单向的（更狭义的只是指正向的）语言模型。语言模型的关键点是要防止看到“未来信息”。如上式，预测x1的时候，是没有任何外部输入的；而预测x2的时候，只能输入x1，预测x3的时候，只能输入x1,x2；依此类推。如下图所示：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ulm/2.png" alt="图片"></p>
<p>比如ELMo是基于LSTM的两个单向语言模型的拼接：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ulm/3.png" alt="图片"></p>
<p>比如GPT预测文本序列是基于Transformer从左到右逐个word预测：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ulm/4.png" alt="图片"></p>
<p>BERT使用双向的Transformer encoder，融合双向的上下文信息，从而预测被masked掉的words：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ulm/5.png" alt="图片"></p>
<p>此外，BERT能够确切地对一组文本对的关系进行建模，这已经被证明是对许多成对的自然语言理解任务（如自然语言推理）是有益的。尽管BERT在多个自然语言理解任务上提升显著，但是其双向的天然特性，使其在自然语言生成任务上困难重重。</p>
<h2 id="乱序语言模型"><a href="#乱序语言模型" class="headerlink" title="乱序语言模型"></a>乱序语言模型</h2><p>乱序语言模型是XLNet提出来的概念，它主要用于XLNet的预训练上。它要建模的是如下概率：</p>
<p>$\begin{aligned}<br>p\left(x_{1}, x_{2}, x_{3}, \ldots, x_{n}\right) &amp;=p\left(x_{1}\right) p\left(x_{2} \mid x_{1}\right) p\left(x_{3} \mid x_{1}, x_{2}\right) \ldots p\left(x_{n} \mid x_{1}, \ldots, x_{n}\right) \\<br>&amp;=p\left(x_{3}\right) p\left(x_{1} \mid x_{3}\right) p\left(x_{2} \mid x_{1}, x_{3}\right) \ldots p\left(x_{n} \mid x_{1}, \ldots, x_{n}\right) \\<br>&amp;=\ldots \\<br>&amp;=p\left(x_{n-1}\right) p\left(x_{1} \mid x_{n-1}\right) p\left(x_{n} \mid x_{n-1}, x_{1}\right) \ldots p\left(x_{2} \mid x_{n-1}, x_{1}, \ldots, x_{3}\right)<br>\end{aligned}$</p>
<p>如何做到的呢？假设要生成：\<s\> → 迎 → 京 → 你 → 欢 → 北 → \<e\>，则使用如下mask</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ulm/6.png" alt="图片"></p>
<p>如果是正向语言模型则是：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ulm/7.png" alt="图片"></p>
<p>这个Mask到底是啥？算Attention的时候，如果Mask==inf对输出就不产生贡献，而Transformer又是完全凭借的Attention计算的，因而可以产生多种灵活的Mask。</p>
<h2 id="Seq2Seq语言模型"><a href="#Seq2Seq语言模型" class="headerlink" title="Seq2Seq语言模型"></a>Seq2Seq语言模型</h2><p>假如输入是“你想吃啥”，目标句子是“白切鸡”，那UNILM将这两个句子拼成一个：[CLS] 你 想 吃 啥 [SEP] 白 切 鸡 [SEP]。经过这样转化之后，最简单的方案就是训练一个语言模型，然后输入“[CLS] 你 想 吃 啥 [SEP]”来逐字预测“白 切 鸡”，直到出现“[SEP]”为止，即如下图：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ulm/8.png" alt="图片"></p>
<p>模型结构大概如下：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ulm/9.png" alt="图片"></p>
<p>本文所提出的的UNILM模型能够同时处理自然语言理解任务也能够处理自然语言生成任务。UNILM是一个深度的Transformer网络，其预训练过程采用3种无监督的语言模型目标：双向LM、单向LM和Sequence-to-Sequence LM：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ulm/10.png" alt="图片"></p>
<p>通过加入seq2seq LM 和 Left-to-right 使得模型可以进行自然语言生成任务。Sequence-to-Sequence LM中第一个sequence称为source，第2个sequence称为target。target sequence的上下文是第1个sequence和其左边的words组成。</p>
<p>本文提出的UNILM模型有以下3个优点：</p>
<ul>
<li>统一的预训练流程，使得仅仅使用一个Transformer语言模型即可。该Transformer模型在不同的LM（上述Table 2中的3个LM）上共享参数，这就无需在多个LM上分别训练和配置。</li>
<li>多个LM之间的参数共享使得学习到的文本表征具有更强的泛化能力。在不同的语言模型目标上联合优化，使得上下文在不同方式中被使用，也减缓了在单一LM上的过拟合。</li>
<li>除了可以应用到自然语言理解任务上，本文模型还能够作为一个sequence-to-sequence LM来处理自然语言生成任务，如摘要生成和问题生成。</li>
</ul>
<p>UNILM的预训练是基于大量语料，论文用如下实验阐述了其效果：</p>
<ul>
<li>UNILM作为一个双向encoder：与BERT在GLUE数据集、SQuAD 2.0和CoQA的抽取式问答对比， 此时的UNILM作为一个双向的encoder。</li>
<li>UNILM作为一个sequence-to-sequence模型：在摘要生成(CNN/DailyMail)、问题生成(SQuAD)和生成式问答(CoQA)中UNILM作为一个sequence-to-sequence模型</li>
</ul>
<h1 id="模型整体框架"><a href="#模型整体框架" class="headerlink" title="模型整体框架"></a>模型整体框架</h1><p>给定输入sequence x=x1 .. x|x|， 通过多层的Transformer网络对每个token得到一个带有上下文信息的向量表征。输入tokens的表征联合word embedding、位置embedding和text segment。再将输入向量输入到多层Transformer网络中，利用其中的self-attention机制联合整个输入sequence计算得到文本的表征。</p>
<p>如下图所示，统一的语言模型预训练在多个无监督语言模型目标（单向LM、双向LM和Sequence-to-Sequence）中共享Transformer网络参数。为了待预测token的上下文范围，本文在self-attention中使用了不同的masks。换句话说，本文使用mask方式来控制待预测token所考虑到的上下文。</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ulm/11.png" alt="图片"></p>
<ul>
<li>当训练双向语言模型的时候，前后的单词都可以被看到，</li>
<li>训练单向语言模型的时候，只有一个方向的可以被看到</li>
<li>当训练 seq-seq 模型的时候，输入和输出的一个方向可以被看到，</li>
<li>具体的实现是通过 mask 掉对应位置的 attention 值导致对应位置的信息丢失。</li>
<li>具体的实现是在 softmax 中加入了 M(i,j)值，表示位置i是否需要被j 关注到。<ul>
<li>$\mathbf{H}^{l}=\text { Transformer } l\left(\mathbf{H}^{l-1}\right)$</li>
<li>$\mathbf{Q}=\mathbf{H}^{l-1} \mathbf{W}_{l}^{Q}, \quad \mathbf{K}=\mathbf{H}^{l-1} \mathbf{W}^{K}$</li>
<li>$\mathbf{M}_{i j}=\left\{\begin{array}{ll}<br>0, &amp; \text { allow to attend } \\<br>-\infty, &amp; \text { prevent from attending }<br>\end{array}\right.$</li>
<li>$\mathbf{A}_{l}=\operatorname{softmax}\left(\begin{array}{c}<br>\mathrm{QK}^{\top} \\<br>\sqrt{d k}<br>\end{array}+\mathbf{M}\right)\left(\mathbf{H}^{l-1} \mathbf{V}_{l}\right)$</li>
</ul>
</li>
</ul>
<h1 id="模型输入"><a href="#模型输入" class="headerlink" title="模型输入"></a>模型输入</h1><p>对于输入的词序列xxx，在起始位置添加一个预定义的[SOS] token作为标志符。[SOS] token对应的输出向量可以作为整个输入的表征。此外，在每个segment的末尾添加一个预定义的标志符[EOS]。这个[EOS] token用来表示segments之间的边界。其作用不仅仅是自然语言理解任务中的句子边界，还作为自然语言生成任务中生成word的停止符。</p>
<p>输入表征与BERT相同，文本通过WordPiece被tokenized为subword。例如，单词”forecasted”被分割为”forecast”和”##ed”，其中”##”表明该部分是属于某个word。每个输入的token，其向量表征是token embedding、position embedding和segment embedding三者之和。本文使用绝对位置embedding。segment embedding是为了区分一对文本中的不同segment。第1个segment和第2个segment分别有不同的segment embedding，即为了区分句子对中的2个句子。由于不同LM目标使用不同的segment embeddings，所以可以作为LM辨识器。</p>
<h1 id="主要计算过程"><a href="#主要计算过程" class="headerlink" title="主要计算过程"></a>主要计算过程</h1><p>给定输入向量，先将整理为H，再将其输入到一个L层的Transformer网络中对输入进行encode：</p>
<ul>
<li>输入向量：$\{\mathbf{x} i\}_{i=1}^{|x|}$</li>
<li>整理后的向量：$\mathbf{H}^{0}=[\mathbf{x} 1, \cdots, \mathbf{x}|x|]$</li>
<li>encode后的向量：$\mathbf{H}^{l}=\text { Transformer } l\left(\mathbf{H}^{l-1}\right)$</li>
</ul>
<p>在每个Transformer Block中，有多头self-attention以聚合之前的输出向量。对于第l−th层的Transformer，其中一个self-attention head Al的计算过程如下：</p>
<ul>
<li>$\mathbf{Q}=\mathbf{H}^{l-1} \mathbf{W}_{l}^{Q}, \quad \mathbf{K}=\mathbf{H}^{l-1} \mathbf{w}_{l}^{K}$</li>
<li>$\mathbf{M}_{i j}=\left\{\begin{array}{ll}<br>0, &amp; \text { allow to attend } \\<br>-\infty, &amp; \text { prevent from attending }<br>\end{array}\right.$</li>
<li>$\mathbf{A}_{l}=\operatorname{softmax}\left(\begin{array}{c}<br>\mathrm{QK}^{\top} \\<br>\sqrt{d k}<br>\end{array}+\mathbf{M}\right)\left(\mathbf{H}^{l-1} \mathbf{V}_{l}\right)$</li>
</ul>
<p>Mask矩阵控制着tokens之间是否存在被attended。不同的mask 矩阵M控制attent到不同的上下文，具体如整体框架图所示，在双向LM中，mask矩阵的值为0，表示所有的tokens之间都能够相互注意到。</p>
<ul>
<li>使用「mask」替换单词，模型拿到对应位置的输出猜测对应单词<ul>
<li>在对输入进行mask的时候，是选取一个WordPiece tokens随机mask，所谓的mask操作是用预定义的token[MASK]对其进行替换。再将输入到Transformer网络中计算得到输出向量，再将输出向量输入到softmax分类器中以预测被mask掉的token。</li>
</ul>
</li>
<li>交叉熵损失<ul>
<li>NILM模型的参数是通过最小化预测的tokens和原始真实tokens之间的交叉熵损失学习到的。</li>
</ul>
</li>
<li>单向LM<ul>
<li>单向LM包括从左到右和从右到左的LM。在单向LM的预训练过程中，输入仅仅使用单个segment。以从左都右的LM为例，每个token都是基于其左边的上文及其自身的encode结果。为预测x1x2[MASK]x4 ，只有x1、x2及其自身[MASK]会被使用到。其具体实现是通过三角矩阵形式的self-attention mask M。从框架图中可以看出上三角矩阵为深灰色，值为无穷大(表示阻塞，token之间没有行方向的token注意不到列方向的token)，其他位置为浅灰色，值为0(表示可以注意到早期，即左边的token)</li>
</ul>
</li>
<li>双向LM<ul>
<li>Bert</li>
</ul>
</li>
<li>Sequence-to-Sequence LM<ul>
<li>sequence-to-sequence LM的输入分为2个segments，如框架图所示，第一个segment(即source segment)在整个segment内部能够attend到任何的tokens，第2个segment(target segment)中的tokens只能attend到target segment中上文tokens及其自身，还有source segment。框架图中的sequence-to-sequence LM的self-attention mask可以看出，左边的mask 矩阵设置为全0，对应的source segment，即所有的token之间是可以相互注意到的。右上角被设置为无穷大，使得source segment看不到target segment；往下的右上角值设置为无穷大，其他值为0是实现target segment只能注意到上文，忽略其下文。</li>
</ul>
</li>
<li>24层1024隐藏单元，16 头 attention，总模型参数量 340M</li>
<li>使用 bert做初始化</li>
<li>Mask的标准：<ul>
<li>输入数据中随机选择15%的词用于预测，这15%的词中，<ul>
<li>80%的词向量输入时被替换为\<MASK\></li>
<li>10%的词的词向量在输入时被替换为其他词的词向量</li>
<li>另外10%保持不动</li>
</ul>
</li>
<li>这样一来就相当于告诉模型，我可能给你答案，也可能不给你答案，也可能给你错误的答案，有\<MASK\>的地方我会检查你的答案，没\<MASK\>的地方我也可能检查你的答案，所以\<MASK\>标签对你来说没有什么特殊意义，所以无论如何，你都要好好预测所有位置的输出。</li>
</ul>
</li>
<li>三种多任务学习，一个batch，比例按照 1：1：1<ul>
<li>预训练目标函数是多个语言模型的平均似然的求和。在每个训练batch中，1/3的时间使用双向LM目标，1/3的时间使用sequence-to-sequence LM作为目标，从左到右和从右到左的LM目标各自占1/6。</li>
</ul>
</li>
</ul>
<h1 id="Fine-tunning"><a href="#Fine-tunning" class="headerlink" title="Fine-tunning"></a>Fine-tunning</h1><p>Fine-tunning过程随着下游任务不同而有变化，但是大致的思路是：</p>
<ul>
<li>对于序列生成任务， 输入格式为 [SOS] S1 [EOS] S2 [EOS]，按照 15%的概率随机 mask S2 [EOS]这个部分，模型预测对应位置</li>
<li>其他任务参考 bert</li>
</ul>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="摘要生成"><a href="#摘要生成" class="headerlink" title="摘要生成"></a>摘要生成</h2><p>数据集： CNN/日常邮件摘要 该数据集包含带有多句摘要（平均 3.75 个句子或 56 个词）的在线新闻文章（平均 781 个词）。经处理的版本包含 287226 个训练对、13368 个验证对和 11490 个测试对。模型基于 ROUGE-1、ROUGE-2 和 ROUGE-L 进行评估</p>
<p>按照序列生成任务进行 fine-tunning，在解码期间，使用beam search，并且删除重复的三元组，并在开发集上调整最大的摘要长度。</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ulm/12.png" alt="图片"></p>
<h2 id="阅读理解式QA"><a href="#阅读理解式QA" class="headerlink" title="阅读理解式QA"></a>阅读理解式QA</h2><p>finetuning 的时候，answer每个位置对应的输出，经过一个 softmax 分类器，输出是否是答案的开始或者结尾，梯度下降，测试的时候根据分类器输出得到answer。</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ulm/13.png" alt="图片"></p>
<p>精准匹配度（Exact Match，EM）：计算预测结果与标准答案是否完全匹配。</p>
<p>模糊匹配度（F1）：计算预测结果与标准答案之间字级别的匹配程度。</p>
<h2 id="问题生成"><a href="#问题生成" class="headerlink" title="问题生成"></a>问题生成</h2><p>输入形式：[SOS] passage [SEP] answer [EOS] question [EOS] ， question 前面的作为第一个句子，question 作为第二个句子，进行 seq2seq 式训练，生成</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ulm/14.png" alt="图片"></p>
<h2 id="GLUE-任务"><a href="#GLUE-任务" class="headerlink" title="GLUE 任务"></a>GLUE 任务</h2><p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ulm/15.png" alt="图片"></p>
<p>如果不算预训练的时候加入的数据，那么就是在没有使用外部数据的情况下，一些数据集上超过了 bert，是很厉害的。</p>
<h2 id="生成回复"><a href="#生成回复" class="headerlink" title="生成回复"></a>生成回复</h2><p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/ulm/16.png" alt="图片"></p>
]]></content>
      <tags>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello Edge: Keyword spotting on Microcontrollers论文源码阅读及实验结论</title>
    <url>/2019/04/23/Hello-Edge-Keyword-spotting-on-Microcontrollers%E8%AE%BA%E6%96%87%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E5%8F%8A%E5%AE%9E%E9%AA%8C%E7%BB%93%E8%AE%BA/</url>
    <content><![CDATA[<p>高通的大哥想做语音唤醒实验，我自己对这个也很感兴趣，所以阅读了一下相关论文，并做了一次实验。本篇主要对源码进行解读。</p>
<a id="more"></a>
<h2 id="论文结论"><a href="#论文结论" class="headerlink" title="论文结论"></a>论文结论</h2><ul>
<li>LSTM使用的内存最少</li>
<li>整体DS-CNN &gt; CRNN &gt; GRU &gt; LSTM &gt; Basic_LSTM &gt; CNN &gt; DNN</li>
</ul>
<h2 id="KWS-for-FIXLEN代码阅读"><a href="#KWS-for-FIXLEN代码阅读" class="headerlink" title="KWS_for_FIXLEN代码阅读"></a>KWS_for_FIXLEN代码阅读</h2><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><h4 id="音频相关"><a href="#音频相关" class="headerlink" title="音频相关"></a>音频相关</h4><ul>
<li>words_list：_silence_,_unknown_,yes,no,up,down,left,right,on,off,stop,go<ul>
<li>label_count（How many classes are to be recognized）：12</li>
<li>wanted_words: yes,no,up,down,left,right,on,off,stop,go</li>
</ul>
</li>
<li>sample_rate（需要和提供的wav文件的采样率匹配）：16000 </li>
<li>clip_duration_ms（录音文件的时长）：1000 <ul>
<li>desired_samples （sample_rate * clip_duration_ms / 1000 语音需要的样本点个数）：16000</li>
</ul>
</li>
<li>window_size_ms（帧长）：40.0<ul>
<li>window_size_samples（sample_rate * window_size_ms / 1000）：640</li>
</ul>
</li>
<li>window_stride_ms（帧移）：40.0 帧移<ul>
<li>window_stride_samples（sample_rate * window_stride_ms / 1000）：640</li>
<li>spectrogram_length（0:1 + int((desired_samples - window_size_samples) / window_stride_samples) ? desired_samples - window_size_samples &lt; 0 声音有多少帧）: 25</li>
</ul>
</li>
<li>dct_coefficient_count（对MFCC来说每一帧有多少系数）：10<ul>
<li>fingerprint_size （dct_coefficient_count * spectrogram_length）: 250</li>
</ul>
</li>
<li>background_volume（背景噪声的音量，默认0.1。这是一种Data Augmentation的技术，通过给语音增加噪声来提高模型的泛化能力）</li>
<li>background_frequency（多少比例的训练数据会增加噪声）：0.8</li>
<li>silence_percentage（How much of the training data should be silence）：10.0</li>
<li>unknown_percentage(How much of the training data should be unknown words):10.0</li>
<li>validation_percentage(What percentage of wavs to use as a validation set):10</li>
<li>testing_percentage(What percentage of wavs to use as a test set):10</li>
<li>time_shift_ms（录音都是长度1秒的文件，但是在实际预测的时候用户开始的实际是不固定的，为了模拟这种情况，我们这里会随机的把录音文件往前或者往后平移一段时间，这个参数就是指定平移的范围。默认100(ms)，说明会随机的在[-100,100]之间平移数据）：100.0 <ul>
<li>time_shift_samples（time_shift_ms * sample_rate/ 1000）：16</li>
</ul>
</li>
</ul>
<h4 id="模型相关"><a href="#模型相关" class="headerlink" title="模型相关"></a>模型相关</h4><ul>
<li>网络参数<ul>
<li>model_architecture：single_fc, conv, low_latency_conv, low_latency_svdf, dnn, cnn, basic_lstm, lstm, gru, crnn, ds_cnn</li>
<li>model_size_info：[144, 144, 144]</li>
</ul>
</li>
<li>训练参数<ul>
<li>learning_rate: [0.0005, 0.0001, 0.00002]</li>
<li>how_many_training_steps: [10000,10000,10000]</li>
<li>summaries_dir</li>
<li>train_dir</li>
<li>eval_step_interval: 400</li>
<li>save_step_interval：100</li>
</ul>
</li>
</ul>
<h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><ul>
<li><p>python train.py —mode prepare —dct_coefficient_count 10 —window_size_ms 40 —window_stride_ms 40 —data_dir data/speech_commands</p>
<ul>
<li>如果需要下载原始数据：python train.py —mode prepare —dct_coefficient_count 10 —window_size_ms 40 —window_stride_ms 40 —data_url <a href="http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz" target="_blank" rel="noopener">http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz</a> —data_dir data/speech_commands</li>
<li><p><strong>prepare_data_index</strong>：根据音频文件名哈希到不同集合中（尽量让同一个发声人在一个集合中），将静音根据比例添加到不同集合中，把一些unknown的wav根据比例添加到不同集合中，最后进行集合内shuffle （先忽略background文件夹下的wav）。</p>
<ul>
<li><p>这里涉及到划分训练集、验证集和测试集的一个小技巧。通常我们的训练数据是不断增加的，如果按照随机的按比例划分训练集、验证集和测试集，那幺增加一个新的数据重新划分后有可能把原来的训练集中的数据划分到测试数据里。因为我们的模型可能要求incremental的训练，因此这就相对于把测试数据也拿来训练了。因此我们需要一种“稳定”的划分方法——原来在训练集中的数据仍然在训练数据中。这里我们使用的技巧就是对于文件名进行hash，然后根据hash的结果对总量取模来划分到不同的集合里。这样就能保证同一个数据第一次如果是在训练集合里，那幺它永远都会划分到训练集合里。不过它只能大致保证三个集合的比例而不能绝对的保证比例</p>
</li>
<li><p>另一点是每个集合里都要加入一定比例(silence_percentage)的silence和unknown词</p>
</li>
<li><p><strong>input</strong>：silence_percentage, unknown_percentage, validation_percentage, testing_percentage, wanted_words</p>
</li>
<li><p><strong>output</strong>：data_index: {‘validation’: [{‘label’: word, ‘file’: wav_path}], ‘testing’: […], ‘training’: […]}</p>
<ul>
<li><p>words_list：[_silence_, _unknown_, yes, no, up, down, left, right, on, off, stop, go]</p>
</li>
<li><p>word_to_index（不需要识别的词都映射成1）：{‘marvin’: 1, ‘tree’: 1, ‘learn’: 1, ‘dog’: 1, ‘sheila’: 1, ‘bird’: 1, ‘right’: 7, ‘off’: 9, ‘backward’: 1, ‘six’: 1, ‘two’: 1, ‘no’: 3, ‘yes’: 2, ‘one’: 1, ‘follow’: 1, ‘up’: 4, ‘three’: 1, ‘forward’: 1, ‘happy’: 1, ‘nine’: 1, ‘bed’: 1, ‘zero’: 1, ‘house’: 1, ‘visual’: 1, ‘five’: 1, ‘seven’: 1, ‘cat’: 1, ‘left’: 6, ‘stop’: 10, ‘go’: 11, ‘four’: 1, ‘on’: 8, ‘wow’: 1, ‘down’: 5, ‘eight’: 1}</p>
</li>
</ul>
</li>
</ul>
</li>
<li>prepare_background_data: <ul>
<li>单独处理background_noise文件夹下的音频</li>
<li>input：BACKGROUND_NOISE_DIR_NAME</li>
<li>output:<ul>
<li>background_data: （16000, 0）的float tensor</li>
<li>重要函数：<strong>wav_decoder = contrib_audio.decode_wav(wav_loader, desired_channels=1)</strong> ：将16-bit PCM Wave 文件解码成一维 float tensor</li>
</ul>
</li>
</ul>
</li>
<li>prepare_processing_graph：先将音频解码的tensorflow graph创建好，等session.run的时候执行<ul>
<li>加载一个WAVE文件 —&gt; 解码（(16000,0)的float tensor） —&gt; 缩放音量（element-wise乘，得到还是(16000,0)的float tensor） —&gt; 语音平移 —&gt; 加入背景噪声（(desired_samples,)且在-1.0到1.0的float tensor） —&gt; 计算频谱 （(1, 25, 513)这里第一个维度是通道数，单通道是1）—&gt; 建立一个MFCC指纹（<strong>(1, 25, 10)</strong>）</li>
<li>语音平移：如果是右移，那幺需要在左边补零；如果是左移，则要右边补零。这个padding的量也是在生成batch数据的时候动态产生的，所以也定义为一个placeholder。因为语音的tensor是(16000,1)的，所有padding是一个[2,2]的tensor，不过通常只在第一个维度(时间)padding。比如右移100个点，那幺传入的tensor是[[100,0],[0,0]]。如果是左移，我们除了要padding，还要把左边的部分“切掉“，因此还会传入一个time_shift_offset_placeholder_，如果是右移，那幺这个值是零。比如我们要实现左移100个点，那幺传入的time_shift_padding_placeholder_应该是[[0,100],[0,0]],而time_shift_offset_placeholder_应该是[100]。_</li>
<li>混入噪声：placeholder background_data_placeholder_表示噪声，而background_volume_placeholder_表示混入的音量(比例)，如果background_volume_placeholder_是零就表示没有噪声。把它们乘起来就得到background_mul，然后把它加到sliced_foreground就得到background_add，因为加起来音量可能超过1，所有需要把大于1的变成1，这可以使用clip_by_value函数把音量限制在[-1,1]的区间里</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="模型声明"><a href="#模型声明" class="headerlink" title="模型声明"></a>模型声明</h3><ul>
<li>输入、输出<ul>
<li>input: fingerprint_input（batch_size, fingerprint_size）, model_settings, model_size_info, is_training</li>
<li>output: logits, dropout_prob(if training, 是一个placeholder)</li>
</ul>
</li>
</ul>
<ul>
<li>dnn: create_dnn_model<ul>
<li>参数：<ul>
<li>W1：（fingerprint_size, model_size_info[0]）</li>
<li>b1: (model_size_info[0])</li>
<li>W2: （model_size_info[0], model_size_info[1]）</li>
<li>b2: （model_size_info[1]）</li>
<li>W3：（model_size_info[1], model_size_info[2]）</li>
<li>b3: （model_size_info[2]）</li>
<li>weights: （model_size_info[2], label_count）</li>
<li>bias: （label_count）</li>
<li>总参数个数：250 <em> 144 + 144 + 144 </em> 144 + 144 + 144 <em> 144 + 144+ 144 </em> 12 + 12 = 79644</li>
<li>操作个数：</li>
</ul>
</li>
<li>前向计算过程：fingerprint_input用flow代替, * 表示矩阵乘<ul>
<li>flow  —&gt;（batch_size, fingerprint_size）</li>
<li>flow1 = dropout(relu(flow * W1 + b1 ))  —&gt;（batch_size, model_size_info[0]）</li>
<li>flow2 = dropout(relu(flow1 * W2 + b2 ))  —&gt; （batch_size, model_size_info[1]）</li>
<li>flow3 = dropout(relu(flow2 * W2 + b2 ))  —&gt; （batch_size, model_size_info[2] </li>
<li>logits = flow3 * weights + bias  —&gt; （batch_size, label_count）</li>
</ul>
</li>
</ul>
</li>
<li>conv: create_conv_model  参考【2】<ul>
<li>参数：<ul>
<li>first_weights：（first_filter_height=20, first_filter_width=8, 1, first_filter_count=64）注：卷积核</li>
<li>first_bias：（first_filter_count,）</li>
<li>second_weights: （second_filter_height=10, second_filter_width=4, first_filter_count=64, second_filter_count=64）</li>
<li>second_bias： （second_filter_count,）</li>
<li>final_fc_weights：（13 <em> 5 </em> 64=4160, label_count）</li>
<li>final_fc_bias：（label_count,）</li>
</ul>
</li>
<li>前向计算过程：<ul>
<li>flow  —&gt;（batch_size, fingerprint_size）</li>
<li>fingerprint_4d = reshape(flow) —&gt; (batch_size, spectrogram_length=25, dct_coefficient_count=10, 1) 注：类似图像的[batch_size, in_height, in_width, in_channels]</li>
<li>first_conv = dropout(relu(conv2d(fingerprint_4d, first_weights, [1, 1, 1, 1], ‘SAME’) + first_bias)) —&gt; （batch_size, spectrogram_length=25, dct_coefficient_count=10, first_filter_count=64）卷积</li>
<li>max_pool = max_pool(first_conv, [1, 2, 2, 1], [1, 2, 2, 1], ‘SAME’) —&gt; (batch_size, (25-2 + 2 <em> 1)/2 + 1=13, (10 - 2 + 2</em>1)/2 = 5, first_filter_count=64)</li>
<li>second_conv = dropout(relu(conv2d(max_pool, second_weights, [1,1,1,1], ‘SAME’) + second_bias)) —&gt; （batch_size, 13, 5, 64）</li>
<li>flattened_second_conv = reshape(second_conv) —&gt; (batch_size, 13 <em> 5 </em> 64=4160)</li>
<li>final_fc = flattened_second_conv * final_fc_weights + final_fc_bias —&gt; (batch_size, label_count)</li>
<li>总参数个数：224140 = 20 <em> 8 </em> 64 + 64 + 10 <em> 4 </em> 64 <em> 64 + 64 + 4160 </em> 12 + 12</li>
<li>总操作个数</li>
</ul>
</li>
</ul>
</li>
<li>low_latency_conv：create_low_latency_conv_model 参考【2】相比于conv有更少的参数，但是有准确度的损失<ul>
<li>参数<ul>
<li>first_weights：（spectrogram_length=25, first_filter_width=8, 1, first_filter_count=186）</li>
<li>first_bias：（first_filter_count,）</li>
<li>first_fc_weights：（first_conv_element_count=558, first_fc_output_channels = 128）</li>
<li>first_fc_bias：（first_fc_output_channels, ）</li>
<li>second_fc_weights：（first_fc_output_channels=128, second_fc_output_channels=128）</li>
<li>second_fc_bias：（second_fc_output_channels=128, ）</li>
<li>final_fc_weights：（second_fc_output_channels=128, label_count=12）</li>
<li>final_fc_bias：（label_count,）</li>
<li>总参数个数：126998</li>
<li>总操作个数</li>
</ul>
</li>
<li>前向计算过程<ul>
<li>flow  —&gt;（batch_size, fingerprint_size）</li>
<li>fingerprint_4d = reshape(flow) —&gt; (batch_size, spectrogram_length=25, dct_coefficient_count=10, 1)</li>
<li>first_conv = dropout(relu(conv2d(fingerprint_4d, first_weights, [1, 1, 1, 1], ‘VALID’ )+ first_bias)) —&gt; （batch_size, 1, 3, 186）</li>
<li>flattened_first_conv = reshape(first_conv) —&gt; （batch_size, 3 * 186=558）</li>
<li>first_fc = dropout(flattened_first_conv * first_fc_weights + first_fc_bias) —&gt; （batch_size, first_fc_output_channels = 128）</li>
<li>second_fc = dropout(first_fc * second_fc_weights + second_fc_bias) —&gt; （batch_size, second_fc_output_channels = 128）</li>
<li>final_fc = second_fc * final_fc_weights + final_fc_bias —&gt; (batch_size, label_count)</li>
</ul>
</li>
</ul>
</li>
<li>basic-lstm: 和lstm差了use_peepholes和num_proj，参数个数为43916 = 10 <em> 98 </em> 4 + 98 <em> 98 </em> 4 + 98 <em> 4 + 98 </em> 12 + 12</li>
<li>lstm: model_size_info [0]对应projection size，model_size_info[1]对应memory cells in LSTM<ul>
<li>参数 ?加peephole之后参数也不太对<ul>
<li>lstm_cell: （隐藏层使用了全链接，peephole connections）<ul>
<li>W_f、W_i、W_o、W_c：（dct_coefficient_count,  model_size_info[1]）</li>
<li>U_f、U_i、U_o、U_c: （model_size_info [0], model_size_info[1]）</li>
<li>bias_f、bias_i、bias_o、bias_c: (model_size_info[1])</li>
<li>W_h: （model_size_info[1], model_size_info[0]）</li>
<li>bias_h: （model_size_info[0]）</li>
</ul>
</li>
<li>W_o：（model_size_info[0]=98, label_count=12）</li>
<li>b_o: （label_count=12,）</li>
<li>参数总个数：? 78516 = <strong>5760 + 56448 + 576 + 14112 + 98 + 1176 + 12 = 78182</strong></li>
<li>总操作个数</li>
</ul>
</li>
<li>前向计算过程 * 是矩阵乘法，∘是元素乘<ul>
<li>flow  —&gt;（batch_size, fingerprint_size）</li>
<li>fingerprint_4d = reshape(flow) —&gt; (batch_size, spectrogram_length=25, dct_coefficient_count=10)<ul>
<li>一个时间步的输入<ul>
<li>隐状态h_t-1：（batch_size，model_size_info[0]）</li>
<li>x_t：（batch_size, dct_coefficient_count=10）</li>
<li>c_t-1: （batch_size, model_size_info[1]）</li>
</ul>
</li>
<li>f_t = sigmoid(x_t <em> W_f + h_t-1 </em> U_f + bias_f) —&gt;（batch_size，model_size_info[1]）</li>
<li>i_t = sigmoid(x_t <em> W_i + h_t-1 </em> U_i + bias_i) —&gt; （batch_size，model_size_info[1]）</li>
<li>o_t = sigmoid(x_t <em> W_o + h_t-1 </em> U_o + bias_o ) —&gt; （batch_size，model_size_info[1]）</li>
<li>c’ = tanh(x_t <em> W_c + h_t-1 </em> U_c + bias_c) —&gt; （batch_size，model_size_info[1]）</li>
<li>c_t = f_t ∘ c_t-1 + i_t ∘ c’ —&gt; （batch_size, model_size_info[1]）</li>
<li>h_t = tanh(o_t ∘ c_t) —&gt; （batch_size, model_size_info[1]）</li>
<li>h_t = h_t * W_h + bias_h —&gt; （batch_size, model_size_info[0]）</li>
</ul>
</li>
<li>flow = dynamic_rnn(lstm_cell, fingerprint_4d) 的最后一个输出 —&gt; (batch_size, model_size_info[0]=98)</li>
<li>logits = flow * W_o + b_o —&gt; （batch_size, label_count）</li>
</ul>
</li>
</ul>
</li>
<li><p>gru: </p>
<ul>
<li>参数 model_size_info[0]对应num_layers，model_size_info[1]对应gru_units<ul>
<li>使用layer_normalize</li>
<li>不使用layer_normalize<ul>
<li>W_rx、W_zx、W_hx: （dct_coefficient_count=10, gru_units=154）</li>
<li>W_rh、W_zh、W_hh: （gru_units, gru_units）</li>
<li>bias_r、bias_z、bias_h: （gru_units）</li>
<li>W_o：（gru_units, label_count）</li>
<li>b_o：（label_count）</li>
<li>总参数个数：10 <em> 154 </em> 3 + 154 <em> 154 </em> 3 + 154 <em> 3 + 154 </em> 12 + 12  = 78080 ? 91950</li>
</ul>
</li>
</ul>
</li>
<li><p>前向计算过程 </p>
<ul>
<li>flow  —&gt;（batch_size, fingerprint_size）</li>
<li><p>fingerprint_4d = reshape(flow) —&gt; (batch_size, spectrogram_length=25, dct_coefficient_count=10)</p>
<ul>
<li>一个时间步的输入<ul>
<li>隐状态h_t-1：（batch_size，gru_units）</li>
<li>x_t：（batch_size, dct_coefficient_count=10）</li>
</ul>
</li>
<li><p>使用layer_normalize</p>
<ul>
<li>单向</li>
<li>双向：前向和后向logits的拼接</li>
</ul>
</li>
<li><p>不使用layer_normalize</p>
<ul>
<li>单向<ul>
<li>r_t = sigmoid(x_t <em> W_rx + h_t-1 </em> W_rh + bias_r) —&gt; (batch_size, gru_units)</li>
<li>z_t = sigmoid(x_t <em> W_zx + h_t-1 </em> W_zh + bias_z) —&gt; (batch_size, gru_units)</li>
<li>h’ = tanh(x_t <em> W_hx + (r_t ∘ h_t-1) </em> W_hh + bias_h) —&gt; (batch_size, gru_units)</li>
<li>h_t = (1 - z_t) ∘ h_t-1 + z_t ∘ h’ —&gt;  (batch_size, gru_units)</li>
<li>flow = dynamic_rnn(gru_cell, fingerprint_4d) 的最后一个输出 —&gt; (batch_size, gru_units) </li>
<li>logits = flow * W_o + b_o —&gt; （batch_size, label_count）</li>
</ul>
</li>
<li>双向<ul>
<li>前向和后向logits的拼接</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>crnn</li>
<li>ds_cnn</li>
</ul>
<h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><ul>
<li>定义loss：softmax cross entropy<ul>
<li>定义optimize：Adam</li>
<li>重点是audio_processor.get_data 随机生成一个batch的训练数据的过程<ul>
<li>input：<ul>
<li>how_many ：batch大小，如果是-1则返回所有</li>
<li>offset： 如果是非随机的生成数据，这个参数指定开始的offset</li>
<li>background_frequency： 0.0-1.0之间的值，表示需要混入噪音的数据的比例</li>
<li>background_volume_range ：背景噪音的音量</li>
<li>time_shift ：平移的范围，为[-time_shift, time_shift]</li>
<li>sess ：用于执行前面用于产生数据的Operation，参考prepare_processing_graph函数</li>
</ul>
</li>
<li>output：<ul>
<li>data：</li>
<li>labels</li>
<li>过程看代码中注释</li>
</ul>
</li>
</ul>
</li>
<li>实验结果：<ul>
<li>Final test accuracy = 84.68% (N=3081)：python train.py —model_architecture dnn —model_size_info 144 144 144 —dct_coefficient_count 10 —window_size_ms 40 —window_stride_ms 40 —learning_rate 0.0005,0.0001,0.00002 —how_many_training_steps 10000,10000,10000 —data_dir ./data/speech_commands  —summaries_dir work/DNN/DNN1/retrain_logs —train_dir work/DNN/DNN1/training</li>
<li>python train.py —model_architecture conv —model_size_info 144 144 144 —dct_coefficient_count 10 —window_size_ms 40 —window_stride_ms 40 —learning_rate 0.0005,0.0001,0.00002 —how_many_training_steps 10000,10000,10000 —data_dir ./data/speech_commands  —summaries_dir work/CNN/CNN1/retrain_logs —train_dir work/CNN/CNN1/training</li>
<li>python train.py —model_architecture low_latency_conv —model_size_info 144 144 144 —dct_coefficient_count 10 —window_size_ms 40 —window_stride_ms 40 —learning_rate 0.0005,0.0001,0.00002 —how_many_training_steps 10000,10000,10000 —data_dir ./data/speech_commands  —summaries_dir work/CNN/CNN2/retrain_logs —train_dir work/CNN/CNN2/training</li>
<li>python train.py —model_architecture basic_lstm —model_size_info 98 144 —dct_coefficient_count 10 —window_size_ms 40 —window_stride_ms 40 —learning_rate 0.0005,0.0001,0.00002 —how_many_training_steps 10000,10000,10000 —data_dir ./data/speech_commands  —summaries_dir work/LSTM/LSTM1/retrain_logs —train_dir work/LSTM/LSTM1/training</li>
<li>python train.py —model_architecture lstm —model_size_info 98 —dct_coefficient_count 10 —window_size_ms 40 —window_stride_ms 40 —learning_rate 0.0005,0.0001,0.00002 —how_many_training_steps 10000,10000,10000 —data_dir ./data/speech_commands  —summaries_dir work/LSTM/LSTM2/retrain_logs —train_dir work/LSTM/LSTM2/training</li>
<li>python train.py —model_architecture gru —model_size_info 1 154 —window_size_ms 40 —window_stride_ms 40 —learning_rate 0.0005,0.0001,0.00002 —how_many_training_steps 10000,10000,10000 —data_dir ./data/speech_commands  —summaries_dir work/GRU/retrain_logs —train_dir work/GRU/training</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="模型测试"><a href="#模型测试" class="headerlink" title="模型测试"></a>模型测试</h3><ul>
<li>使用checkpoint<ul>
<li>python test.py —model_architecture dnn —model_size_info 144 144 144 —dct_coefficient_count 10 —window_size_ms 40 —window_stride_ms 40 —checkpoint work/DNN/DNN1/training/best/dnn_8503.ckpt-24000</li>
<li>使用freeze后的图： 可生成在安卓和IOS上可执行的图<ul>
<li>python freeze.py —model_architecture dnn —model_size_info 144 144 144 —dct_coefficient_count 10 —window_size_ms 40 —window_stride_ms 40 —data_dir ./data/speech_commands —checkpoint work/DNN/DNN1/training/best/dnn_8503.ckpt-24000 —output_file work/DNN/DNN1/training/best/dnn.pb<ul>
<li>创建图：create_inference_graph，将wave_data的filepath placeholder经过一系列计算转为reshaped_input，用create_model创建图得到logits, 再用softmax得到输出</li>
<li>load weights：models.load_variables_from_checkpoint</li>
<li>将变量替换为inline constant：graph_util.convert_variables_to_constants</li>
<li>写入目标图文件中：tf.train.write_graph</li>
</ul>
</li>
<li>python test_pb.py —model_architecture dnn —model_size_info 144 144 144 —dct_coefficient_count 10 —window_size_ms 40 —window_stride_ms 40 —data_dir ./data/speech_commands —graph work/DNN/DNN1/training/best/dnn.pb</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>模型单个音频预测（使用pb文件预测）</li>
<li>python predict_pb.py —model_architecture dnn —model_size_info 144 144 144 —dct_coefficient_count 10 —window_size_ms 40 —window_stride_ms 40 —data_dir ./data/speech_commands —graph work/DNN/DNN1/training/best/dnn.pb —test_wave data/speech_commands/yes/377e916b_nohash_0.wav</li>
</ul>
<h2 id="日常思考"><a href="#日常思考" class="headerlink" title="日常思考"></a>日常思考</h2><ul>
<li><p>KWS是否可以做成两阶段的，小模型先粗检（得到一个probability），再进入语音系统里用稍大一点的模型进行细检（根据上一个probability和声学特征）</p>
</li>
<li><ul>
<li>参考：<a href="https://mp.weixin.qq.com/s?__biz=MzAxMzc2NDAxOQ==&amp;mid=2650364421&amp;idx=1&amp;sn=7d0bb873fc8912919313c7f7f93f11e3&amp;chksm=83906ed9b4e7e7cfd86895b8c988da3f9b9658cbd3e59757f330667195e66ba691a437cade7c&amp;mpshare=1&amp;scene=24&amp;srcid=0420VEnXWAfwQt5bzrWKExbk&amp;key=6deb43fc298651d2d979d26548ea622f8207610665103536b676bc530064991d7785e44ddc7d09213a5dd65cfbf8f7e10de1f77f152a738e362c2ae53d33be353a3e56ad09614fcc3c3e752935aee8f9&amp;ascene=0&amp;uin=NjI1NjAyODg0&amp;devicetype=iMac+MacBookPro12%2C1+OSX+OSX+10.12.6+build(16G29" target="_blank" rel="noopener">苹果发布长文，揭秘「Hey Siri」的语音唤醒触发器和它背后的DNN模型</a></li>
</ul>
</li>
<li><p>KWS的安全性问题，是谁都可以唤醒吗？会不会根据说话人的几句话自己进行调整</p>
</li>
<li><ul>
<li>参考：<a href="https://mp.weixin.qq.com/s?__biz=MjM5NDM4NjQyNA==&amp;mid=2650918986&amp;idx=1&amp;sn=647ef8da86aea7ff4782fec1b2cde2f6&amp;chksm=bd7de8dd8a0a61cb30368d6d81a848e43171ea704de0eeb5bf81a394783d707d5445c2f8656f&amp;mpshare=1&amp;scene=24&amp;srcid=0420XnfffJnnWilAASoO14l1&amp;key=6deb43fc298651d21d2ecf5eb271a88dc4d0986ba4728fe4872d7b2f71f761167d621e2d64797ba83534f1e94cabe22a28e29077744a42bdc22d5a2afd384fd0ea33b9b15f9cf0aa286a9f8473858011&amp;ascene=0&amp;uin=NjI1NjAyODg0&amp;devicetype=iMac+MacBookPro12%2C1+OSX+OSX+10.12.6+build(16G29" target="_blank" rel="noopener">思必驰锁屏语音唤醒</a></li>
</ul>
</li>
<li><p>KWS在联网时可以将误检、漏检、正常唤醒的音频数据传到服务端，服务端进行标注并训练，但是模型能动态更新到硬件上吗？如果不行，这就是个一锤子买卖，是不是可以考虑先放到少儿词典这样的app中去收集一些数据</p>
</li>
<li><p>网络的输出：是否要标明开始和结束的位置</p>
</li>
</ul>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li>KWS相关资料<ul>
<li>不变长：<a href="https://github.com/ARM-software/ML-KWS-for-MCU" target="_blank" rel="noopener">代码</a>、<a href="https://arxiv.org/pdf/1711.07128.pdf" target="_blank" rel="noopener">论文</a></li>
<li>变长：<a href="https://github.com/mindorii/kws" target="_blank" rel="noopener">代码</a>、<a href="https://arxiv.org/pdf/1611.09405.pdf" target="_blank" rel="noopener">论文</a></li>
</ul>
</li>
<li>GRU推导：<a href="https://www.cnblogs.com/YiXiaoZhou/p/6075777.html" target="_blank" rel="noopener">GRU(Gated Recurrent Unit) 更新过程推导及简单代码实现</a></li>
<li>LSTM和GRU讲解：[<a href="https://www.cnblogs.com/zyly/p/9029591.html" target="_blank" rel="noopener">使用TensorFlow实现LSTM和GRU网络</a>]</li>
<li>关于卷积的详细讲解：<ul>
<li>卷积：<a href="https://blog.csdn.net/u011630575/article/details/78062452" target="_blank" rel="noopener">tensorflow：卷积函数——tf.nn.conv2d</a></li>
<li>卷积：<a href="https://blog.csdn.net/mao_xiao_feng/article/details/78004522" target="_blank" rel="noopener">tf.nn.conv2d是怎样实现卷积的？</a></li>
<li>池化：<a href="https://blog.csdn.net/u010402786/article/details/51541465" target="_blank" rel="noopener">卷积神经网络中图像池化操作全解析</a></li>
<li>池化：<a href="https://blog.csdn.net/mao_xiao_feng/article/details/53453926" target="_blank" rel="noopener">tf.nn.max_pool实现池化操作</a></li>
</ul>
</li>
<li>tensorflow rnn源码解读：<a href="https://panxiaoxie.cn/2018/09/01/tensorflow-rnn-api-源码阅读/" target="_blank" rel="noopener">Tensorflow RNN API 源码阅读</a></li>
<li><a href="https://flashgene.com/archives/25812.html" target="_blank" rel="noopener">使用Tensorflow识别语音关键词</a></li>
<li><a href="http://www.isca-speech.org/archive/interspeech_2015/papers/i15_1478.pdf" target="_blank" rel="noopener">Convolutional Neural Networks for Small-footprint Keyword Spotting</a></li>
</ul>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>KWS</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络基础系列之《优化器——原理及应用》</title>
    <url>/2019/03/27/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8A%E4%BC%98%E5%8C%96%E5%99%A8%E2%80%94%E2%80%94%E5%8E%9F%E7%90%86%E5%8F%8A%E5%BA%94%E7%94%A8%E3%80%8B/</url>
    <content><![CDATA[<p>神经网络在每次迭代中，梯度下降根据⾃变量当前位置，沿着当前位置的梯度更新⾃变量。然而，如果⾃变量的 迭代⽅向仅仅取决于⾃变量当前位置，这可能会带来⼀些问题。比如下图：</p>
<a id="more"></a>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/optimizer/1.png" alt="fig1"></p>
<p>给定学习率，梯度下降迭代⾃变量时会使⾃变量在竖直⽅向⽐在⽔平⽅向移动幅度更⼤。那么，我们需要⼀个较小的学习率从而避免⾃变量在竖直⽅向上越过⽬标函数最优解。然而，这会造成⾃变量在⽔平⽅向上朝最优解移动变慢。</p>
<p>为了解决梯度更新的问题，提出了很多优化器算法，下面一一介绍。</p>
<p>给定学习率，梯度下降迭代⾃变量时会使⾃变量在竖直⽅向⽐在⽔平⽅向移动幅度更⼤。那么，我们需要⼀个较小的学习率从而避免⾃变量在竖直⽅向上越过⽬标函数最优解。然而，这会造成⾃变量在⽔平⽅向上朝最优解移动变慢。</p>
<p>为了解决梯度更新的问题，提出了很多优化器算法，下面一一介绍。</p>
<h2 id="SGD随机梯度下降"><a href="#SGD随机梯度下降" class="headerlink" title="SGD随机梯度下降"></a>SGD随机梯度下降</h2><p>就是对minibtch做梯度下降。其优缺点如下：</p>
<ul>
<li><p>大部分时候你向着全局最小值靠近，有时候你会远离最小值，因为那个样本恰好给你指的方向不对，因此随机梯度下降法是有很多噪声的，平均来看，它最终会靠近最小值，不过有时候也会方向错误，因为随机梯度下降法永远不会收敛，而是会一直在最小值附近波动。一次性只处理了一个训练样本，这样效率过于低下。</p>
</li>
<li><p>实践中最好选择不大不小的 mini-batch，得到了大量向量化，效率高，收敛快。</p>
</li>
</ul>
<p>下面以LR算法为例，说明SGD的计算过程。</p>
<ul>
<li><p>模拟数据并归一化</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">x &#x3D; [30,35,37,59,70,76,88,100]</span><br><span class="line">y &#x3D; [1100,1423,1377,1800,2304,2588,3495,4839]</span><br><span class="line"></span><br><span class="line">x_max &#x3D; max(x)</span><br><span class="line">x_min &#x3D; min(x)</span><br><span class="line">y_max &#x3D; max(y)</span><br><span class="line">y_min &#x3D; min(y)</span><br><span class="line"></span><br><span class="line">for i in range(0,len(x)):</span><br><span class="line">    x[i] &#x3D; (x[i] - x_min)&#x2F;(x_max - x_min)</span><br><span class="line">    y[i] &#x3D; (y[i] - y_min)&#x2F;(y_max - y_min)</span><br><span class="line">    </span><br><span class="line">print (x)</span><br><span class="line">print (y)</span><br></pre></td></tr></table></figure>
</li>
<li><p>数据等高线</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">def calc_loss(a,b,x,y):</span><br><span class="line">    tmp &#x3D; y - (a * x + b)</span><br><span class="line">    tmp &#x3D; tmp ** 2  # 对矩阵内的每一个元素平方</span><br><span class="line">    SSE &#x3D; sum(tmp) &#x2F; (2 * len(x))</span><br><span class="line">    return SSE</span><br><span class="line"></span><br><span class="line">def draw_hill(x,y):</span><br><span class="line">    a &#x3D; np.linspace(-20,20,100)</span><br><span class="line">    b &#x3D; np.linspace(-20,20,100)</span><br><span class="line">    x &#x3D; np.array(x)</span><br><span class="line">    y &#x3D; np.array(y)</span><br><span class="line"></span><br><span class="line">    allSSE &#x3D; np.zeros(shape&#x3D;(len(a),len(b)))</span><br><span class="line">    for ai in range(0,len(a)):</span><br><span class="line">        for bi in range(0,len(b)):</span><br><span class="line">            a0 &#x3D; a[ai]</span><br><span class="line">            b0 &#x3D; b[bi]</span><br><span class="line">            SSE &#x3D; calc_loss(a&#x3D;a0,b&#x3D;b0,x&#x3D;x,y&#x3D;y)</span><br><span class="line">            allSSE[ai][bi] &#x3D; SSE</span><br><span class="line"></span><br><span class="line">    a,b &#x3D; np.meshgrid(a, b)</span><br><span class="line">    return [a,b,allSSE]</span><br><span class="line"></span><br><span class="line">[ha,hb,hallSSE] &#x3D; draw_hill(x,y)</span><br><span class="line">hallSSE &#x3D; hallSSE.T# 重要，将所有的losses做一个转置。原因是矩阵是以左上角至右下角顺序排列元素，而绘图是以左下角为原点。</span><br><span class="line"></span><br><span class="line">print (ha.shape)</span><br><span class="line">print (hb.shape)</span><br><span class="line">print (hallSSE.shape)</span><br></pre></td></tr></table></figure>
</li>
<li><p>绘制学习率的曲线</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">rate &#x3D; 0.1 # learning rate</span><br><span class="line">fig &#x3D; plt.figure(1, figsize&#x3D;(12, 8))</span><br><span class="line">fig.suptitle(&#39;learning rate: %.2f method:momentum&#39;%(rate), fontsize&#x3D;15)</span><br></pre></td></tr></table></figure>
</li>
<li><p>绘制曲面</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from mpl_toolkits.mplot3d import Axes3D</span><br><span class="line">ax &#x3D; fig.add_subplot(2, 2, 1, projection&#x3D;&#39;3d&#39;)</span><br><span class="line">ax.set_top_view()</span><br><span class="line">ax.plot_surface(ha, hb, hallSSE, rstride&#x3D;2, cstride&#x3D;2, cmap&#x3D;&#39;rainbow&#39;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>绘制等高线图</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">plt.subplot(2,2,2)</span><br><span class="line">ta &#x3D; np.linspace(-20, 20, 100)</span><br><span class="line">tb &#x3D; np.linspace(-20, 20, 100)</span><br><span class="line">plt.contourf(ha,hb,hallSSE,15,alpha&#x3D;0.5,cmap&#x3D;plt.cm.hot)</span><br><span class="line">C &#x3D; plt.contour(ha,hb,hallSSE,15,colors&#x3D;&#39;black&#39;)</span><br><span class="line">plt.clabel(C,inline&#x3D;True)</span><br><span class="line">plt.xlabel(&#39;a&#39;)</span><br><span class="line">plt.ylabel(&#39;b&#39;)</span><br><span class="line"></span><br><span class="line">plt.ion() # iteration on</span><br></pre></td></tr></table></figure>
</li>
<li><p>求loss的梯度</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def da(y,y_p,x):</span><br><span class="line">    return (y-y_p)*(-x)</span><br><span class="line"></span><br><span class="line">def db(y,y_p):</span><br><span class="line">    return (y-y_p)*(-1)</span><br></pre></td></tr></table></figure>
</li>
<li><p>进行迭代训练</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import random</span><br><span class="line"></span><br><span class="line">def shuffle_data(x,y):</span><br><span class="line">    # 随机打乱x，y的数据，并且保持x和y一一对应</span><br><span class="line">    seed &#x3D; random.random()</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    random.shuffle(x)</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    random.shuffle(y)</span><br><span class="line"></span><br><span class="line">def get_batch_data(x,y,batch&#x3D;3):</span><br><span class="line">    shuffle_data(x,y)</span><br><span class="line">    x_new &#x3D; x[0:batch]</span><br><span class="line">    y_new &#x3D; y[0:batch]</span><br><span class="line">    return [x_new,y_new]</span><br><span class="line"></span><br><span class="line">a &#x3D; 10.0</span><br><span class="line">b &#x3D; -20.0</span><br><span class="line">rate &#x3D; 0.2</span><br><span class="line"></span><br><span class="line">all_loss &#x3D; []</span><br><span class="line">all_step &#x3D; []</span><br><span class="line">last_a &#x3D; a</span><br><span class="line">last_b &#x3D; b</span><br><span class="line">for step in range(1,200):</span><br><span class="line">    loss &#x3D; 0</span><br><span class="line">    all_da &#x3D; 0</span><br><span class="line">    all_db &#x3D; 0</span><br><span class="line">    shuffle_data(x,y)</span><br><span class="line">    [x_new,y_new] &#x3D; get_batch_data(x,y,batch&#x3D;4)</span><br><span class="line">    for i in range(0,len(x_new)):</span><br><span class="line">        y_p &#x3D; a*x_new[i] + b</span><br><span class="line">        loss &#x3D; loss + (y_new[i] - y_p)*(y_new[i] - y_p)&#x2F;2</span><br><span class="line">        all_da &#x3D; all_da + da(y_new[i],y_p,x_new[i])</span><br><span class="line">        all_db &#x3D; all_db + db(y_new[i],y_p)</span><br><span class="line">    #loss_ &#x3D; calc_loss(a &#x3D; a,b&#x3D;b,x&#x3D;np.array(x),y&#x3D;np.array(y))</span><br><span class="line">    loss &#x3D; loss&#x2F;len(x_new)</span><br><span class="line"></span><br><span class="line">    # 绘制图1中的loss点</span><br><span class="line">    ax.scatter(a, b, loss, color&#x3D;&#39;black&#39;)</span><br><span class="line">    # 绘制图2中的loss点</span><br><span class="line">    plt.subplot(2,2,2)</span><br><span class="line">    plt.scatter(a,b,s&#x3D;5,color&#x3D;&#39;blue&#39;)</span><br><span class="line">    plt.plot([last_a,a],[last_b,b],color&#x3D;&#39;aqua&#39;)</span><br><span class="line">    # 绘制图3中的回归直线</span><br><span class="line">    plt.subplot(2, 2, 3)</span><br><span class="line">    plt.plot(x, y)</span><br><span class="line">    plt.plot(x, y, &#39;o&#39;)</span><br><span class="line">    x_ &#x3D; np.linspace(0, 1, 2)</span><br><span class="line">    y_draw &#x3D; a * x_ + b</span><br><span class="line">    plt.plot(x_, y_draw)</span><br><span class="line">    # 绘制图4的loss更新曲线</span><br><span class="line">    all_loss.append(loss)</span><br><span class="line">    all_step.append(step)</span><br><span class="line">    plt.subplot(2,2,4)</span><br><span class="line">    plt.plot(all_step,all_loss,color&#x3D;&#39;orange&#39;)</span><br><span class="line">    plt.xlabel(&quot;step&quot;)</span><br><span class="line">    plt.ylabel(&quot;loss&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # print(&#39;a &#x3D; %.3f,b &#x3D; %.3f&#39; % (a,b))</span><br><span class="line">    last_a &#x3D; a</span><br><span class="line">    last_b &#x3D; b</span><br><span class="line">    a &#x3D; a - rate*all_da</span><br><span class="line">    b &#x3D; b - rate*all_db</span><br><span class="line"></span><br><span class="line">    if step%1 &#x3D;&#x3D; 0:</span><br><span class="line">        print(&quot;step: &quot;, step, &quot; loss: &quot;, loss)</span><br><span class="line">        plt.show()</span><br><span class="line">        plt.pause(0.01)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="Momentum-动量算法"><a href="#Momentum-动量算法" class="headerlink" title="Momentum 动量算法"></a>Momentum 动量算法</h2><p>如果把梯度下降法想象成一个小球从山坡到山谷的过程，那么前面几篇文章的小球是这样移动的：从A点开始，计算当前A点的坡度，沿着坡度最大的方向走一段路，停下到B。在B点再看一看周围坡度最大的地方，沿着这个坡度方向走一段路，再停下。确切的来说，这并不像一个球，更像是一个正在下山的盲人，每走一步都要停下来，用拐杖来来探探四周的路，再走一步停下来，周而复始，直到走到山谷。而一个真正的小球要比这聪明多了，从A点滚动到B点的时候，小球带有一定的初速度，在当前初速度下继续加速下降，小球会越滚越快，更快的奔向谷底。momentum 动量法就是模拟这一过程来加速神经网络的优化的。可以用下图说明：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/optimizer/2.png" alt="fig2"></p>
<p>根据图示，其计算过程如下：</p>
<ul>
<li><p>A为起始点，首先计算A点的梯度∇a，然后下降到B点：$\Theta_{new}=\Theta−\alpha∇a$，其中$\Theta$为参数，$\alpha$为学习率。</p>
</li>
<li><p>到了B点需要加上A点的梯度，这里梯度需要有一个衰减值$\gamma$，推荐取0.9。这样的做法可以让早期的梯度对当前梯度的影响越来越小，如果没有衰减值，模型往往会震荡难以收敛，甚至发散。所以B点的参数更新公式是这样的：$v_t=\gamma v_{t-1} + \alpha∇b$，$\Theta_{new}=\Theta - v_t$。其中$v_{t-1}$表示之前所有步骤所累积的动量和。</p>
</li>
</ul>
<p>Momentum算法的特点如下：</p>
<ul>
<li><p>下降初期时，使用上一次参数更新，下降方向一致，乘上较大的$\gamma$能够进行很好的加速</p>
</li>
<li><p>下降中后期时，在局部最小值来回震荡的时候，$∇b-&gt;0$，$\gamma$使得更新幅度增大，跳出陷阱</p>
</li>
<li><p>在梯度改变方向的时候，$\gamma$能够减少更新 总而言之，momentum项能够在相关方向加速SGD，抑制振荡，从而加快收敛</p>
</li>
</ul>
<p>下面以LR算法为例，说明动量的计算过程：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def da(y,y_p,x):</span><br><span class="line">    return (y-y_p)*(-x)</span><br><span class="line"></span><br><span class="line">def db(y,y_p):</span><br><span class="line">    return (y-y_p)*(-1)</span><br><span class="line"></span><br><span class="line">a &#x3D; 10.0</span><br><span class="line">b &#x3D; -20.0</span><br><span class="line">all_loss &#x3D; []</span><br><span class="line">all_step &#x3D; []</span><br><span class="line">last_a &#x3D; a</span><br><span class="line">last_b &#x3D; b</span><br><span class="line">va &#x3D; 0</span><br><span class="line">vb &#x3D; 0</span><br><span class="line">gamma &#x3D; 0.9</span><br><span class="line">for step in range(1,100):</span><br><span class="line">    loss &#x3D; 0</span><br><span class="line">    all_da &#x3D; 0</span><br><span class="line">    all_db &#x3D; 0</span><br><span class="line">    for i in range(0,len(x)):</span><br><span class="line">        y_p &#x3D; a*x[i] + b</span><br><span class="line">        loss &#x3D; loss + (y[i] - y_p)*(y[i] - y_p)&#x2F;2  </span><br><span class="line">        # loss &#x3D; (y-ax[i]-b)(y-ax[i]-b)&#x2F;2&#x3D;(y^2 + (ax[i])^2 + b^2 - 2yax[i] - 2yb + 2ax[i]b&#x2F;2</span><br><span class="line">        # 对loss中的参数a求偏导：(2ax[i]^2 - 2yx[i] + 2x[i]b)&#x2F;2 &#x3D; x[i](ax[i]+b-y) &#x3D; x[i](y[i]-y) &#x3D; -x[i](y - y[i]) </span><br><span class="line">        all_da &#x3D; all_da + da(y[i],y_p,x[i])</span><br><span class="line">        # 对loss中的参数b求偏导：(2b - 2y + 2ax[i])&#x2F;2 &#x3D; (ax[i] + b - y) &#x3D; (y - y[i]) * (-1)</span><br><span class="line">        all_db &#x3D; all_db + db(y[i],y_p)</span><br><span class="line">    #loss_ &#x3D; calc_loss(a &#x3D; a,b&#x3D;b,x&#x3D;np.array(x),y&#x3D;np.array(y))</span><br><span class="line">    loss &#x3D; loss&#x2F;len(x)</span><br><span class="line"></span><br><span class="line">    # 绘制图1中的loss点</span><br><span class="line">    ax.scatter(a, b, loss, color&#x3D;&#39;black&#39;)</span><br><span class="line">    # 绘制图2中的loss点</span><br><span class="line">    plt.subplot(2,2,2)</span><br><span class="line">    plt.scatter(a,b,s&#x3D;5,color&#x3D;&#39;blue&#39;)</span><br><span class="line">    plt.plot([last_a,a],[last_b,b],color&#x3D;&#39;aqua&#39;)</span><br><span class="line">    # 绘制图3中的回归直线</span><br><span class="line">    plt.subplot(2, 2, 3)</span><br><span class="line">    plt.plot(x, y)</span><br><span class="line">    plt.plot(x, y, &#39;o&#39;)</span><br><span class="line">    x_ &#x3D; np.linspace(0, 1, 2)</span><br><span class="line">    y_draw &#x3D; a * x_ + b</span><br><span class="line">    plt.plot(x_, y_draw)</span><br><span class="line">    # 绘制图4的loss更新曲线</span><br><span class="line">    all_loss.append(loss)</span><br><span class="line">    all_step.append(step)</span><br><span class="line">    plt.subplot(2,2,4)</span><br><span class="line">    plt.plot(all_step,all_loss,color&#x3D;&#39;orange&#39;)</span><br><span class="line">    plt.xlabel(&quot;step&quot;)</span><br><span class="line">    plt.ylabel(&quot;loss&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # print(&#39;a &#x3D; %.3f,b &#x3D; %.3f&#39; % (a,b))</span><br><span class="line">    last_a &#x3D; a</span><br><span class="line">    last_b &#x3D; b</span><br><span class="line">    va &#x3D; gamma * va+ rate*all_da</span><br><span class="line">    vb &#x3D; gamma * vb+ rate*all_db</span><br><span class="line">    a &#x3D; a - va</span><br><span class="line">    b &#x3D; b - vb</span><br><span class="line"></span><br><span class="line">    if step%1 &#x3D;&#x3D; 0:</span><br><span class="line">        print(&quot;step: &quot;, step, &quot; loss: &quot;, loss)</span><br><span class="line">        plt.show()</span><br><span class="line">        plt.pause(0.01)</span><br><span class="line">plt.show()</span><br><span class="line"># plt.pause(9999)</span><br></pre></td></tr></table></figure>
<p>总结：优化算法中，⽬标函数⾃变量的每⼀个元素在相同时间步都使⽤同⼀个学习率来⾃我迭代。在“动量法”⾥我们看到当x1和x2的梯度值有较⼤差别时，需要选择⾜够小的学习率使得⾃变量在梯度值较⼤的维度上不发散。但这样会导致⾃变量在梯度值较小的维度上迭代过慢。动量法依赖指数加权移动平均使得⾃变量的更新⽅向更加⼀致，从而降低发散的可能。</p>
<h2 id="AdaGrad算法"><a href="#AdaGrad算法" class="headerlink" title="AdaGrad算法"></a>AdaGrad算法</h2><p>AdaGrad思路基本是借鉴L2 Regularizer，不过此时调节的不是𝑊，而是𝐺𝑟𝑎𝑑𝑖𝑒𝑛𝑡。AdaGrad算法根据⾃变量在每个维度的梯度值的⼤小来调整各个维度上的学习率，从而避免统⼀的学习率难以适应所有维度的问题。</p>
<p>这个算法的优点是可以对低频的参数做较大的更新，对高频的做较小的更新，也因此，对于稀疏的数据它的表现很好，很好地提高了 SGD 的鲁棒性，例如识别 Youtube 视频里面的猫，训练 GloVe word embeddings，因为它们都是需要在低频的特征上有更大的更新。</p>
<p>它的缺点是当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到⼀个有⽤的解。具体计算过程如下：</p>
<ul>
<li><p>$s_t = s_{t-1} + g_t \odot g_t$</p>
</li>
<li><p>$x_t=x_{t-1}-\frac{\eta }{s_t + \varepsilon } \odot g_t$</p>
</li>
</ul>
<p>在时间步0，AdaGrad将$s_0$中每个元素初始化为0。在时间步t，⾸先将小批量随机梯度$g_t$按元素平⽅后累加到变量$s_t$，接着，我们将⽬标函数⾃变量中每个元素的学习率通过按元素运算重新调整⼀下。其中$\eta$是学习率，$\varepsilon$是为了维持数值稳定性而添加的常数，如10的-6次方。这⾥开⽅、除法和乘法的运算都是按元素运算的。这些按元素运算使得⽬标函数⾃变量中每个元素都分别拥有⾃⼰的学习率。一般$\varepsilon$选取0.01</p>
<p>需要强调的是，小批量随机梯度按元素平⽅的累加变量$s_t$出现在学习率的分⺟项中。因此：</p>
<ul>
<li><p>如果⽬标函数有关⾃变量中某个元素的偏导数⼀直都较⼤，那么该元素的学习率将下降较快；也就是说，在训练前期，梯度较小，使得Regularizer项很大，放大梯度。[激励阶段]</p>
</li>
<li><p>反之，如果⽬标函数有关⾃变量中某个元素的偏导数⼀直都较小，那么该元素的学习率将下降较慢；也就是说，训练后期，梯度较大，使得Regularizer项很小，缩小梯度。[惩罚阶段]</p>
</li>
</ul>
<p>可以这样理解：AdaGrad过程，是一个递推过程，次从𝜏=1，推到𝜏=𝑡，把沿路的𝐺𝑟𝑎𝑑𝑖𝑒𝑛𝑡的平方根，作为Regularizer。由于Regularizer是专门针对Gradient的，所以有利于解决Gradient Vanish/Expoloding问题。</p>
<p>下图为AdaGrad优化的一个过程：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/optimizer/3.png" alt="fig3"></p>
<p>AdaGrad的缺点是：</p>
<ul>
<li><p>由公式可以看出，仍依赖于人工设置一个全局学习率。$\eta$设置过大的话，会使regularizer过于敏感，对梯度的调节太大</p>
</li>
<li><p>中后期，分母上梯度平方的累加将会越来越大，使$g_t-&gt;0$，使得训练提前结束</p>
</li>
</ul>
<p>下面继续以LR为例，说明其计算过程。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a &#x3D; 10.0</span><br><span class="line">b &#x3D; -20.0</span><br><span class="line">all_loss &#x3D; []</span><br><span class="line">all_step &#x3D; []</span><br><span class="line">last_a &#x3D; a</span><br><span class="line">last_b &#x3D; b</span><br><span class="line">n &#x3D; np.array([0,0])</span><br><span class="line">epsilon &#x3D; 1e-8</span><br><span class="line">for step in range(1,500):</span><br><span class="line">    loss &#x3D; 0</span><br><span class="line">    all_da &#x3D; 0</span><br><span class="line">    all_db &#x3D; 0</span><br><span class="line">    for i in range(0,len(x)):</span><br><span class="line">        y_p &#x3D; a*x[i] + b</span><br><span class="line">        loss &#x3D; loss + (y[i] - y_p)*(y[i] - y_p)&#x2F;2</span><br><span class="line">        all_da &#x3D; all_da + da(y[i],y_p,x[i])</span><br><span class="line">        all_db &#x3D; all_db + db(y[i],y_p)</span><br><span class="line">    #loss_ &#x3D; calc_loss(a &#x3D; a,b&#x3D;b,x&#x3D;np.array(x),y&#x3D;np.array(y))</span><br><span class="line">    loss &#x3D; loss&#x2F;len(x)</span><br><span class="line"></span><br><span class="line">    # 绘制图1中的loss点</span><br><span class="line">    ax.scatter(a, b, loss, color&#x3D;&#39;black&#39;)</span><br><span class="line">    # 绘制图2中的loss点</span><br><span class="line">    plt.subplot(2,2,2)</span><br><span class="line">    plt.scatter(a,b,s&#x3D;5,color&#x3D;&#39;blue&#39;)</span><br><span class="line">    plt.plot([last_a,a],[last_b,b],color&#x3D;&#39;aqua&#39;)</span><br><span class="line">    # 绘制图3中的回归直线</span><br><span class="line">    plt.subplot(2, 2, 3)</span><br><span class="line">    plt.plot(x, y)</span><br><span class="line">    plt.plot(x, y, &#39;o&#39;)</span><br><span class="line">    x_ &#x3D; np.linspace(0, 1, 2)</span><br><span class="line">    y_draw &#x3D; a * x_ + b</span><br><span class="line">    plt.plot(x_, y_draw)</span><br><span class="line">    # 绘制图4的loss更新曲线</span><br><span class="line">    all_loss.append(loss)</span><br><span class="line">    all_step.append(step)</span><br><span class="line">    plt.subplot(2,2,4)</span><br><span class="line">    plt.plot(all_step,all_loss,color&#x3D;&#39;orange&#39;)</span><br><span class="line">    plt.xlabel(&quot;step&quot;)</span><br><span class="line">    plt.ylabel(&quot;loss&quot;)</span><br><span class="line"></span><br><span class="line">    # print(&#39;a &#x3D; %.3f,b &#x3D; %.3f&#39; % (a,b))</span><br><span class="line">    last_a &#x3D; a</span><br><span class="line">    last_b &#x3D; b</span><br><span class="line"></span><br><span class="line">    #-- 参数更新</span><br><span class="line">    n[0] &#x3D; n[0]+np.square(all_da)</span><br><span class="line">    n[1] &#x3D; n[1]+np.square(all_db)</span><br><span class="line">    rate_new &#x3D; rate&#x2F;(np.sqrt(n + epsilon))</span><br><span class="line">    print(&#39;rate_new a:&#39;,rate_new[0],&#39; b:&#39;,rate_new[1])</span><br><span class="line">    a &#x3D; a - (rate&#x2F;(np.sqrt(n[0] + epsilon)))*all_da</span><br><span class="line">    b &#x3D; b - (rate&#x2F;(np.sqrt(n[1] + epsilon)))*all_db</span><br><span class="line"></span><br><span class="line">    if step%1 &#x3D;&#x3D; 0:</span><br><span class="line">        print(&quot;step: &quot;, step, &quot; loss: &quot;, loss)</span><br><span class="line">        plt.show()</span><br><span class="line">        plt.pause(0.01)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="RMSProp算法"><a href="#RMSProp算法" class="headerlink" title="RMSProp算法"></a>RMSProp算法</h2><p>当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到⼀个有⽤的解。为了解决这⼀问题，RMSProp算法对AdaGrad算法做了⼀点小小的修改：</p>
<ul>
<li><p>$s_t = \lambda s_{t-1} + (1-\lambda)g_t \odot g_t$</p>
</li>
<li><p>$x_t=x_{t-1}-\frac{\eta }{s_t + \varepsilon } \odot g_t$</p>
</li>
</ul>
<p>因为RMSProp算法的状态变量st是对平⽅项$g_t \odot g_t$的指数加权移动平均，所以可以看作是最近$\frac{1}{1-\lambda}$个时间步的小批量随机梯度平⽅项的加权平均。如此⼀来，⾃变量每个元素的学习率在迭代过程中就不再⼀直降低（或不变）。</p>
<p>下图为RMSProp优化的一个过程：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/optimizer/4.png" alt="fig4"></p>
<p>RMSProp有如下特点：</p>
<ul>
<li><p>训练初中期，加速效果不错，很快</p>
</li>
<li><p>训练后期，反复在局部最小值附近抖动（下面实验中，如果step设置为500，可能出现loss非常大的情况）</p>
</li>
<li><p>其实RMSprop依然依赖于全局学习率</p>
</li>
<li><p>RMSprop算是Adagrad的一种发展，和Adadelta的变体，效果趋于二者之间</p>
</li>
<li><p>适合处理非平稳目标 - 对于RNN效果很好</p>
</li>
<li><p>RMSProp利用了二阶信息做了Gradient优化，在BatchNorm之后，对其需求不是很大。</p>
</li>
</ul>
<p>下面继续以LR为例，说明其计算过程。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a &#x3D; 10.0</span><br><span class="line">b &#x3D; -20.0</span><br><span class="line">all_loss &#x3D; []</span><br><span class="line">all_step &#x3D; []</span><br><span class="line">last_a &#x3D; a</span><br><span class="line">last_b &#x3D; b</span><br><span class="line">lambdas &#x3D; 0.9</span><br><span class="line">n &#x3D; np.array([0,0])</span><br><span class="line">epsilon &#x3D; 1e-8</span><br><span class="line">for step in range(1,200):</span><br><span class="line">    loss &#x3D; 0</span><br><span class="line">    all_da &#x3D; 0</span><br><span class="line">    all_db &#x3D; 0</span><br><span class="line">    for i in range(0,len(x)):</span><br><span class="line">        y_p &#x3D; a*x[i] + b</span><br><span class="line">        loss &#x3D; loss + (y[i] - y_p)*(y[i] - y_p)&#x2F;2</span><br><span class="line">        all_da &#x3D; all_da + da(y[i],y_p,x[i])</span><br><span class="line">        all_db &#x3D; all_db + db(y[i],y_p)</span><br><span class="line">    #loss_ &#x3D; calc_loss(a &#x3D; a,b&#x3D;b,x&#x3D;np.array(x),y&#x3D;np.array(y))</span><br><span class="line">    loss &#x3D; loss&#x2F;len(x)</span><br><span class="line"></span><br><span class="line">    # 绘制图1中的loss点</span><br><span class="line">    ax.scatter(a, b, loss, color&#x3D;&#39;black&#39;)</span><br><span class="line">    # 绘制图2中的loss点</span><br><span class="line">    plt.subplot(2,2,2)</span><br><span class="line">    plt.scatter(a,b,s&#x3D;5,color&#x3D;&#39;blue&#39;)</span><br><span class="line">    plt.plot([last_a,a],[last_b,b],color&#x3D;&#39;aqua&#39;)</span><br><span class="line">    # 绘制图3中的回归直线</span><br><span class="line">    plt.subplot(2, 2, 3)</span><br><span class="line">    plt.plot(x, y)</span><br><span class="line">    plt.plot(x, y, &#39;o&#39;)</span><br><span class="line">    x_ &#x3D; np.linspace(0, 1, 2)</span><br><span class="line">    y_draw &#x3D; a * x_ + b</span><br><span class="line">    plt.plot(x_, y_draw)</span><br><span class="line">    # 绘制图4的loss更新曲线</span><br><span class="line">    all_loss.append(loss)</span><br><span class="line">    all_step.append(step)</span><br><span class="line">    plt.subplot(2,2,4)</span><br><span class="line">    plt.plot(all_step,all_loss,color&#x3D;&#39;orange&#39;)</span><br><span class="line">    plt.xlabel(&quot;step&quot;)</span><br><span class="line">    plt.ylabel(&quot;loss&quot;)</span><br><span class="line"></span><br><span class="line">    # print(&#39;a &#x3D; %.3f,b &#x3D; %.3f&#39; % (a,b))</span><br><span class="line">    last_a &#x3D; a</span><br><span class="line">    last_b &#x3D; b</span><br><span class="line"></span><br><span class="line">    #-- 参数更新</span><br><span class="line">    n[0] &#x3D; lambdas * n[0] + (1 - lambdas) * np.square(all_da)</span><br><span class="line">    n[1] &#x3D; lambdas * n[1] + (1 - lambdas) * np.square(all_db)</span><br><span class="line">    rate_new &#x3D; rate&#x2F;(np.sqrt(n + epsilon))</span><br><span class="line">    print(&#39;rate_new a:&#39;,rate_new[0],&#39; b:&#39;,rate_new[1])</span><br><span class="line">    a &#x3D; a - (rate&#x2F;(np.sqrt(n[0] + epsilon)))*all_da</span><br><span class="line">    b &#x3D; b - (rate&#x2F;(np.sqrt(n[1] + epsilon)))*all_db</span><br><span class="line"></span><br><span class="line">    if step%1 &#x3D;&#x3D; 0:</span><br><span class="line">        print(&quot;step: &quot;, step, &quot; loss: &quot;, loss)</span><br><span class="line">        plt.show()</span><br><span class="line">        plt.pause(0.01)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="AdaDelta算法"><a href="#AdaDelta算法" class="headerlink" title="AdaDelta算法"></a>AdaDelta算法</h2><p>AdaDelta基本思想是用一阶的方法，近似模拟二阶牛顿法。除了RMSProp算法以外，另⼀个常⽤优化算法AdaDelta算法也针对AdaGrad算法在迭代后期可能较难找到有⽤解的问题做了改进。有意思的是，AdaDelta算法没有学习率这⼀超参数。其计算过程如下：</p>
<ul>
<li><p>$s_t = \lambda s_{t-1} + (1-\lambda)g_t \odot g_t$</p>
</li>
<li><p>$g’_t=\sqrt{\frac{\Delta x_t + \varepsilon }{s_t +  \varepsilon}} \odot g_t$</p>
</li>
<li><p>$\Delta x_t=px_{t-1}-(1-p)g’_t \odot g’_t$</p>
</li>
<li><p>$x_t = x_{t-1} - g’_t$</p>
</li>
</ul>
<p>AdaDelta算法的第一步和RMSProp算法⼀样，与RMSProp算法不同的是，AdaDelta算法还维护⼀个额外的状态变量$\Delta x_t$来计算自变量的变化量（按$g’_t$元素平⽅的指数加权移动平均），其元素同样在时间步0时被初始化为0。</p>
<p>可以这样理解：相比于同样是基于Gradient的Regularizer，不过只取最近的w个状态，这样不会让梯度被惩罚至0。</p>
<p>与RMSProp相比，AdaDelta算法没有学习率超参数，它通过使用有关自变量更新量平方的指数加权移动平均的项来替代RMSProp算法中的学习率。</p>
<p>AdaDelta的特点：</p>
<ul>
<li><p>从多个数据集情况来看，AdaDelta在训练初期和中期，具有非常不错的加速效果。</p>
</li>
<li><p>但是到训练后期，进入局部最小值雷区之后，AdaDelta就会反复在局部最小值附近抖动。主要体现在验证集错误率上，脱离不了局部最小值吸引盆。这时候，切换成动量SGD，如果把学习率降低一个量级，就会发现验证集正确率有2%~5%的提升。[注]：使用Batch Norm之后，这样从AdaDelta切到SGD会导致数值体系崩溃，原因未知。</p>
</li>
</ul>
<p>下面继续以LR为例，说明其计算过程。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a &#x3D; 10.0</span><br><span class="line">b &#x3D; -20.0</span><br><span class="line">all_loss &#x3D; []</span><br><span class="line">all_step &#x3D; []</span><br><span class="line">last_a &#x3D; a</span><br><span class="line">last_b &#x3D; b</span><br><span class="line">n &#x3D; np.array([0,0])</span><br><span class="line">theta &#x3D; np.array([0,0]).astype(np.float32) # 每一次a,b迭代的更新值</span><br><span class="line"></span><br><span class="line">apple &#x3D; np.array([0,0]).astype(np.float32)</span><br><span class="line">pear &#x3D; np.array([0,0]).astype(np.float32)</span><br><span class="line"># 迭代</span><br><span class="line">for step in range(1,500):</span><br><span class="line">    loss &#x3D; 0</span><br><span class="line">    all_da &#x3D; 0</span><br><span class="line">    all_db &#x3D; 0</span><br><span class="line">    all_d &#x3D; np.array([0,0]).astype(np.float32)</span><br><span class="line">    for i in range(0,len(x)):</span><br><span class="line">        y_p &#x3D; a*x[i] + b</span><br><span class="line">        loss &#x3D; loss + (y[i] - y_p)*(y[i] - y_p)&#x2F;2</span><br><span class="line">        all_da &#x3D; all_da + da(y[i],y_p,x[i])</span><br><span class="line">        all_db &#x3D; all_db + db(y[i],y_p)</span><br><span class="line">    #loss_ &#x3D; calc_loss(a &#x3D; a,b&#x3D;b,x&#x3D;np.array(x),y&#x3D;np.array(y))</span><br><span class="line">    all_d &#x3D; np.array([all_da,all_db])</span><br><span class="line">    loss &#x3D; loss&#x2F;len(x)</span><br><span class="line"></span><br><span class="line">    # 绘制图1中的loss点</span><br><span class="line">    ax.scatter(a, b, loss, color&#x3D;&#39;black&#39;)</span><br><span class="line">    # 绘制图2中的loss点</span><br><span class="line">    plt.subplot(2,2,2)</span><br><span class="line">    plt.scatter(a,b,s&#x3D;5,color&#x3D;&#39;blue&#39;)</span><br><span class="line">    plt.plot([last_a,a],[last_b,b],color&#x3D;&#39;aqua&#39;)</span><br><span class="line">    # 绘制图3中的回归直线</span><br><span class="line">    plt.subplot(2, 2, 3)</span><br><span class="line">    plt.plot(x, y)</span><br><span class="line">    plt.plot(x, y, &#39;o&#39;)</span><br><span class="line">    x_ &#x3D; np.linspace(0, 1, 2)</span><br><span class="line">    y_draw &#x3D; a * x_ + b</span><br><span class="line">    plt.plot(x_, y_draw)</span><br><span class="line">    # 绘制图4的loss更新曲线</span><br><span class="line">    all_loss.append(loss)</span><br><span class="line">    all_step.append(step)</span><br><span class="line">    plt.subplot(2,2,4)</span><br><span class="line">    plt.plot(all_step,all_loss,color&#x3D;&#39;orange&#39;)</span><br><span class="line">    plt.xlabel(&quot;step&quot;)</span><br><span class="line">    plt.ylabel(&quot;loss&quot;)</span><br><span class="line"></span><br><span class="line">    # print(&#39;a &#x3D; %.3f,b &#x3D; %.3f&#39; % (a,b))</span><br><span class="line">    last_a &#x3D; a</span><br><span class="line">    last_b &#x3D; b</span><br><span class="line"></span><br><span class="line">    #-- 参数更新</span><br><span class="line">    apple &#x3D; gamma*apple + (1-gamma)*(all_d**2) # apple with all_d of this step</span><br><span class="line">#     rms_apple &#x3D; np.sqrt(apple + epsilon)</span><br><span class="line">    rms_apple &#x3D; apple + epsilon</span><br><span class="line"></span><br><span class="line">    pear &#x3D; gamma*pear + (1-gamma)*(theta**2) # pear with theta of last step</span><br><span class="line">#     rms_pear &#x3D; np.sqrt(pear + epsilon)</span><br><span class="line"></span><br><span class="line">#     theta &#x3D; -(rms_pear&#x2F;rms_apple)*all_d</span><br><span class="line">    theta &#x3D; -np.sqrt(rms_pear&#x2F;rms_apple) * all_d</span><br><span class="line">    [a,b] &#x3D; [a,b] + theta</span><br><span class="line"></span><br><span class="line">    if step%1 &#x3D;&#x3D; 0:</span><br><span class="line">        print(&quot;step: &quot;, step, &quot; loss: &quot;, loss,&quot;rms_pear: &quot;,rms_pear,&quot; rms_apple&quot;,rms_apple)</span><br><span class="line">        plt.show()</span><br><span class="line">        plt.pause(0.01)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="Adam算法"><a href="#Adam算法" class="headerlink" title="Adam算法"></a>Adam算法</h2><p>Adam算法在RMSProp算法基础上对小批量随机梯度也做了指数加权移动平均。</p>
<ul>
<li><p>$v_t=\beta_1v_{t-1} + (1-\beta_1)g_t$</p>
<ul>
<li>Adam算法使⽤了动量变量$v_t$和RMSProp算法中小批量随机梯度按元素平⽅的指数加权移动平均变量$s_t$，并在时间步0将它们中每个元素初始化为0。给定超参数0 ≤ $\beta_1$ &lt; 1（算法作者建议设为0.9），时间步t的动量变量$v_t$即小批量随机梯度$g_t$的指数加权移动平均</li>
</ul>
</li>
<li><p>$s_t=\beta_2s_{t-1}+(1-\beta_2)g_t\odot g_t$</p>
<ul>
<li>和RMSProp算法中⼀样，给定超参数0 ≤ $\beta_2$ &lt; 1（算法作者建议设为0.999），将小批量随机梯度按元素平⽅后的项$g_t\odot g_t$做指数加权移动平均得到$s_t$</li>
</ul>
</li>
<li><p>$v’_t=\frac{v_t}{1-\beta^t_1}$、$s’_t=\frac{s_t}{1-\beta^t_2}$</p>
<ul>
<li>因为当t较小时，过去各时间步小批量随机梯度权值之和会较小。例如，当$\beta_1=0.9$时，$v_1=0.1g_1$。为了消除这样的影响，对于任意时间步t，我们可以将$v_t$再除以${1-\beta^t_1}$，从而使过去各时间步小批量随机梯度权值之和为1。这也叫作偏差修正。在Adam算法中，我们对变量$v_t$和$s_t$均作偏差修正得到</li>
</ul>
</li>
<li><p>$g’_t=\frac{\eta v’_t}{\sqrt{s’_t}+\varepsilon }$</p>
<ul>
<li>接下来，Adam算法使⽤以上偏差修正后的变量$v_t$和$s_t$，将模型参数中每个元素的学习率通过按元素运算重新调整得到（5）。其中$\eta$是学习率，$\varepsilon$是为了维持数值稳定性而添加的常数，如10的-8次方。和AdaGrad算法、RMSProp算法以及AdaDelta算法⼀样，⽬标函数⾃变量中每个元素都分别拥有⾃⼰的学习率</li>
</ul>
</li>
<li><p>$x_t=x_{t-1}-g’_t$</p>
<ul>
<li>使用$g’_t$迭代自变量</li>
</ul>
</li>
</ul>
<p>Adam算法有如下特点：</p>
<ul>
<li>除了像Adadelta和RMSprop一样存储了过去梯度的平方$s_t$的指数衰减平均值 ，也像momentum一样保持了过去梯度$v_t$的指数衰减平均值</li>
<li>如果$v_t$和$s_t$被初始化为0向量，那它们就会向0偏置，所以做了偏差校正，通过计算偏差校正后的$v_t$和$s_t$来抵消这些偏差</li>
</ul>
<p>下面继续以LR为例，说明其计算过程。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a &#x3D; 10.0</span><br><span class="line">b &#x3D; -20.0</span><br><span class="line">all_loss &#x3D; []</span><br><span class="line">all_step &#x3D; []</span><br><span class="line">last_a &#x3D; a</span><br><span class="line">last_b &#x3D; b</span><br><span class="line">m &#x3D; 0.0</span><br><span class="line">v &#x3D; 0.0</span><br><span class="line">theta &#x3D; np.array([0,0]).astype(np.float32)</span><br><span class="line"></span><br><span class="line">rate &#x3D; 0.001</span><br><span class="line">beta1 &#x3D; 0.9</span><br><span class="line">beta2 &#x3D; 0.999</span><br><span class="line">epsilon &#x3D; 1e-8</span><br><span class="line"></span><br><span class="line">for step in range(1,500):</span><br><span class="line">    loss &#x3D; 0</span><br><span class="line">    all_da &#x3D; 0</span><br><span class="line">    all_db &#x3D; 0</span><br><span class="line">    for i in range(0,len(x)):</span><br><span class="line">        y_p &#x3D; a*x[i] + b</span><br><span class="line">        loss &#x3D; loss + (y[i] - y_p)*(y[i] - y_p)&#x2F;2</span><br><span class="line">        all_da &#x3D; all_da + da(y[i],y_p,x[i])</span><br><span class="line">        all_db &#x3D; all_db + db(y[i],y_p)</span><br><span class="line">    loss &#x3D; loss&#x2F;len(x)</span><br><span class="line">    all_d &#x3D; np.array([all_da,all_db]).astype(np.float32)</span><br><span class="line">    # 绘制图1中的loss点</span><br><span class="line">    ax.scatter(a, b, loss, color&#x3D;&#39;black&#39;)</span><br><span class="line">    # 绘制图2中的loss点</span><br><span class="line">    plt.subplot(2,2,2)</span><br><span class="line">    plt.scatter(a,b,s&#x3D;5,color&#x3D;&#39;blue&#39;)</span><br><span class="line">    plt.plot([last_a,a],[last_b,b],color&#x3D;&#39;aqua&#39;)</span><br><span class="line">    # 绘制图3中的回归直线</span><br><span class="line">    plt.subplot(2, 2, 3)</span><br><span class="line">    plt.plot(x, y)</span><br><span class="line">    plt.plot(x, y, &#39;o&#39;)</span><br><span class="line">    x_ &#x3D; np.linspace(0, 1, 2)</span><br><span class="line">    y_draw &#x3D; a * x_ + b</span><br><span class="line">    plt.plot(x_, y_draw)</span><br><span class="line">    # 绘制图4的loss更新曲线</span><br><span class="line">    all_loss.append(loss)</span><br><span class="line">    all_step.append(step)</span><br><span class="line">    plt.subplot(2,2,4)</span><br><span class="line">    plt.plot(all_step,all_loss,color&#x3D;&#39;orange&#39;)</span><br><span class="line">    plt.xlabel(&quot;step&quot;)</span><br><span class="line">    plt.ylabel(&quot;loss&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # print(&#39;a &#x3D; %.3f,b &#x3D; %.3f&#39; % (a,b))</span><br><span class="line">    last_a &#x3D; a</span><br><span class="line">    last_b &#x3D; b</span><br><span class="line"></span><br><span class="line">    m &#x3D; beta1*m + (1-beta1)*all_d</span><br><span class="line">    v &#x3D; beta2*v + (1-beta2)*(all_d**2)</span><br><span class="line"></span><br><span class="line">    m_ &#x3D; m&#x2F;(1 - beta1)</span><br><span class="line">    v_ &#x3D; v&#x2F;(1 - beta2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    theta &#x3D; -(rate*m_&#x2F;(np.sqrt(v_) + epsilon))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    [a,b] &#x3D; [a,b] + theta</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    if step%1 &#x3D;&#x3D; 0:</span><br><span class="line">        print(&quot;step: &quot;, step, &quot; loss: &quot;, loss)</span><br><span class="line">        plt.show()</span><br><span class="line">        plt.pause(0.01)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="如何选择优化算法"><a href="#如何选择优化算法" class="headerlink" title="如何选择优化算法"></a>如何选择优化算法</h2><ul>
<li><p>如果数据是稀疏的，就用自适用方法，即 Adagrad, Adadelta, RMSprop, Adam。</p>
</li>
<li><p>RMSprop, Adadelta, Adam 在很多情况下的效果是相似的。</p>
</li>
<li><p>Adam 就是在 RMSprop 的基础上加了 bias-correction 和 momentum，</p>
</li>
<li><p>随着梯度变的稀疏，Adam 比 RMSprop 效果会好。</p>
</li>
<li><p>整体来讲，Adam 是最好的选择。</p>
</li>
<li><p>很多论文里都会用 SGD，没有 momentum 等。SGD 虽然能达到极小值，但是比其它算法用的时间长，而且可能会被困在鞍点。</p>
</li>
<li><p>如果需要更快的收敛，或者是训练更深更复杂的神经网络，需要用一种自适应的算法。</p>
</li>
</ul>
<h2 id="keras中优化器的用法"><a href="#keras中优化器的用法" class="headerlink" title="keras中优化器的用法"></a>keras中优化器的用法</h2><p>以全连接网络做分类任务为例子，说明keras中optimizer的用法</p>
<h3 id="生成数据"><a href="#生成数据" class="headerlink" title="生成数据"></a>生成数据</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">X &#x3D; np.linspace(-1, 1, 200) #在返回（-1, 1）范围内的等差序列</span><br><span class="line">np.random.shuffle(X)    # 打乱顺序</span><br><span class="line">Y &#x3D; 0.5 * X + 2 + np.random.normal(0, 0.05, (200, )) #生成Y并添加噪声</span><br><span class="line"></span><br><span class="line">X_train, Y_train &#x3D; X[:160], Y[:160]     # 前160组数据为训练数据集</span><br><span class="line">X_test, Y_test &#x3D; X[160:], Y[160:]      #后40组数据为测试数据集</span><br></pre></td></tr></table></figure>
<h3 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from keras import optimizers</span><br><span class="line">from keras.layers import Dense, Activation</span><br><span class="line">from keras.models import Sequential</span><br><span class="line"></span><br><span class="line">model &#x3D; Sequential()</span><br><span class="line">model.add(Dense(input_dim&#x3D;1, units&#x3D;1))</span><br></pre></td></tr></table></figure>
<h3 id="使用SGD"><a href="#使用SGD" class="headerlink" title="使用SGD"></a>使用SGD</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># clipvalue&#x3D;0.5表示保留(-0.5, 0.5)之间的梯度，其他的需要做梯度裁剪</span><br><span class="line"># momentum&#x3D;0.9使用了动量平滑，可以设置为0.0表示纯SGD，nesterov&#x3D;True表示使用Nesterov动量</span><br><span class="line">sgd &#x3D; optimizers.SGD(lr&#x3D;0.01, decay&#x3D;1e-6, momentum&#x3D;0.9, nesterov&#x3D;True, clipvalue&#x3D;0.5)</span><br><span class="line">model.compile(loss&#x3D;&#39;mean_squared_error&#39;, optimizer&#x3D;sgd)</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<h3 id="训练并测试"><a href="#训练并测试" class="headerlink" title="训练并测试"></a>训练并测试</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(&#39;Training -----------&#39;)</span><br><span class="line">for step in range(501):</span><br><span class="line">    cost &#x3D; model.train_on_batch(X_train, Y_train)</span><br><span class="line">    if step % 50 &#x3D;&#x3D; 0:</span><br><span class="line">        print(&quot;After %d trainings, the cost: %f&quot; % (step, cost))</span><br><span class="line">        </span><br><span class="line">print(&#39;\nTesting ------------&#39;)</span><br><span class="line">cost &#x3D; model.evaluate(X_test, Y_test, batch_size&#x3D;40)</span><br><span class="line">print(&#39;test cost:&#39;, cost)</span><br><span class="line">W, b &#x3D; model.layers[0].get_weights()</span><br><span class="line">print(&#39;Weights&#x3D;&#39;, W, &#39;\nbiases&#x3D;&#39;, b)</span><br></pre></td></tr></table></figure>
<h3 id="使用RMSprop"><a href="#使用RMSprop" class="headerlink" title="使用RMSprop"></a>使用RMSprop</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rmsprop &#x3D; optimizers.RMSprop(lr&#x3D;0.001, rho&#x3D;0.9, epsilon&#x3D;1e-06)</span><br></pre></td></tr></table></figure>
<h3 id="使用Adagrad"><a href="#使用Adagrad" class="headerlink" title="使用Adagrad"></a>使用Adagrad</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">adagrad &#x3D; optimizers.Adagrad(lr&#x3D;0.01, epsilon&#x3D;1e-06)</span><br></pre></td></tr></table></figure>
<h3 id="使用Adadelta"><a href="#使用Adadelta" class="headerlink" title="使用Adadelta"></a>使用Adadelta</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">adadelta &#x3D; optimizers.Adadelta(lr&#x3D;1.0, rho&#x3D;0.95, epsilon&#x3D;1e-06)</span><br></pre></td></tr></table></figure>
<h3 id="使用Adam"><a href="#使用Adam" class="headerlink" title="使用Adam"></a>使用Adam</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">adam &#x3D; optimizers.Adam(lr&#x3D;0.001, beta_1&#x3D;0.9, beta_2&#x3D;0.999, epsilon&#x3D;1e-08)</span><br></pre></td></tr></table></figure>
<h2 id="BERT中使用的优化器解析"><a href="#BERT中使用的优化器解析" class="headerlink" title="BERT中使用的优化器解析"></a>BERT中使用的优化器解析</h2><p>bert使用的optimizer叫做AdamWeightDecayOptimizer，先放代码为敬：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">def create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps, use_tpu):</span><br><span class="line">  &quot;&quot;&quot;Creates an optimizer training op.&quot;&quot;&quot;</span><br><span class="line">  global_step &#x3D; tf.train.get_or_create_global_step()</span><br><span class="line"></span><br><span class="line">  learning_rate &#x3D; tf.constant(value&#x3D;init_lr, shape&#x3D;[], dtype&#x3D;tf.float32)</span><br><span class="line"></span><br><span class="line">  # Implements linear decay of the learning rate.</span><br><span class="line">  learning_rate &#x3D; tf.train.polynomial_decay(</span><br><span class="line">      learning_rate,</span><br><span class="line">      global_step,</span><br><span class="line">      num_train_steps,</span><br><span class="line">      end_learning_rate&#x3D;0.0,</span><br><span class="line">      power&#x3D;1.0,</span><br><span class="line">      cycle&#x3D;False)</span><br><span class="line"></span><br><span class="line">  # Implements linear warmup. I.e., if global_step &lt; num_warmup_steps, the</span><br><span class="line">  # learning rate will be &#96;global_step&#x2F;num_warmup_steps * init_lr&#96;.</span><br><span class="line">  if num_warmup_steps:</span><br><span class="line">    global_steps_int &#x3D; tf.cast(global_step, tf.int32)</span><br><span class="line">    warmup_steps_int &#x3D; tf.constant(num_warmup_steps, dtype&#x3D;tf.int32)</span><br><span class="line"></span><br><span class="line">    global_steps_float &#x3D; tf.cast(global_steps_int, tf.float32)</span><br><span class="line">    warmup_steps_float &#x3D; tf.cast(warmup_steps_int, tf.float32)</span><br><span class="line"></span><br><span class="line">    warmup_percent_done &#x3D; global_steps_float &#x2F; warmup_steps_float</span><br><span class="line">    warmup_learning_rate &#x3D; init_lr * warmup_percent_done</span><br><span class="line"></span><br><span class="line">    is_warmup &#x3D; tf.cast(global_steps_int &lt; warmup_steps_int, tf.float32)</span><br><span class="line">    learning_rate &#x3D; (</span><br><span class="line">        (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)</span><br><span class="line"></span><br><span class="line">  # It is recommended that you use this optimizer for fine tuning, since this</span><br><span class="line">  # is how the model was trained (note that the Adam m&#x2F;v variables are NOT</span><br><span class="line">  # loaded from init_checkpoint.)</span><br><span class="line">  optimizer &#x3D; AdamWeightDecayOptimizer(</span><br><span class="line">      learning_rate&#x3D;learning_rate,</span><br><span class="line">      weight_decay_rate&#x3D;0.01,</span><br><span class="line">      beta_1&#x3D;0.9,</span><br><span class="line">      beta_2&#x3D;0.999,</span><br><span class="line">      epsilon&#x3D;1e-6,</span><br><span class="line">      exclude_from_weight_decay&#x3D;[&quot;LayerNorm&quot;, &quot;layer_norm&quot;, &quot;bias&quot;])</span><br><span class="line"></span><br><span class="line">  if use_tpu:</span><br><span class="line">    optimizer &#x3D; tf.contrib.tpu.CrossShardOptimizer(optimizer)</span><br><span class="line"></span><br><span class="line">  tvars &#x3D; tf.trainable_variables()</span><br><span class="line">  grads &#x3D; tf.gradients(loss, tvars)</span><br><span class="line"></span><br><span class="line">  # This is how the model was pre-trained.</span><br><span class="line">  (grads, _) &#x3D; tf.clip_by_global_norm(grads, clip_norm&#x3D;1.0)</span><br><span class="line"></span><br><span class="line">  train_op &#x3D; optimizer.apply_gradients(</span><br><span class="line">      zip(grads, tvars), global_step&#x3D;global_step)</span><br><span class="line"></span><br><span class="line">  # Normally the global step update is done inside of &#96;apply_gradients&#96;.</span><br><span class="line">  # However, &#96;AdamWeightDecayOptimizer&#96; doesn&#39;t do this. But if you use</span><br><span class="line">  # a different optimizer, you should probably take this line out.</span><br><span class="line">  new_global_step &#x3D; global_step + 1</span><br><span class="line">  train_op &#x3D; tf.group(train_op, [global_step.assign(new_global_step)])</span><br><span class="line">  return train_op</span><br><span class="line"></span><br><span class="line">class AdamWeightDecayOptimizer(tf.train.Optimizer):</span><br><span class="line">  &quot;&quot;&quot;A basic Adam optimizer that includes &quot;correct&quot; L2 weight decay.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">  def __init__(self,</span><br><span class="line">               learning_rate,</span><br><span class="line">               weight_decay_rate&#x3D;0.0,</span><br><span class="line">               beta_1&#x3D;0.9,</span><br><span class="line">               beta_2&#x3D;0.999,</span><br><span class="line">               epsilon&#x3D;1e-6,</span><br><span class="line">               exclude_from_weight_decay&#x3D;None,</span><br><span class="line">               name&#x3D;&quot;AdamWeightDecayOptimizer&quot;):</span><br><span class="line">    &quot;&quot;&quot;Constructs a AdamWeightDecayOptimizer.&quot;&quot;&quot;</span><br><span class="line">    super(AdamWeightDecayOptimizer, self).__init__(False, name)</span><br><span class="line"></span><br><span class="line">    self.learning_rate &#x3D; learning_rate</span><br><span class="line">    self.weight_decay_rate &#x3D; weight_decay_rate</span><br><span class="line">    self.beta_1 &#x3D; beta_1</span><br><span class="line">    self.beta_2 &#x3D; beta_2</span><br><span class="line">    self.epsilon &#x3D; epsilon</span><br><span class="line">    self.exclude_from_weight_decay &#x3D; exclude_from_weight_decay</span><br><span class="line"></span><br><span class="line">  def apply_gradients(self, grads_and_vars, global_step&#x3D;None, name&#x3D;None):</span><br><span class="line">    &quot;&quot;&quot;See base class.&quot;&quot;&quot;</span><br><span class="line">    assignments &#x3D; []</span><br><span class="line">    for (grad, param) in grads_and_vars:</span><br><span class="line">      if grad is None or param is None:</span><br><span class="line">        continue</span><br><span class="line"></span><br><span class="line">      param_name &#x3D; self._get_variable_name(param.name)</span><br><span class="line"></span><br><span class="line">      m &#x3D; tf.get_variable(</span><br><span class="line">          name&#x3D;param_name + &quot;&#x2F;adam_m&quot;,</span><br><span class="line">          shape&#x3D;param.shape.as_list(),</span><br><span class="line">          dtype&#x3D;tf.float32,</span><br><span class="line">          trainable&#x3D;False,</span><br><span class="line">          initializer&#x3D;tf.zeros_initializer())</span><br><span class="line">      v &#x3D; tf.get_variable(</span><br><span class="line">          name&#x3D;param_name + &quot;&#x2F;adam_v&quot;,</span><br><span class="line">          shape&#x3D;param.shape.as_list(),</span><br><span class="line">          dtype&#x3D;tf.float32,</span><br><span class="line">          trainable&#x3D;False,</span><br><span class="line">          initializer&#x3D;tf.zeros_initializer())</span><br><span class="line"></span><br><span class="line">      # Standard Adam update.</span><br><span class="line">      next_m &#x3D; (</span><br><span class="line">          tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad))</span><br><span class="line">      next_v &#x3D; (</span><br><span class="line">          tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2,</span><br><span class="line">                                                    tf.square(grad)))</span><br><span class="line"></span><br><span class="line">      update &#x3D; next_m &#x2F; (tf.sqrt(next_v) + self.epsilon)</span><br><span class="line"></span><br><span class="line">      # Just adding the square of the weights to the loss function is *not*</span><br><span class="line">      # the correct way of using L2 regularization&#x2F;weight decay with Adam,</span><br><span class="line">      # since that will interact with the m and v parameters in strange ways.</span><br><span class="line">      #</span><br><span class="line">      # Instead we want ot decay the weights in a manner that doesn&#39;t interact</span><br><span class="line">      # with the m&#x2F;v parameters. This is equivalent to adding the square</span><br><span class="line">      # of the weights to the loss with plain (non-momentum) SGD.</span><br><span class="line">      if self._do_use_weight_decay(param_name):</span><br><span class="line">        update +&#x3D; self.weight_decay_rate * param</span><br><span class="line"></span><br><span class="line">      update_with_lr &#x3D; self.learning_rate * update</span><br><span class="line"></span><br><span class="line">      next_param &#x3D; param - update_with_lr</span><br><span class="line"></span><br><span class="line">      assignments.extend(</span><br><span class="line">          [param.assign(next_param),</span><br><span class="line">           m.assign(next_m),</span><br><span class="line">           v.assign(next_v)])</span><br><span class="line">    return tf.group(*assignments, name&#x3D;name)</span><br><span class="line"></span><br><span class="line">  def _do_use_weight_decay(self, param_name):</span><br><span class="line">    &quot;&quot;&quot;Whether to use L2 weight decay for &#96;param_name&#96;.&quot;&quot;&quot;</span><br><span class="line">    if not self.weight_decay_rate:</span><br><span class="line">      return False</span><br><span class="line">    if self.exclude_from_weight_decay:</span><br><span class="line">      for r in self.exclude_from_weight_decay:</span><br><span class="line">        if re.search(r, param_name) is not None:</span><br><span class="line">          return False</span><br><span class="line">    return True</span><br><span class="line"></span><br><span class="line">  def _get_variable_name(self, param_name):</span><br><span class="line">    &quot;&quot;&quot;Get the variable name from the tensor name.&quot;&quot;&quot;</span><br><span class="line">    m &#x3D; re.match(&quot;^(.*):\\d+$&quot;, param_name)</span><br><span class="line">    if m is not None:</span><br><span class="line">      param_name &#x3D; m.group(1)</span><br><span class="line">    return param_name</span><br></pre></td></tr></table></figure>
<p>可以看到里面使用了warmup+Adam+weight_dacay，现在一一进行说明。</p>
<p>（1）warmup机制在初期使用warmup_step阶段使用较小的学习率（随着warmup_step增大逐步增大到init_lr）</p>
<p>（2）Weight decay是在每次更新的梯度基础上减去一个梯度（ $\Theta$为模型参数向量，$\bigtriangledown f_t(\theta_t) $为t时刻loss函数的梯度，$\alpha$为学习率）:$\theta_t=(1-\lambda ) \theta_t - \alpha \bigtriangledown f_t(\theta_t)$</p>
<p>（3）L2 regularization是给参数加上一个L2惩罚($f_t(\theta )$为loss函数)：$f^{reg}_t(\theta )=f_t(\theta )+\frac{\lambda ‘}{2}||\theta ||^2$。(当$\lambda ‘=\frac{\lambda }{\alpha }$时，与weight decay等价，仅在使用标准SGD优化时成立)</p>
<p>（4）Adam自动调整学习率，大幅提高了训练速度，也很少需要调整学习率，但是有相当多的资料报告Adam优化的最终精度略低于SGD。问题出在哪呢，其实Adam本身没有问题，问题在于目前大多数DL框架的L2 regularization实现用的是weight decay的方式，而weight decay在与Adam共同使用的时候有互相耦合。理由如下图，其中红色是传统的Adam+L2 regularization的方式，绿色是bert使用的接入weight decay的方式，能够完成梯度下降与weight decay的解耦：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/optimizer/5.png" alt="fig5"></p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/optimizer/6.png" alt="fig6"></p>
<p>大部分的模型都会有L2 regularization约束项，因此很有可能出现adam的最终效果没有sgd的好。如果在tf里面要对不同区域的tensor做不同的L2 regularization调整的化可以参考<a href="https://zhuanlan.zhihu.com/p/40814046" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/40814046</a> 先做adam后在手工更新L2 regularization梯度的方法。</p>
<p>推荐：一个框架看懂优化算法之异同 SGD/AdaGrad/Adam：<a href="https://zhuanlan.zhihu.com/p/32230623" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32230623</a></p>
]]></content>
      <tags>
        <tag>基础系列</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：《Lattice CNNs for Matching Based Chinese Question Answering》</title>
    <url>/2019/03/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E3%80%8ALattice-CNNs-for-Matching-Based-Chinese-Question-Answering%E3%80%8B/</url>
    <content><![CDATA[<p>问答系统是普通用户使用知识库最直接的渠道。匹配用户问题这种短文本，通常面临相同语义的单词和表达方式不唯一的挑战。 中文这种还需要额外分词的语言中，这种现象尤为严重。在论文《基于Lattice CNN的中文问答匹配方法（Lattice CNNs for Matching Based Chinese Question Answering）》中，研究者提出一个基于Lattice CNN的模型，能够利用word-lattice中固有的多粒度信息，且具有很强的处理噪声的能力。 对基于文档的问答和基于知识的问答任务进行了广泛的实验，实验结果表明，LCNs模型可以提取word lattice中丰富且有差别的信息，性能优于其他。</p>
<a id="more"></a>
<h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h1><p>短文本匹配受分词效果影响。针对中文或类似语言的匹配文本序列经常会遭受分词的困扰，因为在这种情况下，通常没有完美的中文分词工具可以适合每种情况。 文本匹配通常需要捕获多个粒度的两个序列之间的相关性。 例如，在图1中，示例短语通常被标记为“中国–公民–生活–质量–高”，但是当我们计划将其与“中国人–生活–好”相匹配时，将其细分为“中国人-生计-生活”更有帮助，而不是普通的细分。</p>
<p><img src="https://uploader.shimo.im/f/KIUVXnEHurrHp0a0.png!thumbnail?fileGuid=J8rhpwtJ8CkV8GXD" alt="图片"></p>
<p>融合词级和字级信息的方法，在一定程度上可以缓解不同分词方法之间的不匹配问题，但这些方法仍然受到原有词序结构的影响。 它们通常依赖于一个现有的词的标记，这必须在同一时间做出分词选择，例如，“中国”(China)和“中国人”(Chinese)处理“中国人民”(Chinese people)。 混合只是在它们的框架中的一个位置进行指导。</p>
<p>特定的任务，如问题回答(QA)可以提出进一步的挑战，短文本匹配：</p>
<ul>
<li>基于文档的问答系统(DBQA)。匹配度反映对一个给定的问题，一个句子是他的回答的概率，问题和回答来源不同，因此会存在风格和句法结构都不同的问题。</li>
<li>基于知识的问题回答(KBQA)。一个关键任务是对知识库的谓词短语来匹配问题的关系表达式。</li>
</ul>
<p>最近的研究进展致力于多粒度信息的匹配建模。  Seo等人将单词和字符混合成一个简单的序列(单词级) ，并且Chen等人利用多个卷积内核大小来捕获不同的 n-grams。 但是汉语中的大多数字都可以看作是单独的词，因此直接将汉字与相应的词组合起来可能会失去这些字所能表达的意义。 由于顺序输入的原因，他们要么在处理字符序列时丢失字级信息，要么不得不做出分词选择。</p>
<h1 id="本文方法"><a href="#本文方法" class="headerlink" title="本文方法"></a>本文方法</h1><p>本文提出了一种用于中文问答中短文本匹配的多粒度方法，该方法利用基于Lattice的CNN提取单词格上的句子级特征。具体而言，LCN不再依赖于字符或单词级别序列，而是将单词格作为输入，其中每个可能的单词和字符将被平等对待并具有各自的上下文，以便它们可以在每一层进行交互。 对于每层中的每个单词，LCN可以通过合并方法以不同的粒度捕获不同的上下文单词。</p>
<h2 id="SIAMESE-ARCHITECTURE"><a href="#SIAMESE-ARCHITECTURE" class="headerlink" title="SIAMESE ARCHITECTURE"></a>SIAMESE ARCHITECTURE</h2><p>SIAMESE ARCHITECTURE 及其变体已被广泛应用于句子匹配和基于匹配的问答中，这种结构具有对称分量，可以从不同的输入通道中提取高级特征，这些特征共享相同的矢量空间中的参数和映射输入。 然后对句子表征进行合并，并与输出的相似性进行比较。</p>
<p>例如可以用多层 cnn 学习句子表示。 卷积层之间采用残差连接的方法以丰富特征且易于训练。然后采用池化层总结全局特征以获得句子级别的表示，并通过逐元素乘法进行合并。匹配分数通过多层感知器产生：$s=\sigma\left(\boldsymbol{W}_{2} \operatorname{ReLU}\left(\boldsymbol{W}_{1}\left(\boldsymbol{f}_{q u} \odot \boldsymbol{f}_{c a n}\right)+\boldsymbol{b}_{1}^{T}\right)+\boldsymbol{b}_{2}^{T}\right)$</p>
<p>其中fqu和fcan是问题和候选回答经过CNN编码的特征向量，σ是sigmoid函数W2，W1，b1T,b2T是参数，⊙是逐元素乘法。训练目标是最小化二进制交叉熵：$L=-\sum_{i=1}^{N}\left[y_{i} \log \left(s_{i}\right)+\left(1-y_{i}\right) \log \left(1-s_{i}\right)\right]$</p>
<p>其中，yi第i个训练对的标签。注意，句子表示可以是原始CNN，也可以是Lattice CNN。在原始CN中，卷积核按照顺序扫面每n-gram，并得到一个特征向量，该向量可以看作是中心词的表示，被传递至下一层。但是，每一个词在每一个lattice中可能具有不同粒度的上下文词，并且可以被视为具有相同长度的卷积核的中心。因此，不同于原始CNN，<strong>lattice CNN对于一个词可能产生多个特征向量</strong>，这是将标准CNN直接用于lattice输入的关键挑战。</p>
<p>如下图所示，“citizen”具有四个长度为3的上下文的中心词（China -citizen - life, China - citizen - alive, country - citizen -life, country - citizen - alive）。对于“citizen”，尺寸为3的卷积核会产生4个特征向量：</p>
<p><img src="https://uploader.shimo.im/f/GowtC1TcmsbJWxRu.png!thumbnail?fileGuid=J8rhpwtJ8CkV8GXD" alt="图片"></p>
<p>将”citizen“看作中心词，卷积核尺寸为3时，将会涉及到5个词和四个上下文组成，如上文所述，每个词用不同的颜色标记。然后，使用3个卷积核扫描所有的位置，产生4个3维特征向量。门控权重是通过dense layer根据这些特征向量计算的，反映了不同上下文构成的重要性。中心词的输出向量是他们的权重和，嘈杂上下文通过平滑处理会具有较小的权重。这些在不同上下文的池化操作，允许LCNs在word lattice上工作。</p>
<h2 id="Word-Lattice"><a href="#Word-Lattice" class="headerlink" title="Word Lattice"></a>Word Lattice</h2><p>如下图所示，一个word lattice是一个有向图 $G=<V，E>$ ，$V$是点集，$E$是边集。对于一个中文句子，是一个中文字符序列$S=c_{1:n}$，其所有可被视为单词的字符的子字符串都被视为顶点，即 V={ c_{i:j} |c_{i:j} 是单词}，然后所有相邻单词都根据其在原始句子中的位置，以有向边相连。即$E={e(c_{i:j}，c_{j:k}) | ∀ i，j，k s.t. c_{i:j}，c_{j:k} ∈ V }$</p>
<p>关键问题： 如何决定一个字符序列可以被描述成一个词？作者通过查询现有的词汇来解决这个问题，这些词是百度百科中的频繁词。注意到，大部分中国字符本身都可以被看作词，百度百科的语料库中就会包含这些词。 但这样就会不可避免的对word lattice引入噪声，通过模型的池化过程平滑处理。 构造的图可能会因为存在词汇表以外的词断开，因此，添加$$标签来替换这些词，以连接图。 显然，word lattice 是字符和所有可能的单词的集合。因此不需要进行分词，只需要将所有可能的信息嵌入lattice并且将其输入CNN。 word lattice内在的图结构允许所有可能的单词都被清晰表示。</p>
<p><img src="https://uploader.shimo.im/f/On7kSaZ6rWKH3598.png!thumbnail?fileGuid=J8rhpwtJ8CkV8GXD" alt="图片"></p>
<h2 id="lattice-based-CNN-layer"><a href="#lattice-based-CNN-layer" class="headerlink" title="lattice based CNN layer"></a>lattice based CNN layer</h2><p>采用 lattice based CNN layer 作为输入，而不是标准的 CNN，并利用池化机制合并由多个卷积核在不同上下文产生的特征向量。</p>
<p>形式上，卷积核尺寸为n 的lattice CNN层对词w在word lattice $G=<V,E>$ 下的输出特征向量是：</p>
<p><img src="https://uploader.shimo.im/f/KStboepUiyCCniUa.png!thumbnail?fileGuid=J8rhpwtJ8CkV8GXD" alt="图片"></p>
<p>其中，$f$ 为激活函数，$v_{w_{i}}$ 为该层中词 $w_{i}$ 对应的输入向量，$v_{w_{1}}:…:v_{w_{n}}$ 为这些向量的级联，$g$ 为池化函数，有最大池化、均值池化、门池化。 门池化的公式表示如下所示：</p>
<ul>
<li>$\alpha_{1}, \ldots, \alpha_{t}=\operatorname{softmax}\left\{\boldsymbol{v}_{g}^{T} \boldsymbol{v}_{1}+b_{g}, \ldots, \boldsymbol{v}_{g}^{T} \boldsymbol{v}_{t}+b_{g}\right\}$</li>
<li>$\text { gated-pooling }\left\{\boldsymbol{v}_{1}, \ldots, \boldsymbol{v}_{t}\right\}=\sum_{i=1}^{n} \alpha_{i} \times \boldsymbol{v}_{i}$</li>
</ul>
<p>其中，$v_g$ 和 $b_g$ 为参数，$ɑ_i$ 为 softmax 函数归一化的门控权重。门代表n-gram上下文的重要性，加权和可以控制嘈杂上下文词的传输。必要的时候进行padding。</p>
<p>word lattice可以看作有向图，并通过 Direct Graph Convolutional networks(DGCs)建模，该模型在相邻顶点上采用池化操作，忽略了n-gram的语义结构。但是，对某些情况来说，他们的构想可能和我们的相似。比如，如果我们将LCNs的卷积核尺寸设置为3，使用线性激活函数，假设LCN和DGC的池化是均值池化，则在每一层的每个词处，DGC计算中心词及其一阶邻居的平均值，LCN计算分别计算前一个和后一个词的均值，然后将他们加在中心词上。具体见实验部分。</p>
<p>最后，给定一个已经被构造成word lattice格式的句子，对于lattice的每一个节点，LCN层会产生一个类似于原始CNN的特征向量，这使得堆叠多个LCN以获得更抽象的特征表示更加容易。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="实验目标"><a href="#实验目标" class="headerlink" title="实验目标"></a>实验目标</h2><ul>
<li>word lattice 中的多粒度信息是否有助于基于问答任务的匹配；</li>
<li>LCNs 是否能很好地通过 lattice 捕获多粒度信息；</li>
<li>如何平衡 word lattice 引入的噪声和带有丰富信息的词。</li>
</ul>
<h2 id="数据集选取"><a href="#数据集选取" class="headerlink" title="数据集选取"></a>数据集选取</h2><p>数据集选自 NLPCC-2016 评估任务的中文问答数据集：</p>
<ul>
<li>DBQA：是一个基于文档的问题回答数据集。 在测试集中有8.8 k 的问题和182k 的问句对用于训练，6k 的问题和123k 的问句对用于测试。 平均每个问题有20.6个候选句子和1.04个黄金答案。 问题的平均长度为15.9个字符，每个候选句子平均有38.4个字符。 问句和句子都是自然语言的句子，可能比 KBQA 更多地共享相似的词语和表达方式。 但是候选句子是从网页上提取出来的，而且通常比问句长得多，还有许多不相关的从句。</li>
<li>KBQA：是一种基于知识的关系抽取数据集。 我们按照与[ Lai 等人2017]相同的预处理过程来清理数据集，并将问题中提到的实体替换为特殊标记。 在训练集中有14.3 k 问题，其中问题谓词对为273k，问题谓词对为156k，问题谓词对为9.4 k。 每个问题只包含一个黄金谓词。 每个问题平均有18.1个候选谓词和8.1个字符长度，而一个 KB 谓词平均只有3.4个字符长。 注意，知识库谓词通常是一个简洁的短语，与自然语言问题相比，用词选择有很大的不同，这给解决带来了不同的挑战。</li>
</ul>
<p>我们用来构造 word lattice 的词汇表包含 156k 个单词，其中包括 9.1k 个单字符的单词。 平均而言，每个 DBQA 问题在其 lattice 中包含 22.3 个标记(单词或字符) ，每个 DBQA 候选句子有 55.8 个标记，每个 KBQA 问题有 10.7 个标记，每个 KBQA 谓词包含 5.1 个标记。</p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img src="https://uploader.shimo.im/f/ww9nHeYK5OnuFVCV.png!thumbnail?fileGuid=J8rhpwtJ8CkV8GXD" alt="图片"></p>
]]></content>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Rasa框架解读</title>
    <url>/2019/02/22/Rasa%E6%A1%86%E6%9E%B6%E8%A7%A3%E8%AF%BB/</url>
    <content><![CDATA[<p>Rasa是一个很活跃的开源对话框架，笔者在做语音助手时调研了很多对话框架，个人比较喜欢Rasa，今天就来讲讲它。</p>
<a id="more"></a>
<h1 id="Rasa的组成"><a href="#Rasa的组成" class="headerlink" title="Rasa的组成"></a>Rasa的组成</h1><h2 id="Rasa-NLU"><a href="#Rasa-NLU" class="headerlink" title="Rasa-NLU"></a>Rasa-NLU</h2><p>主要实现自然语言理解（即NLU）功能，本质上就是识别句子的意图和实体。如“买一张去北京的票”，我们可以定义一个意图是“购票”，实体是“北京”和“一张”。</p>
<p>意图识别本质是短文本分类任务（当然在学术界可能称为Intent Detection来和Text Classification分开）。 单纯短文本分类任务的SOTA基本上就是BERT了。</p>
<p>抽取本质是信息抽取任务。 抽取的SOTA现在一般还是BiLSTM-CRF的各种变型，或BERT之类。现在学术界的主要研究方向是多种工作结合，例如同一模型同时做意图识别和信息抽取，互相配合增加总体准确率。</p>
<p>Rasa的NLU，主要是当前的社区版，主要还是使用了各种开源技术，并没有追求学术上的SOTA。 它使用的工具包括Spacy、sklearn-crfsuite</p>
<h2 id="Rasa-Core"><a href="#Rasa-Core" class="headerlink" title="Rasa Core"></a>Rasa Core</h2><p>Rasa的核心部分，NLU有各种实现，开源的也有snips nlu等，但是core却独一无二。</p>
<p>Rasa Core主要完成了基于故事的对话管理，包括解析故事并生成对话系统中的对话管理模型（Dialog Management），输出系统决策（System Action/System Policy）。</p>
<p>学术上一般认为这部分会包含两个模型：</p>
<ul>
<li>对话状态跟踪（Dialog State Tracking / Belief Tracking）</li>
<li>对话策略（Dialog Policy / Policy Optimization）</li>
</ul>
<p>对于1，其实Rasa实现很简单，具体在它的论文 Few-Shot Generalization Across Dialogue Tasks, Vlasov et at., 2018 中说的比较具体。就是简单的基于策略的槽状态替换。</p>
<p>对于2，Rasa使用基于LSTM的Learn to Rank方法，大体上是将当前轮用户意图、上一轮系统行为、当前槽值状态向量化，然后与所有系统行为做相似度学习，以此决定当前轮次的一个或多个系统行为。</p>
<p><img src="https://uploader.shimo.im/f/m5MliMSDjPuV7q1e.png!thumbnail?fileGuid=yhpTXq3xXjJcVQR6" alt="图片"></p>
<h2 id="Rasa-X"><a href="#Rasa-X" class="headerlink" title="Rasa-X"></a>Rasa-X</h2><p>Rasa的可视化编辑工具，更方便NLU、NLG数据的管理，故事的编写。Rasa X可能暂时还不能让所有非开发人员也能快速方便的使用。不过它本质上可以方便开发人员快速开发，快速训练模型验证。</p>
<h1 id="Rasa的PipeLine"><a href="#Rasa的PipeLine" class="headerlink" title="Rasa的PipeLine"></a>Rasa的PipeLine</h1><p><img src="https://uploader.shimo.im/f/Gml5qyuefePTthMN.png!thumbnail?fileGuid=yhpTXq3xXjJcVQR6" alt="图片"></p>
<ul>
<li>用户输入文字，送入解释器，即Rasa NLU</li>
<li>NLU给出结果，如图</li>
</ul>
<p><img src="https://uploader.shimo.im/f/CfVtDjI9fJwiEqeK.png!thumbnail?fileGuid=yhpTXq3xXjJcVQR6" alt="图片"></p>
<ul>
<li>从Tracker到Policy，Tracker用于跟踪对话状态，Tracker输出的是Embedding<ul>
<li>用户意图的Embedding</li>
<li>系统动作（上一步）的Embedding</li>
<li>实体（槽值/Slot）的Embedding</li>
</ul>
</li>
<li>Policy给出系统行为</li>
<li>Tracker记录系统行为，下一次会提供给Policy使用</li>
<li>返回消息给用户</li>
</ul>
<h1 id="Rasa-NLU-1"><a href="#Rasa-NLU-1" class="headerlink" title="Rasa-NLU"></a>Rasa-NLU</h1><h2 id="基本组成"><a href="#基本组成" class="headerlink" title="基本组成"></a>基本组成</h2><h3 id="component"><a href="#component" class="headerlink" title="component"></a>component</h3><p>在我们做任何自然语言处理的任务时，不止是用单纯模型去做一些分类或者标注任务，在此之前，有相当一部分工作是对文本做一些预处理工作，包括但不限于：分词（尤其是中文文本），词性标注，特征提取（传统ML或者统计型方法），词库构建等等。在rasa中，这些不同的预处理工作以及后续的意图分类和实体识别都是通过单独的组件来完成，因此component在NLU中承担着完成NLU不同阶段任务的责任。component类型大致有以下几种：tokenizer、featurizer、extractor、classifier，当然还有emulators，这个主要用于进行对话仿真测试，我目前还没使用过，就不多描述这个组件了。</p>
<h3 id="pipeline"><a href="#pipeline" class="headerlink" title="pipeline"></a>pipeline</h3><p>有了组件之后，如何将组件按部就班，井然有序地拼装起来，并正常工作呢？因此就有了pipeline这个概念，其实在机器学习领域，pipeline这个概念已经存在很长时间了，它在很多框架中都有，比如大名鼎鼎的sklearn。使用pipeline的好处在于可以合理有序管理不同任务阶段的不同组件工具，当组件数量较多时，pipeline的好处就非常明显了。而在rasa中，pipeline的使用更为便捷，是通过yml配置文件实现。即开发者只需要定义好自己的组件，然后将组件配置在配置文件中就可以，即插即用。下图是一个简单的pipeline配置实例：</p>
<p><img src="https://uploader.shimo.im/f/D4AVlWvATEPclpc7.png!thumbnail?fileGuid=yhpTXq3xXjJcVQR6" alt="图片"></p>
<h3 id="message"><a href="#message" class="headerlink" title="message"></a>message</h3><p>在rasa中，用户发送到chatbot的所有对话内容，都需要被封装在一个对象中，这个对象就是Message.而在整个rasa工作流中，存在两个不同的message封装对象，一个是UserMessage，另一个是Message。其中UserMessage是最上层的封装对象，即直接接收用户从某个平台接口传送过来的消息。而Message则是当用户消息流到NLU模块时，将用户消息进行封装。关于UserMessage的内容在后面代码详解时会涉及到，这里先解释一下Message对象。看一下它的类部分定义，其实很简单，就是将用户的对话文本，以及时间进行封装，由于这个Message是贯穿整个NLU工作流的统一数据对象，因此还承载着记忆各个组件临时生成的中间结果（比如分词和词性标注的结果）以及最终得到的意图和实体信息。其中data存放的是意图和实体信息，在后续组件处理时，还会再Message中增加一些变量存储中间结果，即set成员方法的职责。</p>
<p>在Rasa-Core中UserMessage定义如下：</p>
<p><img src="https://uploader.shimo.im/f/nJYXcKewulgGkY3T.png!thumbnail?fileGuid=yhpTXq3xXjJcVQR6" alt="图片"></p>
<p>可以看到作为贯穿整个rasa_core处理流程的用户消息对象，它的成员结构还是比较清晰的，包括了用户发送的文本，定义的OutputChannel类型，用户的id，parse_data(主要存放用户自己定义的实体键值对，开发调试用)，inputChannel类型，以及message的id。</p>
<h2 id="技术细节"><a href="#技术细节" class="headerlink" title="技术细节"></a>技术细节</h2><h3 id="Embedding-方法"><a href="#Embedding-方法" class="headerlink" title="Embedding 方法"></a>Embedding 方法</h3><p>用户意图和系统行为会通过bag-of-word的方法分词，然后向量化，很有趣的结果。在官方论文没有仔细探讨为什么，笔者猜测是为了增加不同的意图、行为之间的语义关联。论文原文：“A bag-of-words representations for the user and the system labels are then created using token counts inside each label.”  例如:action_search_restaurant = {action, search, restaurant} 实体/槽值（Slot）的向量化就非常简单了，只是走了是否存在的binary向量</p>
<h3 id="Learn-to-Rank方法"><a href="#Learn-to-Rank方法" class="headerlink" title="Learn to Rank方法"></a>Learn to Rank方法</h3><p>很多对话系统的系统决策都采用的是分类（Classification）方法，也就是每次总是在多个系统行为中选择唯一一个。</p>
<p>而Rasa选择了排序方法，即判断当前对话状态和系统行为的相似度，这有两个可能的好处：</p>
<ul>
<li>可以更容易实现多个系统行为的同时输出。能让一个对话状态输出多个系统行为是Rasa的特色。至于为什么如此，可能有工程上的一些考虑，例如这样更方便，例如两个系统行为，一个是机器人说“请等待”，一个是真的去查询数据。</li>
<li>更方便扩展系统行为。如果是分类模型，增加一个分类，那必须重新训练整个分类器。如果是Ranking模型，如果只是增加或减少分类，可以考虑只训练新增的系统行为相关的和不相关的部分数据集，可能增加总体的训练速度。更方便快速实验、迭代。</li>
</ul>
<h2 id="给Rasa-NLU添加组件"><a href="#给Rasa-NLU添加组件" class="headerlink" title="给Rasa-NLU添加组件"></a>给Rasa-NLU添加组件</h2><p>以CRFEntityExtractor为例，讲解一下Component的主要核心要素。</p>
<p><img src="https://uploader.shimo.im/f/6kzsP0JMqFIHIDe2.png!thumbnail?fileGuid=yhpTXq3xXjJcVQR6" alt="图片"></p>
<p>首先看到，该类继承了一个EntityExtractor，这是一个二级组件抽象类（我自己定义的说法），这个二层抽象类继承自Component这个一级抽象类。因为不同组件承担的任务不同，有些组件任务比较单一，可以直接继承Component比如tokenizer,classifier，而有些组件的任务比较复杂，则需要制定这一类型的二级接口，方便扩展，如featurizer,extractor。</p>
<p>其次，每个component需要定义一个类变量provides和requires,分别表示这个组件所提供的中间成果和依赖的上游任务。对于CRFEntityExtractor来说，它提供了实体的抽取，同时为了进行实体抽取，需要先对文本进行分词，因此需要上游任务先完成tokenizer任务，提供tokens的中间成果。</p>
<p><img src="https://uploader.shimo.im/f/HdsywcZpNPzSe9ly.png!thumbnail?fileGuid=yhpTXq3xXjJcVQR6" alt="图片"></p>
<p>既然是使用条件随机场来进行实体抽取，那么就需要进行模型训练。因此需要定义train方法，来训练模型。关注train方法的两个参数training_data和config。其中，config就是之前提到的配置pipeline的配置文件的读取对象。training_data是TrainingData类型的对象。你可以将其类比于pytorch中的data_loader功能，它的主要作用是对训练数据进行封装，拆分训练集验证集，做数据校验等工作。说到这里，提一下rasa支持的原始训练数据的存放格式，主要支持markdown，wit，luis等文件格式，当然也可以提供json格式的数据。rasa如何读取这些格式的训练数据则是在training_data中定义。</p>
<p><img src="https://uploader.shimo.im/f/51JQbndhDVvb3Zd3.png!thumbnail?fileGuid=yhpTXq3xXjJcVQR6" alt="图片"></p>
<p>当模型训练完成后，需要保存和加载模型，对生产环境上的实时业务流进行处理，因此需要定义persist和load方法加载模型。</p>
<p><img src="https://uploader.shimo.im/f/RXNVPf8kSYa5csT0.png!thumbnail?fileGuid=yhpTXq3xXjJcVQR6" alt="图片"></p>
<p>process方法，这个可以说是组件里面最重要的一个方法。当前面一通操作之后，只得到了模型，如何调用这个模型并处理文本，就是process方法的工作了。最后在message中增加一个dict，名为entities，用来存放提取的实体信息，包括实体的类型，实体的在文本中的start和end的位置信息等。</p>
<h1 id="Rasa-Core-1"><a href="#Rasa-Core-1" class="headerlink" title="Rasa-Core"></a>Rasa-Core</h1><p>Rasa-Core的基本流程图如下：</p>
<p><img src="https://uploader.shimo.im/f/r5s6gqbcJHTpKryp.png!thumbnail?fileGuid=yhpTXq3xXjJcVQR6" alt="图片"></p>
<h2 id="基本组成-1"><a href="#基本组成-1" class="headerlink" title="基本组成"></a>基本组成</h2><h3 id="actions"><a href="#actions" class="headerlink" title="actions"></a>actions</h3><p>该包下面主要存放的是action具体的实现类。关于action的具体定义和描述在后面会有详细讲解，简单说就是chatbot执行的一些动作。Event对象是rasa中定义的chatbot能执行的最小粒度的动作。而Action则是比event更高层次的对象，会根据用户发送过来的消息，执行一些操作，这些操作可以是自定义的一些逻辑，也可以是系统预置的events。rasa中，action可以分为三大类。</p>
<h4 id="utterance-actions"><a href="#utterance-actions" class="headerlink" title="utterance actions"></a>utterance actions</h4><p>直接发送文本给用户，action文本模板是在domain.yml中进行定义。</p>
<h4 id="custom-actions"><a href="#custom-actions" class="headerlink" title="custom actions"></a>custom actions</h4><p>自定义action，由开发者自定义功能的action。个人认为这个是功能最强大的action，因为开发自由度很大，支持使用任何开发语言进行开发。最后只需要将其打包成一个restful服务接口暴露出来即可。因此这种action是可以和对话主系统分离部署的。下面给出自定义action server与bot agent和用户的交互流程图：</p>
<p><img src="https://uploader.shimo.im/f/Pfft9UET4KizYMJN.png!thumbnail?fileGuid=yhpTXq3xXjJcVQR6" alt="图片"></p>
<p>Rasa action支持node.js, .NET, java等开发语言，当然也支持Python。但是对于Python来说，需要安装rasa-sdk工具包。这个工具包里预置了一些有用的action模板，例如form action。</p>
<p>当然，form action以及其他预置的action模板只能实现最简单的场景，如果要实现复杂的场景，需要根据不同场景，自定义action，可以选择继承这些模板，在上面进行功能的添加和完善。</p>
<h4 id="default-action"><a href="#default-action" class="headerlink" title="default action"></a>default action</h4><p>Rasa系统内置的粒度较小的action。与rasa_sdk中的action不同，这个是直接在rasa_core/actions下面的。相对于上面的form action来说，这里的action功能更单一，与events比较像，但是还是略有不同，下面举个实例ActionRestart：</p>
<p><img src="https://uploader.shimo.im/f/D4HXcEwS8uITGsco.png!thumbnail?fileGuid=yhpTXq3xXjJcVQR6" alt="图片"></p>
<p>可以看到它使用了一个Restarted()的event，这个event的功能是重启整个对话流程，重置对话状态。除此之外，该action需要先执行读取话术模板组装bot message，并将其发送给用户后，才去重启整个会话。</p>
<h3 id="channels"><a href="#channels" class="headerlink" title="channels"></a>channels</h3><p>该包下面主要存放的是rasa与前端平台进行对接的接口。因为rasa本身只提供对话系统的功能服务，具体还需要与用户在前端界面进行交互，这个包里定义了不同的接口和不同平台进行对接。例如，console.py，定义了最简单的直接在shell命令行中进行对话交互的接口。</p>
<p>在流程图中的OutputChannel封装了chatbot需要返回给用户的信息，需要注意，chatbot返回的消息不一定是纯文本，还可能是html，json，文件附件等等，因此需要OutputChannel这个统一接口进行封装处理，因此chatbot可以支持让用户进行点选功能（当然，前提是前端界面支持点选的适配），关于如何实现用户点选功能后续会单独开一个功能小讲。</p>
<p>在流程图中的InputChannel主要负责将用户输入连同用户的身份信息封装成UserMessage对象，方便后面的Processor处理。对应的，如果在上一轮对话中，OutputChannel是点选或者其他非单纯文本输出，那么本轮对话中的InputChannel也需要接受用户点选或者其他非单纯文本的输入，封装成最终的UserMessage。</p>
<h3 id="events"><a href="#events" class="headerlink" title="events"></a>events</h3><p>这个是rasa中定义的chatbot能执行的最小粒度的动作。与action有一些关系，我们可以通过action调用不同的events来实现不同的操作。events的实例有“SlotSet”(槽位填充)，”Restarted”（重启对话，将所有状态重置）等等。</p>
<h3 id="nlg"><a href="#nlg" class="headerlink" title="nlg"></a>nlg</h3><p>rasa的response生成模块，即生产chatbot返回给用户的消息。目前，rasa支持通过模板生成话术，也支持通过machine learning的方式做NLG。nlg模块中定义了方法读取domain.yml中的预定义的话术模板，然后生成具体的消息。</p>
<h3 id="policies"><a href="#policies" class="headerlink" title="policies"></a>policies</h3><p>此模块是rasa_core最上层的对话管理控制模块。该包中，定义了不同类型的对话管理策略，rasa将依据这些策略，执行不同actions，完成多轮对话任务。这些策略包括人工规则策略如form_policy、memoization等，也包括通过机器学习、深度学习进行训练得到策略模型，如sklearn_policy、keras_policy等。</p>
<p>对话管理策略是多轮对话系统的核心功能，相当于对话系统的大脑，它负责根据当前用户的反馈，告诉Processor当前轮对话中需要采取的后续action，以及如何更新对话状态信息等。rasa支持人工规则的策略，也支持机器学习、深度学习得到的数据驱动策略。</p>
<p>以Form_policy为例，这个策略是一种表单策略，对应的rasa预置了一种类型的action，叫form的action。这种action会将所有槽位作为表单的属性column，每一轮对话，都会去主动询问用户，引导用户将这些表单的属性填充，直到所有属性填充完成。而form_policy的核心就是检索当前是否配置了form类型的action，如果是，则将下一步的action置为form。有关action的描述将在后面详细给出。可以看出这是一个典型的人工规则策略。</p>
<p>在一次对话任务中，可以使用多个policy的组合来帮助bot完成既定的任务。比如策略A是一个使用深度学习训练得到的一个策略模型，但是一般使用data-driven得到的模型不会达到100%的准确率，总会有bad case的情况，此时如果只是用该策略，那么会话极有可能会陷入到bad case中，因此需要一个兜底的策略在策略A的bad case发生时，让对话能够平稳进行下去。rasa就预置了这样一个策略，叫fall_back，将fall_back与策略A进行组合，就能得到一个更加鲁棒的一个对话策略。</p>
<p>在实际项目生产中，如果在项目初期，领域数据比较少的情况下，通常会选择form policy或者其他规则型策略。当产品上线，在积累到一定的数据后，可以使用一些data-driven的模型来做策略。</p>
<h3 id="schemas-domain"><a href="#schemas-domain" class="headerlink" title="schemas/domain"></a>schemas/domain</h3><p>这里主要放置rasa_core的配置文件domain.yml，这个配置文件主要配置槽位定义，实体定义，话术模板，使用的actions的名称定义以及其他系统配置。开发者在开发自己的对话系统时，需要自定义这个配置文件来覆盖源码中预定义的配置。</p>
<p>Domain对象的数据来自于前述章节提到的配置文件domain.yml。该对象定义不同的方法，从配置文件domain.yml读取槽位模板，话术模板，定义的action名称，自定义的policy名称等信息，并封装到domain对象中。domain对象可以在action执行时为其提供槽位信息以及话术模板等字段。</p>
<p>设计domain的好处在哪儿呢？个人认为主要是方便管理对话系统需要使用的模板信息。这里的模板信息包含定义的槽位，意图、实体、话术模板、自定义action、自定义policy。如果需要添加或者修改这些信息，只需要修改domain.yml里面的信息就可以了，不需要去修改任何代码，让配置和代码解耦。</p>
<h3 id="agent"><a href="#agent" class="headerlink" title="agent"></a>agent</h3><p>这是rasa_core专门设计的一个接口，可以将其视作bot主体，主要作用是封装和调用rasa中最重要的一些功能方法，包括上述提到的几个包里的功能模块。</p>
<h3 id="training"><a href="#training" class="headerlink" title="training"></a>training</h3><p>这里主要存放的是如何将准备的数据转化为对话系统可训练的转化方法以及可视化方法。</p>
<h3 id="interpreter"><a href="#interpreter" class="headerlink" title="interpreter"></a>interpreter</h3><p>这个方法是rasa_core与rasa_nlu的一个纽带，rasa管理模块通过定义interpreter类方法，调用rasa_nlu中的parser方法来对用户的发送到bot的消息文本进行实体抽取、意图识别等操作。</p>
<h3 id="processor"><a href="#processor" class="headerlink" title="processor"></a>processor</h3><p>定义了MessageProcess类，供agent调用，功能是有序得调用不同对话功能组件，例如调用interpreter解析用户文本、调用本轮对话的action完成一些操作、根据policy得到下一步的action、记录对话状态等。</p>
<p>Processor是对话系统的核心处理模块。它通过execute_action完成bot处理对话的流程。这里需要注意一点，在processor执行action之前，agent将会调用processor的log_message方法，使用nlu_interpreter来对用户发送的文本做实体识别和意图识别，然后将信息保存在tracker中。execute_action方法核心内容如下：</p>
<p><img src="https://uploader.shimo.im/f/NtaZ2VxnkfLIjD0v.png!thumbnail?fileGuid=yhpTXq3xXjJcVQR6" alt="图片"></p>
<h3 id="trackers"><a href="#trackers" class="headerlink" title="trackers"></a>trackers</h3><p>这个也是rasa中比较重要的一个对象，它的作用是rasa对话系统中的状态记录器，每一轮对话中，对话的状态信息都会进行更新并保存在这个对象中。例如当前已填充的槽位、用户最后一次发送的文本、当前用户的意图等等。</p>
<h4 id="DialogueStateTracker"><a href="#DialogueStateTracker" class="headerlink" title="DialogueStateTracker"></a>DialogueStateTracker</h4><p>从名字上就可以看到这个对象的功能：在多轮对话过程中全程记录对话状态信息。这个对象在开发自己的对话系统时，作用可是非常大的。很多对话状态信息，都可以从它这里得到。当然， 我们并不能直接去读写其定义的成员变量信息，需要通过其成员方法来操作成员变量，例如current_sate()，其核心内容如下：</p>
<p><img src="https://uploader.shimo.im/f/1MLXml1bdOxbxLOH.png!thumbnail?fileGuid=yhpTXq3xXjJcVQR6" alt="图片"></p>
<p>注意该方法的返回对象是一个字典，其包含了丰富的对话信息，例如用户的id、当前所有的槽位键值对（包括已填充和未被填充的）、用户最近一次发送的消息等等。</p>
<h1 id="其他参考"><a href="#其他参考" class="headerlink" title="其他参考"></a>其他参考</h1><ul>
<li><a href="https://wechaty.js.org/" target="_blank" rel="noopener">wechaty</a></li>
<li><a href="https://github.com/paschmann/rasa-ui" target="_blank" rel="noopener">rasa-ui</a></li>
<li><a href="https://juejin.cn/post/6844903921060839431" target="_blank" rel="noopener">Rasa 聊天机器人框架开发使用</a></li>
</ul>
]]></content>
      <tags>
        <tag>对话系统</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer细节思考</title>
    <url>/2019/02/20/Transformer%E7%BB%86%E8%8A%82%E6%80%9D%E8%80%83/</url>
    <content><![CDATA[<p>今天研究一下Transformer的一些细节，总结一下。</p>
<a id="more"></a>
<blockquote>
<p>参考：</p>
<p><a href="https://state-of-art.top/2019/01/17/Transformer%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E7%8E%B0-%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/" target="_blank" rel="noopener">Transformer原理和实现-从入门到精通</a></p>
<p><a href="https://mp.weixin.qq.com/s?__biz=MjM5MzY4NzE3MA==&amp;mid=2247484632&amp;idx=2&amp;sn=9170e3f036098b5d428123abc1ac1520&amp;source=41#wechat_redirect" target="_blank" rel="noopener">绝对干货！NLP预训练模型：从transformer到albert</a></p>
</blockquote>
<h1 id="Transformer图解"><a href="#Transformer图解" class="headerlink" title="Transformer图解"></a>Transformer图解</h1><p>Transformer使用self-attention和position-encoding替代原始的RNN。</p>
<p><img src="https://uploader.shimo.im/f/WVDQjP11Yteuyi8D.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>每一层encoder</p>
<p><img src="https://uploader.shimo.im/f/bpxADrZe0G5t4yKl.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>每一层decoder</p>
<p><img src="https://uploader.shimo.im/f/TCRzJC0pMGDm5MJz.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<h1 id="Transformer具体结构"><a href="#Transformer具体结构" class="headerlink" title="Transformer具体结构"></a>Transformer具体结构</h1><p><img src="https://uploader.shimo.im/f/uXqNpDFaou7fzJlo.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<h2 id="加入Tensor"><a href="#加入Tensor" class="headerlink" title="加入Tensor"></a>加入Tensor</h2><p>输入的句子是一个词(ID)的序列，我们首先通过Embedding把它变成一个连续稠密的向量，如下图所示。</p>
<p><img src="https://uploader.shimo.im/f/7cQDKKuf7RJsUD3D.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>Embedding之后的序列会输入Encoder，首先经过Self-Attention层然后再经过全连接层，如下图所示。</p>
<p><img src="https://uploader.shimo.im/f/QCmk9C9Ee0OH3T6l.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>我们在计算𝑧𝑖时需要依赖所有时刻的输入𝑥1,…,𝑥𝑛，不过我们可以用矩阵运算一下子把所有的𝑧𝑖计算出来。而全连接网络的计算则完全是独立的，计算i时刻的输出只需要输入𝑧𝑖就足够了，因此很容易并行计算。下图更加明确的表达了这一点（注意全连接层每一个时刻的参数共享）。</p>
<p><img src="https://uploader.shimo.im/f/JA9x4GiSygcDqMiU.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<h2 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h2><p>对于输入的每一个向量(第一层是词的Embedding，其它层是前一层的输出)，我们首先需要生成3个新的向量Q、K和V，分别代表查询(Query)向量、Key向量和Value向量。Q表示为了编码当前词，需要去注意(attend to)其它(其实也包括它自己)的词，我们需要有一个查询向量。而Key向量可以认为是这个词的关键的用于被检索的信息，而Value向量是真正的内容。</p>
<p>对于普通的Attention机制，其计算过程如下：</p>
<p><img src="https://uploader.shimo.im/f/EVMl6DTRHZFNfBj1.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>每个向量的Key和Value向量都是它本身，而Q是当前隐状态ℎ𝑡，计算energy的时候我们计算Q(ℎ𝑡)和Key(ℎ¯𝑗)。然后用softmax变成概率，最后把所有的ℎ¯𝑗加权平均得到context向量。</p>
<p>而Transformer使用的self-attention的计算，以t1时刻为例，如下图：</p>
<p><img src="https://uploader.shimo.im/f/OWcXBG6iT4eJlB1M.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>Self-Attention里的Query不是隐状态，并且来自当前输入向量本身，因此叫作Self-Attention。另外Key和Value都不是输入向量，而是输入向量做了一下线性变换。当然理论上这个线性变换矩阵可以是Identity矩阵，也就是使得Key=Value=输入向量。因此可以认为普通的Attention是这里的特例。这样做的好处是模型可以根据数据从输入向量中提取最适合作为Key(可以看成一种索引)和Value的部分。类似的，Query也是对输入向量做一下线性变换，它让系统可以根据任务学习出最适合的Query，从而可以注意到(attend to)特定的内容。</p>
<p>具体的计算过程：比如图中的输入是两个词”thinking”和”machines”，我们对它们进行Embedding(这是第一层，如果是后面的层，直接输入就是向量了)，得到向量𝑥1,𝑥2。接着我们用3个矩阵分别对它们进行变换，得到向量𝑞1,𝑘1,𝑣1和𝑞2,𝑘2,𝑣2。比如𝑞1=𝑥1𝑊𝑄。图中𝑥1的shape是1x4，𝑊𝑄是4x3，得到的𝑞1是1x3。其它的计算也是类似的，为了能够使得Key和Query可以内积，我们要求𝑊𝐾和𝑊𝑄的shape是一样的，但是并不要求𝑊𝑉和它们一定一样(虽然实际论文实现是一样的)。</p>
<p>每个时刻t都计算出𝑄𝑡,𝐾𝑡,𝑉𝑡之后，我们就可以来计算Self-Attention了。以第一个时刻为例，我们首先计算𝑞1和𝑘1,𝑘2的内积，得到score。接下来使用softmax把得分变成概率，注意这里把得分除以8, 即sqrt(𝑑𝑘)之后再计算的softmax，根据论文的说法，这样计算梯度时会更加稳定(stable)。接下来用softmax得到的概率对所有时刻的V求加权平均，这样就可以认为得到的向量根据Self-Attention的概率综合考虑了所有时刻的输入信息</p>
<h3 id="Self-Attention的矩阵运算"><a href="#Self-Attention的矩阵运算" class="headerlink" title="Self-Attention的矩阵运算"></a>Self-Attention的矩阵运算</h3><p>第一步还是计算Q、K和V，不过不是计算某个时刻的𝑞𝑡,𝑘𝑡,𝑣𝑡了，而是一次计算所有时刻的Q、K和V。计算过程如下图所示。这里的输入是一个矩阵，矩阵的第i行表示第i个时刻的输入𝑥𝑖。</p>
<p><img src="https://uploader.shimo.im/f/wAyzetkWzdIE6WcA.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>接下来就是计算Q和K得到score，然后除以sqrt(dk)，然后再softmax，最后加权平均得到输出。全过程如下图所示。</p>
<p><img src="https://uploader.shimo.im/f/7jg3ECgtzDcFQee7.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head-Attention"></a>Multi-Head-Attention</h3><p><img src="https://uploader.shimo.im/f/Gvl4v25HaKoSjmaW.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>对于输入矩阵(time_step, num_input)，每一组Q、K和V都可以得到一个输出矩阵Z(time_step, num_features):</p>
<p><img src="https://uploader.shimo.im/f/K4i3L8oDT17I4zq1.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>但是后面的全连接网络需要的输入是一个矩阵而不是多个矩阵，因此我们可以把多个head输出的Z按照第二个维度拼接起来，但是这样的特征有一些多，因此Transformer又用了一个线性变换(矩阵𝑊𝑂)对它进行了压缩:</p>
<p><img src="https://uploader.shimo.im/f/SxjNbR105DKJ0XLE.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>整体过程：</p>
<p><img src="https://uploader.shimo.im/f/5ybvl5baeI1eC0L5.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>可以Q、K、V在维度上比词嵌入向量更低。他们的维度是64，而词嵌入和编码器的输入/输出向量的维度是512. 但实际上不强求维度更小，这只是一种基于架构上的选择，它可以使多头注意力（multiheaded attention）的大部分计算保持不变。</p>
<h3 id="使用self-attention的好处"><a href="#使用self-attention的好处" class="headerlink" title="使用self-attention的好处"></a>使用self-attention的好处</h3><p>比如我们要翻译如下句子”The animal didn’t cross the street because it was too tired”(这个动物无法穿越马路，因为它太累了)。这里的it到底指代什么呢，是animal还是street？要知道具体的指代，我们需要在理解it的时候同时关注所有的单词，重点是animal、street和tired，然后根据知识(常识)我们知道只有animal才能tired，而street是不能tired的。Self-Attention用Encoder在编码一个词的时候会考虑句子中所有其它的词，从而确定怎么编码当前词。如果把tired换成narrow，那么it就指代的是street了。</p>
<p>而LSTM(即使是双向的)是无法实现上面的逻辑的。为什么呢？比如前向的LSTM，我们在编码it的时候根本没有看到后面是tired还是narrow，所有它无法把it编码成哪个词。而后向的LSTM呢？当然它看到了tired，但是到it的时候它还没有看到animal和street这两个单词，当然就更无法编码it的内容了。</p>
<p>当然多层的LSTM理论上是可以编码这个语义的，它需要下层的LSTM同时编码了animal和street以及tired三个词的语义，然后由更高层的LSTM来把it编码成animal的语义。但是这样模型更加复杂。</p>
<p>但如果使用multo-head的attention，在编码it的时候有一个Attention Head(后面会讲到)注意到了Animal，因此编码后的it有Animal的语义。</p>
<h2 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h2><p>位置编码有很多方法，其中需要考虑的一个重要因素就是需要它编码的是相对位置的关系。比如两个句子：”北京到上海的机票”和”你好，我们要一张北京到上海的机票”。显然加入位置编码之后，两个北京的向量是不同的了，两个上海的向量也是不同的了，但是我们期望Query(北京1)Key(上海1)却是等于Query(北京2)Key(上海2)的。具体的编码算法我们在代码部分再介绍。位置编码加入后的模型如下图所示：</p>
<p><img src="https://uploader.shimo.im/f/SRIZ4VF3sKXfDYIP.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<h2 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h2><p>几乎所有的归一化方法都能起到平滑损失平面的作用，LN也一样。前面我们介绍过Batch Normalization，这个技巧能够让模型收敛的更快。但是Batch Normalization有一个问题——它需要一个minibatch的数据，而且这个minibatch不能太小(比如1)。另外一个问题就是它不能用于RNN，因为同样一个节点在不同时刻的分布是明显不同的。</p>
<p>假设我们的输入是一个minibatch的数据，我们再假设每一个数据都是一个向量，则输入是一个矩阵，每一行是一个训练数据，每一列都是一个特征。BatchNorm是对每个特征进行Normalization，而LayerNorm是对每个样本的不同特征进行Normalization，因此LayerNorm的输入可以是一行(一个样本)。</p>
<p>如下图所示，输入是(3,6)的矩阵，minibatch的大小是3，每个样本有6个特征。BatchNorm会对6个特征维度分别计算出6个均值和方差，然后用这两个均值和方差来分别对6个特征进行Normalization。而LayerNorm是分别对3个样本的6个特征求均值和方差，因此可以得到3个均值和方差，然后用这3个均值和方差对3个样本来做Normalization。</p>
<p><img src="https://uploader.shimo.im/f/TOvA2LPKMM1fykBk.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>BatchNorm看起来比较直观，我们在数据预处理也经常会把输入Normalize成均值为0，方差为1的数据，只不过它引入了可以学习的参数使得模型可以更加需要重新缓慢(不能剧烈)的调整均值和方差。而LayerNorm似乎有效奇怪，比如第一个特征是年龄，第二个特征是身高，把一个人的这两个特征求均值和方差似乎没有什么意义。论文里有一些讨论，都比较抽象。当然把身高和年龄平均并没有什么意义，但是对于其它层的特征，我们通过平均”期望”它们的取值范围大体一致，也可能使得神经网络调整参数更加容易，如果这两个特征实在有很大的差异，模型也可以学习出合适的参数让它来把取值范围缩放到更合适的区间。</p>
<h3 id="LN原理详解"><a href="#LN原理详解" class="headerlink" title="LN原理详解"></a>LN原理详解</h3><p>在Transformer中，LN被一笔带过，但却是不可或缺的一部分。每个子层的输出值为$\text { Layer } N o r m(x+\text { Sublayer }(x))$，这在网络结构图上非常明显（Norm即LN）。基本上所有的规范化技术，都可以概括为如下的公式：</p>
<ul>
<li>调整前：$h_{i}=f\left(a_{i}\right)$</li>
<li>调整后：$h_{i}^{\prime}=f\left(\frac{g_{i}}{\sigma_{i}}\left(a_{i}-u_{i}\right)+b_{i}\right)$</li>
</ul>
<p>对于隐层中某个节点的输出为对激活值 a_i 进行非线性变换 f() 后的h_i ，先使用均值$u$和方差$\sigma_{i}$对 $a_i$ 进行分布调整。如果将其理解成正态分布，就是把“高瘦”和“矮胖”的都调整回正常体型（深粉色），把偏离x=0的拉回中间来（淡紫色）。</p>
<p><img src="https://uploader.shimo.im/f/KOXOhpHIVBFM7fWl.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>这样做的第一个好处（平移）是，可以让激活值落入f()的梯度敏感区间（红色虚线的中间段）。梯度更新幅度变大，模型训练加快。第二个好处是，可以将每一次迭代的数据调整为相同分布（相当于“白化”），消除极端值，提升训练稳定性。</p>
<p>然而，在梯度敏感区内，隐层的输出接近于“线性”，模型表达能力会大幅度下降。引入 gain 因子g_i和 bias 因子b_i，为规范化后的分布再加入一点“个性”。需要注意的是， g_i和 b_i作为模型参数训练得到，$u_i$和$\sigma_{i}$在限定的数据范围内统计得到。BN 和 LN 的差别就在这里，前者在某一个 Batch 内统计某特定神经元节点的输出分布（跨样本），后者在某一次迭代更新中统计同一层内的所有神经元节点的输出分布（同一样本下）。</p>
<p>那么，为什么要舍弃 BN 改用 LN 呢？朴素版的 BN 是为 CNN 任务提出的，需要较大的 BatchSize 来保证统计量的可靠性，并在训练阶段记录全局的$u$和$\sigma$供预测任务使用。对于天然变长的 RNN 任务，需要对每个神经元进行在每个时序的状态进行统计。这不仅把原本非常简单的 BN 流程变复杂，更导致偏长的序列位置统计量不足。相比之下，LN 的使用限制就小很多，不需要在预测中使用训练阶段的统计量，即使 BatchSize = 1 也毫无影响。</p>
<p>个人理解，对于 CNN 图像类任务，每个卷积核可以看做特定的特征抽取器，对其输出做统计是有理可循的；对于 RNN 序列类任务，统计特定时序每个隐层的输出，毫无道理可言——序列中的绝对位置并没有什么显著的相关性。相反，同一样本同一时序同一层内，不同神经元节点处理的是相同的输入，在它们的输出间做统计合理得多。</p>
<p>从上面的分析可以看出，Normalization 通常被放在非线性化函数之前。以 GRU 为例，来看看 LN 是怎么设置的：</p>
<ul>
<li>$\left(\begin{array}{l}<br>\mathbf{z}_{t} \\<br>\mathbf{r}_{t}<br>\end{array}\right)=\mathbf{W}_{h} \mathbf{h}_{t-1}+\mathbf{W}_{x} \mathbf{x}_{t}$</li>
<li>$\hat{\mathbf{h}}_{t}=\tanh \left(\mathbf{W}_{\mathbf{x}_{t}}+\sigma\left(\mathbf{r}_{t}\right) \odot\left(\mathbf{U} \mathbf{h}_{t-1}\right) \mathbf{h}\right.$</li>
<li>$\mathbf{h}_{t}=\left(1-\sigma\left(\mathbf{z}_{t}\right)\right) \mathbf{h}_{t-1}+\sigma\left(\mathbf{z}_{t}\right) \hat{\mathbf{h}}_{\mathbf{t}}$</li>
</ul>
<p>可以看到，总体的原则是在“非线性之前单独处理各个矩阵”。对于 Transformer，主要的非线性部分在 FFN（ReLU） 和 Self-Attention（Softmax） 的内部，已经没有了显式的循环，但这些逐个叠加的同构子层像极了 GRU 和 LSTM 等 RNN 单元。信息的流动由沿着时序变成了穿过子层，把 LN 设置在每个子层的输出位置，意义上已经不再是“落入sigmoid 的梯度敏感空间来加速训练”了，个人认为更重要的是前文提到的“白化”—— 让每个词的向量化数值更加均衡，以消除极端情况对模型的影响，获得更稳定的深层网络结构 —— 就像这些词从 Embdding 层出来时候那样，彼此只有信息的不同，没有能量的多少。在和之前的 TWWT 实验一样的配置中，删除了全部的 LN 层后模型不再收敛。LN 正如 LSTM 中的tanh，它为模型提供非线性以增强表达能力，同时将输出限制在一定范围内。 因此，对于 Transformer 来说，LN 的效果已经不是“有多好“的范畴了，而是“不能没有”。</p>
<h3 id="MLP中的LN"><a href="#MLP中的LN" class="headerlink" title="MLP中的LN"></a>MLP中的LN</h3><p>设H是一层中隐层节点的数量，l是MLP的层数，我们可以计算LN的归一化统计量$u$和$\sigma$：</p>
<ul>
<li>$\mu^{l}=\frac{1}{H} \sum_{i=1}^{H} a_{i}^{l}$</li>
<li>$\sigma^{l}=\sqrt{\frac{1}{H} \sum_{i=1}^{H}\left(a_{i}^{l}-\mu^{l}\right)^{2}}$</li>
</ul>
<p>注意上面统计量的计算是和样本数量没有关系的，它的数量只取决于隐层节点的数量，所以只要隐层节点的数量足够多，我们就能保证LN的归一化统计量足够具有代表性。通过$u^l$和$\sigma^l$可以得到归一化后的值$\hat{\mathbf{a}}^{l}=\frac{\mathbf{a}^{l}-\mu^{l}}{\sqrt{\left(\sigma^{l}\right)^{2}+\epsilon}}$（其中$\epsilon$是一个很小的小数，防止除0）。</p>
<p>在LN中我们也需要一组参数来保证归一化操作不会破坏之前的信息，在LN中这组参数叫做增益（gain）g和偏置（bias) b（等同于BN中的$\gamma$和）$\beta$。假设激活函数为f，最终LN的输出为：$\mathbf{h}^{l}=f\left(\mathbf{g}^{l} \odot \hat{\mathbf{a}}^{l}+\mathbf{b}^{l}\right)$。合并公式并忽略参数l可以得到：$\mathbf{h}=f\left(\frac{\mathbf{g}}{\sqrt{\sigma^{2}+\epsilon}} \odot(\mathbf{a}-\mu)+\mathbf{b}\right)$</p>
<h3 id="RNN中的LN"><a href="#RNN中的LN" class="headerlink" title="RNN中的LN"></a>RNN中的LN</h3><p>在RNN中，我们可以非常简单的在每个时间片中使用LN，而且在任何时间片我们都能保证归一化统计量统计的是H个节点的信息。对于RNN时刻 t 时的节点，其输入是 t-1 时刻的隐层状态 h^{t-1}和 t 时刻的输入数据x_t，可以表示为：$\mathbf{a}^{t}=W_{h h} h^{t-1}+W_{x h} \mathbf{x}^{t}$。</p>
<p>接着我们便可以在$\mathbf{a}^{t}$上采取归一化过程：</p>
<ul>
<li>$\mu^{t}=\frac{1}{H} \sum_{i=1}^{H} a_{i}^{t}$</li>
<li>$\sigma^{t}=\sqrt{\frac{1}{H} \sum_{i=1}^{H}\left(a_{i}^{t}-\mu^{t}\right)^{2}}$</li>
<li>$\mathbf{h}^{t}=f\left(\frac{\mathbf{g}}{\sqrt{\left(\sigma^{t}\right)^{2}+\epsilon}} \odot\left(\mathbf{a}^{t}-\mu^{t}\right)+\mathbf{b}\right)$</li>
</ul>
<h4 id="LSTM中的LN"><a href="#LSTM中的LN" class="headerlink" title="LSTM中的LN"></a>LSTM中的LN</h4><p>LSTM的计算公式为：</p>
<ul>
<li>$\left(\begin{array}{c}<br>\mathbf{f}_{t} \\<br>\mathbf{i}_{t} \\<br>\mathbf{o}_{t} \\<br>\mathbf{g}_{t}<br>\end{array}\right)=\mathbf{W}_{h} \mathbf{h}_{t-1}+\mathbf{W}_{x} \mathbf{x}_{t}+b$</li>
<li>$\mathbf{c}_{t}=\sigma\left(\mathbf{f}_{t}\right) \odot \mathbf{c}_{t-1}+\sigma\left(\mathbf{i}_{t}\right) \odot \tanh \left(\mathbf{g}_{t}\right)$</li>
<li>$\mathbf{h}_{t}=\sigma\left(\mathbf{o}_{t}\right) \odot \tanh \left(\mathbf{c}_{t}\right)$</li>
</ul>
<p>使用了LN后的计算公式为：</p>
<ul>
<li>$\left(\begin{array}{c}<br>\mathbf{f}_{t} \\<br>\mathbf{i}_{t} \\<br>\mathbf{o}_{t} \\<br>g_{t}<br>\end{array}\right)=\quad L N\left(\mathbf{W}_{h} \mathbf{h}_{t-1} ; \boldsymbol{\alpha}_{1}, \boldsymbol{\beta}_{1}\right)+L N\left(\mathbf{W}_{x} \mathbf{x}_{t} ; \boldsymbol{\alpha}_{2}, \boldsymbol{\beta}_{2}\right)+b$</li>
<li>$\mathbf{c}_{t}=\sigma\left(\mathbf{f}_{t}\right) \odot \mathbf{c}_{t-1}+\sigma\left(\mathbf{i}_{t}\right) \odot \tanh \left(\mathbf{g}_{t}\right)$</li>
<li>$\mathbf{h}_{t}=\sigma\left(\mathbf{o}_{t}\right) \odot \tanh \left(L N\left(\mathbf{c}_{t} ; \boldsymbol{\alpha}_{3}, \boldsymbol{\beta}_{3}\right)\right)$</li>
</ul>
<h3 id="对照实验"><a href="#对照实验" class="headerlink" title="对照实验"></a>对照实验</h3><p>这里我们设置了一组对照试验来对比普通网络，BN以及LN在MLP和RNN上的表现。</p>
<h4 id="MLP上的归一化"><a href="#MLP上的归一化" class="headerlink" title="MLP上的归一化"></a>MLP上的归一化</h4><p>这里使用的是MNIST数据集，但是归一化操作只添加到了后面的MLP部分。Keras官方源码中没有LN的实现，我们可以通过<code>pip install keras-layer-normalization</code>进行安装，使用方法如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from keras_layer_normalization import LayerNormalization</span><br><span class="line"># 构建LN CNN网络</span><br><span class="line">model_ln &#x3D; Sequential()</span><br><span class="line">model_ln.add(Conv2D(input_shape &#x3D; (28,28,1), filters&#x3D;6, kernel_size&#x3D;(5,5), padding&#x3D;&#39;valid&#39;, activation&#x3D;&#39;tanh&#39;))</span><br><span class="line">model_ln.add(MaxPool2D(pool_size&#x3D;(2,2), strides&#x3D;2))</span><br><span class="line">model_ln.add(Conv2D(input_shape&#x3D;(14,14,6), filters&#x3D;16, kernel_size&#x3D;(5,5), padding&#x3D;&#39;valid&#39;, activation&#x3D;&#39;tanh&#39;))</span><br><span class="line">model_ln.add(MaxPool2D(pool_size&#x3D;(2,2), strides&#x3D;2))</span><br><span class="line">model_ln.add(Flatten())</span><br><span class="line">model_ln.add(Dense(120, activation&#x3D;&#39;tanh&#39;))</span><br><span class="line">model_ln.add(LayerNormalization()) # 添加LN运算</span><br><span class="line">model_ln.add(Dense(84, activation&#x3D;&#39;tanh&#39;))</span><br><span class="line">model_ln.add(LayerNormalization())</span><br><span class="line">model_ln.add(Dense(10, activation&#x3D;&#39;softmax&#39;))</span><br></pre></td></tr></table></figure>
<p>另外两个对照试验也使用了这个网络结构，不同点在于归一化部分。图3左侧是batchsize=128时得到的收敛曲线，从中我们可以看出BN和LN均能取得加速收敛的效果，且BN的效果要优于LN。图3右侧是batchsize=8是得到的收敛曲线，这时BN反而会减慢收敛速度，验证了我们上面的结论，对比之下LN要轻微的优于无归一化的网络，说明了LN在小尺度批量上的有效性：<br><img src="https://uploader.shimo.im/f/8jLUlejv9kLBLD4n.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"><img src="https://uploader.shimo.im/f/hpHSt6yZW9dhXRKY.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<h4 id="LSTM上的归一化"><a href="#LSTM上的归一化" class="headerlink" title="LSTM上的归一化"></a>LSTM上的归一化</h4><p>另外一组对照实验是基于imdb的二分类任务，使用了glove作为词嵌入。这里设置了无LN的LSTM和带LN的LSTM的作为对照试验，网络结构如下面代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from lstm_ln import LSTM_LN</span><br><span class="line">model_ln &#x3D; Sequential()</span><br><span class="line">model_ln.add(Embedding(max_features,100))</span><br><span class="line">model_ln.add(LSTM_LN(128))</span><br><span class="line">model_ln.add(Dense(1, activation&#x3D;&#39;sigmoid&#39;))</span><br><span class="line">model_ln.summary()</span><br></pre></td></tr></table></figure>
<p>LN对于RNN系列动态网络的收敛加速上的效果是略有帮助的。LN的有点主要体现在两个方面：</p>
<ul>
<li>LN得到的模型更稳定；</li>
<li>LN有正则化的作用，得到的模型更不容易过拟合。</li>
</ul>
<h2 id="残差连接"><a href="#残差连接" class="headerlink" title="残差连接"></a>残差连接</h2><p>每个Self-Attention层都会加一个残差连接，然后是一个LayerNorm层，如下图所示：</p>
<p><img src="https://uploader.shimo.im/f/9JnJi8vn7Izidhmp.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>下图展示了更多细节：输入𝑥1、𝑥2经self-attention层之后变成𝑧1、𝑧2，然后和残差连接的输入𝑥1、𝑥2加起来，然后经过LayerNorm层输出给全连接层。全连接层也是有一个残差连接和一个LayerNorm层，最后再输出给上一层：</p>
<p><img src="https://uploader.shimo.im/f/smiABSb61r8rvfqg.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>这样做的好处不言而喻，避免了梯度消失（求导时多了一个常数项）。</p>
<h2 id="Decoder层"><a href="#Decoder层" class="headerlink" title="Decoder层"></a>Decoder层</h2><p>Decoder和Encoder是类似的，如下图所示，区别在于它多了一个Encoder-Decoder Attention层，这个层的输入除了来自Self-Attention之外还有Encoder最后一层的所有时刻的输出。Encoder-Decoder Attention层的Query来自下一层，而Key和Value则来自Encoder的输出。</p>
<p><img src="https://uploader.shimo.im/f/IuBCl1ZxZG9BKh5z.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>在这个框架下，解码器实际上可看成一个神经网络语言模型，预测的时候，target中的每一个单词是逐个生成的，当前词的生成依赖两方面：一是Encoder的输出，二是target的前面的单词。例如，在生成第一个单词是，不仅依赖于Encoder的输出，还依赖于起始标志[CLS]；生成第二个单词是，不仅依赖Encoder的输出，还依赖起始标志和第一个单词…依此类推。这其实是说，在翻译当前词的时候，是看不到后面的要翻译的词。由上可以看出，这里的mask是动态的。</p>
<h1 id="Transformer细节讨论"><a href="#Transformer细节讨论" class="headerlink" title="Transformer细节讨论"></a>Transformer细节讨论</h1><h2 id="Scaled-Dot-product-Attention"><a href="#Scaled-Dot-product-Attention" class="headerlink" title="Scaled Dot-product Attention"></a>Scaled Dot-product Attention</h2><p><img src="https://uploader.shimo.im/f/wUrUGWRx6fOH6zaP.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>首先， Q 与 K 进行了一个点积操作，这个其实就是我在Attention讲到的 score 操作。然后，有一个 Scale 操作，其实就是为了防止结果过大，除以一个尺度标度 sqrt(dk)（注：主要因为dk比较大时，点乘注意力的表现变差，认为是由于点乘后的值过大，导致softmax函数趋近于边缘，梯度较小）， 其中dk是Q中一个向量的维度。再然后，经过一个Mask操作； Mask操作是将需要隐藏的数据设置为负无穷，这样经过后面的 Softmax 后就接近于 0，这样这些数据就不会对后续结果产生影响了。最后经过一个 Softmax 层， 然后计算 Attention Value。其公式：$\text {Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V$</p>
<p>这里解释一下 Scaled Dot-product Attention 在本文中的应用，也是称为 Self-Attention 的原因所在，这里的Q，K， V 都是一样的，意思就是说，这里是句子对句子自己进行Attention 来查找句子中词之间的关系，这是一件很厉害的事情，回想LSTM是怎么做的，再比较 Self-Attention， 直观的感觉，Self-Attention更能把握住词与词的语义特征，而LSTM对长依赖的句子，往往毫无办法，表征极差。</p>
<p>常用的attention主要有“Add-相加”和“Mul-相乘”两种：</p>
<ul>
<li>$\operatorname{score}\left(h_{j}, s_{i}\right)=<v, \tanh \left(W_{1} h_{j}+W_{2} s_{i}\right)>[A d d]$</li>
<li>$\operatorname{score}\left(h_{j}, s_{i}\right)=\left\langle W_{1} h_{j}, W_{2} s_{i}&gt;\quad[M u l]\right.$</li>
</ul>
<p>矩阵加法的计算更简单，但是外面套着tanh和 v，相当于一个完整的隐层。在整体计算复杂度上两者接近，但考虑到矩阵乘法已经有了非常成熟的加速算法，Transformer采用了Mul形式。</p>
<p>在模型效果上，《Massive Exploration of Neural Machine Translation Architectures》对不同Attention-Dimension (d_k) 下的Add和Mul进行了对比，如下图：</p>
<p><img src="https://uploader.shimo.im/f/I3AO3zusDdNUXzMC.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>可以看到，在$d_k$较小的时候，Add和Mul相差不大；随着$d_k$增大，Add明显超越了Mul。Transformer 设置 d_k=64虽然不在表格的范围内，但是可以推测Add仍略优于Mul。作者认为，$d_k$的增大将点积结果推向了softmax函数的梯度平缓区，影响了训练的稳定性。原话是这么说的：</p>
<blockquote>
<p>We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients.</p>
</blockquote>
<p>因此，Transformer 为“dot-product attention”加了一个前缀“scaled”，即引入一个温度因子（temperature）$\sqrt{d_{k}}$，中文全称“缩放的点积注意力网络”：$\text {Attention}(Q, K, V)=\operatorname{softmax}\left(Q K^{T} / \sqrt{d_{k}}\right) V$</p>
<p>等等，Add也离不开softmax，怎么没有这个问题呢？首先，左侧v的导数就是 tanh的输出，后者本身就是 [-1,1]之间的，一定不会超限。然后，右侧 W_1的导数就是隐层h_j ，必然是sigmoid或者其他激活函数的输出，值也不会太大。</p>
<p>回到Mul。左侧W_1的导数来自于$h_{j}\left(W_{2} s_{i}\right)$。如果s_i分布在(0,1)，那么$W_{2} s_{i}$就会扩展到(0, d_k) ，即点积可能会造成一个非常大的梯度值。</p>
<h2 id="MultiHeadAttention"><a href="#MultiHeadAttention" class="headerlink" title="MultiHeadAttention"></a>MultiHeadAttention</h2><p>Transformer 中使用 Multi-head Attention要注意以下几点：</p>
<p>首先， 在Encoder与Decoder中的黑色框中，采用的都是是 Self-Attention ，Q，K，V 同源。</p>
<p>其次，需要注意的是只有在Decoder中的Muti-head中才有 Mask 操作，而在Encoder中却没有，这是因为我们在预测第t个词时，需要将 t 时刻及以后的词遮住，只对前面的词进行 self-attention。</p>
<p>最后， 在黄色框中， 采用的是传统的Attention思路，Q 来自Decoder层， 而 K， V来自Encoder的输出 。</p>
<h2 id="最后的Linear与Softmax"><a href="#最后的Linear与Softmax" class="headerlink" title="最后的Linear与Softmax"></a>最后的Linear与Softmax</h2><p>一般都会在最后一层加一个前馈神经网络来增加泛化能力，最后用一个 softmax 来进行预测。</p>
<p>线性层的参数个数为d_model vocab_size， 一般来说，vocab_size会比较大，拿20000为例，那么只这层的参数就有51220000个，约为10的8次方，非常惊人。而在词向量那一层，同样也是这个数值，所以，一种比较好的做法是将这两个全连接层的参数共享，会节省不少内存，而且效果也不会差。</p>
<p>注意：在Encoder的输入embedding、Decoder的输入embedding后，增加了1个scale因子，而生成下一词汇的linear transformation前却未添加scale因子。</p>
<h2 id="Position-Embedding"><a href="#Position-Embedding" class="headerlink" title="Position Embedding"></a>Position Embedding</h2><p>在下图中，每一行对应一个词向量的位置编码，所以第一行对应着输入序列的第一个词。每行包含512个值，每个值介于1和-1之间。我们已经对它们进行了颜色编码，所以图案是可见的。20字(行)的位置编码实例，词嵌入大小为512(列)。你可以看到它从中间分裂成两半。这是因为左半部分的值由一个函数(使用正弦)生成，而右半部分由另一个函数(使用余弦)生成。然后将它们拼在一起而得到每一个位置编码向量。这种编码的优点是能够扩展到未知的序列长度(例如，当我们训练出的模型需要翻译远比训练集里的句子更长的句子时)。</p>
<ul>
<li>$P E_{(p o s, 2 i)}=\sin \left(p o s / 10000^{2 i / d_{\text {mod }}}\right)$</li>
<li>$P E_{(p o s, 2 i+1)}=\cos \left(p o s / 10000^{2 i / d_{\text {model }}}\right)$</li>
</ul>
<p><img src="https://uploader.shimo.im/f/Su3SrWtdY1L1R7zr.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>首先，可以证明出每个位置都能获得唯一的编码。其次，i决定了频率的大小，不同的i可以看成是不同的频率空间中的编码，是相互正交的，通过改变i的值，就能得到多维度的编码，类似于词向量的维度。这里2i&lt;=512（$d_{model}$）, 一共512维。想象一下，当2i大于$d_{model}$时会出现什么情况，这时sin函数的周期会变得非常大，函数值会非常接近于0，这显然不是我们希望看到的，因为这样和词向量就不在一个量级了，位置编码的作用被削弱了。另外，值得注意的是，位置编码是不参与训练的，而词向量是参与训练的。作者通过实验发现，位置编码参与训练与否对最终的结果并无影响。</p>
<p>其次，之所以对奇偶位置分别编码，论文中是这样解释的：</p>
<blockquote>
<p>We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of PEpos.</p>
</blockquote>
<p>相隔 k 个词的两个位置 pos 和 pos+k 的位置编码是由 k 的位置编码定义的一个线性变换，推导公式如下：</p>
<ul>
<li>$P E(p o s+k, 2 i)=P E(p o s, 2 i) P E(k, 2 i+1)+P E(p o s, 2 i+1) P E(k, 2 i)$</li>
<li>$P E(p o s+k, 2 i+1)=P E(p o s, 2 i+1) P E(k, 2 i+1)-P E(p o s, 2 i) P E(k, 2 i)$</li>
</ul>
<p>或者下面的推导更清晰一些：</p>
<ul>
<li>$\begin{aligned}<br>P E_{(p o s+k, 2 i)} &amp;=\sin \left((p o s+k) / 10000^{2 i / d_{\text {model}}}\right) \\<br>&amp;=\sin \left(p o s / 10000^{2 i / d_{\text {model}}}\right) \cos \left(k / 10000^{2 i / d_{\text {model}}}\right)+\cos \left(p o s / 10000^{2 i / d_{\text {model}}}\right) \sin \left(k / 10000^{2 i / d_{\text {model}}}\right) \\<br>&amp;=\cos \left(k / 10000^{2 i / d_{\text {model}}}\right) P E_{(p o s, 2 i)}+\sin \left(k / 10000^{2 i / d_{\text {model}}}\right) P E_{(p o s, 2 i+1)}<br>\end{aligned}$</li>
<li>$\begin{aligned}<br>P E_{(p o s+k, 2 i+1)} &amp;=\cos \left((p o s+k) / 10000^{2 i / d_{\text {model}}}\right) \\<br>&amp;=\cos \left(\text {pos} / 10000^{2 i / d_{\text {model}}}\right) \cos \left(k / 10000^{2 i / d_{\text {model}}}\right)-\sin \left(p o s / 10000^{2 i / d_{\text {model}}}\right) \sin \left(k / 10000^{2 i / d_{\text {model}}}\right) \\<br>&amp;=\cos \left(k / 10000^{2 i / d_{\text {model}}}\right) P E_{(p o s, 2 i+1)}-\sin \left(k / 10000^{2 i / d_{\text {model}}}\right) P E_{(\text {pos}, 2 i)}<br>\end{aligned}$</li>
</ul>
<h2 id="前馈网络"><a href="#前馈网络" class="headerlink" title="前馈网络"></a>前馈网络</h2><p>每个encoderLayer中，多头attention后会接一个前馈网络。这个前馈网络其实是两个全连接层，进行了如下操作：$FFN(x)=max(0,xW1+b1)W2+b2$</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">self.w_1 &#x3D; nn.Linear(d_model, d_ff)</span><br><span class="line"># self.w_1 &#x3D; nn.Conv1d(in_features&#x3D;d_model, out_features&#x3D;d_ff, kenerl_size&#x3D;1)</span><br><span class="line">self.w_2 &#x3D; nn.Linear(d_ff, d_model)</span><br><span class="line"># self.w_2 &#x3D; nn.Conv1d(in_features&#x3D;d_ff, out_features&#x3D;d_model, kenerl_size&#x3D;1)</span><br></pre></td></tr></table></figure>
<p>这两层的作用等价于两个 kenerl_size=1的一维卷积操作。<br>FFN 相当于将每个位置的Attention结果映射到一个更大维度的特征空间，然后使用ReLU引入非线性进行筛选，最后恢复回原始维度。需要说明的是，在抛弃了 LSTM 结构后，FFN 中的 ReLU成为了一个主要的能提供非线性变换的单元。</p>
<h2 id="Weight-Tying"><a href="#Weight-Tying" class="headerlink" title="Weight Tying"></a>Weight Tying</h2><p>论文的3.4小节Embeddings and Softmax，有这样一句话：</p>
<blockquote>
<p>In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [29].</p>
</blockquote>
<p>《Using the Output Embedding to Improve Language Models》这篇文章主要介绍的是RNNLM中的Weight Tying技术，不仅能压缩LM的大小，还能显著改善PPL表现。作者为了证明算法的普适性，在NMT任务上也进行了扩展实验。在seq2seq模型中，decoder可以近似地看作RNNLM，它必不可少地有一个embedding矩阵$U \in R^{C \times H}$和一个pre-softmax矩阵$V \in R^{C \times H}$，来完成词表大小C到隐层大小H的尺度转换：$h_{i n}=U^{T} C, \ldots, h_{p r e_{-} s o f t m a x}=V h_{o u t}$</p>
<p>。Weight Tying在操作上非常简单，即令U=V 。在OPEN-NMT的Pytorch实现版本中，仅仅一行代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if model_opt.share_decoder_embeddings:</span><br><span class="line">    generator[0].weight &#x3D; decoder.embeddings.word_lut.weight</span><br></pre></td></tr></table></figure>
<p>Transformer在英法和英德上，混用了源语言和目标语言的词表，因此使用了升级版的TWWT（Three way weight tying），把encoder的embedding层权重，也加入共享：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if model_opt.share_embeddings:</span><br><span class="line">    tgt_emb.word_lut.weight &#x3D; src_emb.word_lut.weight</span><br></pre></td></tr></table></figure>
<p>虽然weight共享了，但是embedding和pre-softmax仍然是两个不同的层，因为bias是彼此独立的。<br>在我个人的理解中，one-hot向量和对U的操作是“指定抽取”，即取出某个单词的向量行；pre-softmax对U的操作是“逐个点积”，对隐层的输出，依次计算其和每个单词向量行的变换结果。虽然具体的操作不同，但在本质上，U和V都是对任一的单词进行向量化表示（H列），然后按词表序stack起来（C行）。因此，两个权重矩阵在语义上是相通的。</p>
<p>也是由于上面两种操作方式的不同，U在反向传播中不如V训练得充分。将两者绑定在一起，缓和了这一问题，可以训练得到质量更高的新矩阵。另外，由于词表大小C通常比模型隐层大小H高出一个数量级，Weight Tying 可以显著减小模型的参数量。这个数据在论文中是28%~51%，我使用Transformer-base在非共享词表的英中方向上进行测试，模型参数量从 131,636,930 减小到 102,177,474，足足的22%。模型更小，收敛更快更容易。</p>
<p>下图是知乎上网友用Transformer-base模型跑的实验结果，绿色是Weight Tying版本，蓝色是基础版本。X轴为step，Y轴为Appromix BLEU。可以看到，两者的收敛速度接近，曲线平稳后绿色明显优于蓝色，峰值相差0.17。</p>
<p><img src="https://uploader.shimo.im/f/wi59MAoPcUomITKw.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<h2 id="Label-Smoothing"><a href="#Label-Smoothing" class="headerlink" title="Label Smoothing"></a>Label Smoothing</h2><p>Label Smoothing是一种正则化技术，核心功能就是防止过拟合，它的全称是 Label Smoothing Regularization（LSR），是 2015 年的经典论文《Rethinking the Inception Architecture for Computer Vision》的一个副产品。作者认为 LSR 可以避免模型 too confident。关于这一点论文里没有详细的解释，不过对于 NMT 任务来说，鼓励模型产生多样化的译文确实会帮助提升 BLEU 值，毕竟标准译文并不是唯一的。</p>
<p>最著名的正则化技术，dropout，通过随机抹掉一些节点来削弱彼此之间的依赖，通常设置在网络内部隐层。作为 dropout 的配合策略，LSR 考虑的是 softmax 层。</p>
<p>假设目标类别为y，任意类别为，ground-truth 分布为q(k)，模型预测分布为p(k)。显然，当k=y时，q(k)=1。当k!=y时，q(k)=0。LSR 为了让模型的输出不要过于贴合单点分布，选择在 gound-truth 中加入噪声。即削弱y的概率，并整体叠加一个独立于训练样例的均匀分布u(k)：$q^{\prime}(k)=(1-\epsilon) q(k)+\epsilon u(k)=(1-\epsilon) q(k)+\epsilon / K$。其中K是 softmax 的类别数。拆开来写可以看得清楚一点：</p>
<ul>
<li>$q^{\prime}(k)=1-\epsilon+\epsilon / K, \quad k=y$</li>
<li>$q^{\prime}(k)=\epsilon / K, \quad k \neq y$</li>
</ul>
<p>所有类别的概率和仍然是归一的。说白了就是把最高点砍掉一点，多出来的概率平均分给所有人。调整之后，交叉熵（损失函数）也随之变化：$H\left(q^{\prime}, p\right)=(1-\epsilon) H(q, p)+\epsilon H(u \cdot p)$。对于两个完全一致的分布，其交叉熵为0。LSR 可以看作是在优化目标中加入了正则项 H(u, p)，在模型输出偏离均匀分布时施以惩罚。</p>
<h2 id="Warmup-amp-Noam学习率更新"><a href="#Warmup-amp-Noam学习率更新" class="headerlink" title="Warmup &amp; Noam学习率更新"></a>Warmup &amp; Noam学习率更新</h2><h3 id="warmup"><a href="#warmup" class="headerlink" title="warmup"></a>warmup</h3><p>论文提出了一个全新的学习率更新公式：$\text {lrate}=d_{\text {model}}^{-0.5} \cdot \min \left(\text {step_num}^{-0.5}, \text {step_num} \cdot \text {warmup_steps}^{-1.5}\right)$</p>
<p>如果把min去掉的话，就变成一个以warmup_steps为分界点的分段函数。在该点之后，$\text { lrate }=d_{\text {model }}^{-0.5} \cdot \text { step_num }^{-0.5}$，是 decay 的部分。常用方法有指数衰减（exponential）、分段常数衰减（piecewise-constant）、反时限衰减（inverse-time）等等。Transformer 采用了负幂的形式，衰减速度先快后慢。</p>
<p>在该点之前，$\text { lrate }=d_{\text {model}}^{-0.5} \cdot \text { step_num } \cdot \text { warmup_steps}^{-1.5}$，是 warmup 的部分。Transformer 采用了线性函数的形式，warmup_steps 越大，斜率越小。</p>
<p><img src="https://uploader.shimo.im/f/bxxdHqj3LdOeu2mt.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>画在图上明显很多。Transformer 的学习率更新公式叫作“noam”，它将 warmup 和 decay 两个部分组合在一起，总体趋势是先增加后减小。</p>
<p>warmup为什么有效? 可参考<a href="https://www.zhihu.com/question/338066667" target="_blank" rel="noopener">https://www.zhihu.com/question/338066667</a>、<a href="https://zhuanlan.zhihu.com/p/45410279" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/45410279</a>。</p>
<h3 id="weight-decay"><a href="#weight-decay" class="headerlink" title="weight-decay"></a>weight-decay</h3><h4 id="指数衰减"><a href="#指数衰减" class="headerlink" title="指数衰减"></a>指数衰减</h4><p>指数衰减学习率是先使用较大的学习率来快速得到一个较优的解，然后随着迭代的继续,逐步减小学习率，使得模型在训练后期更加稳定。Transfomer使用的是指数衰减。指数衰减学习率的公式：$\text { decayed_learning_rate = learning_rate\cdotdecay_rate }^{(\text {global_step} / \text {decay_steps})}$</p>
<p>global_step是计数器,从0计数到训练的迭代次数，learning_rate是初始化的学习率，decayed_learning_rate是随着 global_step递增而衰减。显然，当global_step为初值0时， 有下面等式：$\text { decayed_learning_rate = learning_rate }$。</p>
<p>decay_steps用来控制衰减速度，如果decay_steps大一些, global_step/decay_steps就会增长缓慢一些。从而指数衰减学习率decayed_learning_rate就会衰减得慢一否则学习率很快就会衰减为趋近于0。</p>
<p>具体代码可参考：<a href="https://zhuanlan.zhihu.com/p/29421235" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29421235</a></p>
<h4 id="分段常数衰减"><a href="#分段常数衰减" class="headerlink" title="分段常数衰减"></a>分段常数衰减</h4><pre><code>参考：[https://www.jianshu.com/p/125fe2ab085b](https://www.jianshu.com/p/125fe2ab085b)
</code></pre><h4 id="自然指数衰减"><a href="#自然指数衰减" class="headerlink" title="自然指数衰减"></a>自然指数衰减</h4><p>它与指数衰减方式相似，不同的在于它的衰减底数是e，故而其收敛的速度更快，一般用于相对比较容易训练的网络，便于较快的收敛，其更新规则如下：$\text { decayed_learning_rate = learning_rate } * e^{\frac{-d e c a y_{r a t e}}{g l b b a_{s t e p}}}$</p>
<p>下图为为分段常数衰减、指数衰减、自然指数衰减三种方式的对比图，红色的即为分段常数衰减图，阶梯型曲线。蓝色线为指数衰减图，绿色即为自然指数衰减图，很明可以看到自然指数衰减方式下的学习率衰减程度要大于一般指数衰减方式，有助于更快的收敛。</p>
<p><img src="https://uploader.shimo.im/f/Pw9NQo5Yu503saf7.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<h4 id="多项式衰减"><a href="#多项式衰减" class="headerlink" title="多项式衰减"></a>多项式衰减</h4><p>应用多项式衰减的方式进行更新学习率，这里会给定初始学习率和最低学习率取值，然后将会按照给定的衰减方式将学习率从初始值衰减到最低值,其更新规则如下式所示：</p>
<ul>
<li>$\text { global_step }=\min (\text {global_step, decay_steps})$</li>
<li>$\begin{array}{c}<br>\text { decayed_learning_rate }=(\text {learning_rate}-\text {end_learning_rate}) *\left(1-\frac{\text {global_step}}{\text {decay_steps}}\right)^{\text {powe}} \\<br>+\text {end_learning_rate}<br>\end{array}$</li>
</ul>
<p>需要注意的是，有两个机制，降到最低学习率后，到训练结束可以一直使用最低学习率进行更新，另一个是再次将学习率调高，使用 decay_steps 的倍数，取第一个大于 global_steps 的结果，它是用来防止神经网络在训练的后期由于学习率过小而导致的网络一直在某个局部最小值附近震荡，这样可以通过在后期增大学习率跳出局部极小值。</p>
<p>如下图所示，红色线代表学习率降低至最低后，一直保持学习率不变进行更新，绿色线代表学习率衰减到最低后，又会再次循环往复的升高降低。</p>
<p><img src="https://uploader.shimo.im/f/TWHJqCaUQytQ7SLS.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<h4 id="余弦衰减"><a href="#余弦衰减" class="headerlink" title="余弦衰减"></a>余弦衰减</h4><p>余弦衰减就是采用余弦的相关方式进行学习率的衰减，衰减图和余弦函数相似。其更新机制如下式所示：</p>
<ul>
<li>$\text { global_step }=\min (\text {global_step, decay_steps})$</li>
<li>$\text {cosine_decay}=0.5 <em>\left(1+\cos \left(\pi </em> \frac{\text {global_step}}{\text {decay_steps}}\right)\right)$</li>
<li>$\text { decayed }=(1-\alpha) * \text { cosine_decay }+\alpha$</li>
<li>$\text { decayed_learning_rate = learning_rate * decayed }$</li>
</ul>
<p>如下图所示，红色即为标准的余弦衰减曲线，学习率从初始值下降到最低学习率后保持不变。蓝色的线是线性余弦衰减方式曲线，它是学习率从初始学习率以线性的方式下降到最低学习率值。绿色噪声线性余弦衰减方式：</p>
<p><img src="https://uploader.shimo.im/f/B2zQIXMV8trD31KQ.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<h2 id="Masking"><a href="#Masking" class="headerlink" title="Masking"></a>Masking</h2><h3 id="pad-mask"><a href="#pad-mask" class="headerlink" title="pad mask"></a>pad mask</h3><p>通常也是编码端的mask</p>
<p><img src="https://uploader.shimo.im/f/n9R8KVx3vwWbU3Wu.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<h3 id="sequence-mask"><a href="#sequence-mask" class="headerlink" title="sequence mask"></a>sequence mask</h3><p><img src="https://uploader.shimo.im/f/UcgsvQ46WN33NjqJ.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<h3 id="解码端的mask要同时考虑pad-mask和sequece-mask"><a href="#解码端的mask要同时考虑pad-mask和sequece-mask" class="headerlink" title="解码端的mask要同时考虑pad mask和sequece mask"></a>解码端的mask要同时考虑pad mask和sequece mask</h3><p><img src="https://uploader.shimo.im/f/D2Nwlz6tyDopu7un.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<h2 id=""><a href="#" class="headerlink" title=" "></a> </h2><h2 id="Word-Piece"><a href="#Word-Piece" class="headerlink" title="Word Piece"></a>Word Piece</h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>现在基本性能好一些的NLP模型，例如OpenAI GPT，google的BERT，在数据预处理的时候都会有WordPiece的过程。WordPiece字面理解是把word拆成piece一片一片，其实就是这个意思。</p>
<p>WordPiece的一种主要的实现方式叫做BPE（Byte-Pair Encoding）双字节编码。</p>
<p>BPE的过程可以理解为把一个单词再拆分，使得我们的此表会变得精简，并且寓意更加清晰。</p>
<p>比如”loved”,”loving”,”loves”这三个单词。其实本身的语义都是“爱”的意思，但是如果我们以单词为单位，那它们就算不一样的词，在英语中不同后缀的词非常的多，就会使得词表变的很大，训练速度变慢，训练的效果也不是太好。</p>
<p>BPE算法通过训练，能够把上面的3个单词拆分成”lov”,”ed”,”ing”,”es”几部分，这样可以把词的本身的意思和时态分开，有效的减少了词表的数量。</p>
<h3 id="BPE算法"><a href="#BPE算法" class="headerlink" title="BPE算法"></a>BPE算法</h3><blockquote>
<p>参考：<a href="https://plmsmile.github.io/2017/10/19/subword-units/" target="_blank" rel="noopener">https://plmsmile.github.io/2017/10/19/subword-units/</a></p>
</blockquote>
<p>BPE的大概训练过程：首先将词分成一个一个的字符，然后在词的范围内统计字符对出现的次数，每次将次数最多的字符对保存起来，直到循环次数结束。</p>
<p>我们模拟一下BPE算法。</p>
<ul>
<li>我们原始词表如下：{‘l o w e r ‘: 2, ‘n e w e s t ‘: 6, ‘w i d e s t ‘: 3, ‘l o w ‘: 5}</li>
<li>其中的key是词表的单词拆分层字母，再加代表结尾，value代表词出现的频率。</li>
<li>下面我们每一步在整张词表中找出频率最高相邻序列，并把它合并，依次循环。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">原始词表 &#123;&#39;l o w e r &lt;&#x2F;w&gt;&#39;: 2, &#39;n e w e s t &lt;&#x2F;w&gt;&#39;: 6, &#39;w i d e s t &lt;&#x2F;w&gt;&#39;: 3, &#39;l o w &lt;&#x2F;w&gt;&#39;: 5&#125;</span><br><span class="line">出现最频繁的序列 (&#39;s&#39;, &#39;t&#39;) 9</span><br><span class="line">合并最频繁的序列后的词表 &#123;&#39;n e w e st &lt;&#x2F;w&gt;&#39;: 6, &#39;l o w e r &lt;&#x2F;w&gt;&#39;: 2, &#39;w i d e st &lt;&#x2F;w&gt;&#39;: 3, &#39;l o w &lt;&#x2F;w&gt;&#39;: 5&#125;</span><br><span class="line">出现最频繁的序列 (&#39;e&#39;, &#39;st&#39;) 9</span><br><span class="line">合并最频繁的序列后的词表 &#123;&#39;l o w e r &lt;&#x2F;w&gt;&#39;: 2, &#39;l o w &lt;&#x2F;w&gt;&#39;: 5, &#39;w i d est &lt;&#x2F;w&gt;&#39;: 3, &#39;n e w est &lt;&#x2F;w&gt;&#39;: 6&#125;</span><br><span class="line">出现最频繁的序列 (&#39;est&#39;, &#39;&lt;&#x2F;w&gt;&#39;) 9</span><br><span class="line">合并最频繁的序列后的词表 &#123;&#39;w i d est&lt;&#x2F;w&gt;&#39;: 3, &#39;l o w e r &lt;&#x2F;w&gt;&#39;: 2, &#39;n e w est&lt;&#x2F;w&gt;&#39;: 6, &#39;l o w &lt;&#x2F;w&gt;&#39;: 5&#125;</span><br><span class="line">出现最频繁的序列 (&#39;l&#39;, &#39;o&#39;) 7</span><br><span class="line">合并最频繁的序列后的词表 &#123;&#39;w i d est&lt;&#x2F;w&gt;&#39;: 3, &#39;lo w e r &lt;&#x2F;w&gt;&#39;: 2, &#39;n e w est&lt;&#x2F;w&gt;&#39;: 6, &#39;lo w &lt;&#x2F;w&gt;&#39;: 5&#125;</span><br><span class="line">出现最频繁的序列 (&#39;lo&#39;, &#39;w&#39;) 7</span><br><span class="line">合并最频繁的序列后的词表 &#123;&#39;w i d est&lt;&#x2F;w&gt;&#39;: 3, &#39;low e r &lt;&#x2F;w&gt;&#39;: 2, &#39;n e w est&lt;&#x2F;w&gt;&#39;: 6, &#39;low &lt;&#x2F;w&gt;&#39;: 5&#125;</span><br><span class="line">出现最频繁的序列 (&#39;n&#39;, &#39;e&#39;) 6</span><br><span class="line">合并最频繁的序列后的词表 &#123;&#39;w i d est&lt;&#x2F;w&gt;&#39;: 3, &#39;low e r &lt;&#x2F;w&gt;&#39;: 2, &#39;ne w est&lt;&#x2F;w&gt;&#39;: 6, &#39;low &lt;&#x2F;w&gt;&#39;: 5&#125;</span><br><span class="line">出现最频繁的序列 (&#39;w&#39;, &#39;est&lt;&#x2F;w&gt;&#39;) 6</span><br><span class="line">合并最频繁的序列后的词表 &#123;&#39;w i d est&lt;&#x2F;w&gt;&#39;: 3, &#39;low e r &lt;&#x2F;w&gt;&#39;: 2, &#39;ne west&lt;&#x2F;w&gt;&#39;: 6, &#39;low &lt;&#x2F;w&gt;&#39;: 5&#125;</span><br><span class="line">出现最频繁的序列 (&#39;ne&#39;, &#39;west&lt;&#x2F;w&gt;&#39;) 6</span><br><span class="line">合并最频繁的序列后的词表 &#123;&#39;w i d est&lt;&#x2F;w&gt;&#39;: 3, &#39;low e r &lt;&#x2F;w&gt;&#39;: 2, &#39;newest&lt;&#x2F;w&gt;&#39;: 6, &#39;low &lt;&#x2F;w&gt;&#39;: 5&#125;</span><br><span class="line">出现最频繁的序列 (&#39;low&#39;, &#39;&lt;&#x2F;w&gt;&#39;) 5</span><br><span class="line">合并最频繁的序列后的词表 &#123;&#39;w i d est&lt;&#x2F;w&gt;&#39;: 3, &#39;low e r &lt;&#x2F;w&gt;&#39;: 2, &#39;newest&lt;&#x2F;w&gt;&#39;: 6, &#39;low&lt;&#x2F;w&gt;&#39;: 5&#125;</span><br><span class="line">出现最频繁的序列 (&#39;i&#39;, &#39;d&#39;) 3</span><br><span class="line">合并最频繁的序列后的词表 &#123;&#39;w id est&lt;&#x2F;w&gt;&#39;: 3, &#39;newest&lt;&#x2F;w&gt;&#39;: 6, &#39;low&lt;&#x2F;w&gt;&#39;: 5, &#39;low e r &lt;&#x2F;w&gt;&#39;: 2&#125;</span><br></pre></td></tr></table></figure>
<p>这样我们通过BPE得到了更加合适的词表了，这个词表可能会出现一些不是单词的组合，但是这个本身是有意义的一种形式，加速NLP的学习，提升不同词之间的语义的区分度。</p>
<h3 id="join-BPE"><a href="#join-BPE" class="headerlink" title="join BPE"></a>join BPE</h3><p>为目标语言和原语言一起使用BPE，即联合两种语言的词典去做BPE。提高了源语言和目标语言的分割一致性。训练中一般concat两种语言。</p>
<p>机器翻译时，编码与解码共享wordpiece model，可以处理翻译时目标语言和当前语言直接拷贝的词情况。</p>
<h2 id="阻止训练发散的方法"><a href="#阻止训练发散的方法" class="headerlink" title="阻止训练发散的方法"></a>阻止训练发散的方法</h2><p>降低学习速率、增加warmup_steps以及引入梯度截断。</p>
<h2 id="Dropout使用"><a href="#Dropout使用" class="headerlink" title="Dropout使用"></a>Dropout使用</h2><p>dropout设置在word embedding与position embedding相加之后，以及add+layernorm之前。</p>
<h2 id="Checkpoint-average"><a href="#Checkpoint-average" class="headerlink" title="Checkpoint average"></a>Checkpoint average</h2><p>指将训练一段时间的存储的checkpoint（文中选择为5个或20个），将这些checkpoint的所有模型的参数求平均。</p>
<h2 id="Xavier-初始化"><a href="#Xavier-初始化" class="headerlink" title="Xavier 初始化"></a>Xavier 初始化</h2><p>在官方 tensor2tensor 代码中，可以看到关于 weight 参数初始化的时候，采用了一种叫作xavier_uniform 的方法。也可以简称为 Xavier（读作 [ˈzeɪvjər]） 或者 Glorot，来自于作者 Xavier Glorot 在 2010 年和 Bengio 大神一起发表的《Understanding the difficulty of training deep feedforward neural networks》。</p>
<p>Xavier 是一种均匀初始化。其基线是朴素版本，即对于包含 n_i 个输入单元的网络层 layer_i ，其 weight 的初始值均匀采样自$\left[-1 / \sqrt{\left.\left.\left(n_{i}\right), 1 / \sqrt{(} n_{i}\right)\right]}\right.$。作者发现，在深层神经网络中，朴素版本的均匀初始化和激活函数（sigmoid 与 tanh）配合得不好。具体说来，就是不仅收敛得慢，训练平稳后的模型效果还差。如果使用预训练的模型来初始化网络，则能够明显改善训练过程。因此，作者认为，更换初始化方式是可取的。</p>
<p><img src="https://uploader.shimo.im/f/jhBiphXgghzG5p9R.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>首先来看看 sigmoid。它的线性区在 0.5 附近，偏向 1 或 0 表示则表示饱和。在理想情况下，激活值应当落在表达能力最强的非线性区间，远离线性区与饱和区。然而，在实际训练中，输出层 layer-4 在很长一段时间内都接近下饱和（黑）；其他层由上往下离线性区越来越近（蓝绿红）。虽然从 epoch-100 开始慢慢正常了，但这显然拖慢了收敛过程。</p>
<p><img src="https://uploader.shimo.im/f/kv4jPjIkXP58VeD1.png!thumbnail?fileGuid=vTCtKG6hgHqVrCJD" alt="图片"></p>
<p>然后是正负对称的 tanh。随着训练进行，自下而上（红绿蓝黑青），各层慢慢地都落入了饱和区。明明验证集的效果还达不到预期，模型却“训不动”了。</p>
<p>那么，我们需要什么样的激活值分布呢？一个标准的第 i 层可以分为线性$s^{i}=z^{i} W^{i}+b^{i}$和非线性$z^{i+1}=f\left(s^{i}\right)$两部分。在深层网络中，激活值的方差会自下而上逐层累积， 链式推导可得：$\operatorname{Var}\left[z^{i}\right]=\operatorname{Var}[x] \prod_{j=0}^{i-1} n_{j} \operatorname{Var}\left[W^{j}\right]$</p>
<p>对于反向传播，假定输出落在激活函数的线性区，即$f^{\prime}\left(s_{k}^{i}\right) \approx 1$。同样应用链式法则，各参数的梯度为（d 为总层数）：</p>
<ul>
<li>$\operatorname{Var}\left[\frac{\partial \text { cost }}{s^{i}}\right]=\operatorname{Var}\left[\frac{\partial \text { cost }}{s^{d}}\right] \prod_{j=i}^{d} n_{j+1} \operatorname{Var}\left[W^{j}\right]$</li>
<li>$\operatorname{Var}\left[\frac{\partial \operatorname{cost}}{w^{i}}\right]=\operatorname{Var}\left[\frac{\partial \operatorname{cost}}{s^{i}}\right] \operatorname{Var}\left[z^{i}\right]$</li>
</ul>
<p>现在要回到正题上了。一个稳定的深层网络，要求$\operatorname{Var}\left[z^{i}\right]$和$\operatorname{Var}\left[\partial \operatorname{cost} / s^{i}\right]$在各层保持一致，这样可以让激活值（forward）始终远离饱和区，并且梯度（backward）不会消失或爆炸。根据上面的公式，可以得到约束目标：$\forall i, n_{i} \operatorname{Var}\left[W^{i}\right]=1=n^{i+1} \operatorname{Var}\left[W^{i}\right]$</p>
<p>于方差是和输入单元的个数有关的，所以正向和反向分别使用了$n_i$和$n_{i+1}$。这两个等式显然不能同时成立，只好折中一下，让$\operatorname{Var}\left[W^{i}\right]=2 /\left(n_{i}+n_{i+1}\right)$。补充一条基础知识，对于均匀分布U[a,b] ，其方差为$(b-a)^{2} / 12$。现在开始解方程，让 a=b 并且$(b-a)^{2} / 12=2 /\left(n_{i}+n_{i+1}\right)$，得到 Xavier 均匀分布的最终形式：$U\left[-\sqrt{\frac{6}{n_{i}+n_{i+1}}}, \quad \sqrt{\frac{6}{n_{i}+n_{i+1}}}\right]$</p>
<h2 id="Transformer有哪些缺点"><a href="#Transformer有哪些缺点" class="headerlink" title="Transformer有哪些缺点"></a>Transformer有哪些缺点</h2><ul>
<li>评价不同模型需要考虑的是：<ul>
<li>句法特征提取能力</li>
<li>语义特征提取能力：Transformer &gt;&gt; 原生CNN == 原生RNN</li>
<li>长距离特征捕获能力：Transformer &gt; 原生RNN &gt;&gt; 原生CNN</li>
<li>任务综合特征抽取能力：Transformer最强</li>
<li>并行计算能力及运行效率：Transformer = 原生CNN &gt;&gt; 原生RNN</li>
</ul>
</li>
<li>超长文本：Transformer-XL</li>
<li>计算简化：ALBert</li>
</ul>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>用GitHub+Hexo搭建个人网站</title>
    <url>/2019/02/07/%E7%94%A8GitHub-Hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/</url>
    <content><![CDATA[<p>假期宅在家里，研究了一下用github搭建个人网站，把里面使用到的工具和命令总结一下。相关代码可参考：<a href="https://github.com/majing2019/myblogs" target="_blank" rel="noopener">https://github.com/majing2019/myblogs</a><br><a id="more"></a></p>
<h1 id="安装相关软件"><a href="#安装相关软件" class="headerlink" title="安装相关软件"></a>安装相关软件</h1><blockquote>
<p>参考：<br><a href="https://zhuanlan.zhihu.com/p/26625249" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26625249</a><br><a href="https://zhuanlan.zhihu.com/p/62555815" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/62555815</a></p>
</blockquote>
<ul>
<li>npm install -g hexo-cli</li>
<li>hexo init blog</li>
<li>cd ~/blog</li>
<li>export CC=/usr/bin/clang</li>
<li>export CXX=/usr/bin/clang++</li>
<li>npm install</li>
<li>npm install hexo-server —save</li>
<li>hexo server</li>
<li>在<a href="http://localhost:4000" target="_blank" rel="noopener">http://localhost:4000</a>访问网站首页</li>
<li>npm install hexo-deployer-git —save</li>
</ul>
<h1 id="建立repository"><a href="#建立repository" class="headerlink" title="建立repository"></a>建立repository</h1><blockquote>
<p>参考：<br><a href="https://help.github.com/en/github/working-with-github-pages" target="_blank" rel="noopener">https://help.github.com/en/github/working-with-github-pages</a><br><a href="https://github.community/t5/GitHub-Pages/404-Error/td-p/14331" target="_blank" rel="noopener">https://github.community/t5/GitHub-Pages/404-Error/td-p/14331</a></p>
</blockquote>
<ul>
<li>创建username.github.io的repository</li>
<li>在Settings-&gt;Github Pages中升级账户</li>
</ul>
<h1 id="修改相关配置"><a href="#修改相关配置" class="headerlink" title="修改相关配置"></a>修改相关配置</h1><ul>
<li>修改_config.yml<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: &lt;repository url&gt; #git@github.com:sufaith&#x2F;sufaith.github.io.git</span><br><span class="line">  branch: [branch] #master</span><br><span class="line">  message: [message]</span><br><span class="line">url: majing2019.github.io</span><br></pre></td></tr></table></figure></li>
<li>在source文件夹下创建CNAME文件，内容为二级域名</li>
<li>在~/blog目录下运行hexo generate</li>
<li>hexo clean &amp;&amp; hexo deploy</li>
<li>访问 <a href="https://majing2019.github.io/archives/" target="_blank" rel="noopener">https://majing2019.github.io/archives/</a></li>
</ul>
<h1 id="绑定域名"><a href="#绑定域名" class="headerlink" title="绑定域名"></a>绑定域名</h1><ul>
<li>登录<a href="https://www.aliyun.com/" target="_blank" rel="noopener">https://www.aliyun.com/</a>注册了一个域名majsunflower.cn</li>
<li>添加一个域名解析<ul>
<li>类型CNAME，主机记录www，记录值majing2019.github.io</li>
<li>类型A，主机记录@，记录值是对应的ip地址，可通过ping majing2019.github.io获得</li>
</ul>
</li>
<li>在github仓库中设置custom domain</li>
<li>在blog下创建source/CNAME文件，并写入majsunflower.cn</li>
</ul>
<h1 id="编写自己的个性化网站"><a href="#编写自己的个性化网站" class="headerlink" title="编写自己的个性化网站"></a>编写自己的个性化网站</h1><blockquote>
<p>参考：<br><a href="https://zhwangart.github.io/2018/11/30/Ocean/" target="_blank" rel="noopener">https://zhwangart.github.io/2018/11/30/Ocean/</a></p>
</blockquote>
<h2 id="更换主题"><a href="#更换主题" class="headerlink" title="更换主题"></a>更换主题</h2><ul>
<li>在<a href="https://hexo.io/themes/" target="_blank" rel="noopener">https://hexo.io/themes/</a>中选择一个主题</li>
<li>git clone <a href="https://github.com/zhwangart/hexo-theme-ocean.git" target="_blank" rel="noopener">https://github.com/zhwangart/hexo-theme-ocean.git</a> themes/ocean</li>
<li>修改_config.yml中theme为ocean</li>
</ul>
<h2 id="配置语言"><a href="#配置语言" class="headerlink" title="配置语言"></a>配置语言</h2><ul>
<li>_config.yml中language改为zh-CN</li>
</ul>
<h2 id="评论功能"><a href="#评论功能" class="headerlink" title="评论功能"></a>评论功能</h2><blockquote>
<p>参考：<br><a href="https://zhwangart.github.io/2018/12/06/Gitalk/" target="_blank" rel="noopener">https://zhwangart.github.io/2018/12/06/Gitalk/</a></p>
</blockquote>
<ul>
<li>在<a href="https://github.com/settings/applications/new" target="_blank" rel="noopener">https://github.com/settings/applications/new</a>申请<ul>
<li>后续可在<a href="https://github.com/settings/developers" target="_blank" rel="noopener">https://github.com/settings/developers</a>中修改app相关内容</li>
<li>注意Authorization callback URL在网站绑定域名后需要写域名</li>
</ul>
</li>
<li>填写themes/ocean/_config.yml中gitalk相关字段</li>
</ul>
<h2 id="使用图床"><a href="#使用图床" class="headerlink" title="使用图床"></a>使用图床</h2><blockquote>
<p>参考：<br><a href="https://zhuanlan.zhihu.com/p/26625249" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26625249</a><br><a href="https://blog.csdn.net/qq_36305327/article/details/71578290" target="_blank" rel="noopener">https://blog.csdn.net/qq_36305327/article/details/71578290</a></p>
</blockquote>
<ul>
<li><p>到<a href="https://www.qiniu.com/" target="_blank" rel="noopener">https://www.qiniu.com/</a>上添加对象存储<a href="https://portal.qiniu.com/kodo/bucket/" target="_blank" rel="noopener">https://portal.qiniu.com/kodo/bucket/</a></p>
</li>
<li><p>在markdown中可直接引用图片</p>
</li>
</ul>
<h2 id="添加关于"><a href="#添加关于" class="headerlink" title="添加关于"></a>添加关于</h2><ul>
<li>hexo new page about</li>
<li>使用markdown编写source/about/index.md</li>
</ul>
<h2 id="添加标签"><a href="#添加标签" class="headerlink" title="添加标签"></a>添加标签</h2><ul>
<li>hexo new page tags // 创建标签页面</li>
<li>修改source/tags/index.md为<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: Tags</span><br><span class="line">date: 2019-04-19 17:28:54</span><br><span class="line">type: tags</span><br><span class="line">layout: &quot;tags&quot;</span><br><span class="line">---</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="添加相册"><a href="#添加相册" class="headerlink" title="添加相册"></a>添加相册</h2><ul>
<li>hexo new page gallery</li>
<li>编辑source/gallery/index.md<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: Gallery</span><br><span class="line">albums: [</span><br><span class="line">        [&quot;img_url&quot;,&quot;img_caption&quot;],</span><br><span class="line">        [&quot;img_url&quot;,&quot;img_caption&quot;]</span><br><span class="line">        ]</span><br><span class="line">---</span><br></pre></td></tr></table></figure></li>
<li>如果出现相册加载过慢的问题，可以参考<a href="https://zhwangart.github.io/2019/07/02/Ocean-Issues/" target="_blank" rel="noopener">https://zhwangart.github.io/2019/07/02/Ocean-Issues/</a>解决</li>
</ul>
<h2 id="添加分类"><a href="#添加分类" class="headerlink" title="添加分类"></a>添加分类</h2><ul>
<li>hexo new page categories</li>
</ul>
<h2 id="本地搜索"><a href="#本地搜索" class="headerlink" title="本地搜索"></a>本地搜索</h2><blockquote>
<p>参考：<br><a href="https://github.com/zhwangart/gitalk/issues/7#issuecomment-451877736" target="_blank" rel="noopener">https://github.com/zhwangart/gitalk/issues/7#issuecomment-451877736</a></p>
</blockquote>
<ul>
<li>npm install hexo-generator-searchdb —save</li>
<li>在blog/_config.yml中添加配置<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">search:</span><br><span class="line">  path: search.xml</span><br><span class="line">  field: post</span><br><span class="line">  content: true</span><br></pre></td></tr></table></figure></li>
<li>hexo g</li>
<li><del>修改themes/ocean/layout/_partial/after-footer.ejs中修改如下内容</del><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;% if (theme.local_search.enable)&#123; %&gt;</span><br><span class="line">  &lt;%- js(&#39;&#x2F;js&#x2F;search&#39;) %&gt;</span><br><span class="line">&lt;% &#125; %&gt;</span><br><span class="line"></span><br><span class="line">&lt;%- js(&#39;&#x2F;js&#x2F;ocean&#39;) %&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="用Markdown写文章"><a href="#用Markdown写文章" class="headerlink" title="用Markdown写文章"></a>用Markdown写文章</h2><h3 id="创建文章"><a href="#创建文章" class="headerlink" title="创建文章"></a>创建文章</h3><blockquote>
<p>参考：<br><a href="https://chaxiaoniu.oschina.io/2017/07/10/Markdown-Grammar/" target="_blank" rel="noopener">https://chaxiaoniu.oschina.io/2017/07/10/Markdown-Grammar/</a></p>
</blockquote>
<ul>
<li>hexo new “用GitHub+Hexo搭建个人网站”</li>
<li>文章格式如下<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: 用GitHub+Hexo搭建个人网站 #文章页面上的显示名称，可以任意修改</span><br><span class="line">date: date  #文章生成时间，一般不改，当然也可以任意修改</span><br><span class="line">tags: [Hexo, Ocean] #文章标签，可空。也可以按照你的习惯写分类名字，注意后面有空格，多个标签可以用[]包含，以&#96;,&#96;隔开</span><br><span class="line">categories: [技术] #分类</span><br><span class="line">---</span><br><span class="line">这里是你博客列表显示的摘要文字</span><br><span class="line">&lt;!--more--&gt;</span><br><span class="line">以下是博客的正文，以上面的格式为分隔线</span><br></pre></td></tr></table></figure></li>
<li>如果不希望显示时有目录，需要添加<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">toc: false</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="添加公式"><a href="#添加公式" class="headerlink" title="添加公式"></a>添加公式</h3><blockquote>
<p>参考：<br><a href="https://blog.csdn.net/Aoman_Hao/article/details/81381507" target="_blank" rel="noopener">https://blog.csdn.net/Aoman_Hao/article/details/81381507</a></p>
</blockquote>
<ul>
<li>npm uninstall hexo-renderer-marked —save</li>
<li>npm install hexo-renderer-kramed —save</li>
<li>修改node_modules/hexo-renderer-kramed/lib/renderer.js<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">function formatText(text) &#123;</span><br><span class="line">  &#x2F;&#x2F; Fit kramed&#39;s rule: $$ + \1 + $$</span><br><span class="line">  &#x2F;&#x2F; return text.replace(&#x2F;&#96;\$(.*?)\$&#96;&#x2F;g, &#39;$$$$$1$$$$&#39;);</span><br><span class="line">  return text;&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>npm uninstall hexo-math —save</p>
</li>
<li><p>npm install hexo-renderer-mathjax —save</p>
</li>
<li>修改node_modules/hexo-renderer-mathjax/mathjax.html，注释掉最后一行script并改为<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;script src&#x3D;&quot;https:&#x2F;&#x2F;cdnjs.cloudflare.com&#x2F;ajax&#x2F;libs&#x2F;mathjax&#x2F;2.7.1&#x2F;MathJax.js?config&#x3D;TeX-MML-AM_CHTML&quot;&gt;&lt;&#x2F;script&gt;</span><br></pre></td></tr></table></figure></li>
<li>修改node_modules/kramed/lib/rules/inline.js<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">escape: &#x2F;^\\([&#96;*\[\]()# +\-.!_&gt;])&#x2F;,</span><br><span class="line">em: &#x2F;^\*((?:\*\*|[\s\S])+?)\*(?!\*)&#x2F;,</span><br></pre></td></tr></table></figure></li>
<li>修改themes/ocean/_config.yml增加<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mathjax: true</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="添加文章封面"><a href="#添加文章封面" class="headerlink" title="添加文章封面"></a>添加文章封面</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: Post name</span><br><span class="line">photos: [</span><br><span class="line">        [&quot;img_url&quot;],</span><br><span class="line">        [&quot;img_url&quot;]</span><br><span class="line">        ]</span><br><span class="line">---</span><br></pre></td></tr></table></figure>
<h3 id="添加视频"><a href="#添加视频" class="headerlink" title="添加视频"></a>添加视频</h3><blockquote>
<p>参考：<br><a href="https://blog.csdn.net/u010953692/article/details/79075884" target="_blank" rel="noopener">https://blog.csdn.net/u010953692/article/details/79075884</a><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;iframe height&#x3D;498 width&#x3D;510 src&#x3D;&quot;http:&#x2F;&#x2F;q503tsu73.bkt.clouddn.com&#x2F;IMG_0018.mp4?e&#x3D;1580557032&amp;token&#x3D;05Ii263bPN3Z-CT3JPRaRfWi5sXIj8pwX6V1bN2j:-rUb7zOxk-WfRrhdJtNdOOGfy58&#x3D;&amp;attname&#x3D;&quot; frameborder&#x3D;0 allowfullscreen&gt;&lt;&#x2F;iframe&gt;</span><br></pre></td></tr></table></figure></p>
</blockquote>
<h3 id="文章置顶"><a href="#文章置顶" class="headerlink" title="文章置顶"></a>文章置顶</h3><ul>
<li>npm uninstall hexo-generator-index —save</li>
<li>npm install hexo-generator-index-pin-top —save</li>
<li>在需要置顶的文章上加入<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line"> title: 新增文章置顶</span><br><span class="line"> top: ture</span><br><span class="line"> ---</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="同时部署在Github和Coding上"><a href="#同时部署在Github和Coding上" class="headerlink" title="同时部署在Github和Coding上"></a>同时部署在Github和Coding上</h1><blockquote>
<p>参考：<br><a href="https://tomatoro.cn/archives/3de92cb5.html" target="_blank" rel="noopener">https://tomatoro.cn/archives/3de92cb5.html</a></p>
</blockquote>
<ul>
<li><a href="https://coding.net/" target="_blank" rel="noopener">https://coding.net/</a>上创建devops项目</li>
<li>修改blog/_config.yml中的deploy<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: </span><br><span class="line">    github: git@github.com:majing2019&#x2F;majing2019.github.io.git</span><br><span class="line">    coding: git@e.coding.net:majsunflower&#x2F;myblog.git</span><br><span class="line">  branch: master</span><br><span class="line">  message: my blog</span><br></pre></td></tr></table></figure></li>
<li>将id_rsa.pub的公钥复制到个人账户下，ssh -T git@git.coding.net验证是否成功</li>
<li>hexo deploy -g部署到coding上</li>
<li>配置静态页面即可访问：<a href="http://02ss3u.coding-pages.com/" target="_blank" rel="noopener">https://02ss3u.coding-pages.com/</a></li>
<li>在自定义域名里增加：<a href="http://majsunflower.cn/">majsunflower.cn</a></li>
<li>阿里云<a href="https://homenew.console.aliyun.com/" target="_blank" rel="noopener">https://homenew.console.aliyun.com/</a>中修改域名相关配置，区分境内和境外的访问</li>
</ul>
<h1 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h1><ul>
<li><p>部署：hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</p>
</li>
<li><p>本地测试：hexo server</p>
</li>
</ul>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Ocean</tag>
      </tags>
  </entry>
</search>

<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Spark入门</title>
    <url>/2020/02/26/Spark%E5%85%A5%E9%97%A8/</url>
    <content><![CDATA[<p>最近需要用spark比较多，重新学习一下。今天先学习一些基础。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://classroom.udacity.com/courses/ud2002" target="_blank" rel="noopener">https://classroom.udacity.com/courses/ud2002</a></p>
</blockquote>
<h1 id="Spark处理数据"><a href="#Spark处理数据" class="headerlink" title="Spark处理数据"></a>Spark处理数据</h1><h2 id="函数式编程"><a href="#函数式编程" class="headerlink" title="函数式编程"></a>函数式编程</h2><p>首先用下图来看一下，函数式编程和过程式编程的区别。<img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/maj1.png" alt="图片"></p>
<p>函数式编程非常适合分布式系统。Python并不是函数编程语言，但使用PySparkAPI 可以让你编写Spark程序，并确保你的代码使用了函数式编程。在底层，Python 代码使用 py4j 来调用 Java 虚拟机(JVM)。</p>
<p>假设有下面一段代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">log_of_songs &#x3D; [</span><br><span class="line">        &quot;Despacito&quot;,</span><br><span class="line">        &quot;Nice for what&quot;,</span><br><span class="line">        &quot;No tears left to cry&quot;,</span><br><span class="line">        &quot;Despacito&quot;,</span><br><span class="line">        &quot;Havana&quot;,</span><br><span class="line">        &quot;In my feelings&quot;,</span><br><span class="line">        &quot;Nice for what&quot;,</span><br><span class="line">        &quot;Despacito&quot;,</span><br><span class="line">        &quot;All the stars&quot;</span><br><span class="line">]</span><br><span class="line">play_count &#x3D; 0</span><br><span class="line">def count_plays(song_title):</span><br><span class="line">    global play_count</span><br><span class="line">    for song in log_of_songs:</span><br><span class="line">        if song &#x3D;&#x3D; song_title:</span><br><span class="line">            play_count &#x3D; play_count + 1</span><br><span class="line">    return play_count</span><br></pre></td></tr></table></figure>
<p>调用两次count_plays(“Despacito”)会得到不同的结果，这是因为play_count是作为全局变量，在函数内部进行了修改。解决这个问题可以采用如下代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def count_plays(song_title, play_count):</span><br><span class="line">    for song in log_of_songs:</span><br><span class="line">        if song &#x3D;&#x3D; song_title:</span><br><span class="line">            play_count &#x3D; play_count + 1</span><br><span class="line">    return play_count</span><br></pre></td></tr></table></figure>
<p>这就是Spark解决问题的方式。<br>在Spark中我们使用Pure Function（纯函数），就像面包制造厂，不同的面包机器之间是互不干扰的，且不会损坏原材料。Spark会在函数执行前，将数据复制多分，以输入到不同函数中。为了防止内存溢出，Spark会在代码中建立一个数据的有向无环图，在运行前检查是否有必要对某一分数据进行复制。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/maj2.png" alt="图片"></p>
<h2 id="运行时参数设置"><a href="#运行时参数设置" class="headerlink" title="运行时参数设置"></a>运行时参数设置</h2><blockquote>
<p>参考：<br><a href="https://tech.meituan.com/2016/04/29/spark-tuning-basic.html" target="_blank" rel="noopener">https://tech.meituan.com/2016/04/29/spark-tuning-basic.html</a><br><a href="https://spark.apache.org/docs/1.6.1/running-on-yarn.html" target="_blank" rel="noopener">https://spark.apache.org/docs/1.6.1/running-on-yarn.html</a></p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line">    --master yarn \</span><br><span class="line">    --deploy-mode cluster  \</span><br><span class="line">    --num-executors 100 \</span><br><span class="line">    --driver-memory 2g \</span><br><span class="line">    --executor-memory 14g \</span><br><span class="line">    --executor-cores 6 \</span><br><span class="line">    --conf spark.default.parallelism&#x3D;1000 \</span><br><span class="line">    --conf spark.storage.memoryFraction&#x3D;0.2 \</span><br><span class="line">    --conf spark.shuffle.memoryFraction&#x3D;0.6 \</span><br><span class="line">    --conf spark.executor.extraJavaOptions&#x3D;&#39;-Dlog4j.configuration&#x3D;log4j.properties&#39; \</span><br><span class="line">    --driver-java-options -Dlog4j.configuration&#x3D;log4j.properties \</span><br><span class="line">    python文件  \</span><br></pre></td></tr></table></figure>
<ul>
<li>spark-submit: which spark-submit 查看该命令是 spark 系统的还是 pyspark 包自带的，应该使用 spark 系统的<ul>
<li>master:</li>
<li>standaloone: spark 自带的集群资源管理器</li>
<li>yarn</li>
<li>local: 本地运行</li>
</ul>
</li>
<li>deploy-mode：<ul>
<li>client: driver 在本机上，能够直接使用本机文件系统</li>
<li>cluster: driver 指不定在哪台机器上，不能读取本机文件系统</li>
</ul>
</li>
<li>spark 运行时配置：主要的有运行内存和节点数量:<ul>
<li>num_executors</li>
<li>spark_driver_memory</li>
<li>spark_executor_memory</li>
</ul>
</li>
<li>addFiles 与 —files（将需要使用的文件分发到每台机器上）：<ul>
<li>addFiles()：能够分发到每台机器上,包括 driver 上</li>
<li>—files: 只能分发到 executor 上</li>
</ul>
</li>
<li>引用其他模块的问题：<ul>
<li>第三方库：需要将第三方库打包上传供使用</li>
<li>自己的模块：也需要打包上传,以供使用</li>
</ul>
</li>
<li>运行下面前请确认<ul>
<li>export SPARK_HOME=…../spark-1.6.2-bin-hadoop2.6</li>
<li>export PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH</li>
<li>export JAVA_HOME=…/jdk1.8.0_60</li>
<li>PYSPARK_PYTHON=./NLTK/conda-env/bin/python spark-submit —conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./NLTK/conda-env/bin/python —master yarn-cluster —archives conda-env.zip#NLTK clean_step_two.py</li>
</ul>
</li>
</ul>
<h2 id="Maps和Lambda"><a href="#Maps和Lambda" class="headerlink" title="Maps和Lambda"></a>Maps和Lambda</h2><blockquote>
<p>lambda函数起源：<br><a href="http://palmstroem.blogspot.com/2012/05/lambda-calculus-for-absolute-dummies.html" target="_blank" rel="noopener">http://palmstroem.blogspot.com/2012/05/lambda-calculus-for-absolute-dummies.html</a></p>
</blockquote>
<p>Maps会复制原始数据，并把副本数据按照Maps中的函数进行转换。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from pyspark import SparkContext</span><br><span class="line">log_of_songs &#x3D; [</span><br><span class="line">    &quot;Despacito&quot;,</span><br><span class="line">    &quot;Nice for what&quot;,</span><br><span class="line">    &quot;No tears left to cry&quot;,</span><br><span class="line">    &quot;Despacito&quot;,</span><br><span class="line">    &quot;Havana&quot;,</span><br><span class="line">    &quot;In my feelings&quot;,</span><br><span class="line">    &quot;Nice for what&quot;,</span><br><span class="line">    &quot;Despacito&quot;,</span><br><span class="line">    &quot;All the stars&quot;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">def convert_song_to_lowercase(song):</span><br><span class="line">    return song.lower()</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">    conf &#x3D; SparkConf()</span><br><span class="line">    conf.setAppName(&quot;Testing&quot;).setMaster(&quot;local&quot;)</span><br><span class="line">    sc &#x3D; SparkContext(conf&#x3D;conf)</span><br><span class="line">    sc.setLogLevel(&quot;WARN&quot;)</span><br><span class="line"></span><br><span class="line">    # parallelize将对象分配到不同节点上</span><br><span class="line">    distributed_song_log &#x3D; sc.parallelize(log_of_songs)</span><br><span class="line">    # 定义不同节点的所有数据执行convert_song_to_lowercase的操作</span><br><span class="line">    # 但此时spark还未执行，它在等待所有定义结束后，看是否可以优化某些操作</span><br><span class="line">    distributed_song_log.map(convert_song_to_lowercase)</span><br><span class="line">    # 如果想强制spark执行，则可以使用collect，则会将所有数据汇总</span><br><span class="line">    # 注意此时spark并没有改变原始数据的大小写，它将原始数据进行了拷贝，再做的处理</span><br><span class="line">    distributed_song_log.collect()</span><br><span class="line">    # 也可以使用python的匿名函数进行map</span><br><span class="line">    distributed_song_log.map(lambda song: song.lower()).collect()</span><br></pre></td></tr></table></figure>
<h2 id="Data-Frame"><a href="#Data-Frame" class="headerlink" title="Data Frame"></a>Data Frame</h2><p>数据处理有两种方式，一种使用Data Frame和Python进行命令式编程，另一种使用SQL进行声明式编程。命令式编程关注的是”How”，声明式编程关注的是”What”。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/maj3.png" alt="图片"></p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/maj4.png" alt="图片"></p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/maj5.png" alt="图片"></p>
<h3 id="Data-Frame的读取和写入"><a href="#Data-Frame的读取和写入" class="headerlink" title="Data Frame的读取和写入"></a>Data Frame的读取和写入</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import pyspark</span><br><span class="line">from pyspark import SparkConf</span><br><span class="line">from pyspark.sql import SparkSession</span><br><span class="line"></span><br><span class="line">spark &#x3D; SparkSession \</span><br><span class="line">    .builder \</span><br><span class="line">    .appName(&quot;Our first Python Spark SQL example&quot;) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"># 检查一下是否生效了。</span><br><span class="line">spark.sparkContext.getConf().getAll()</span><br><span class="line">path &#x3D; &quot;data&#x2F;sparkify_log_small.json&quot;</span><br><span class="line">user_log &#x3D; spark.read.json(path)</span><br><span class="line">user_log.printSchema()</span><br><span class="line">user_log.describe()</span><br><span class="line">user_log.show(n&#x3D;1)</span><br><span class="line"># 取数据的前5条</span><br><span class="line">user_log.take(5)</span><br><span class="line">out_path &#x3D; &quot;data&#x2F;sparkify_log_small.csv&quot;</span><br><span class="line">user_log.write.save(out_path, format&#x3D;&quot;csv&quot;, header&#x3D;True)</span><br><span class="line"># 读取另一个daraframe</span><br><span class="line">user_log_2 &#x3D; spark.read.csv(out_path, header&#x3D;True)</span><br><span class="line">user_log_2.printSchema()</span><br><span class="line">user_log_2.take(2)</span><br><span class="line">user_log_2.select(&quot;userID&quot;).show()</span><br></pre></td></tr></table></figure>
<h3 id="Data-Frame数据处理"><a href="#Data-Frame数据处理" class="headerlink" title="Data Frame数据处理"></a>Data Frame数据处理</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from pyspark.sql import SparkSession</span><br><span class="line">from pyspark.sql.functions import udf</span><br><span class="line">from pyspark.sql.types import StringType</span><br><span class="line">from pyspark.sql.types import IntegerType</span><br><span class="line">from pyspark.sql.functions import desc</span><br><span class="line">from pyspark.sql.functions import asc</span><br><span class="line">from pyspark.sql.functions import sum as Fsum</span><br><span class="line"></span><br><span class="line">import datetime</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">%matplotlib inline</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">spark &#x3D; SparkSession \</span><br><span class="line">    .builder \</span><br><span class="line">    .appName(&quot;Wrangling Data&quot;) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line">path &#x3D; &quot;data&#x2F;sparkify_log_small.json&quot;</span><br><span class="line">user_log &#x3D; spark.read.json(path)</span><br><span class="line"># 数据搜索</span><br><span class="line">user_log.take(5)</span><br><span class="line">user_log.printSchema()</span><br><span class="line">user_log.describe().show()</span><br><span class="line">user_log.describe(&quot;artist&quot;).show()</span><br><span class="line">user_log.describe(&quot;sessionId&quot;).show()</span><br><span class="line">user_log.count()</span><br><span class="line">user_log.select(&quot;page&quot;).dropDuplicates().sort(&quot;page&quot;).show()</span><br><span class="line">user_log.select([&quot;userId&quot;, &quot;firstname&quot;, &quot;page&quot;, &quot;song&quot;]).where(user_log.userId &#x3D;&#x3D; &quot;1046&quot;).collect()</span><br><span class="line"># 按小时统计数据</span><br><span class="line">get_hour &#x3D; udf(lambda x: datetime.datetime.fromtimestamp(x &#x2F; 1000.0). hour)</span><br><span class="line">user_log &#x3D; user_log.withColumn(&quot;hour&quot;, get_hour(user_log.ts))</span><br><span class="line">user_log.head()</span><br><span class="line">songs_in_hour &#x3D; user_log.filter(user_log.page &#x3D;&#x3D; &quot;NextSong&quot;).groupby(user_log.hour).count().orderBy(user_log.hour.cast(&quot;float&quot;))</span><br><span class="line">songs_in_hour.show()</span><br><span class="line">songs_in_hour_pd &#x3D; songs_in_hour.toPandas()</span><br><span class="line">songs_in_hour_pd.hour &#x3D; pd.to_numeric(songs_in_hour_pd.hour)</span><br><span class="line">plt.scatter(songs_in_hour_pd[&quot;hour&quot;], songs_in_hour_pd[&quot;count&quot;])</span><br><span class="line">plt.xlim(-1, 24);</span><br><span class="line">plt.ylim(0, 1.2 * max(songs_in_hour_pd[&quot;count&quot;]))</span><br><span class="line">plt.xlabel(&quot;Hour&quot;)</span><br><span class="line">plt.ylabel(&quot;Songs played&quot;);</span><br><span class="line"></span><br><span class="line"># 删除空值的行</span><br><span class="line">user_log_valid &#x3D; user_log.dropna(how &#x3D; &quot;any&quot;, subset &#x3D; [&quot;userId&quot;, &quot;sessionId&quot;])</span><br><span class="line">user_log_valid.count()</span><br><span class="line">user_log.select(&quot;userId&quot;).dropDuplicates().sort(&quot;userId&quot;).show()</span><br><span class="line">user_log_valid &#x3D; user_log_valid.filter(user_log_valid[&quot;userId&quot;] !&#x3D; &quot;&quot;)</span><br><span class="line">user_log_valid.count()</span><br><span class="line"># 降级服务的用户</span><br><span class="line">user_log_valid.filter(&quot;page &#x3D; &#39;Submit Downgrade&#39;&quot;).show()</span><br><span class="line">user_log.select([&quot;userId&quot;, &quot;firstname&quot;, &quot;page&quot;, &quot;level&quot;, &quot;song&quot;]).where(user_log.userId &#x3D;&#x3D; &quot;1138&quot;).collect()</span><br><span class="line">flag_downgrade_event &#x3D; udf(lambda x: 1 if x &#x3D;&#x3D; &quot;Submit Downgrade&quot; else 0, IntegerType())</span><br><span class="line">user_log_valid &#x3D; user_log_valid.withColumn(&quot;downgraded&quot;, flag_downgrade_event(&quot;page&quot;))</span><br><span class="line">user_log_valid.head()</span><br><span class="line">from pyspark.sql import Window</span><br><span class="line">windowval &#x3D; Window.partitionBy(&quot;userId&quot;).orderBy(desc(&quot;ts&quot;)).rangeBetween(Window.unboundedPreceding, 0)</span><br><span class="line">user_log_valid &#x3D; user_log_valid.withColumn(&quot;phase&quot;, Fsum(&quot;downgraded&quot;).over(windowval))</span><br><span class="line">user_log_valid.select([&quot;userId&quot;, &quot;firstname&quot;, &quot;ts&quot;, &quot;page&quot;, &quot;level&quot;, &quot;phase&quot;]).where(user_log.userId &#x3D;&#x3D; &quot;1138&quot;).sort(&quot;ts&quot;).collect()</span><br></pre></td></tr></table></figure>
<h2 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from pyspark.sql import SparkSession</span><br><span class="line">from pyspark.sql.functions import udf</span><br><span class="line">from pyspark.sql.types import StringType</span><br><span class="line">from pyspark.sql.types import IntegerType</span><br><span class="line">from pyspark.sql.functions import desc</span><br><span class="line">from pyspark.sql.functions import asc</span><br><span class="line">from pyspark.sql.functions import sum as Fsum</span><br><span class="line"></span><br><span class="line">import datetime</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">%matplotlib inline</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">spark &#x3D; SparkSession \</span><br><span class="line">    .builder \</span><br><span class="line">    .appName(&quot;Data wrangling with Spark SQL&quot;) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line">path &#x3D; &quot;data&#x2F;sparkify_log_small.json&quot;</span><br><span class="line">user_log &#x3D; spark.read.json(path)</span><br><span class="line"></span><br><span class="line">user_log.take(1)</span><br><span class="line"># 下面的代码创建了一个临时视图，你可以使用该视图运行 SQL 查询</span><br><span class="line">user_log.createOrReplaceTempView(&quot;user_log_table&quot;)</span><br><span class="line">spark.sql(&quot;SELECT * FROM user_log_table LIMIT 2&quot;).show()</span><br><span class="line">spark.sql(&#39;&#39;&#39;</span><br><span class="line">          SELECT * </span><br><span class="line">          FROM user_log_table </span><br><span class="line">          LIMIT 2</span><br><span class="line">          &#39;&#39;&#39;</span><br><span class="line">          ).show()</span><br><span class="line">spark.sql(&#39;&#39;&#39;</span><br><span class="line">          SELECT COUNT(*) </span><br><span class="line">          FROM user_log_table </span><br><span class="line">          &#39;&#39;&#39;</span><br><span class="line">          ).show()</span><br><span class="line">spark.sql(&#39;&#39;&#39;</span><br><span class="line">          SELECT userID, firstname, page, song</span><br><span class="line">          FROM user_log_table </span><br><span class="line">          WHERE userID &#x3D;&#x3D; &#39;1046&#39;</span><br><span class="line">          &#39;&#39;&#39;</span><br><span class="line">          ).collect()</span><br><span class="line">spark.sql(&#39;&#39;&#39;</span><br><span class="line">          SELECT DISTINCT page</span><br><span class="line">          FROM user_log_table </span><br><span class="line">          ORDER BY page ASC</span><br><span class="line">          &#39;&#39;&#39;</span><br><span class="line">          ).show()</span><br><span class="line"></span><br><span class="line"># 自定义函数</span><br><span class="line">spark.udf.register(&quot;get_hour&quot;, lambda x: int(datetime.datetime.fromtimestamp(x &#x2F; 1000.0).hour))</span><br><span class="line">spark.sql(&#39;&#39;&#39;</span><br><span class="line">          SELECT *, get_hour(ts) AS hour</span><br><span class="line">          FROM user_log_table </span><br><span class="line">          LIMIT 1</span><br><span class="line">          &#39;&#39;&#39;</span><br><span class="line">          ).collect()</span><br><span class="line">songs_in_hour &#x3D; spark.sql(&#39;&#39;&#39;</span><br><span class="line">          SELECT get_hour(ts) AS hour, COUNT(*) as plays_per_hour</span><br><span class="line">          FROM user_log_table</span><br><span class="line">          WHERE page &#x3D; &quot;NextSong&quot;</span><br><span class="line">          GROUP BY hour</span><br><span class="line">          ORDER BY cast(hour as int) ASC</span><br><span class="line">          &#39;&#39;&#39;</span><br><span class="line">          )</span><br><span class="line">songs_in_hour.show()</span><br><span class="line"># 用 Pandas 转换数据</span><br><span class="line">songs_in_hour_pd &#x3D; songs_in_hour.toPandas()</span><br><span class="line">print(songs_in_hour_pd)</span><br></pre></td></tr></table></figure>
<h2 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h2><blockquote>
<p>参考：<br><a href="https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html" target="_blank" rel="noopener">https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html</a><br><a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/rdd-programming-guide.html</a></p>
</blockquote>
<p>不管使用Pyspark还是其他语言，Spark的底层都会通过Catalyst转成执行DAG序列：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/spark1.png" alt="图片"></p>
<p>DAG在底层使用RDD对象进行操作。</p>
<h1 id="Spark中的机器学习"><a href="#Spark中的机器学习" class="headerlink" title="Spark中的机器学习"></a>Spark中的机器学习</h1><h2 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from pyspark.sql import SparkSession</span><br><span class="line">from pyspark.ml.feature import RegexTokenizer, VectorAssembler, Normalizer, StandardScaler</span><br><span class="line">from pyspark.sql.functions import udf</span><br><span class="line">from pyspark.sql.types import IntegerType</span><br><span class="line"></span><br><span class="line">import re</span><br><span class="line"># create a SparkSession: note this step was left out of the screencast</span><br><span class="line">spark &#x3D; SparkSession.builder \</span><br><span class="line">    .master(&quot;local&quot;) \</span><br><span class="line">    .appName(&quot;Word Count&quot;) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line"># 如何读取数据集</span><br><span class="line">stack_overflow_data &#x3D; &#39;Train_onetag_small.json&#39;</span><br><span class="line">df &#x3D; spark.read.json(stack_overflow_data)</span><br><span class="line">df.head()</span><br><span class="line"># 把字符串分为单独的单词。Spark有一个[Tokenizer]（https:&#x2F;&#x2F;spark.apache.org&#x2F;docs&#x2F;latest&#x2F;ml-features.html#tokenizer） 类以及RegexTokenizer。 后者在分词时有更大的自由度。</span><br><span class="line"># split the body text into separate words</span><br><span class="line">regexTokenizer &#x3D; RegexTokenizer(inputCol&#x3D;&quot;Body&quot;, outputCol&#x3D;&quot;words&quot;, pattern&#x3D;&quot;\\W&quot;)</span><br><span class="line">df &#x3D; regexTokenizer.transform(df)</span><br><span class="line">df.head()</span><br><span class="line"># count the number of words in each body tag</span><br><span class="line">body_length &#x3D; udf(lambda x: len(x), IntegerType())</span><br><span class="line">df &#x3D; df.withColumn(&quot;BodyLength&quot;, body_length(df.words))</span><br><span class="line"># count the number of paragraphs and links in each body tag</span><br><span class="line">number_of_paragraphs &#x3D; udf(lambda x: len(re.findall(&quot;&lt;&#x2F;p&gt;&quot;, x)), IntegerType())</span><br><span class="line">number_of_links &#x3D; udf(lambda x: len(re.findall(&quot;&lt;&#x2F;a&gt;&quot;, x)), IntegerType())</span><br><span class="line">df &#x3D; df.withColumn(&quot;NumParagraphs&quot;, number_of_paragraphs(df.Body))</span><br><span class="line">df &#x3D; df.withColumn(&quot;NumLinks&quot;, number_of_links(df.Body))</span><br><span class="line">df.head(2)</span><br><span class="line"># 将内容长度，段落数和内容中的链接数合并为一个向量</span><br><span class="line">assembler &#x3D; VectorAssembler(inputCols&#x3D;[&quot;BodyLength&quot;, &quot;NumParagraphs&quot;, &quot;NumLinks&quot;], outputCol&#x3D;&quot;NumFeatures&quot;)</span><br><span class="line">df &#x3D; assembler.transform(df)</span><br><span class="line">df.head()</span><br><span class="line"># 归一化向量</span><br><span class="line">scaler &#x3D; Normalizer(inputCol&#x3D;&quot;NumFeatures&quot;, outputCol&#x3D;&quot;ScaledNumFeatures&quot;)</span><br><span class="line">df &#x3D; scaler.transform(df)</span><br><span class="line">df.head(2)</span><br><span class="line"># 缩放向量</span><br><span class="line">scaler2 &#x3D; StandardScaler(inputCol&#x3D;&quot;NumFeatures&quot;, outputCol&#x3D;&quot;ScaledNumFeatures2&quot;, withStd&#x3D;True)</span><br><span class="line">scalerModel &#x3D; scaler2.fit(df)</span><br><span class="line">df &#x3D; scalerModel.transform(df)</span><br><span class="line">df.head(2)</span><br></pre></td></tr></table></figure>
<h2 id="文本特征"><a href="#文本特征" class="headerlink" title="文本特征"></a>文本特征</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from pyspark.sql import SparkSession</span><br><span class="line">from pyspark.ml.feature import RegexTokenizer, CountVectorizer, \</span><br><span class="line">    IDF, StringIndexer</span><br><span class="line">from pyspark.sql.functions import udf</span><br><span class="line">from pyspark.sql.types import IntegerType</span><br><span class="line"></span><br><span class="line">import re</span><br><span class="line"># create a SparkSession: note this step was left out of the screencast</span><br><span class="line">spark &#x3D; SparkSession.builder \</span><br><span class="line">    .master(&quot;local&quot;) \</span><br><span class="line">    .appName(&quot;Word Count&quot;) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line"># 如何读取数据集</span><br><span class="line">stack_overflow_data &#x3D; &#39;Train_onetag_small.json&#39;</span><br><span class="line">df &#x3D; spark.read.json(stack_overflow_data)</span><br><span class="line">df.head()</span><br><span class="line"></span><br><span class="line"># 分词将字符串拆分为单独的单词。Spark 有一个[Tokenizer] （https:&#x2F;&#x2F;spark.apache.org&#x2F;docs&#x2F;latest&#x2F;ml-features.html#tokenizer） 类和RegexTokenizer。后者在分词时有更大的自由度 。</span><br><span class="line"># split the body text into separate words</span><br><span class="line">regexTokenizer &#x3D; RegexTokenizer(inputCol&#x3D;&quot;Body&quot;, outputCol&#x3D;&quot;words&quot;, pattern&#x3D;&quot;\\W&quot;)</span><br><span class="line">df &#x3D; regexTokenizer.transform(df)</span><br><span class="line">df.head()</span><br><span class="line"></span><br><span class="line"># CountVectorizer</span><br><span class="line"># find the term frequencies of the words</span><br><span class="line">cv &#x3D; CountVectorizer(inputCol&#x3D;&quot;words&quot;, outputCol&#x3D;&quot;TF&quot;, vocabSize&#x3D;1000)</span><br><span class="line">cvmodel &#x3D; cv.fit(df)</span><br><span class="line">df &#x3D; cvmodel.transform(df)</span><br><span class="line">df.take(1)</span><br><span class="line"># show the vocabulary in order of </span><br><span class="line">cvmodel.vocabulary</span><br><span class="line"># show the last 10 terms in the vocabulary</span><br><span class="line">cvmodel.vocabulary[-10:]</span><br><span class="line"></span><br><span class="line"># 逆文本频率指数（Inter-document Frequency ）</span><br><span class="line">idf &#x3D; IDF(inputCol&#x3D;&quot;TF&quot;, outputCol&#x3D;&quot;TFIDF&quot;)</span><br><span class="line">idfModel &#x3D; idf.fit(df)</span><br><span class="line">df &#x3D; idfModel.transform(df)</span><br><span class="line">df.head()</span><br><span class="line"></span><br><span class="line"># StringIndexer</span><br><span class="line">indexer &#x3D; StringIndexer(inputCol&#x3D;&quot;oneTag&quot;, outputCol&#x3D;&quot;label&quot;)</span><br><span class="line">df &#x3D; indexer.fit(df).transform(df)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>一首小诗：做最好的自己</title>
    <url>/2020/02/16/%E9%9A%8F%E6%84%9F%E4%B8%80%E7%AF%87/</url>
    <content><![CDATA[<p>今天看一个纪录片《人生第一次》时听到的小诗，来自美国诗人、短片小说作家——道格拉斯·马拉赫。</p>
<a id="more"></a>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=475218187&auto=1&height=66"></iframe>

<h4 id="如果你不能成为山顶上的高松，"><a href="#如果你不能成为山顶上的高松，" class="headerlink" title="如果你不能成为山顶上的高松，"></a>如果你不能成为山顶上的高松，</h4><h4 id="那就当棵山谷里的小树吧，"><a href="#那就当棵山谷里的小树吧，" class="headerlink" title="那就当棵山谷里的小树吧，"></a>那就当棵山谷里的小树吧，</h4><h4 id="但要当棵溪边最好的小树。"><a href="#但要当棵溪边最好的小树。" class="headerlink" title="但要当棵溪边最好的小树。"></a>但要当棵溪边最好的小树。</h4><p><br/></p>
<h4 id="如果你不能成为一棵大树，"><a href="#如果你不能成为一棵大树，" class="headerlink" title="如果你不能成为一棵大树，"></a>如果你不能成为一棵大树，</h4><h4 id="那就当丛小灌木；"><a href="#那就当丛小灌木；" class="headerlink" title="那就当丛小灌木；"></a>那就当丛小灌木；</h4><h4 id="如果你不能成为一丛小灌木，"><a href="#如果你不能成为一丛小灌木，" class="headerlink" title="如果你不能成为一丛小灌木，"></a>如果你不能成为一丛小灌木，</h4><h4 id="那就当一片小草地。"><a href="#那就当一片小草地。" class="headerlink" title="那就当一片小草地。"></a>那就当一片小草地。</h4><p><br/></p>
<h4 id="如果你不能是一只香獐，"><a href="#如果你不能是一只香獐，" class="headerlink" title="如果你不能是一只香獐，"></a>如果你不能是一只香獐，</h4><h4 id="那就当尾小鲈鱼，"><a href="#那就当尾小鲈鱼，" class="headerlink" title="那就当尾小鲈鱼，"></a>那就当尾小鲈鱼，</h4><h4 id="但要当湖里最活泼的小鲈鱼。"><a href="#但要当湖里最活泼的小鲈鱼。" class="headerlink" title="但要当湖里最活泼的小鲈鱼。"></a>但要当湖里最活泼的小鲈鱼。</h4><p><br/></p>
<h4 id="我们不能全是船长，"><a href="#我们不能全是船长，" class="headerlink" title="我们不能全是船长，"></a>我们不能全是船长，</h4><h4 id="必须有人也是水手。"><a href="#必须有人也是水手。" class="headerlink" title="必须有人也是水手。"></a>必须有人也是水手。</h4><p><br/></p>
<h4 id="这里有许多事让我们去做，"><a href="#这里有许多事让我们去做，" class="headerlink" title="这里有许多事让我们去做，"></a>这里有许多事让我们去做，</h4><h4 id="有大事，有小事，"><a href="#有大事，有小事，" class="headerlink" title="有大事，有小事，"></a>有大事，有小事，</h4><h4 id="但最重要的是我们身旁的事。"><a href="#但最重要的是我们身旁的事。" class="headerlink" title="但最重要的是我们身旁的事。"></a>但最重要的是我们身旁的事。</h4><p><br/></p>
<h4 id="如果你不能成为大道，"><a href="#如果你不能成为大道，" class="headerlink" title="如果你不能成为大道，"></a>如果你不能成为大道，</h4><h4 id="那就当一条小路，"><a href="#那就当一条小路，" class="headerlink" title="那就当一条小路，"></a>那就当一条小路，</h4><h4 id="如果你不能成为太阳，"><a href="#如果你不能成为太阳，" class="headerlink" title="如果你不能成为太阳，"></a>如果你不能成为太阳，</h4><h4 id="那就当一颗星星。"><a href="#那就当一颗星星。" class="headerlink" title="那就当一颗星星。"></a>那就当一颗星星。</h4><p><br/></p>
<h4 id="决定成败的不是你的身材"><a href="#决定成败的不是你的身材" class="headerlink" title="决定成败的不是你的身材"></a>决定成败的不是你的身材</h4><h4 id="而是做一个最好的你。"><a href="#而是做一个最好的你。" class="headerlink" title="而是做一个最好的你。"></a>而是做一个最好的你。</h4><p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/suigan1.png" alt="图片"></p>
]]></content>
      <tags>
        <tag>小诗</tag>
      </tags>
  </entry>
  <entry>
    <title>机器翻译检测</title>
    <url>/2020/02/13/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A3%80%E6%B5%8B/</url>
    <content><![CDATA[<p>因为本周要做一个机器翻译检测的任务，因此搜到了几篇论文，看一下大概有哪些思路。论文基本上只简单扫了一眼，简单介绍一下其中的3篇。</p>
<a id="more"></a>
<blockquote>
<p>参考：</p>
<p>Machine Translation Detection from Monolingual Web-Text</p>
<p>Automatic Detection of Machine Translated Text and Translation Quality</p>
<p>Detecting Machine-Translated Paragraphs by Matching Similar Words</p>
<p>Automatic Detection of Translated Text and its Impact on Machine Translation</p>
<p>BLEU: a Method for Automatic Evaluation of Machine Translation</p>
<p>Building a Web-based parallel corpus and filtering out machinetranslated text</p>
<p>Machine Translation Detection from MonolingualWeb-Text</p>
<p>Machine Translationness: a Concept for Machine Translation Evaluation and Detection</p>
<p>MT Detection in Web-Scraped Parallel Corpora</p>
<p>On the Features of Translationese</p>
<p>Translationese and Its Dialects</p>
</blockquote>
<h2 id="Machine-Translation-Detection-from-Monolingual-Web-Text"><a href="#Machine-Translation-Detection-from-Monolingual-Web-Text" class="headerlink" title="Machine Translation Detection from Monolingual Web-Text"></a>Machine Translation Detection from Monolingual Web-Text</h2><p>首先强调的是，这篇论文检测的是SMT机器翻译。看到论文摘要时我想到，针对不同的机器翻译模型，检测的机制也是不一样的，要有这点意识。这篇论文关注到的是SMT系统中“phrase salad”现象，并使用单语语料就可以达到95.8%的准确率。</p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul>
<li>SMT翻译中的‘phrase salad’现象，指的是翻译结果的每个短语单独拿出来是对的，但组合到一起是错的</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mtcheck1.png" alt="图片"></p>
<ul>
<li>比如上面这个例子，not only后面应该有but also，但这个短语在SMT翻译系统里只有一半被翻译了</li>
<li>使用了一个分类器对句子是否是‘phrase salad’进行检测，使用到的特征包括两个，一个是语言模型，另外是一些人们常用但对SMT来说难以生成的短语</li>
</ul>
<h3 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h3><p>基于SMT翻译的特点，在特征选择时主要考虑3点：句子流畅度、语法正确度、短语完整度。从人工翻译提取到的特征表达了它和人工产生句子的相似性，从机器翻译中提取到的特征表达了它和机器翻译句子的相似性。</p>
<h3 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h3><ul>
<li>Fluency Feature<ul>
<li>使用两个语言模型，f(w,H)和f(w,MT)，前者表示人工翻译的语言模型，后者是机器翻译的语言模型</li>
</ul>
</li>
<li>Grammaticality Feature<ul>
<li>使用POS语言模型，f(pos,H)和f(pos, MT)，前者表示人工翻译的POS序列的语言模型，后者是机器翻译的</li>
<li>对提取出的function word的POS语言模型，f(fw, H)和f(fw, MT)<ul>
<li>function word: <ul>
<li>Prepositions: of, at, in, without, between</li>
<li>Pro<a href="https://www.baidu.com/s?wd=no&amp;tn=SE_PcZhidaonwhc_ngpagmjz&amp;rsv_dl=gh_pc_zhidao" target="_blank" rel="noopener">no</a>uns: he, they, anybody, it, one</li>
<li>Determiners: the, a, that, my, more, much, either,neither</li>
<li>Conjunctions: and, that, when, while, although, or</li>
<li>Auxiliary verbs: be (is, am, are), have, got, do</li>
<li>Particles: <a href="https://www.baidu.com/s?wd=no&amp;tn=SE_PcZhidaonwhc_ngpagmjz&amp;rsv_dl=gh_pc_zhidao" target="_blank" rel="noopener">no</a>, <a href="https://www.baidu.com/s?wd=no&amp;tn=SE_PcZhidaonwhc_ngpagmjz&amp;rsv_dl=gh_pc_zhidao" target="_blank" rel="noopener">no</a>t, nor, as</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Gappy-Phrase Feature<ul>
<li>中间有间隔的短语：如not only * but also</li>
<li>使用character级别的LM衡量</li>
<li>Sequential Pattern Mining<ul>
<li>使用sequential pattern挖掘的方法找到所有Gappy-Phrase</li>
</ul>
</li>
<li>使用的信息增益进行的短语选择，但是没看懂是如何计算特征的</li>
</ul>
</li>
<li>最后使用SVM进行分类</li>
<li>其他可考虑的feature：<ul>
<li>Translationese and its dialects论文</li>
<li>On the features of translationese论文（比较学术）</li>
<li>average token length</li>
<li>type-token ratio</li>
</ul>
</li>
</ul>
<h2 id="Automatic-Detection-of-Machine-Translated-Text-and-Translation-Quality"><a href="#Automatic-Detection-of-Machine-Translated-Text-and-Translation-Quality" class="headerlink" title="Automatic Detection of Machine Translated Text and Translation Quality"></a>Automatic Detection of Machine Translated Text and Translation Quality</h2><p>这篇论文也是使用的单语语料训练的分类器，用来进行机器翻译的检测和翻译质量的评估。这篇文章的重点强调的是，通过分类器检测的方法，只能对质量特别差的翻译系统起作用，如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mtcheck2.png" alt="图片"></p>
<p>可以看出来，翻译系统越好，检测的准确率就越低。</p>
<h3 id="特征选择-1"><a href="#特征选择-1" class="headerlink" title="特征选择"></a>特征选择</h3><ul>
<li>本文使用的特征都是二元特征，即是否存在POS ngram和467个function word。</li>
<li>从4个方面构建特征：word、lemma、pos、mixed</li>
</ul>
<h2 id="Detecting-Machine-Translated-Paragraphs-by-Matching-Similar-Words"><a href="#Detecting-Machine-Translated-Paragraphs-by-Matching-Similar-Words" class="headerlink" title="Detecting Machine-Translated Paragraphs by Matching Similar Words"></a>Detecting Machine-Translated Paragraphs by Matching Similar Words</h2><p>这篇文章主要是检查段落级别的机器翻译，方法是通过计算word的match情况，和段落的coherence来检测机器翻译，整体流程如下：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mtcheck3.png" alt="图片"></p>
<h3 id="计算相似词"><a href="#计算相似词" class="headerlink" title="计算相似词"></a>计算相似词</h3><p>先把段落内所有词打上POS标签，依次计算和其他词的相似度（如果POS相同则保留）。能够看出人工翻译的整体相似度比较低，机器翻译的相似度高一些，如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mtcheck4.png" alt="图片"></p>
<h3 id="计算Coherence"><a href="#计算Coherence" class="headerlink" title="计算Coherence"></a>计算Coherence</h3><p>基于POS对统计根据的均值和方差。如下图：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/mtcheck5.png" alt="图片"></p>
<h3 id="进行分类"><a href="#进行分类" class="headerlink" title="进行分类"></a>进行分类</h3><p>使用SVM分类。</p>
]]></content>
      <tags>
        <tag>机器翻译</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读：《Solving and Generating Chinese Character Riddles》</title>
    <url>/2020/02/11/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E3%80%8ASolving-and-Generating-Chinese-Character-Riddles%E3%80%8B/</url>
    <content><![CDATA[<p>之前想给语音助手增加一个猜字谜的功能，这两天不忙就读了一下这篇机器解谜语的论文。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://www.aclweb.org/anthology/D16-1081.pdf" target="_blank" rel="noopener">https://www.aclweb.org/anthology/D16-1081.pdf</a></p>
</blockquote>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>解谜语类似于下面的过程：</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/solgene1.png" alt="图片"></p>
<ul>
<li>解谜的pipeline<ul>
<li>解题过程<ul>
<li>学习谜语中的短语和部首的对齐关系</li>
<li>学习谜语和rule的关系</li>
<li>使用上面两个关系，用算法得到候选答案</li>
<li>使用Ranking-SVM来对candidate进行排序</li>
</ul>
</li>
<li>生成谜语过程<ul>
<li>使用模版方法</li>
<li>使用替代的方法</li>
<li>使用Ranking-SVM来对candidate进行排序</li>
</ul>
</li>
<li>整体过程如下图：</li>
</ul>
</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/solgene2.png" alt="图片"></p>
<h2 id="Phrase-Radical-Alignments-and-Rules"><a href="#Phrase-Radical-Alignments-and-Rules" class="headerlink" title="Phrase-Radical Alignments and Rules"></a>Phrase-Radical Alignments and Rules</h2><h3 id="Alignment"><a href="#Alignment" class="headerlink" title="Alignment"></a>Alignment</h3><ul>
<li>希望将“千里”和“马”进行对齐</li>
<li>方法一<ul>
<li>将谜语分词$\left(w_{1}, w_{2}, \ldots, w_{n}\right)$</li>
<li>将答案分成不同部首$\left(r_{1}, r_{2}, \ldots, r_{m}\right)$</li>
<li>统计对齐$\left(\left[w_{i}, w_{j}\right], r_{k}\right)(i, j \in[1, n], k \in[1, m])$</li>
</ul>
</li>
<li>方法二<ul>
<li>谜语中两个连续字符$\left(w_{1}, w_{2}\right)$，如果w1是w2的部首，且w2的其余部分r出现在答案q中，则$\left(\left(w_{1}, w_{2}\right), r\right)$是一个对齐</li>
</ul>
</li>
<li>统计所有的对齐，并过滤掉出现频次小于3的</li>
<li>特别常见的对齐如下图：</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/solgne3.png" alt="图片"></p>
<h3 id="Rules"><a href="#Rules" class="headerlink" title="Rules"></a>Rules</h3><ul>
<li>总结了6类规则</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/solgne4.png" alt="图片"></p>
<ul>
<li>对于$\left(\left[w_{1}, w_{n}\right], r\right)$，如果r是wi的部首，则$\left(w_{1}, \dots, w_{i-1},(.), w_{i+1}, \dots, w_{n}\right)$就是一个潜在的规则，我们从数据中最终总结193条规则，归纳为上面6类</li>
<li>1000个汉字有至少1个alignment，27个汉字有至少100个alignment</li>
</ul>
<h2 id="Riddle-Solving-and-Generation"><a href="#Riddle-Solving-and-Generation" class="headerlink" title="Riddle Solving and Generation"></a>Riddle Solving and Generation</h2><h3 id="Solving-Chinese-Character-Riddles"><a href="#Solving-Chinese-Character-Riddles" class="headerlink" title="Solving Chinese Character Riddles"></a>Solving Chinese Character Riddles</h3><ul>
<li>解谜算法的伪代码如下</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/solgne5.png" alt="图片"></p>
<ul>
<li>以“上岗必戴安全帽”为例，“上岗”通过规则“上(up) (.)”和山对齐，“必” 和 “戴”跟自己对齐，“安全帽”因为analogical shape和“宀”对齐，最终得到结果“密”</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/solgene6.png" alt="图片"></p>
<ul>
<li>对答案进行排序，排序时使用的特征如下：</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/solgene7.png" alt="图片"></p>
<h3 id="Generating-Chinese-Character-Riddles"><a href="#Generating-Chinese-Character-Riddles" class="headerlink" title="Generating Chinese Character Riddles"></a>Generating Chinese Character Riddles</h3><ul>
<li>基于模板的方法</li>
<li>基于替换的方法</li>
<li>对候选的description进行排序，排序使用的特征如下：</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/solgene8.png" alt="图片"></p>
<h3 id="Ranking-Model"><a href="#Ranking-Model" class="headerlink" title="Ranking Model"></a>Ranking Model</h3><ul>
<li>score的计算：$\text { Score }(c)=\sum_{i=1}^{m} \lambda_{i} * g_{i}(c)$，其中c表示一个候选，gi(c)表示c的第i个特征，m是特征的总数，$\lambda_{i}​$表示特征的权重</li>
<li>使用Ranking SVM算法求解特征权重参数</li>
</ul>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><ul>
<li>数据：从网络上爬取的7w+谜语，3k+的笔画，古代诗词和对联等用于训练语言模型</li>
<li>使用准确率评价解谜的效果，使用人工评测来评价生成谜题</li>
</ul>
]]></content>
      <tags>
        <tag>论文阅读</tag>
        <tag>机器猜字谜</tag>
      </tags>
  </entry>
  <entry>
    <title>论文粗读:《Generating Sentences by Editing Prototypes》</title>
    <url>/2020/02/11/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94%E3%80%8AGenerating-Sentences-by-Editing-Prototypes%E3%80%8B/</url>
    <content><![CDATA[<p>因为想做一个根据不同年龄段的人生成不同故事内容的demo，所以阅读了这篇文本风格转换的论文。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://arxiv.org/abs/1709.08878" target="_blank" rel="noopener">https://arxiv.org/abs/1709.08878</a><br><a href="https://github.com/kelvinguu/neural-editor" target="_blank" rel="noopener">https://github.com/kelvinguu/neural-editor</a></p>
</blockquote>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h3><ul>
<li>通过从训练集中挑选一个prototype sentence，产生一个能捕捉到句子相似度等句子级别信息的latent edit vector，并通过这个向量生成句子</li>
<li>模型的整体框架如下图，这个模型的由来是基于人们的一个经验，通常人们写一个复杂的句子时，都是根据一个简单的句子，逐步修改而来的</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/gsep1.png" alt="图片"></p>
<ul>
<li>目标函数：最大化生成模型的log likelihood</li>
<li>使用locality sensitive hashing寻找相似的句子</li>
</ul>
<h2 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h2><h3 id="解决问题分两步"><a href="#解决问题分两步" class="headerlink" title="解决问题分两步"></a>解决问题分两步</h3><ul>
<li>从语料库里选择一句话<ul>
<li>prototype distribution: p(x’) （uniform over X）</li>
<li>以p(x’)的概率从语料库中随机选择一个prototype sentence</li>
</ul>
</li>
<li>把这句话进行修改<ul>
<li>以edit prior: p(z)的概率sample出一个edit vector: z （实际是对edit type进行编码）</li>
<li>将z和x’送入到$p_{\text {edit }}\left(x | x^{\prime}, z\right)$的神经网络中，产生新的句子x</li>
</ul>
</li>
</ul>
<p>整体公式如下：</p>
<p>$p(x)=\sum_{x^{\prime} \in \mathcal{X}} p\left(x | x^{\prime}\right) p\left(x^{\prime}\right)$</p>
<p>$p\left(x | x^{\prime}\right)=\mathbb{E}_{z \sim p(z)}\left[p_{\mathrm{edit}}\left(x | x^{\prime}, z\right)\right]$</p>
<h3 id="模型满足两个条件"><a href="#模型满足两个条件" class="headerlink" title="模型满足两个条件"></a>模型满足两个条件</h3><ul>
<li>Semantic smoothness：一次edit只能对文本进行小改动；多次edit可以产生大的改动</li>
<li>Consistent edit behavior：对edit有类型的控制，对不同句子同一类型的edit应该产生相似的效果</li>
</ul>
<h2 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h2><h3 id="对p-x-进行近似"><a href="#对p-x-进行近似" class="headerlink" title="对p(x)进行近似"></a>对p(x)进行近似</h3><ul>
<li>大多数的prototype x都是不相关的，即p(x|x’)非常小；因此我们只考虑和x有非常高的lexical overlap的prototype x’</li>
<li>定义一个lexical similarity neighborhor $\mathcal{N}(x) \stackrel{\text { def }}{=}\left\{x^{\prime} \in \mathcal{X}: d_{J}\left(x, x^{\prime}\right)&lt;0.5\right\}$，其中dj是x和x’的Jaccard距离</li>
<li>利用neighborhood prototypes和Jensen不等式求解</li>
</ul>
<p>$\begin{aligned} \log p(x) &amp;=\log \left[\sum_{x^{\prime} \in \mathcal{X}} p\left(x | x^{\prime}\right) p\left(x^{\prime}\right)\right] \ &amp; \geq \log \left[\sum_{x^{\prime} \in \mathcal{N}(x)} p\left(x | x^{\prime}\right) p\left(x^{\prime}\right)\right] \end{aligned}$</p>
<p>$\begin{array}{l}{=\log \left[|\mathcal{N}(x)|^{-1} \sum_{x^{\prime} \in \mathcal{N}(x)} p\left(x | x^{\prime}\right)\right]+R(x)} \ {\geq|\mathcal{N}(x)|^{-1} \sum_{x^{\prime} \in \mathcal{N}(x)} \log p\left(x | x^{\prime}\right)+R(x)} \ {\underbrace{x^{\prime} \in \mathcal{N}(x)}_{\text {def }_{\mathrm{LEX}}(x)}+R(x)}\end{array}$</p>
<ul>
<li>$\begin{array}{l}{p\left(x^{\prime}\right)=1 /|\mathcal{X}|} \ {\mathrm{R}(\mathrm{x})=\log (|\mathcal{N}(x)| /|\mathcal{X}|)}\end{array}$</li>
<li>$|\mathcal{N}(x)|​$是跟x相关的常数，x的邻居使用locality sensitive hashing (LSH) and minhashing进行预先的计算</li>
</ul>
<h3 id="对log-p-x-x’-进行近似"><a href="#对log-p-x-x’-进行近似" class="headerlink" title="对log p(x|x’)进行近似"></a>对log p(x|x’)进行近似</h3><ul>
<li><p>使用蒙特卡洛对$z \sim p(z)​$进行采样时可能会产生比较高的方差，因为$p_{\text {edit }}\left(x | x^{\prime}, z\right)​$对于大部分从p(z) sample出来的z来说都输出0，只对一部分不常见的值输出较大的值</p>
</li>
<li><p>使用inverse neural editor：$q\left(z | x^{\prime}, x\right)$</p>
<ul>
<li>对prototype x’和修正后的句子x，生成一个x’到x的转换的edit vector z，这个z在重要的值上会有较大的概率</li>
</ul>
</li>
<li><p>使用evidence lower bound（ELBO）来计算log p(x|x’)</p>
<p>$\begin{aligned} \log p\left(x | x^{\prime}\right) \geq &amp; \underbrace{\mathbb{E}_{z \sim q\left(z | x^{\prime}, x\right)}\left[\log p_{\text {edit }}\left(x | x^{\prime}, z\right)\right]}_{\mathcal{L}_{\text {gen }}} \ &amp;-\underbrace{\operatorname{KL}\left(q\left(z | x^{\prime}, x\right) | p(z)\right)}_{\mathcal{E}_{\mathrm{KL}}^{L}} \ \stackrel{\text { def }}{=} \operatorname{ELBO}\left(x, x^{\prime}\right) \end{aligned}$</p>
</li>
<li><p>q(z|x’,x)可以看成是VAE的encoder，pedit(x|x’,z)可以看成是VAE的decoder</p>
</li>
</ul>
<h3 id="目标函数-sum-x-prime-in-mathcal-N-x-operatorname-ELBO-left-x-x-prime-right"><a href="#目标函数-sum-x-prime-in-mathcal-N-x-operatorname-ELBO-left-x-x-prime-right" class="headerlink" title="目标函数 $\sum_{x^{\prime} \in \mathcal{N}(x)} \operatorname{ELBO}\left(x, x^{\prime}\right)$"></a>目标函数 $\sum_{x^{\prime} \in \mathcal{N}(x)} \operatorname{ELBO}\left(x, x^{\prime}\right)$</h3><ul>
<li>参数：$\Theta=\left(\Theta_{p}, \Theta_{q}\right)$，包含neural editor的参数和inverse neural editor的参数</li>
</ul>
<h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><h3 id="Neural-editor-p-text-edit-left-x-x-prime-z-right"><a href="#Neural-editor-p-text-edit-left-x-x-prime-z-right" class="headerlink" title="Neural editor $p_{\text {edit }}\left(x | x^{\prime}, z\right)$"></a>Neural editor $p_{\text {edit }}\left(x | x^{\prime}, z\right)$</h3><ul>
<li>input: prototype x’</li>
<li>output: revised sentence x</li>
<li>seq2seq<ul>
<li>encoder: 3层双向LSTM，使用Glove词向量初始化</li>
<li>decoder: 3层包含attention的LSTM<ul>
<li>最上面一层的hidden state用来和encoder输出的hidden state一起算attention</li>
<li>将attention向量和z向量concate一起，再送入softmax中</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Edit-prior-p-z"><a href="#Edit-prior-p-z" class="headerlink" title="Edit prior $p(z)$"></a>Edit prior $p(z)$</h3><ul>
<li>edit vector z的sample方法<ul>
<li>先采样其scalar length：$z_{\text {norm }} \sim  \operatorname{Unif}(0,10)​$</li>
<li>再采样其direction:  在uniform distribution中采样一个zdir向量</li>
<li>$z=z_{\text {norm }} \cdot z_{\text {dir }}$</li>
<li>这样做是为了方便计算KL散度</li>
</ul>
</li>
</ul>
<h3 id="Inverse-neural-editor-q-left-z-x-prime-x-right"><a href="#Inverse-neural-editor-q-left-z-x-prime-x-right" class="headerlink" title="Inverse neural editor $q\left(z | x^{\prime}, x\right)$"></a>Inverse neural editor $q\left(z | x^{\prime}, x\right)$</h3><ul>
<li>假设x’和x只差了一个word，那么edit vector z跟word的词向量应该是一样的，那么多个word的插入就相当于多个word的词向量的和，删除同理</li>
<li>加入到x’的词的集合：$I=x \backslash x^{\prime}​$</li>
<li>从x’中删除的词的集合：$D=x^{\prime}\backslash  x$</li>
<li>x’和x的差异：$f\left(x, x^{\prime}\right)=\sum_{w \in I} \Phi(w) \oplus \sum_{w \in D} \Phi(w)$<ul>
<li>$\Phi(w)$表示w的词向量，它同时也是inverse neural editor q的参数，使用300维的Glove向量初始化</li>
<li>$\oplus$表示concate操作</li>
</ul>
</li>
<li>认为q是在f的基础上加入噪声获得的（先旋转，在rescale）：</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/gsep2.png" alt="图片"></p>
<ul>
<li>$\begin{array}{l}{f_{\text {norm }}=|f|} \ {f_{\text {dir }}=f / f_{\text {norm }}}\end{array}$</li>
<li>$\operatorname{vMF}(v ; \mu, \kappa)​$表示点v的unit空间中的vMF分布，参数包含mean vector$\mu​$和concentration parameter $ \kappa​$</li>
<li><p>因此可得：$\begin{aligned} q\left(z_{\text {dir }} | x^{\prime}, x\right) &amp;=\operatorname{vMF}\left(z_{\text {dir }} ; f_{\text {dir }}, \kappa\right) \ q\left(z_{\text {norm }} | x^{\prime}, x\right) &amp;=\text { Unif }\left(z_{\text {norn }} ;\left[\tilde{f}_{\text {norn }}, \tilde{f}_{\text {nom }}+\epsilon\right]\right) \end{aligned}$</p>
</li>
<li><p>其中 $\tilde{f}_{\text {norm }}=\min \left(f_{\text {norm }}, 10-\epsilon\right)$</p>
</li>
<li>最终 $z=z_{\mathrm{dir}} \cdot z_{\mathrm{norm}}$</li>
</ul>
<h2 id="Details"><a href="#Details" class="headerlink" title="Details"></a>Details</h2><h3 id="计算目标函数的梯度-nabla-Theta-q-operatorname-ELBO-left-x-x-prime-right-nabla-Theta-q-mathcal-L-mathrm-gen-nabla-Theta-q-mathcal-L-mathrm-KL"><a href="#计算目标函数的梯度-nabla-Theta-q-operatorname-ELBO-left-x-x-prime-right-nabla-Theta-q-mathcal-L-mathrm-gen-nabla-Theta-q-mathcal-L-mathrm-KL" class="headerlink" title="计算目标函数的梯度$\nabla_{\Theta_{q}} \operatorname{ELBO}\left(x, x^{\prime}\right)=\nabla_{\Theta_{q}} \mathcal{L}_{\mathrm{gen}}-\nabla_{\Theta_{q}} \mathcal{L}_{\mathrm{KL}}$"></a>计算目标函数的梯度$\nabla_{\Theta_{q}} \operatorname{ELBO}\left(x, x^{\prime}\right)=\nabla_{\Theta_{q}} \mathcal{L}_{\mathrm{gen}}-\nabla_{\Theta_{q}} \mathcal{L}_{\mathrm{KL}}$</h3><ul>
<li>使用重参数计算：$\nabla_{\Theta_{q}} \mathcal{L}_{\mathrm{gen}}$<ul>
<li>将z~q(z|x’,x)重写为$z=h(\alpha)$</li>
<li>$\begin{aligned} \nabla \Theta_{q} \mathcal{L}_{\mathrm{gen}} &amp;=\nabla \Theta_{q} \mathbb{E}_{z \sim q\left(z | x^{\prime}, x\right)}\left[\log p_{\text {edit }}\left(x | x^{\prime}, z\right)\right] \ &amp;=\mathbb{E}_{\alpha \sim p(\alpha)}\left[\nabla \Theta_{q} \log p_{\text {edit }}\left(x | x^{\prime}, h(\alpha)\right)\right] \end{aligned}$</li>
</ul>
</li>
<li>计算$\nabla_{\Theta_{q}} \mathcal{L}_{\mathrm{KL}}$<ul>
<li>$\begin{aligned} \mathcal{L}_{\mathrm{KL}} &amp;=\mathrm{KL}\left(q\left(z_{\text {norm }} | x^{\prime}, x\right) | p\left(z_{\text {norm }}\right)\right) \ &amp;+\mathrm{KL}\left(q\left(z_{\text {dir }} | x^{\prime}, x\right) | p\left(z_{\text {dir }}\right)\right) \end{aligned}$</li>
<li>$\begin{array}{l}{\operatorname{KL}(\operatorname{vMF}(\mu, \kappa) | \operatorname{vMF}(\mu, 0))=\kappa \frac{I_{d / 2}(\kappa)+I_{d / 2-1}(\kappa) \frac{d-2}{2 \kappa}}{I_{d / 2-1}(\kappa)-\frac{d-2}{2 \kappa}}} \ {-\log \left(I_{d / 2-1}(\kappa)\right)-\log (\Gamma(d / 2))} \ {+\log (\kappa)(d / 2-1)-(d-2) \log (2) / 2}\end{array}$</li>
</ul>
</li>
</ul>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="Datsets"><a href="#Datsets" class="headerlink" title="Datsets"></a>Datsets</h3><ul>
<li>Yelp</li>
<li>One BillionWord Language Model Benchmark</li>
<li>使用Spacy将NER的词替换成其NER category</li>
<li>将出现频次小于10000的词用UNK替换</li>
</ul>
<h3 id="Generative-Modeling"><a href="#Generative-Modeling" class="headerlink" title="Generative Modeling"></a>Generative Modeling</h3><ul>
<li>对比几个生成模型的效果（KENLM语言模型、自回归语言模型）</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/gsep3.png" alt="图片"></p>
<ul>
<li>使用neural editor可以生成跟prototype很不一样的句子</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/gsep4.png" alt="图片"></p>
<h3 id="Human-Evaluation"><a href="#Human-Evaluation" class="headerlink" title="Human Evaluation"></a>Human Evaluation</h3><ul>
<li>降低softmax temperture有助于产生更符合语法的句子，但也会产生short and generic sentence<ul>
<li>从corpus中sample出prototype sentence可以增加生成句子的多样性，因此即便temperature设置为0，也不会影响句子多样子，这是比传统的NLM强的地方</li>
</ul>
</li>
<li>从grammaticality 和 plausibility两方面进行评测</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/gsep5.png" alt="图片"></p>
<h3 id="Semantics-of-NeuralEditor"><a href="#Semantics-of-NeuralEditor" class="headerlink" title="Semantics of NeuralEditor"></a>Semantics of NeuralEditor</h3><ul>
<li>跟sentence variational autoencoder (SVAE)模型对比，SVAE将句子映射到semantic空间向量，再从向量还原句子</li>
<li>semantic smoothness<ul>
<li>smoothness表示每一个小的edit只能对句子有一点点改变</li>
<li>我们先从corpus中选择一个prototype sentence，然后不断对它使用neural editor，并让人工对semantic的变化进行打分。</li>
<li>对比实验有两个，一个是SVAE，一个是从corpus中根据cosine相似度选择出的句子</li>
</ul>
</li>
<li>consistent edit behavior</li>
</ul>
]]></content>
      <tags>
        <tag>论文阅读</tag>
        <tag>文本风格迁移</tag>
      </tags>
  </entry>
  <entry>
    <title>Separable Convolution</title>
    <url>/2020/02/06/Separable-Convolution/</url>
    <content><![CDATA[<p>阅读论文《The Evolved Transformer》时遇到了separable convolution的概念，因此找了相关资料学习了一下。</p>
<blockquote>
<p>参考：<br><a href="https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-al" target="_blank" rel="noopener">https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-al</a></p>
</blockquote>
<a id="more"></a>
<p>在讲Separable Convolution前先了解下常用的卷积网络的定义。</p>
<h2 id="Convolutional-Neural-Networks"><a href="#Convolutional-Neural-Networks" class="headerlink" title="Convolutional Neural Networks"></a>Convolutional Neural Networks</h2><blockquote>
<p>参考：<br><a href="https://towardsdatascience.com/what-is-a-neural-network-6010edabde2b" target="_blank" rel="noopener">https://towardsdatascience.com/what-is-a-neural-network-6010edabde2b</a></p>
</blockquote>
<p>卷积网络中最重要的是卷积核，通过卷积核在图像每个区域的运算，得到图像不同的特征，如下图（可以在<a href="http://setosa.io/ev/image-kernels/" target="_blank" rel="noopener">http://setosa.io/ev/image-kernels/</a>中更好得体验）：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn1.png" alt="图片"></p>
<p>上面这张图使用outliine卷积核，实际中可以使用sharp等不同功能的卷积核以达到不同效果。</p>
<p>对于神经网络的每一层而言，可以使用多个卷积和得到不同的特征图，并将这些特征图一起输入到下一层网络。最终这些特征供给最后一层的分类器进行匹配，得到分类结果。下面的动画展示了这个过程：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn1.gif" alt="图片"></p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn2.gif" alt="图片"></p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn3.gif" alt="图片"></p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn4.gif" alt="图片"></p>
<p>Separable Convolution可以分成spatial separable convolution和depthwise separable convolution。</p>
<p>对于12x12x3的图像，5x5x3的卷积核，能产生8x8x1的输出：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn2.png" alt="图片"></p>
<p>假设我们想要8x8x256的输出，则需要使用256个卷积核来创造256个8x8x1的图像，把他们叠加在一起产生8x8x256的输出：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn3.png" alt="图片"></p>
<p>即12x12x3 — (5x5x3x256) — &gt;12x12x256</p>
<h2 id="Spatial-Separable-Convolutions"><a href="#Spatial-Separable-Convolutions" class="headerlink" title="Spatial Separable Convolutions"></a>Spatial Separable Convolutions</h2><p>Spatial separable convolution将卷积分成两部分，最常见的是把3x3的kernel分解成3x1和1x3的kernel，如：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn4.png" alt="图片"></p>
<p>通过这种方式，原本一次卷积要算9次乘法，现在只需要6次。</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn5.png" alt="图片"></p>
<p>还有一个Sobel kernel（用来检测边）也是用的这种方法：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn6.png" alt="图片"></p>
<p>但spatial separable convolution存在的问题是，不是所有kernel都能转换成2个小的kernel。</p>
<h2 id="Depthwise-Separable-Convolutions"><a href="#Depthwise-Separable-Convolutions" class="headerlink" title="Depthwise Separable Convolutions"></a>Depthwise Separable Convolutions</h2><p>由于卷积并不使用矩阵相乘，为了减少计算量，可以将卷积的过程分成两部分：a depthwise convolution and a pointwise convolution. </p>
<h3 id="depthwise-convolution"><a href="#depthwise-convolution" class="headerlink" title="depthwise convolution"></a>depthwise convolution</h3><p>首先，我们使用3个5x5x1的卷积核产生8x8x3的图像：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn7.png" alt="图片"></p>
<h3 id="pointwise-convolution"><a href="#pointwise-convolution" class="headerlink" title="pointwise convolution"></a>pointwise convolution</h3><p>其次，使用1x1x3 的卷积核对每个像素计算，得到8x8x1 的图像：</p>
<p><img src="http://q503tsu73.bkt.clouddn.com/sc8.png?e=1580956603&amp;token=05Ii263bPN3Z-CT3JPRaRfWi5sXIj8pwX6V1bN2j:BxS4Xqtt2YsqJ5GJVFVlnK9D3xw=&amp;attname=" alt="图片"></p>
<p>使用256个1x1x3 的卷积核，则恶意产生8x8x256的图像：</p>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/cnn9.png" alt="图片"></p>
<p>可以看到，整个过程由原来的12x12x3 — (5x5x3x256) →12x12x256，变成12x12x3 — (5x5x1x1) — &gt; (1x1x3x256) — &gt;12x12x256</p>
<h2 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h2><p>主要就是减少了计算量，原先是256个5x5x3的卷积核移动8x8次，即需要256x3x5x5x8x8=1,228,800次乘法计算。使用depthwise convolution，有3个5x5x1的卷积核移动8x8次，需要3x5x5x8x8 = 4,800次乘法计算。使用pointwise convolution，有256个1x1x3的卷积核移动8x8次，需要256x1x1x3x8x8=49,152次乘法计算，加起来共有53,952次计算。</p>
<h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><ul>
<li>keras doc：<a href="https://keras.io/layers/convolutional/" target="_blank" rel="noopener">https://keras.io/layers/convolutional/</a></li>
<li><a href="https://github.com/alexandrosstergiou/keras-DepthwiseConv3D" target="_blank" rel="noopener">https://github.com/alexandrosstergiou/keras-DepthwiseConv3D</a></li>
<li>[<a href="https://github.com/sara-kassani/Depthwise-Separable-Convolutional-Neural-Network-for-Skin-Lesion-Classification](" target="_blank" rel="noopener">https://github.com/sara-kassani/Depthwise-Separable-Convolutional-Neural-Network-for-Skin-Lesion-Classification](</a></li>
</ul>
]]></content>
      <tags>
        <tag>Convolution</tag>
        <tag>卷积网络</tag>
      </tags>
  </entry>
  <entry>
    <title>论文粗读:《The Evolved Transformer》</title>
    <url>/2020/02/05/%E8%AE%BA%E6%96%87%E7%B2%97%E8%AF%BB%E2%80%94%E2%80%94%E3%80%8AThe-Evolved-Transformer%E3%80%8B/</url>
    <content><![CDATA[<p>今天阅读了论文《The Evolved Transformer》，该论文使用了神经架构搜索方法找到了一个更优的transformer结构。下面是阅读过程中的笔记。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://arxiv.org/pdf/1901.11117.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1901.11117.pdf</a><br><a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/evolved_transformer.py" target="_blank" rel="noopener">https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/evolved_transformer.py</a><br><a href="https://blog.csdn.net/jasonzhoujx/article/details/88875469" target="_blank" rel="noopener">https://blog.csdn.net/jasonzhoujx/article/details/88875469</a></p>
</blockquote>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h3><ul>
<li>神经架构搜索<ul>
<li>tournament selection architecture search </li>
<li>warm start</li>
<li>Progressive Dynamic Hurdles（PDH）</li>
</ul>
</li>
<li>搜索出了一个新的transformer架构：Evolved Transformer</li>
</ul>
<h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><h3 id="搜索空间"><a href="#搜索空间" class="headerlink" title="搜索空间"></a>搜索空间</h3><ul>
<li>encoder stackable cell<ul>
<li>6个NASNet-style block<ul>
<li>左右两个block将输入的hidden state转成左右两个hidden state再归并成为一个新的hidden state，作为self-attention的输入</li>
</ul>
</li>
</ul>
</li>
<li>decoder stackable cell<ul>
<li>8个NASNet-style block</li>
</ul>
</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/et1.png" alt="图片"></p>
<ul>
<li>搜索空间branch<ul>
<li>Input：分支可以从输入池中选择一个隐藏状态作为当前block的输入。单元中的第i个block可以从[0, i]个隐藏状态中进行选择，其中第j个隐藏状态表示该cell中第j个block的输出，第0个候选项为单元的输入。</li>
<li>Normalization：归一化项提供了两个选项， [LAYER NORMALIZATION (Ba et al., 2016), NONE]</li>
<li>Layer：构造一个神经网络层，提供的选项包括：<ul>
<li>标准卷积</li>
<li>深度可分离卷积</li>
<li>LIGHTWEIGHT 卷积</li>
<li>n头注意力层</li>
<li>GATED LINEAR UNIT</li>
<li>ATTEND TO ENCODER（decoder专用）</li>
<li>全等无操作</li>
<li>Dead Branch，切断输出</li>
</ul>
</li>
<li>Relative Output Dimension：决定神经网络层输出的维度。</li>
<li>Activation：搜索中激活函数的选项有[SWISH, RELU, LEAKY RELU, NON]</li>
<li>Combiner Function：表征的是左枝和右枝的结合方式，包括{ADDITION、CONCATENATION、MULTIPLICATION}。如果左右枝最终输出形状不同，则需要使用padding进行填充。短的向量向长的向量对齐，当使用加法进行结合时使用0填充，当使用乘法进行结合时使用1填充。</li>
<li>Number of cells：纵向叠加的cell的数量，搜索范围是[1,6]</li>
</ul>
</li>
</ul>
<h3 id="演进过程"><a href="#演进过程" class="headerlink" title="演进过程"></a>演进过程</h3><ul>
<li>锦标赛选择（Tournament Selection）：<ul>
<li>tournament selection算法是一种遗传算法，首先随机生成一批个体, 这些个体是一个个由不同组件组成的完整的模型，我们在目标任务上训练这些个体并在验证集上面计算他们的表现。</li>
<li>首先在初始种群中进行采样产生子种群，从子种群中选出适应性（fitness）最高的个体作为亲本（parent）。被选中的亲本进行突变——也就是将网络模型中的一些组件改变为其他的组件——以产生子模型，然后在对这些子模型分配适应度（fitness），在训练集和测试集上进行训练和验证。</li>
<li>对种群重新进行采样，用通过评估的子模型代替子种群中的fitness的个体以生成新的种群。</li>
<li>重复上面的步骤，直到种群中出现超过给定指标的模型。</li>
</ul>
</li>
<li>渐进式动态障碍（Progressive Dynamic Hurdle）：<ul>
<li>实验使用的训练集是WMT14英语到德语的机器翻译数据集，完整的训练和验证过程需要很长的时间，如果在所有的子模型上进行完整的训练和验证过程将会耗费很大的计算资源。因此论文中使用渐进式动态障碍的方法来提前停止一些没有前景的模型的训练，转而将更多的计算资源分配那些当前表现更好的子模型。具体来说就是让当前表现最好的一些模型多训练一些step。</li>
<li>假设当前种群经过一次锦标赛选择，生成了m个子模型并且加入到了种群中，这时候计算整个种群fitness的平均值h0，下一次锦标赛选择将会以h0作为对照，生成的另外m个fitness超过h0的子模型可以继续训练s1个step，接着进行种群中的所有的其他个体会继续训练s1个step，然后在新的种群中生成h1，以此类推知道种群中所有的个体的训练step都达到一个指定值。</li>
<li>如果一个子模型是由第iii次锦标赛选择之后的亲本生成的，那么验证的过程将会进行iii次。第一次为该模型分配s0次的训练step并且在验证集上进行验证，若验证的fitness大于h0则再分配s1次训练step，再验证，再与h1比较，只有子样本通过h0,h1,…,hi次比较才能作为新的个体加入到新的种群中。</li>
</ul>
</li>
</ul>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><ul>
<li><p>机器翻译</p>
<ul>
<li>在初始的10K step使用0.01的learning rate</li>
<li><p>Transformer</p>
<ul>
<li>inverse-square-root decay to 0 at 300K steps：$l r=s t e p^{-0.00303926^{\circ}}-.962392$</li>
</ul>
</li>
<li><p>Evolved Transformer</p>
<ul>
<li>single-cycle cosine decay</li>
</ul>
</li>
<li>every decay was paired with the same constant 0.01 warmup.</li>
<li>大模型使用高一点的dropout（0.3），小模型使用0.2 dropout</li>
<li>beam-size=6, lenth-penalty=0.6, max-output=50</li>
</ul>
</li>
<li>语言模型<ul>
<li>跟机器翻译差不多，去掉了label smooth, intra-attention dropout=0.0</li>
</ul>
</li>
<li>Search Configuration<ul>
<li>populatino=100</li>
<li>mutation=2.5%</li>
<li>fitness: negative log perplexity</li>
</ul>
</li>
</ul>
<h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><ul>
<li>最终搜索出来的模型结构</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/et2.png" alt="图片"></p>
<ul>
<li>embedding_size=768, 6 encoder, 6 decoder</li>
<li>attention_head=16</li>
<li>ET比Transformer可以在更小的模型上达到更好的效果，当模型增大时两者的差距就不大了（可能因为模型越大越容易过拟合，而且单独增加embedding_size可能不起作用，需要和depth共同增加）</li>
</ul>
]]></content>
      <tags>
        <tag>论文阅读</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读:《Towards a Human-like Open-Domain Chatbot》</title>
    <url>/2020/02/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94%E3%80%8ATowards-a-Human-like-Open-Domain-Chatbot%E3%80%8B/</url>
    <content><![CDATA[<p>今天阅读了谷歌最新出的一篇论文，《Towards a Human-like Open-Domain Chatbot》，主要提出了端到端对话机器人的一种评测方法和模型框架。下面是阅读过程中的笔记。</p>
<a id="more"></a>
<blockquote>
<p>参考：<br><a href="https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html" target="_blank" rel="noopener">https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html</a><br><a href="https://arxiv.org/pdf/2001.09977.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2001.09977.pdf</a><br><a href="https://github.com/google-research/google-research/tree/master/meena" target="_blank" rel="noopener">https://github.com/google-research/google-research/tree/master/meena</a></p>
</blockquote>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="开放的chatbot-API总结"><a href="#开放的chatbot-API总结" class="headerlink" title="开放的chatbot API总结"></a>开放的chatbot API总结</h3><ul>
<li>cleverbot API: <a href="https://www.cleverbot.com/api/" target="_blank" rel="noopener">https://www.cleverbot.com/api/</a><ul>
<li><a href="https://github.com/plasticuproject/cleverbotfree" target="_blank" rel="noopener">https://github.com/plasticuproject/cleverbotfree</a></li>
</ul>
</li>
<li>xiaobing: <a href="https://www.msxiaobing.com/" target="_blank" rel="noopener">https://www.msxiaobing.com/</a></li>
<li>mitsuku: <a href="https://www.pandorabots.com/mitsuku/" target="_blank" rel="noopener">https://www.pandorabots.com/mitsuku/</a><ul>
<li><a href="https://github.com/hanwenzhu/mitsuku-api" target="_blank" rel="noopener">https://github.com/hanwenzhu/mitsuku-api</a></li>
</ul>
</li>
</ul>
<h3 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h3><ul>
<li>模型架构：Evolved Transformer<ul>
<li>模型输入：多轮对话（最多7轮）</li>
<li>模型输出：回复</li>
<li>最佳模型：2.6B参数，10.2PPL，8K BPE subword vocabulary, 训练数据40B words</li>
</ul>
</li>
<li>评测指标<ul>
<li>PPL</li>
<li>SSA（Sensibleness and Specificity Average）用来评估<ul>
<li>whether make sense</li>
<li>whether specific</li>
</ul>
</li>
<li>人工评测使用static（1477个多轮对话）和interactive（想说啥就说啥）两种数据集，发现SSA和PPL在这两个数据集上高度相关</li>
<li>模型在评测集的表现：<ul>
<li>0.72的SSA</li>
<li>经过filtering mechanism 和 tuned decoding后有0.79的SSA，相比于人提供的0.86SSA的回复已经很接近了</li>
</ul>
</li>
</ul>
</li>
<li>方法的局限性<ul>
<li>评测数据集的局限性，不能解决所有领域的问题</li>
</ul>
</li>
</ul>
<h2 id="对话机器人的评价"><a href="#对话机器人的评价" class="headerlink" title="对话机器人的评价"></a>对话机器人的评价</h2><h3 id="人工进行评测时的参考标准"><a href="#人工进行评测时的参考标准" class="headerlink" title="人工进行评测时的参考标准"></a>人工进行评测时的参考标准</h3><ul>
<li>Sensibleness<ul>
<li>common sense</li>
<li>logical coherence</li>
<li>consistency</li>
<li>人工评测时对于可打的标签：confusing, illogical, out of context, factually wrong, make sense</li>
<li>缺陷：对于安全的回答，如I don’t know，无法区分</li>
</ul>
</li>
<li>Specificity<ul>
<li>A: I love tennis.   B: That’s nice 应该被标记为not specific，如果 B：Me too, I can’t get enough of Roger Federer!则被标记为specific</li>
<li>已经被标记为not sensible的直接标记为not specific</li>
</ul>
</li>
<li>SSA<ul>
<li>可以使用Sensibleness和Specificity标记在所有responses的比例来作为参考标准</li>
<li>使用SSA将Sensibleness和Specificity的比例进行了结合</li>
</ul>
</li>
</ul>
<h3 id="可进行对比的几个开源chatbot框架"><a href="#可进行对比的几个开源chatbot框架" class="headerlink" title="可进行对比的几个开源chatbot框架"></a>可进行对比的几个开源chatbot框架</h3><ul>
<li>基于RNN：<a href="https://github.com/lukalabs/cakechat" target="_blank" rel="noopener">https://github.com/lukalabs/cakechat</a></li>
<li>基于Transformer: <a href="https://github.com/microsoft/DialoGPT" target="_blank" rel="noopener">https://github.com/microsoft/DialoGPT</a><ul>
<li>762M参数的模型效果更好一些</li>
<li>dialogpt没有公开其解码和MMI-reranking的过程，gpt2bot实现了解码：<a href="https://github.com/polakowo/gpt2bot" target="_blank" rel="noopener">https://github.com/polakowo/gpt2bot</a></li>
<li>附加一个中文的基于DialoGPT开发的闲聊模型<ul>
<li><a href="https://github.com/yangjianxin1/GPT2-chitchat" target="_blank" rel="noopener">https://github.com/yangjianxin1/GPT2-chitchat</a></li>
<li><a href="https://blog.csdn.net/kingsonyoung/article/details/103803067" target="_blank" rel="noopener">https://blog.csdn.net/kingsonyoung/article/details/103803067</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="构建静态评测集"><a href="#构建静态评测集" class="headerlink" title="构建静态评测集"></a>构建静态评测集</h3><ul>
<li>从单轮开始：<a href="http://ai.stanford.edu/~quocle/QAresults.pdf" target="_blank" rel="noopener">http://ai.stanford.edu/~quocle/QAresults.pdf</a></li>
<li>增加一些个性化问题，如：Do you like cats?<ul>
<li>A: Do you like movies?; B: Yeah. I like sci-fi mostly; A: Really? Which is your favorite?期待I love Back to the Future这样的回答，对于I don’t like movies这样的回复应标记为not sensible</li>
</ul>
</li>
</ul>
<h3 id="进行动态评测"><a href="#进行动态评测" class="headerlink" title="进行动态评测"></a>进行动态评测</h3><ul>
<li>机器人以Hi开始，评测人员自由与bot对话，并对每一个bot的回复进行评测。每一个对话至少14轮，至多28轮。</li>
</ul>
<h2 id="Meena-Chatbot"><a href="#Meena-Chatbot" class="headerlink" title="Meena Chatbot"></a>Meena Chatbot</h2><h3 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h3><ul>
<li>来源于public social media</li>
<li>清洗流程<ul>
<li>去掉 subword 数目&lt;=2 或 subword 数目 &gt;= 128</li>
<li>去掉 字母比例&lt;0.7</li>
<li>去掉 包含URL</li>
<li>去掉 作者名字bot</li>
<li>去掉 出现100次以上</li>
<li>去掉 跟上文n-gram重复比例过高</li>
<li>去掉 敏感句子</li>
<li>去掉 括号中内容</li>
<li>当一个句子被删除时，则上文全部被删除</li>
</ul>
</li>
<li>共清洗出867M的(context, response)对</li>
<li>使用sentence piece进行BPE分词，得到8K的BPE vocab</li>
<li>最终语料包含341GB的语料(40B word)</li>
</ul>
<h3 id="模型框架"><a href="#模型框架" class="headerlink" title="模型框架"></a>模型框架</h3><ul>
<li>Evolved Transformer<ul>
<li>2.6B parameter</li>
<li>1 ET encoder + 13 ET decoder</li>
</ul>
</li>
<li>最大的模型可达到10.2的PPL</li>
<li>最大的传统Transformer模型（32层decoder）可达到10.7的PPL</li>
<li>hidden size: 2560</li>
<li>attention head: 32</li>
<li>共享编码、解码、softmax的embedding</li>
<li>编码、解码最长是128</li>
</ul>
<h3 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h3><ul>
<li>使用Adafactor optimizer，初始学习率0.01，在前10k step保持不变，使用inverse square root of the number of steps进行衰减</li>
<li>使用<a href="https://github.com/tensorflow/" target="_blank" rel="noopener">https://github.com/tensorflow/</a>tensor2tensor代码进行训练</li>
</ul>
<h3 id="解码细节"><a href="#解码细节" class="headerlink" title="解码细节"></a>解码细节</h3><ul>
<li><p>为了避免产生乏味的回复，可以使用多种方法进行解码</p>
<ul>
<li>reranking</li>
<li>基于profiles, topics, and styles</li>
<li>强化学习</li>
<li>变分自编吗</li>
</ul>
</li>
<li><p>当PPL足够小时，可以使用sample-and-rank策略进行解码</p>
<ul>
<li><p>使用temperature T随机产生N个独立的候选</p>
<ul>
<li><p>$p_{i}=\frac{\exp \left(z_{i} / T\right)}{\sum_{j} \exp \left(z_{j} / T\right)}$</p>
</li>
<li><p>T=1产生不经过修正的分布</p>
</li>
<li><p>T越大，越容易产生不常见的词，如相关的实体名词，但可能产生错误的词</p>
</li>
<li><p>T越小，越容易产生常见的词，如冠词或介词，虽然安全但不specific</p>
</li>
<li><p>解释1</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">温度是神经网络的超参数，用于在应用softmax之前通过缩放对数来控制预测的随机性。 例如，在TensorFlow的LSTM中，温度代表在计算softmax之前将logit除以多少。</span><br><span class="line"></span><br><span class="line">当温度为1时，我们直接在logits（较早层的未缩放输出）上计算softmax，并使用温度为0.6的模型在logits&#x2F;0.6上计算softmax，从而得出较大的值。 在更大的值上执行softmax可使LSTM 更加自信 （需要较少的输入来激活输出层），但在其样本中也更加保守 （从不太可能的候选样本中进行抽样的可能性较小）。 使用较高的温度会在各个类上产生较软的概率分布，并使RNN更容易被样本“激发”，从而导致更多的多样性和更多的错误 。</span><br><span class="line"></span><br><span class="line">softmax函数通过确保网络输出在每个时间步长都在零到一之间，基于其指数值对候选网络的每次迭代进行归一化。</span><br><span class="line"></span><br><span class="line">因此，温度增加了对低概率候选者的敏感性。</span><br></pre></td></tr></table></figure>
</li>
<li><p>解释2</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">当T很大时，即趋于正无穷时，所有的激活值对应的激活概率趋近于相同（激活概率差异性较小）；而当T很低时，即趋于0时，不同的激活值对应的激活概率差异也就越大。</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
<li><p>发现使用beam-search解码会产生重复且无趣的回复，使用sample-and-rank产生的回复会丰富一些</p>
</li>
<li>使用N=20，T=0.88</li>
<li>response score的计算：logP/T，P是response的likelihood，T是token的个数</li>
<li><p>解码时增加detect cross turn repetitions</p>
<ul>
<li>当两个turn的n-gram重复超过一定比例时，则从候选中删除</li>
</ul>
</li>
<li>增加一个分类层，用来过滤掉敏感回复</li>
</ul>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><h3 id="SSA和PPL是相关的"><a href="#SSA和PPL是相关的" class="headerlink" title="SSA和PPL是相关的"></a>SSA和PPL是相关的</h3><ul>
<li>基本呈线性关系</li>
</ul>
<p><img src="http://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/tod1.png" alt="图片"></p>
<h3 id="效果的比较"><a href="#效果的比较" class="headerlink" title="效果的比较"></a>效果的比较</h3><ul>
<li>小冰：呈现出个性化的回复，但有时也会无意义，且经常回复得太平常。小冰另一个特点就是具有同情心，可以在以后的评价指标中考虑这一点。小冰有near-human-level engagingness但not very close to human-level humanness，因此在我们的评测指标上SSA不高。</li>
<li>mitsuku：56%SSA（72%sensibility 40%specifity）, 网站上的对话并不是它参加图灵测试的版本</li>
<li>DialoGPT：48%SSA（57%sensibility 49%specifity）</li>
<li>CleverBot：在interactive评测表现比static上稍微好一些（56% interactive SSA，44% static SSA）。发现cleverbot更擅长将话题引入到它更擅长的领域中，缺少personality</li>
<li>Meena：base（72% SSA），full（79% SSA）</li>
</ul>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>论文阅读</tag>
        <tag>Chatbot</tag>
      </tags>
  </entry>
  <entry>
    <title>各类资源定期汇总</title>
    <url>/2020/02/01/%E5%90%84%E7%B1%BB%E8%B5%84%E6%BA%90%E5%AE%9A%E6%9C%9F%E6%B1%87%E6%80%BB/</url>
    <content><![CDATA[<p>一些学习等资源的总结，不定期更新。</p>
<a id="more"></a>
<h1 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h1><h2 id="视频类资源"><a href="#视频类资源" class="headerlink" title="视频类资源"></a>视频类资源</h2><h3 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h3><ul>
<li>吴恩达机器学习：<a href="https://study.163.com/course/courseMain.htm?courseId=1004570029" target="_blank" rel="noopener">https://study.163.com/course/courseMain.htm?courseId=1004570029</a></li>
<li>统计学习基础：链接: <a href="https://pan.baidu.com/s/1NWE6lEJrSgOFAEVVOW2TmQ" target="_blank" rel="noopener">https://pan.baidu.com/s/1NWE6lEJrSgOFAEVVOW2TmQ</a> 提取码: g7km</li>
<li>林轩田机器学习：<a href="https://www.tinymind.cn/articles/168" target="_blank" rel="noopener">https://www.tinymind.cn/articles/168</a></li>
</ul>
<h3 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h3><ul>
<li>花书：<a href="https://www.bilibili.com/video/av69236102?from=search&amp;seid=4183604428283884229" target="_blank" rel="noopener">https://www.bilibili.com/video/av69236102?from=search&amp;seid=4183604428283884229</a></li>
</ul>
<h3 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h3><ul>
<li>斯坦福自然语言处理：<a href="https://zhuanlan.zhihu.com/p/63199665" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/63199665</a></li>
<li>宗成庆统计自然语言处理：<ul>
<li><a href="https://space.bilibili.com/373951238" target="_blank" rel="noopener">https://space.bilibili.com/373951238</a></li>
<li><a href="https://space.bilibili.com/303667813/video" target="_blank" rel="noopener">https://space.bilibili.com/303667813/video</a></li>
</ul>
</li>
</ul>
<h2 id="CV"><a href="#CV" class="headerlink" title="CV"></a>CV</h2><ul>
<li>CS231n计算机视觉：<a href="https://www.bilibili.com/video/av77752864/" target="_blank" rel="noopener">https://www.bilibili.com/video/av77752864/</a></li>
</ul>
<h3 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h3><ul>
<li>David Silver强化学习：<a href="https://space.bilibili.com/74997410?spm_id_from=333.788.b_765f7570696e666f.2" target="_blank" rel="noopener">https://space.bilibili.com/74997410?spm_id_from=333.788.b_765f7570696e666f.2</a></li>
</ul>
<h2 id="博客类资源"><a href="#博客类资源" class="headerlink" title="博客类资源"></a>博客类资源</h2><ul>
<li>科学空间：<a href="https://kexue.fm/" target="_blank" rel="noopener">https://kexue.fm/</a></li>
</ul>
<h2 id="学习笔记类资源"><a href="#学习笔记类资源" class="headerlink" title="学习笔记类资源"></a>学习笔记类资源</h2><h3 id="机器学习-1"><a href="#机器学习-1" class="headerlink" title="机器学习"></a>机器学习</h3><ul>
<li>吴恩达机器学习笔记：<a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes" target="_blank" rel="noopener">https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes</a></li>
<li>统计学习基础笔记：<a href="https://github.com/SmirkCao/Lihang" target="_blank" rel="noopener">https://github.com/SmirkCao/Lihang</a></li>
<li>百面机器学习：<a href="https://github.com/Relph1119/QuestForMachineLearning-Camp" target="_blank" rel="noopener">https://github.com/Relph1119/QuestForMachineLearning-Camp</a></li>
</ul>
<h3 id="深度学习-1"><a href="#深度学习-1" class="headerlink" title="深度学习"></a>深度学习</h3><ul>
<li>花书笔记：<a href="https://discoverml.github.io/simplified-deeplearning/" target="_blank" rel="noopener">https://discoverml.github.io/simplified-deeplearning/</a><ul>
<li>中文版图书：<a href="https://github.com/exacity/deeplearningbook-chinese" target="_blank" rel="noopener">https://github.com/exacity/deeplearningbook-chinese</a></li>
</ul>
</li>
</ul>
<h3 id="NLP-1"><a href="#NLP-1" class="headerlink" title="NLP"></a>NLP</h3><ul>
<li>斯坦福自然语言处理：<a href="https://zhuanlan.zhihu.com/p/59011576" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/59011576</a></li>
<li>宗成庆统计自然语言处理：<ul>
<li><a href="https://github.com/aicourse/ZMC301-CAS-NLP-2019" target="_blank" rel="noopener">https://github.com/aicourse/ZMC301-CAS-NLP-2019</a></li>
<li><a href="http://www.nlpr.ia.ac.cn/cip/ZongReportandLecture/ReportandLectureIndex.htm" target="_blank" rel="noopener">http://www.nlpr.ia.ac.cn/cip/ZongReportandLecture/ReportandLectureIndex.htm</a></li>
</ul>
</li>
</ul>
<h3 id="CV-1"><a href="#CV-1" class="headerlink" title="CV"></a>CV</h3><ul>
<li>CS231n计算机视觉：<ul>
<li><a href="https://zhuanlan.zhihu.com/p/21930884" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21930884</a></li>
<li><a href="https://github.com/mbadry1/CS231n-2017-Summary" target="_blank" rel="noopener">https://github.com/mbadry1/CS231n-2017-Summary</a></li>
</ul>
</li>
</ul>
<h3 id="强化学习-1"><a href="#强化学习-1" class="headerlink" title="强化学习"></a>强化学习</h3><ul>
<li>David Silver强化学习笔记：<a href="https://zhuanlan.zhihu.com/reinforce" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/reinforce</a></li>
</ul>
<h2 id="工具汇总"><a href="#工具汇总" class="headerlink" title="工具汇总"></a>工具汇总</h2><h3 id="NLP-2"><a href="#NLP-2" class="headerlink" title="NLP"></a>NLP</h3><ul>
<li>jialu: <a href="https://github.com/ownthink/Jiagu" target="_blank" rel="noopener">https://github.com/ownthink/Jiagu</a></li>
</ul>
<h2 id="数据集汇总"><a href="#数据集汇总" class="headerlink" title="数据集汇总"></a>数据集汇总</h2><h3 id="NLP-3"><a href="#NLP-3" class="headerlink" title="NLP"></a>NLP</h3><ul>
<li>百度：<a href="http://ai.baidu.com/broad" target="_blank" rel="noopener">http://ai.baidu.com/broad</a></li>
</ul>
<h2 id="前沿追踪类资源"><a href="#前沿追踪类资源" class="headerlink" title="前沿追踪类资源"></a>前沿追踪类资源</h2><h3 id="NLP-4"><a href="#NLP-4" class="headerlink" title="NLP"></a>NLP</h3><ul>
<li><a href="https://github.com/sebastianruder/NLP-progress" target="_blank" rel="noopener">https://github.com/sebastianruder/NLP-progress</a></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>用GitHub+Hexo搭建个人网站</title>
    <url>/2020/02/01/%E7%94%A8GitHub-Hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/</url>
    <content><![CDATA[<p>假期宅在家里，研究了一下用github搭建个人网站，把里面使用到的工具和命令总结一下。相关代码可参考：<a href="https://github.com/majing2019/myblogs" target="_blank" rel="noopener">https://github.com/majing2019/myblogs</a><br><a id="more"></a></p>
<h1 id="安装相关软件"><a href="#安装相关软件" class="headerlink" title="安装相关软件"></a>安装相关软件</h1><blockquote>
<p>参考：<br><a href="https://zhuanlan.zhihu.com/p/26625249" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26625249</a><br><a href="https://zhuanlan.zhihu.com/p/62555815" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/62555815</a></p>
</blockquote>
<ul>
<li>npm install -g hexo-cli</li>
<li>hexo init blog</li>
<li>cd ~/blog</li>
<li>export CC=/usr/bin/clang</li>
<li>export CXX=/usr/bin/clang++</li>
<li>npm install</li>
<li>npm install hexo-server —save</li>
<li>hexo server</li>
<li>在<a href="http://localhost:4000" target="_blank" rel="noopener">http://localhost:4000</a>访问网站首页</li>
<li>npm install hexo-deployer-git —save</li>
</ul>
<h1 id="建立repository"><a href="#建立repository" class="headerlink" title="建立repository"></a>建立repository</h1><blockquote>
<p>参考：<br><a href="https://help.github.com/en/github/working-with-github-pages" target="_blank" rel="noopener">https://help.github.com/en/github/working-with-github-pages</a><br><a href="https://github.community/t5/GitHub-Pages/404-Error/td-p/14331" target="_blank" rel="noopener">https://github.community/t5/GitHub-Pages/404-Error/td-p/14331</a></p>
</blockquote>
<ul>
<li>创建username.github.io的repository</li>
<li>在Settings-&gt;Github Pages中升级账户</li>
</ul>
<h1 id="修改相关配置"><a href="#修改相关配置" class="headerlink" title="修改相关配置"></a>修改相关配置</h1><ul>
<li>修改_config.yml<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: &lt;repository url&gt; #git@github.com:sufaith&#x2F;sufaith.github.io.git</span><br><span class="line">  branch: [branch] #master</span><br><span class="line">  message: [message]</span><br><span class="line">url: majing2019.github.io</span><br></pre></td></tr></table></figure></li>
<li>在source文件夹下创建CNAME文件，内容为二级域名</li>
<li>在~/blog目录下运行hexo generate</li>
<li>hexo clean &amp;&amp; hexo deploy</li>
<li>访问 <a href="https://majing2019.github.io/archives/" target="_blank" rel="noopener">https://majing2019.github.io/archives/</a></li>
</ul>
<h1 id="绑定域名"><a href="#绑定域名" class="headerlink" title="绑定域名"></a>绑定域名</h1><ul>
<li>登录<a href="https://www.aliyun.com/" target="_blank" rel="noopener">https://www.aliyun.com/</a>注册了一个域名majsunflower.cn</li>
<li>添加一个域名解析<ul>
<li>类型CNAME，主机记录www，记录值majing2019.github.io</li>
<li>类型A，主机记录@，记录值是对应的ip地址，可通过ping majing2019.github.io获得</li>
</ul>
</li>
<li>在github仓库中设置custom domain</li>
<li>在blog下创建source/CNAME文件，并写入majsunflower.cn</li>
</ul>
<h1 id="编写自己的个性化网站"><a href="#编写自己的个性化网站" class="headerlink" title="编写自己的个性化网站"></a>编写自己的个性化网站</h1><blockquote>
<p>参考：<br><a href="https://zhwangart.github.io/2018/11/30/Ocean/" target="_blank" rel="noopener">https://zhwangart.github.io/2018/11/30/Ocean/</a></p>
</blockquote>
<h2 id="更换主题"><a href="#更换主题" class="headerlink" title="更换主题"></a>更换主题</h2><ul>
<li>在<a href="https://hexo.io/themes/" target="_blank" rel="noopener">https://hexo.io/themes/</a>中选择一个主题</li>
<li>git clone <a href="https://github.com/zhwangart/hexo-theme-ocean.git" target="_blank" rel="noopener">https://github.com/zhwangart/hexo-theme-ocean.git</a> themes/ocean</li>
<li>修改_config.yml中theme为ocean</li>
</ul>
<h2 id="配置语言"><a href="#配置语言" class="headerlink" title="配置语言"></a>配置语言</h2><ul>
<li>_config.yml中language改为zh-CN</li>
</ul>
<h2 id="评论功能"><a href="#评论功能" class="headerlink" title="评论功能"></a>评论功能</h2><blockquote>
<p>参考：<br><a href="https://zhwangart.github.io/2018/12/06/Gitalk/" target="_blank" rel="noopener">https://zhwangart.github.io/2018/12/06/Gitalk/</a></p>
</blockquote>
<ul>
<li>在<a href="https://github.com/settings/applications/new" target="_blank" rel="noopener">https://github.com/settings/applications/new</a>申请<ul>
<li>后续可在<a href="https://github.com/settings/developers" target="_blank" rel="noopener">https://github.com/settings/developers</a>中修改app相关内容</li>
<li>注意Authorization callback URL在网站绑定域名后需要写域名</li>
</ul>
</li>
<li>填写themes/ocean/_config.yml中gitalk相关字段</li>
</ul>
<h2 id="使用图床"><a href="#使用图床" class="headerlink" title="使用图床"></a>使用图床</h2><blockquote>
<p>参考：<br><a href="https://zhuanlan.zhihu.com/p/26625249" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26625249</a><br><a href="https://blog.csdn.net/qq_36305327/article/details/71578290" target="_blank" rel="noopener">https://blog.csdn.net/qq_36305327/article/details/71578290</a></p>
</blockquote>
<ul>
<li><p>到<a href="https://www.qiniu.com/" target="_blank" rel="noopener">https://www.qiniu.com/</a>上添加对象存储<a href="https://portal.qiniu.com/kodo/bucket/" target="_blank" rel="noopener">https://portal.qiniu.com/kodo/bucket/</a></p>
</li>
<li><p>在markdown中可直接引用图片</p>
</li>
</ul>
<h2 id="添加关于"><a href="#添加关于" class="headerlink" title="添加关于"></a>添加关于</h2><ul>
<li>hexo new page about</li>
<li>使用markdown编写source/about/index.md</li>
</ul>
<h2 id="添加标签"><a href="#添加标签" class="headerlink" title="添加标签"></a>添加标签</h2><ul>
<li>hexo new page tags // 创建标签页面</li>
<li>修改source/tags/index.md为<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: Tags</span><br><span class="line">date: 2019-04-19 17:28:54</span><br><span class="line">type: tags</span><br><span class="line">layout: &quot;tags&quot;</span><br><span class="line">---</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="添加相册"><a href="#添加相册" class="headerlink" title="添加相册"></a>添加相册</h2><ul>
<li>hexo new page gallery</li>
<li>编辑source/gallery/index.md<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: Gallery</span><br><span class="line">albums: [</span><br><span class="line">        [&quot;img_url&quot;,&quot;img_caption&quot;],</span><br><span class="line">        [&quot;img_url&quot;,&quot;img_caption&quot;]</span><br><span class="line">        ]</span><br><span class="line">---</span><br></pre></td></tr></table></figure></li>
<li>如果出现相册加载过慢的问题，可以参考<a href="https://zhwangart.github.io/2019/07/02/Ocean-Issues/" target="_blank" rel="noopener">https://zhwangart.github.io/2019/07/02/Ocean-Issues/</a>解决</li>
</ul>
<h2 id="添加分类"><a href="#添加分类" class="headerlink" title="添加分类"></a>添加分类</h2><ul>
<li>hexo new page categories</li>
</ul>
<h2 id="本地搜索"><a href="#本地搜索" class="headerlink" title="本地搜索"></a>本地搜索</h2><blockquote>
<p>参考：<br><a href="https://github.com/zhwangart/gitalk/issues/7#issuecomment-451877736" target="_blank" rel="noopener">https://github.com/zhwangart/gitalk/issues/7#issuecomment-451877736</a></p>
</blockquote>
<ul>
<li>npm install hexo-generator-searchdb —save</li>
<li>在blog/_config.yml中添加配置<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">search:</span><br><span class="line">  path: search.xml</span><br><span class="line">  field: post</span><br><span class="line">  content: true</span><br></pre></td></tr></table></figure></li>
<li>hexo g</li>
<li><del>修改themes/ocean/layout/_partial/after-footer.ejs中修改如下内容</del><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;% if (theme.local_search.enable)&#123; %&gt;</span><br><span class="line">  &lt;%- js(&#39;&#x2F;js&#x2F;search&#39;) %&gt;</span><br><span class="line">&lt;% &#125; %&gt;</span><br><span class="line"></span><br><span class="line">&lt;%- js(&#39;&#x2F;js&#x2F;ocean&#39;) %&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="用Markdown写文章"><a href="#用Markdown写文章" class="headerlink" title="用Markdown写文章"></a>用Markdown写文章</h2><h3 id="创建文章"><a href="#创建文章" class="headerlink" title="创建文章"></a>创建文章</h3><blockquote>
<p>参考：<br><a href="https://chaxiaoniu.oschina.io/2017/07/10/Markdown-Grammar/" target="_blank" rel="noopener">https://chaxiaoniu.oschina.io/2017/07/10/Markdown-Grammar/</a></p>
</blockquote>
<ul>
<li>hexo new “用GitHub+Hexo搭建个人网站”</li>
<li>文章格式如下<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: 用GitHub+Hexo搭建个人网站 #文章页面上的显示名称，可以任意修改</span><br><span class="line">date: date  #文章生成时间，一般不改，当然也可以任意修改</span><br><span class="line">tags: [Hexo, Ocean] #文章标签，可空。也可以按照你的习惯写分类名字，注意后面有空格，多个标签可以用[]包含，以&#96;,&#96;隔开</span><br><span class="line">categories: [技术] #分类</span><br><span class="line">---</span><br><span class="line">这里是你博客列表显示的摘要文字</span><br><span class="line">&lt;!--more--&gt;</span><br><span class="line">以下是博客的正文，以上面的格式为分隔线</span><br></pre></td></tr></table></figure></li>
<li>如果不希望显示时有目录，需要添加<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">toc: false</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="添加公式"><a href="#添加公式" class="headerlink" title="添加公式"></a>添加公式</h3><blockquote>
<p>参考：<br><a href="https://blog.csdn.net/Aoman_Hao/article/details/81381507" target="_blank" rel="noopener">https://blog.csdn.net/Aoman_Hao/article/details/81381507</a></p>
</blockquote>
<ul>
<li>npm uninstall hexo-renderer-marked —save</li>
<li>npm install hexo-renderer-kramed —save</li>
<li>修改node_modules/hexo-renderer-kramed/lib/renderer.js<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">function formatText(text) &#123;</span><br><span class="line">  &#x2F;&#x2F; Fit kramed&#39;s rule: $$ + \1 + $$</span><br><span class="line">  &#x2F;&#x2F; return text.replace(&#x2F;&#96;\$(.*?)\$&#96;&#x2F;g, &#39;$$$$$1$$$$&#39;);</span><br><span class="line">  return text;&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>npm uninstall hexo-math —save</p>
</li>
<li><p>npm install hexo-renderer-mathjax —save</p>
</li>
<li>修改node_modules/hexo-renderer-mathjax/mathjax.html，注释掉最后一行script并改为<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;script src&#x3D;&quot;https:&#x2F;&#x2F;cdnjs.cloudflare.com&#x2F;ajax&#x2F;libs&#x2F;mathjax&#x2F;2.7.1&#x2F;MathJax.js?config&#x3D;TeX-MML-AM_CHTML&quot;&gt;&lt;&#x2F;script&gt;</span><br></pre></td></tr></table></figure></li>
<li>修改node_modules/kramed/lib/rules/inline.js<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">escape: &#x2F;^\\([&#96;*\[\]()# +\-.!_&gt;])&#x2F;,</span><br><span class="line">em: &#x2F;^\*((?:\*\*|[\s\S])+?)\*(?!\*)&#x2F;,</span><br></pre></td></tr></table></figure></li>
<li>修改themes/ocean/_config.yml增加<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mathjax: true</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="添加文章封面"><a href="#添加文章封面" class="headerlink" title="添加文章封面"></a>添加文章封面</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: Post name</span><br><span class="line">photos: [</span><br><span class="line">        [&quot;img_url&quot;],</span><br><span class="line">        [&quot;img_url&quot;]</span><br><span class="line">        ]</span><br><span class="line">---</span><br></pre></td></tr></table></figure>
<h3 id="添加视频"><a href="#添加视频" class="headerlink" title="添加视频"></a>添加视频</h3><blockquote>
<p>参考：<br><a href="https://blog.csdn.net/u010953692/article/details/79075884" target="_blank" rel="noopener">https://blog.csdn.net/u010953692/article/details/79075884</a><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;iframe height&#x3D;498 width&#x3D;510 src&#x3D;&quot;http:&#x2F;&#x2F;q503tsu73.bkt.clouddn.com&#x2F;IMG_0018.mp4?e&#x3D;1580557032&amp;token&#x3D;05Ii263bPN3Z-CT3JPRaRfWi5sXIj8pwX6V1bN2j:-rUb7zOxk-WfRrhdJtNdOOGfy58&#x3D;&amp;attname&#x3D;&quot; frameborder&#x3D;0 allowfullscreen&gt;&lt;&#x2F;iframe&gt;</span><br></pre></td></tr></table></figure></p>
</blockquote>
<h3 id="文章置顶"><a href="#文章置顶" class="headerlink" title="文章置顶"></a>文章置顶</h3><ul>
<li>npm uninstall hexo-generator-index —save</li>
<li>npm install hexo-generator-index-pin-top —save</li>
<li>在需要置顶的文章上加入<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line"> title: 新增文章置顶</span><br><span class="line"> top: ture</span><br><span class="line"> ---</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="同时部署在Github和Coding上"><a href="#同时部署在Github和Coding上" class="headerlink" title="同时部署在Github和Coding上"></a>同时部署在Github和Coding上</h1><blockquote>
<p>参考：<br><a href="https://tomatoro.cn/archives/3de92cb5.html" target="_blank" rel="noopener">https://tomatoro.cn/archives/3de92cb5.html</a></p>
</blockquote>
<ul>
<li><a href="https://coding.net/" target="_blank" rel="noopener">https://coding.net/</a>上创建devops项目</li>
<li>修改blog/_config.yml中的deploy<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: </span><br><span class="line">    github: git@github.com:majing2019&#x2F;majing2019.github.io.git</span><br><span class="line">    coding: git@e.coding.net:majsunflower&#x2F;myblog.git</span><br><span class="line">  branch: master</span><br><span class="line">  message: my blog</span><br></pre></td></tr></table></figure></li>
<li>将id_rsa.pub的公钥复制到个人账户下，ssh -T git@git.coding.net验证是否成功</li>
<li>hexo deploy -g部署到coding上</li>
<li>配置静态页面即可访问：<a href="http://02ss3u.coding-pages.com/" target="_blank" rel="noopener">https://02ss3u.coding-pages.com/</a></li>
<li>在自定义域名里增加：<a href="http://majsunflower.cn/">majsunflower.cn</a></li>
<li>阿里云<a href="https://homenew.console.aliyun.com/" target="_blank" rel="noopener">https://homenew.console.aliyun.com/</a>中修改域名相关配置，区分境内和境外的访问</li>
</ul>
<h1 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h1><ul>
<li><p>部署：hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</p>
</li>
<li><p>本地测试：hexo server</p>
</li>
</ul>
]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Ocean</tag>
      </tags>
  </entry>
</search>

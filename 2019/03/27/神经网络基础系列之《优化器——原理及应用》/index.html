<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  
  
  
    <meta name="description" content="记录生活">
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <title>
    神经网络基础系列之《优化器——原理及应用》 |
    
    大嘴怪的小世界</title>
  
    <link rel="shortcut icon" href="/favicon.ico">
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<script src="/js/pace.min.js"></script>

<meta name="generator" content="Hexo 4.2.1"></head>

<body>
<main class="content">
  <section class="outer">
  

<article id="post-神经网络基础系列之《优化器——原理及应用》" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      神经网络基础系列之《优化器——原理及应用》
    </h1>
  
  




      </header>
    

    
      <div class="article-meta">
        <a href="/2019/03/27/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8A%E4%BC%98%E5%8C%96%E5%99%A8%E2%80%94%E2%80%94%E5%8E%9F%E7%90%86%E5%8F%8A%E5%BA%94%E7%94%A8%E3%80%8B/" class="article-date">
  <time datetime="2019-03-27T14:49:45.000Z" itemprop="datePublished">2019-03-27</time>
</a>
        
      </div>
    

    
      
    <div class="tocbot"></div>





    

    <div class="article-entry" itemprop="articleBody">
      


      

      
        <p>神经网络在每次迭代中，梯度下降根据⾃变量当前位置，沿着当前位置的梯度更新⾃变量。然而，如果⾃变量的 迭代⽅向仅仅取决于⾃变量当前位置，这可能会带来⼀些问题。比如下图：</p>
<a id="more"></a>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/optimizer/1.png" alt="fig1"></p>
<p>给定学习率，梯度下降迭代⾃变量时会使⾃变量在竖直⽅向⽐在⽔平⽅向移动幅度更⼤。那么，我们需要⼀个较小的学习率从而避免⾃变量在竖直⽅向上越过⽬标函数最优解。然而，这会造成⾃变量在⽔平⽅向上朝最优解移动变慢。</p>
<p>为了解决梯度更新的问题，提出了很多优化器算法，下面一一介绍。</p>
<p>给定学习率，梯度下降迭代⾃变量时会使⾃变量在竖直⽅向⽐在⽔平⽅向移动幅度更⼤。那么，我们需要⼀个较小的学习率从而避免⾃变量在竖直⽅向上越过⽬标函数最优解。然而，这会造成⾃变量在⽔平⽅向上朝最优解移动变慢。</p>
<p>为了解决梯度更新的问题，提出了很多优化器算法，下面一一介绍。</p>
<h2 id="SGD随机梯度下降"><a href="#SGD随机梯度下降" class="headerlink" title="SGD随机梯度下降"></a>SGD随机梯度下降</h2><p>就是对minibtch做梯度下降。其优缺点如下：</p>
<ul>
<li><p>大部分时候你向着全局最小值靠近，有时候你会远离最小值，因为那个样本恰好给你指的方向不对，因此随机梯度下降法是有很多噪声的，平均来看，它最终会靠近最小值，不过有时候也会方向错误，因为随机梯度下降法永远不会收敛，而是会一直在最小值附近波动。一次性只处理了一个训练样本，这样效率过于低下。</p>
</li>
<li><p>实践中最好选择不大不小的 mini-batch，得到了大量向量化，效率高，收敛快。</p>
</li>
</ul>
<p>下面以LR算法为例，说明SGD的计算过程。</p>
<ul>
<li><p>模拟数据并归一化</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">x &#x3D; [30,35,37,59,70,76,88,100]</span><br><span class="line">y &#x3D; [1100,1423,1377,1800,2304,2588,3495,4839]</span><br><span class="line"></span><br><span class="line">x_max &#x3D; max(x)</span><br><span class="line">x_min &#x3D; min(x)</span><br><span class="line">y_max &#x3D; max(y)</span><br><span class="line">y_min &#x3D; min(y)</span><br><span class="line"></span><br><span class="line">for i in range(0,len(x)):</span><br><span class="line">    x[i] &#x3D; (x[i] - x_min)&#x2F;(x_max - x_min)</span><br><span class="line">    y[i] &#x3D; (y[i] - y_min)&#x2F;(y_max - y_min)</span><br><span class="line">    </span><br><span class="line">print (x)</span><br><span class="line">print (y)</span><br></pre></td></tr></table></figure>
</li>
<li><p>数据等高线</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">def calc_loss(a,b,x,y):</span><br><span class="line">    tmp &#x3D; y - (a * x + b)</span><br><span class="line">    tmp &#x3D; tmp ** 2  # 对矩阵内的每一个元素平方</span><br><span class="line">    SSE &#x3D; sum(tmp) &#x2F; (2 * len(x))</span><br><span class="line">    return SSE</span><br><span class="line"></span><br><span class="line">def draw_hill(x,y):</span><br><span class="line">    a &#x3D; np.linspace(-20,20,100)</span><br><span class="line">    b &#x3D; np.linspace(-20,20,100)</span><br><span class="line">    x &#x3D; np.array(x)</span><br><span class="line">    y &#x3D; np.array(y)</span><br><span class="line"></span><br><span class="line">    allSSE &#x3D; np.zeros(shape&#x3D;(len(a),len(b)))</span><br><span class="line">    for ai in range(0,len(a)):</span><br><span class="line">        for bi in range(0,len(b)):</span><br><span class="line">            a0 &#x3D; a[ai]</span><br><span class="line">            b0 &#x3D; b[bi]</span><br><span class="line">            SSE &#x3D; calc_loss(a&#x3D;a0,b&#x3D;b0,x&#x3D;x,y&#x3D;y)</span><br><span class="line">            allSSE[ai][bi] &#x3D; SSE</span><br><span class="line"></span><br><span class="line">    a,b &#x3D; np.meshgrid(a, b)</span><br><span class="line">    return [a,b,allSSE]</span><br><span class="line"></span><br><span class="line">[ha,hb,hallSSE] &#x3D; draw_hill(x,y)</span><br><span class="line">hallSSE &#x3D; hallSSE.T# 重要，将所有的losses做一个转置。原因是矩阵是以左上角至右下角顺序排列元素，而绘图是以左下角为原点。</span><br><span class="line"></span><br><span class="line">print (ha.shape)</span><br><span class="line">print (hb.shape)</span><br><span class="line">print (hallSSE.shape)</span><br></pre></td></tr></table></figure>
</li>
<li><p>绘制学习率的曲线</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">rate &#x3D; 0.1 # learning rate</span><br><span class="line">fig &#x3D; plt.figure(1, figsize&#x3D;(12, 8))</span><br><span class="line">fig.suptitle(&#39;learning rate: %.2f method:momentum&#39;%(rate), fontsize&#x3D;15)</span><br></pre></td></tr></table></figure>
</li>
<li><p>绘制曲面</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from mpl_toolkits.mplot3d import Axes3D</span><br><span class="line">ax &#x3D; fig.add_subplot(2, 2, 1, projection&#x3D;&#39;3d&#39;)</span><br><span class="line">ax.set_top_view()</span><br><span class="line">ax.plot_surface(ha, hb, hallSSE, rstride&#x3D;2, cstride&#x3D;2, cmap&#x3D;&#39;rainbow&#39;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>绘制等高线图</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">plt.subplot(2,2,2)</span><br><span class="line">ta &#x3D; np.linspace(-20, 20, 100)</span><br><span class="line">tb &#x3D; np.linspace(-20, 20, 100)</span><br><span class="line">plt.contourf(ha,hb,hallSSE,15,alpha&#x3D;0.5,cmap&#x3D;plt.cm.hot)</span><br><span class="line">C &#x3D; plt.contour(ha,hb,hallSSE,15,colors&#x3D;&#39;black&#39;)</span><br><span class="line">plt.clabel(C,inline&#x3D;True)</span><br><span class="line">plt.xlabel(&#39;a&#39;)</span><br><span class="line">plt.ylabel(&#39;b&#39;)</span><br><span class="line"></span><br><span class="line">plt.ion() # iteration on</span><br></pre></td></tr></table></figure>
</li>
<li><p>求loss的梯度</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def da(y,y_p,x):</span><br><span class="line">    return (y-y_p)*(-x)</span><br><span class="line"></span><br><span class="line">def db(y,y_p):</span><br><span class="line">    return (y-y_p)*(-1)</span><br></pre></td></tr></table></figure>
</li>
<li><p>进行迭代训练</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">import random</span><br><span class="line"></span><br><span class="line">def shuffle_data(x,y):</span><br><span class="line">    # 随机打乱x，y的数据，并且保持x和y一一对应</span><br><span class="line">    seed &#x3D; random.random()</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    random.shuffle(x)</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    random.shuffle(y)</span><br><span class="line"></span><br><span class="line">def get_batch_data(x,y,batch&#x3D;3):</span><br><span class="line">    shuffle_data(x,y)</span><br><span class="line">    x_new &#x3D; x[0:batch]</span><br><span class="line">    y_new &#x3D; y[0:batch]</span><br><span class="line">    return [x_new,y_new]</span><br><span class="line"></span><br><span class="line">a &#x3D; 10.0</span><br><span class="line">b &#x3D; -20.0</span><br><span class="line">rate &#x3D; 0.2</span><br><span class="line"></span><br><span class="line">all_loss &#x3D; []</span><br><span class="line">all_step &#x3D; []</span><br><span class="line">last_a &#x3D; a</span><br><span class="line">last_b &#x3D; b</span><br><span class="line">for step in range(1,200):</span><br><span class="line">    loss &#x3D; 0</span><br><span class="line">    all_da &#x3D; 0</span><br><span class="line">    all_db &#x3D; 0</span><br><span class="line">    shuffle_data(x,y)</span><br><span class="line">    [x_new,y_new] &#x3D; get_batch_data(x,y,batch&#x3D;4)</span><br><span class="line">    for i in range(0,len(x_new)):</span><br><span class="line">        y_p &#x3D; a*x_new[i] + b</span><br><span class="line">        loss &#x3D; loss + (y_new[i] - y_p)*(y_new[i] - y_p)&#x2F;2</span><br><span class="line">        all_da &#x3D; all_da + da(y_new[i],y_p,x_new[i])</span><br><span class="line">        all_db &#x3D; all_db + db(y_new[i],y_p)</span><br><span class="line">    #loss_ &#x3D; calc_loss(a &#x3D; a,b&#x3D;b,x&#x3D;np.array(x),y&#x3D;np.array(y))</span><br><span class="line">    loss &#x3D; loss&#x2F;len(x_new)</span><br><span class="line"></span><br><span class="line">    # 绘制图1中的loss点</span><br><span class="line">    ax.scatter(a, b, loss, color&#x3D;&#39;black&#39;)</span><br><span class="line">    # 绘制图2中的loss点</span><br><span class="line">    plt.subplot(2,2,2)</span><br><span class="line">    plt.scatter(a,b,s&#x3D;5,color&#x3D;&#39;blue&#39;)</span><br><span class="line">    plt.plot([last_a,a],[last_b,b],color&#x3D;&#39;aqua&#39;)</span><br><span class="line">    # 绘制图3中的回归直线</span><br><span class="line">    plt.subplot(2, 2, 3)</span><br><span class="line">    plt.plot(x, y)</span><br><span class="line">    plt.plot(x, y, &#39;o&#39;)</span><br><span class="line">    x_ &#x3D; np.linspace(0, 1, 2)</span><br><span class="line">    y_draw &#x3D; a * x_ + b</span><br><span class="line">    plt.plot(x_, y_draw)</span><br><span class="line">    # 绘制图4的loss更新曲线</span><br><span class="line">    all_loss.append(loss)</span><br><span class="line">    all_step.append(step)</span><br><span class="line">    plt.subplot(2,2,4)</span><br><span class="line">    plt.plot(all_step,all_loss,color&#x3D;&#39;orange&#39;)</span><br><span class="line">    plt.xlabel(&quot;step&quot;)</span><br><span class="line">    plt.ylabel(&quot;loss&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # print(&#39;a &#x3D; %.3f,b &#x3D; %.3f&#39; % (a,b))</span><br><span class="line">    last_a &#x3D; a</span><br><span class="line">    last_b &#x3D; b</span><br><span class="line">    a &#x3D; a - rate*all_da</span><br><span class="line">    b &#x3D; b - rate*all_db</span><br><span class="line"></span><br><span class="line">    if step%1 &#x3D;&#x3D; 0:</span><br><span class="line">        print(&quot;step: &quot;, step, &quot; loss: &quot;, loss)</span><br><span class="line">        plt.show()</span><br><span class="line">        plt.pause(0.01)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="Momentum-动量算法"><a href="#Momentum-动量算法" class="headerlink" title="Momentum 动量算法"></a>Momentum 动量算法</h2><p>如果把梯度下降法想象成一个小球从山坡到山谷的过程，那么前面几篇文章的小球是这样移动的：从A点开始，计算当前A点的坡度，沿着坡度最大的方向走一段路，停下到B。在B点再看一看周围坡度最大的地方，沿着这个坡度方向走一段路，再停下。确切的来说，这并不像一个球，更像是一个正在下山的盲人，每走一步都要停下来，用拐杖来来探探四周的路，再走一步停下来，周而复始，直到走到山谷。而一个真正的小球要比这聪明多了，从A点滚动到B点的时候，小球带有一定的初速度，在当前初速度下继续加速下降，小球会越滚越快，更快的奔向谷底。momentum 动量法就是模拟这一过程来加速神经网络的优化的。可以用下图说明：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/optimizer/2.png" alt="fig2"></p>
<p>根据图示，其计算过程如下：</p>
<ul>
<li><p>A为起始点，首先计算A点的梯度∇a，然后下降到B点：$\Theta_{new}=\Theta−\alpha∇a$，其中$\Theta$为参数，$\alpha$为学习率。</p>
</li>
<li><p>到了B点需要加上A点的梯度，这里梯度需要有一个衰减值$\gamma$，推荐取0.9。这样的做法可以让早期的梯度对当前梯度的影响越来越小，如果没有衰减值，模型往往会震荡难以收敛，甚至发散。所以B点的参数更新公式是这样的：$v_t=\gamma v_{t-1} + \alpha∇b$，$\Theta_{new}=\Theta - v_t$。其中$v_{t-1}$表示之前所有步骤所累积的动量和。</p>
</li>
</ul>
<p>Momentum算法的特点如下：</p>
<ul>
<li><p>下降初期时，使用上一次参数更新，下降方向一致，乘上较大的$\gamma$能够进行很好的加速</p>
</li>
<li><p>下降中后期时，在局部最小值来回震荡的时候，$∇b-&gt;0$，$\gamma$使得更新幅度增大，跳出陷阱</p>
</li>
<li><p>在梯度改变方向的时候，$\gamma$能够减少更新 总而言之，momentum项能够在相关方向加速SGD，抑制振荡，从而加快收敛</p>
</li>
</ul>
<p>下面以LR算法为例，说明动量的计算过程：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">def da(y,y_p,x):</span><br><span class="line">    return (y-y_p)*(-x)</span><br><span class="line"></span><br><span class="line">def db(y,y_p):</span><br><span class="line">    return (y-y_p)*(-1)</span><br><span class="line"></span><br><span class="line">a &#x3D; 10.0</span><br><span class="line">b &#x3D; -20.0</span><br><span class="line">all_loss &#x3D; []</span><br><span class="line">all_step &#x3D; []</span><br><span class="line">last_a &#x3D; a</span><br><span class="line">last_b &#x3D; b</span><br><span class="line">va &#x3D; 0</span><br><span class="line">vb &#x3D; 0</span><br><span class="line">gamma &#x3D; 0.9</span><br><span class="line">for step in range(1,100):</span><br><span class="line">    loss &#x3D; 0</span><br><span class="line">    all_da &#x3D; 0</span><br><span class="line">    all_db &#x3D; 0</span><br><span class="line">    for i in range(0,len(x)):</span><br><span class="line">        y_p &#x3D; a*x[i] + b</span><br><span class="line">        loss &#x3D; loss + (y[i] - y_p)*(y[i] - y_p)&#x2F;2  </span><br><span class="line">        # loss &#x3D; (y-ax[i]-b)(y-ax[i]-b)&#x2F;2&#x3D;(y^2 + (ax[i])^2 + b^2 - 2yax[i] - 2yb + 2ax[i]b&#x2F;2</span><br><span class="line">        # 对loss中的参数a求偏导：(2ax[i]^2 - 2yx[i] + 2x[i]b)&#x2F;2 &#x3D; x[i](ax[i]+b-y) &#x3D; x[i](y[i]-y) &#x3D; -x[i](y - y[i]) </span><br><span class="line">        all_da &#x3D; all_da + da(y[i],y_p,x[i])</span><br><span class="line">        # 对loss中的参数b求偏导：(2b - 2y + 2ax[i])&#x2F;2 &#x3D; (ax[i] + b - y) &#x3D; (y - y[i]) * (-1)</span><br><span class="line">        all_db &#x3D; all_db + db(y[i],y_p)</span><br><span class="line">    #loss_ &#x3D; calc_loss(a &#x3D; a,b&#x3D;b,x&#x3D;np.array(x),y&#x3D;np.array(y))</span><br><span class="line">    loss &#x3D; loss&#x2F;len(x)</span><br><span class="line"></span><br><span class="line">    # 绘制图1中的loss点</span><br><span class="line">    ax.scatter(a, b, loss, color&#x3D;&#39;black&#39;)</span><br><span class="line">    # 绘制图2中的loss点</span><br><span class="line">    plt.subplot(2,2,2)</span><br><span class="line">    plt.scatter(a,b,s&#x3D;5,color&#x3D;&#39;blue&#39;)</span><br><span class="line">    plt.plot([last_a,a],[last_b,b],color&#x3D;&#39;aqua&#39;)</span><br><span class="line">    # 绘制图3中的回归直线</span><br><span class="line">    plt.subplot(2, 2, 3)</span><br><span class="line">    plt.plot(x, y)</span><br><span class="line">    plt.plot(x, y, &#39;o&#39;)</span><br><span class="line">    x_ &#x3D; np.linspace(0, 1, 2)</span><br><span class="line">    y_draw &#x3D; a * x_ + b</span><br><span class="line">    plt.plot(x_, y_draw)</span><br><span class="line">    # 绘制图4的loss更新曲线</span><br><span class="line">    all_loss.append(loss)</span><br><span class="line">    all_step.append(step)</span><br><span class="line">    plt.subplot(2,2,4)</span><br><span class="line">    plt.plot(all_step,all_loss,color&#x3D;&#39;orange&#39;)</span><br><span class="line">    plt.xlabel(&quot;step&quot;)</span><br><span class="line">    plt.ylabel(&quot;loss&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # print(&#39;a &#x3D; %.3f,b &#x3D; %.3f&#39; % (a,b))</span><br><span class="line">    last_a &#x3D; a</span><br><span class="line">    last_b &#x3D; b</span><br><span class="line">    va &#x3D; gamma * va+ rate*all_da</span><br><span class="line">    vb &#x3D; gamma * vb+ rate*all_db</span><br><span class="line">    a &#x3D; a - va</span><br><span class="line">    b &#x3D; b - vb</span><br><span class="line"></span><br><span class="line">    if step%1 &#x3D;&#x3D; 0:</span><br><span class="line">        print(&quot;step: &quot;, step, &quot; loss: &quot;, loss)</span><br><span class="line">        plt.show()</span><br><span class="line">        plt.pause(0.01)</span><br><span class="line">plt.show()</span><br><span class="line"># plt.pause(9999)</span><br></pre></td></tr></table></figure>
<p>总结：优化算法中，⽬标函数⾃变量的每⼀个元素在相同时间步都使⽤同⼀个学习率来⾃我迭代。在“动量法”⾥我们看到当x1和x2的梯度值有较⼤差别时，需要选择⾜够小的学习率使得⾃变量在梯度值较⼤的维度上不发散。但这样会导致⾃变量在梯度值较小的维度上迭代过慢。动量法依赖指数加权移动平均使得⾃变量的更新⽅向更加⼀致，从而降低发散的可能。</p>
<h2 id="AdaGrad算法"><a href="#AdaGrad算法" class="headerlink" title="AdaGrad算法"></a>AdaGrad算法</h2><p>AdaGrad思路基本是借鉴L2 Regularizer，不过此时调节的不是𝑊，而是𝐺𝑟𝑎𝑑𝑖𝑒𝑛𝑡。AdaGrad算法根据⾃变量在每个维度的梯度值的⼤小来调整各个维度上的学习率，从而避免统⼀的学习率难以适应所有维度的问题。</p>
<p>这个算法的优点是可以对低频的参数做较大的更新，对高频的做较小的更新，也因此，对于稀疏的数据它的表现很好，很好地提高了 SGD 的鲁棒性，例如识别 Youtube 视频里面的猫，训练 GloVe word embeddings，因为它们都是需要在低频的特征上有更大的更新。</p>
<p>它的缺点是当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到⼀个有⽤的解。具体计算过程如下：</p>
<ul>
<li><p>$s_t = s_{t-1} + g_t \odot g_t$</p>
</li>
<li><p>$x_t=x_{t-1}-\frac{\eta }{s_t + \varepsilon } \odot g_t$</p>
</li>
</ul>
<p>在时间步0，AdaGrad将$s_0$中每个元素初始化为0。在时间步t，⾸先将小批量随机梯度$g_t$按元素平⽅后累加到变量$s_t$，接着，我们将⽬标函数⾃变量中每个元素的学习率通过按元素运算重新调整⼀下。其中$\eta$是学习率，$\varepsilon$是为了维持数值稳定性而添加的常数，如10的-6次方。这⾥开⽅、除法和乘法的运算都是按元素运算的。这些按元素运算使得⽬标函数⾃变量中每个元素都分别拥有⾃⼰的学习率。一般$\varepsilon$选取0.01</p>
<p>需要强调的是，小批量随机梯度按元素平⽅的累加变量$s_t$出现在学习率的分⺟项中。因此：</p>
<ul>
<li><p>如果⽬标函数有关⾃变量中某个元素的偏导数⼀直都较⼤，那么该元素的学习率将下降较快；也就是说，在训练前期，梯度较小，使得Regularizer项很大，放大梯度。[激励阶段]</p>
</li>
<li><p>反之，如果⽬标函数有关⾃变量中某个元素的偏导数⼀直都较小，那么该元素的学习率将下降较慢；也就是说，训练后期，梯度较大，使得Regularizer项很小，缩小梯度。[惩罚阶段]</p>
</li>
</ul>
<p>可以这样理解：AdaGrad过程，是一个递推过程，次从𝜏=1，推到𝜏=𝑡，把沿路的𝐺𝑟𝑎𝑑𝑖𝑒𝑛𝑡的平方根，作为Regularizer。由于Regularizer是专门针对Gradient的，所以有利于解决Gradient Vanish/Expoloding问题。</p>
<p>下图为AdaGrad优化的一个过程：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/optimizer/3.png" alt="fig3"></p>
<p>AdaGrad的缺点是：</p>
<ul>
<li><p>由公式可以看出，仍依赖于人工设置一个全局学习率。$\eta$设置过大的话，会使regularizer过于敏感，对梯度的调节太大</p>
</li>
<li><p>中后期，分母上梯度平方的累加将会越来越大，使$g_t-&gt;0$，使得训练提前结束</p>
</li>
</ul>
<p>下面继续以LR为例，说明其计算过程。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; 10.0</span><br><span class="line">b &#x3D; -20.0</span><br><span class="line">all_loss &#x3D; []</span><br><span class="line">all_step &#x3D; []</span><br><span class="line">last_a &#x3D; a</span><br><span class="line">last_b &#x3D; b</span><br><span class="line">n &#x3D; np.array([0,0])</span><br><span class="line">epsilon &#x3D; 1e-8</span><br><span class="line">for step in range(1,500):</span><br><span class="line">    loss &#x3D; 0</span><br><span class="line">    all_da &#x3D; 0</span><br><span class="line">    all_db &#x3D; 0</span><br><span class="line">    for i in range(0,len(x)):</span><br><span class="line">        y_p &#x3D; a*x[i] + b</span><br><span class="line">        loss &#x3D; loss + (y[i] - y_p)*(y[i] - y_p)&#x2F;2</span><br><span class="line">        all_da &#x3D; all_da + da(y[i],y_p,x[i])</span><br><span class="line">        all_db &#x3D; all_db + db(y[i],y_p)</span><br><span class="line">    #loss_ &#x3D; calc_loss(a &#x3D; a,b&#x3D;b,x&#x3D;np.array(x),y&#x3D;np.array(y))</span><br><span class="line">    loss &#x3D; loss&#x2F;len(x)</span><br><span class="line"></span><br><span class="line">    # 绘制图1中的loss点</span><br><span class="line">    ax.scatter(a, b, loss, color&#x3D;&#39;black&#39;)</span><br><span class="line">    # 绘制图2中的loss点</span><br><span class="line">    plt.subplot(2,2,2)</span><br><span class="line">    plt.scatter(a,b,s&#x3D;5,color&#x3D;&#39;blue&#39;)</span><br><span class="line">    plt.plot([last_a,a],[last_b,b],color&#x3D;&#39;aqua&#39;)</span><br><span class="line">    # 绘制图3中的回归直线</span><br><span class="line">    plt.subplot(2, 2, 3)</span><br><span class="line">    plt.plot(x, y)</span><br><span class="line">    plt.plot(x, y, &#39;o&#39;)</span><br><span class="line">    x_ &#x3D; np.linspace(0, 1, 2)</span><br><span class="line">    y_draw &#x3D; a * x_ + b</span><br><span class="line">    plt.plot(x_, y_draw)</span><br><span class="line">    # 绘制图4的loss更新曲线</span><br><span class="line">    all_loss.append(loss)</span><br><span class="line">    all_step.append(step)</span><br><span class="line">    plt.subplot(2,2,4)</span><br><span class="line">    plt.plot(all_step,all_loss,color&#x3D;&#39;orange&#39;)</span><br><span class="line">    plt.xlabel(&quot;step&quot;)</span><br><span class="line">    plt.ylabel(&quot;loss&quot;)</span><br><span class="line"></span><br><span class="line">    # print(&#39;a &#x3D; %.3f,b &#x3D; %.3f&#39; % (a,b))</span><br><span class="line">    last_a &#x3D; a</span><br><span class="line">    last_b &#x3D; b</span><br><span class="line"></span><br><span class="line">    #-- 参数更新</span><br><span class="line">    n[0] &#x3D; n[0]+np.square(all_da)</span><br><span class="line">    n[1] &#x3D; n[1]+np.square(all_db)</span><br><span class="line">    rate_new &#x3D; rate&#x2F;(np.sqrt(n + epsilon))</span><br><span class="line">    print(&#39;rate_new a:&#39;,rate_new[0],&#39; b:&#39;,rate_new[1])</span><br><span class="line">    a &#x3D; a - (rate&#x2F;(np.sqrt(n[0] + epsilon)))*all_da</span><br><span class="line">    b &#x3D; b - (rate&#x2F;(np.sqrt(n[1] + epsilon)))*all_db</span><br><span class="line"></span><br><span class="line">    if step%1 &#x3D;&#x3D; 0:</span><br><span class="line">        print(&quot;step: &quot;, step, &quot; loss: &quot;, loss)</span><br><span class="line">        plt.show()</span><br><span class="line">        plt.pause(0.01)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="RMSProp算法"><a href="#RMSProp算法" class="headerlink" title="RMSProp算法"></a>RMSProp算法</h2><p>当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到⼀个有⽤的解。为了解决这⼀问题，RMSProp算法对AdaGrad算法做了⼀点小小的修改：</p>
<ul>
<li><p>$s_t = \lambda s_{t-1} + (1-\lambda)g_t \odot g_t$</p>
</li>
<li><p>$x_t=x_{t-1}-\frac{\eta }{s_t + \varepsilon } \odot g_t$</p>
</li>
</ul>
<p>因为RMSProp算法的状态变量st是对平⽅项$g_t \odot g_t$的指数加权移动平均，所以可以看作是最近$\frac{1}{1-\lambda}$个时间步的小批量随机梯度平⽅项的加权平均。如此⼀来，⾃变量每个元素的学习率在迭代过程中就不再⼀直降低（或不变）。</p>
<p>下图为RMSProp优化的一个过程：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/optimizer/4.png" alt="fig4"></p>
<p>RMSProp有如下特点：</p>
<ul>
<li><p>训练初中期，加速效果不错，很快</p>
</li>
<li><p>训练后期，反复在局部最小值附近抖动（下面实验中，如果step设置为500，可能出现loss非常大的情况）</p>
</li>
<li><p>其实RMSprop依然依赖于全局学习率</p>
</li>
<li><p>RMSprop算是Adagrad的一种发展，和Adadelta的变体，效果趋于二者之间</p>
</li>
<li><p>适合处理非平稳目标 - 对于RNN效果很好</p>
</li>
<li><p>RMSProp利用了二阶信息做了Gradient优化，在BatchNorm之后，对其需求不是很大。</p>
</li>
</ul>
<p>下面继续以LR为例，说明其计算过程。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; 10.0</span><br><span class="line">b &#x3D; -20.0</span><br><span class="line">all_loss &#x3D; []</span><br><span class="line">all_step &#x3D; []</span><br><span class="line">last_a &#x3D; a</span><br><span class="line">last_b &#x3D; b</span><br><span class="line">lambdas &#x3D; 0.9</span><br><span class="line">n &#x3D; np.array([0,0])</span><br><span class="line">epsilon &#x3D; 1e-8</span><br><span class="line">for step in range(1,200):</span><br><span class="line">    loss &#x3D; 0</span><br><span class="line">    all_da &#x3D; 0</span><br><span class="line">    all_db &#x3D; 0</span><br><span class="line">    for i in range(0,len(x)):</span><br><span class="line">        y_p &#x3D; a*x[i] + b</span><br><span class="line">        loss &#x3D; loss + (y[i] - y_p)*(y[i] - y_p)&#x2F;2</span><br><span class="line">        all_da &#x3D; all_da + da(y[i],y_p,x[i])</span><br><span class="line">        all_db &#x3D; all_db + db(y[i],y_p)</span><br><span class="line">    #loss_ &#x3D; calc_loss(a &#x3D; a,b&#x3D;b,x&#x3D;np.array(x),y&#x3D;np.array(y))</span><br><span class="line">    loss &#x3D; loss&#x2F;len(x)</span><br><span class="line"></span><br><span class="line">    # 绘制图1中的loss点</span><br><span class="line">    ax.scatter(a, b, loss, color&#x3D;&#39;black&#39;)</span><br><span class="line">    # 绘制图2中的loss点</span><br><span class="line">    plt.subplot(2,2,2)</span><br><span class="line">    plt.scatter(a,b,s&#x3D;5,color&#x3D;&#39;blue&#39;)</span><br><span class="line">    plt.plot([last_a,a],[last_b,b],color&#x3D;&#39;aqua&#39;)</span><br><span class="line">    # 绘制图3中的回归直线</span><br><span class="line">    plt.subplot(2, 2, 3)</span><br><span class="line">    plt.plot(x, y)</span><br><span class="line">    plt.plot(x, y, &#39;o&#39;)</span><br><span class="line">    x_ &#x3D; np.linspace(0, 1, 2)</span><br><span class="line">    y_draw &#x3D; a * x_ + b</span><br><span class="line">    plt.plot(x_, y_draw)</span><br><span class="line">    # 绘制图4的loss更新曲线</span><br><span class="line">    all_loss.append(loss)</span><br><span class="line">    all_step.append(step)</span><br><span class="line">    plt.subplot(2,2,4)</span><br><span class="line">    plt.plot(all_step,all_loss,color&#x3D;&#39;orange&#39;)</span><br><span class="line">    plt.xlabel(&quot;step&quot;)</span><br><span class="line">    plt.ylabel(&quot;loss&quot;)</span><br><span class="line"></span><br><span class="line">    # print(&#39;a &#x3D; %.3f,b &#x3D; %.3f&#39; % (a,b))</span><br><span class="line">    last_a &#x3D; a</span><br><span class="line">    last_b &#x3D; b</span><br><span class="line"></span><br><span class="line">    #-- 参数更新</span><br><span class="line">    n[0] &#x3D; lambdas * n[0] + (1 - lambdas) * np.square(all_da)</span><br><span class="line">    n[1] &#x3D; lambdas * n[1] + (1 - lambdas) * np.square(all_db)</span><br><span class="line">    rate_new &#x3D; rate&#x2F;(np.sqrt(n + epsilon))</span><br><span class="line">    print(&#39;rate_new a:&#39;,rate_new[0],&#39; b:&#39;,rate_new[1])</span><br><span class="line">    a &#x3D; a - (rate&#x2F;(np.sqrt(n[0] + epsilon)))*all_da</span><br><span class="line">    b &#x3D; b - (rate&#x2F;(np.sqrt(n[1] + epsilon)))*all_db</span><br><span class="line"></span><br><span class="line">    if step%1 &#x3D;&#x3D; 0:</span><br><span class="line">        print(&quot;step: &quot;, step, &quot; loss: &quot;, loss)</span><br><span class="line">        plt.show()</span><br><span class="line">        plt.pause(0.01)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="AdaDelta算法"><a href="#AdaDelta算法" class="headerlink" title="AdaDelta算法"></a>AdaDelta算法</h2><p>AdaDelta基本思想是用一阶的方法，近似模拟二阶牛顿法。除了RMSProp算法以外，另⼀个常⽤优化算法AdaDelta算法也针对AdaGrad算法在迭代后期可能较难找到有⽤解的问题做了改进。有意思的是，AdaDelta算法没有学习率这⼀超参数。其计算过程如下：</p>
<ul>
<li><p>$s_t = \lambda s_{t-1} + (1-\lambda)g_t \odot g_t$</p>
</li>
<li><p>$g’_t=\sqrt{\frac{\Delta x_t + \varepsilon }{s_t +  \varepsilon}} \odot g_t$</p>
</li>
<li><p>$\Delta x_t=px_{t-1}-(1-p)g’_t \odot g’_t$</p>
</li>
<li><p>$x_t = x_{t-1} - g’_t$</p>
</li>
</ul>
<p>AdaDelta算法的第一步和RMSProp算法⼀样，与RMSProp算法不同的是，AdaDelta算法还维护⼀个额外的状态变量$\Delta x_t$来计算自变量的变化量（按$g’_t$元素平⽅的指数加权移动平均），其元素同样在时间步0时被初始化为0。</p>
<p>可以这样理解：相比于同样是基于Gradient的Regularizer，不过只取最近的w个状态，这样不会让梯度被惩罚至0。</p>
<p>与RMSProp相比，AdaDelta算法没有学习率超参数，它通过使用有关自变量更新量平方的指数加权移动平均的项来替代RMSProp算法中的学习率。</p>
<p>AdaDelta的特点：</p>
<ul>
<li><p>从多个数据集情况来看，AdaDelta在训练初期和中期，具有非常不错的加速效果。</p>
</li>
<li><p>但是到训练后期，进入局部最小值雷区之后，AdaDelta就会反复在局部最小值附近抖动。主要体现在验证集错误率上，脱离不了局部最小值吸引盆。这时候，切换成动量SGD，如果把学习率降低一个量级，就会发现验证集正确率有2%~5%的提升。[注]：使用Batch Norm之后，这样从AdaDelta切到SGD会导致数值体系崩溃，原因未知。</p>
</li>
</ul>
<p>下面继续以LR为例，说明其计算过程。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; 10.0</span><br><span class="line">b &#x3D; -20.0</span><br><span class="line">all_loss &#x3D; []</span><br><span class="line">all_step &#x3D; []</span><br><span class="line">last_a &#x3D; a</span><br><span class="line">last_b &#x3D; b</span><br><span class="line">n &#x3D; np.array([0,0])</span><br><span class="line">theta &#x3D; np.array([0,0]).astype(np.float32) # 每一次a,b迭代的更新值</span><br><span class="line"></span><br><span class="line">apple &#x3D; np.array([0,0]).astype(np.float32)</span><br><span class="line">pear &#x3D; np.array([0,0]).astype(np.float32)</span><br><span class="line"># 迭代</span><br><span class="line">for step in range(1,500):</span><br><span class="line">    loss &#x3D; 0</span><br><span class="line">    all_da &#x3D; 0</span><br><span class="line">    all_db &#x3D; 0</span><br><span class="line">    all_d &#x3D; np.array([0,0]).astype(np.float32)</span><br><span class="line">    for i in range(0,len(x)):</span><br><span class="line">        y_p &#x3D; a*x[i] + b</span><br><span class="line">        loss &#x3D; loss + (y[i] - y_p)*(y[i] - y_p)&#x2F;2</span><br><span class="line">        all_da &#x3D; all_da + da(y[i],y_p,x[i])</span><br><span class="line">        all_db &#x3D; all_db + db(y[i],y_p)</span><br><span class="line">    #loss_ &#x3D; calc_loss(a &#x3D; a,b&#x3D;b,x&#x3D;np.array(x),y&#x3D;np.array(y))</span><br><span class="line">    all_d &#x3D; np.array([all_da,all_db])</span><br><span class="line">    loss &#x3D; loss&#x2F;len(x)</span><br><span class="line"></span><br><span class="line">    # 绘制图1中的loss点</span><br><span class="line">    ax.scatter(a, b, loss, color&#x3D;&#39;black&#39;)</span><br><span class="line">    # 绘制图2中的loss点</span><br><span class="line">    plt.subplot(2,2,2)</span><br><span class="line">    plt.scatter(a,b,s&#x3D;5,color&#x3D;&#39;blue&#39;)</span><br><span class="line">    plt.plot([last_a,a],[last_b,b],color&#x3D;&#39;aqua&#39;)</span><br><span class="line">    # 绘制图3中的回归直线</span><br><span class="line">    plt.subplot(2, 2, 3)</span><br><span class="line">    plt.plot(x, y)</span><br><span class="line">    plt.plot(x, y, &#39;o&#39;)</span><br><span class="line">    x_ &#x3D; np.linspace(0, 1, 2)</span><br><span class="line">    y_draw &#x3D; a * x_ + b</span><br><span class="line">    plt.plot(x_, y_draw)</span><br><span class="line">    # 绘制图4的loss更新曲线</span><br><span class="line">    all_loss.append(loss)</span><br><span class="line">    all_step.append(step)</span><br><span class="line">    plt.subplot(2,2,4)</span><br><span class="line">    plt.plot(all_step,all_loss,color&#x3D;&#39;orange&#39;)</span><br><span class="line">    plt.xlabel(&quot;step&quot;)</span><br><span class="line">    plt.ylabel(&quot;loss&quot;)</span><br><span class="line"></span><br><span class="line">    # print(&#39;a &#x3D; %.3f,b &#x3D; %.3f&#39; % (a,b))</span><br><span class="line">    last_a &#x3D; a</span><br><span class="line">    last_b &#x3D; b</span><br><span class="line"></span><br><span class="line">    #-- 参数更新</span><br><span class="line">    apple &#x3D; gamma*apple + (1-gamma)*(all_d**2) # apple with all_d of this step</span><br><span class="line">#     rms_apple &#x3D; np.sqrt(apple + epsilon)</span><br><span class="line">    rms_apple &#x3D; apple + epsilon</span><br><span class="line"></span><br><span class="line">    pear &#x3D; gamma*pear + (1-gamma)*(theta**2) # pear with theta of last step</span><br><span class="line">#     rms_pear &#x3D; np.sqrt(pear + epsilon)</span><br><span class="line"></span><br><span class="line">#     theta &#x3D; -(rms_pear&#x2F;rms_apple)*all_d</span><br><span class="line">    theta &#x3D; -np.sqrt(rms_pear&#x2F;rms_apple) * all_d</span><br><span class="line">    [a,b] &#x3D; [a,b] + theta</span><br><span class="line"></span><br><span class="line">    if step%1 &#x3D;&#x3D; 0:</span><br><span class="line">        print(&quot;step: &quot;, step, &quot; loss: &quot;, loss,&quot;rms_pear: &quot;,rms_pear,&quot; rms_apple&quot;,rms_apple)</span><br><span class="line">        plt.show()</span><br><span class="line">        plt.pause(0.01)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="Adam算法"><a href="#Adam算法" class="headerlink" title="Adam算法"></a>Adam算法</h2><p>Adam算法在RMSProp算法基础上对小批量随机梯度也做了指数加权移动平均。</p>
<ul>
<li><p>$v_t=\beta_1v_{t-1} + (1-\beta_1)g_t$</p>
<ul>
<li>Adam算法使⽤了动量变量$v_t$和RMSProp算法中小批量随机梯度按元素平⽅的指数加权移动平均变量$s_t$，并在时间步0将它们中每个元素初始化为0。给定超参数0 ≤ $\beta_1$ &lt; 1（算法作者建议设为0.9），时间步t的动量变量$v_t$即小批量随机梯度$g_t$的指数加权移动平均</li>
</ul>
</li>
<li><p>$s_t=\beta_2s_{t-1}+(1-\beta_2)g_t\odot g_t$</p>
<ul>
<li>和RMSProp算法中⼀样，给定超参数0 ≤ $\beta_2$ &lt; 1（算法作者建议设为0.999），将小批量随机梯度按元素平⽅后的项$g_t\odot g_t$做指数加权移动平均得到$s_t$</li>
</ul>
</li>
<li><p>$v’_t=\frac{v_t}{1-\beta^t_1}$、$s’_t=\frac{s_t}{1-\beta^t_2}$</p>
<ul>
<li>因为当t较小时，过去各时间步小批量随机梯度权值之和会较小。例如，当$\beta_1=0.9$时，$v_1=0.1g_1$。为了消除这样的影响，对于任意时间步t，我们可以将$v_t$再除以${1-\beta^t_1}$，从而使过去各时间步小批量随机梯度权值之和为1。这也叫作偏差修正。在Adam算法中，我们对变量$v_t$和$s_t$均作偏差修正得到</li>
</ul>
</li>
<li><p>$g’_t=\frac{\eta v’_t}{\sqrt{s’_t}+\varepsilon }$</p>
<ul>
<li>接下来，Adam算法使⽤以上偏差修正后的变量$v_t$和$s_t$，将模型参数中每个元素的学习率通过按元素运算重新调整得到（5）。其中$\eta$是学习率，$\varepsilon$是为了维持数值稳定性而添加的常数，如10的-8次方。和AdaGrad算法、RMSProp算法以及AdaDelta算法⼀样，⽬标函数⾃变量中每个元素都分别拥有⾃⼰的学习率</li>
</ul>
</li>
<li><p>$x_t=x_{t-1}-g’_t$</p>
<ul>
<li>使用$g’_t$迭代自变量</li>
</ul>
</li>
</ul>
<p>Adam算法有如下特点：</p>
<ul>
<li>除了像Adadelta和RMSprop一样存储了过去梯度的平方$s_t$的指数衰减平均值 ，也像momentum一样保持了过去梯度$v_t$的指数衰减平均值</li>
<li>如果$v_t$和$s_t$被初始化为0向量，那它们就会向0偏置，所以做了偏差校正，通过计算偏差校正后的$v_t$和$s_t$来抵消这些偏差</li>
</ul>
<p>下面继续以LR为例，说明其计算过程。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; 10.0</span><br><span class="line">b &#x3D; -20.0</span><br><span class="line">all_loss &#x3D; []</span><br><span class="line">all_step &#x3D; []</span><br><span class="line">last_a &#x3D; a</span><br><span class="line">last_b &#x3D; b</span><br><span class="line">m &#x3D; 0.0</span><br><span class="line">v &#x3D; 0.0</span><br><span class="line">theta &#x3D; np.array([0,0]).astype(np.float32)</span><br><span class="line"></span><br><span class="line">rate &#x3D; 0.001</span><br><span class="line">beta1 &#x3D; 0.9</span><br><span class="line">beta2 &#x3D; 0.999</span><br><span class="line">epsilon &#x3D; 1e-8</span><br><span class="line"></span><br><span class="line">for step in range(1,500):</span><br><span class="line">    loss &#x3D; 0</span><br><span class="line">    all_da &#x3D; 0</span><br><span class="line">    all_db &#x3D; 0</span><br><span class="line">    for i in range(0,len(x)):</span><br><span class="line">        y_p &#x3D; a*x[i] + b</span><br><span class="line">        loss &#x3D; loss + (y[i] - y_p)*(y[i] - y_p)&#x2F;2</span><br><span class="line">        all_da &#x3D; all_da + da(y[i],y_p,x[i])</span><br><span class="line">        all_db &#x3D; all_db + db(y[i],y_p)</span><br><span class="line">    loss &#x3D; loss&#x2F;len(x)</span><br><span class="line">    all_d &#x3D; np.array([all_da,all_db]).astype(np.float32)</span><br><span class="line">    # 绘制图1中的loss点</span><br><span class="line">    ax.scatter(a, b, loss, color&#x3D;&#39;black&#39;)</span><br><span class="line">    # 绘制图2中的loss点</span><br><span class="line">    plt.subplot(2,2,2)</span><br><span class="line">    plt.scatter(a,b,s&#x3D;5,color&#x3D;&#39;blue&#39;)</span><br><span class="line">    plt.plot([last_a,a],[last_b,b],color&#x3D;&#39;aqua&#39;)</span><br><span class="line">    # 绘制图3中的回归直线</span><br><span class="line">    plt.subplot(2, 2, 3)</span><br><span class="line">    plt.plot(x, y)</span><br><span class="line">    plt.plot(x, y, &#39;o&#39;)</span><br><span class="line">    x_ &#x3D; np.linspace(0, 1, 2)</span><br><span class="line">    y_draw &#x3D; a * x_ + b</span><br><span class="line">    plt.plot(x_, y_draw)</span><br><span class="line">    # 绘制图4的loss更新曲线</span><br><span class="line">    all_loss.append(loss)</span><br><span class="line">    all_step.append(step)</span><br><span class="line">    plt.subplot(2,2,4)</span><br><span class="line">    plt.plot(all_step,all_loss,color&#x3D;&#39;orange&#39;)</span><br><span class="line">    plt.xlabel(&quot;step&quot;)</span><br><span class="line">    plt.ylabel(&quot;loss&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # print(&#39;a &#x3D; %.3f,b &#x3D; %.3f&#39; % (a,b))</span><br><span class="line">    last_a &#x3D; a</span><br><span class="line">    last_b &#x3D; b</span><br><span class="line"></span><br><span class="line">    m &#x3D; beta1*m + (1-beta1)*all_d</span><br><span class="line">    v &#x3D; beta2*v + (1-beta2)*(all_d**2)</span><br><span class="line"></span><br><span class="line">    m_ &#x3D; m&#x2F;(1 - beta1)</span><br><span class="line">    v_ &#x3D; v&#x2F;(1 - beta2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    theta &#x3D; -(rate*m_&#x2F;(np.sqrt(v_) + epsilon))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    [a,b] &#x3D; [a,b] + theta</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    if step%1 &#x3D;&#x3D; 0:</span><br><span class="line">        print(&quot;step: &quot;, step, &quot; loss: &quot;, loss)</span><br><span class="line">        plt.show()</span><br><span class="line">        plt.pause(0.01)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="如何选择优化算法"><a href="#如何选择优化算法" class="headerlink" title="如何选择优化算法"></a>如何选择优化算法</h2><ul>
<li><p>如果数据是稀疏的，就用自适用方法，即 Adagrad, Adadelta, RMSprop, Adam。</p>
</li>
<li><p>RMSprop, Adadelta, Adam 在很多情况下的效果是相似的。</p>
</li>
<li><p>Adam 就是在 RMSprop 的基础上加了 bias-correction 和 momentum，</p>
</li>
<li><p>随着梯度变的稀疏，Adam 比 RMSprop 效果会好。</p>
</li>
<li><p>整体来讲，Adam 是最好的选择。</p>
</li>
<li><p>很多论文里都会用 SGD，没有 momentum 等。SGD 虽然能达到极小值，但是比其它算法用的时间长，而且可能会被困在鞍点。</p>
</li>
<li><p>如果需要更快的收敛，或者是训练更深更复杂的神经网络，需要用一种自适应的算法。</p>
</li>
</ul>
<h2 id="keras中优化器的用法"><a href="#keras中优化器的用法" class="headerlink" title="keras中优化器的用法"></a>keras中优化器的用法</h2><p>以全连接网络做分类任务为例子，说明keras中optimizer的用法</p>
<h3 id="生成数据"><a href="#生成数据" class="headerlink" title="生成数据"></a>生成数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X &#x3D; np.linspace(-1, 1, 200) #在返回（-1, 1）范围内的等差序列</span><br><span class="line">np.random.shuffle(X)    # 打乱顺序</span><br><span class="line">Y &#x3D; 0.5 * X + 2 + np.random.normal(0, 0.05, (200, )) #生成Y并添加噪声</span><br><span class="line"></span><br><span class="line">X_train, Y_train &#x3D; X[:160], Y[:160]     # 前160组数据为训练数据集</span><br><span class="line">X_test, Y_test &#x3D; X[160:], Y[160:]      #后40组数据为测试数据集</span><br></pre></td></tr></table></figure>
<h3 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from keras import optimizers</span><br><span class="line">from keras.layers import Dense, Activation</span><br><span class="line">from keras.models import Sequential</span><br><span class="line"></span><br><span class="line">model &#x3D; Sequential()</span><br><span class="line">model.add(Dense(input_dim&#x3D;1, units&#x3D;1))</span><br></pre></td></tr></table></figure>
<h3 id="使用SGD"><a href="#使用SGD" class="headerlink" title="使用SGD"></a>使用SGD</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># clipvalue&#x3D;0.5表示保留(-0.5, 0.5)之间的梯度，其他的需要做梯度裁剪</span><br><span class="line"># momentum&#x3D;0.9使用了动量平滑，可以设置为0.0表示纯SGD，nesterov&#x3D;True表示使用Nesterov动量</span><br><span class="line">sgd &#x3D; optimizers.SGD(lr&#x3D;0.01, decay&#x3D;1e-6, momentum&#x3D;0.9, nesterov&#x3D;True, clipvalue&#x3D;0.5)</span><br><span class="line">model.compile(loss&#x3D;&#39;mean_squared_error&#39;, optimizer&#x3D;sgd)</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<h3 id="训练并测试"><a href="#训练并测试" class="headerlink" title="训练并测试"></a>训练并测试</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">print(&#39;Training -----------&#39;)</span><br><span class="line">for step in range(501):</span><br><span class="line">    cost &#x3D; model.train_on_batch(X_train, Y_train)</span><br><span class="line">    if step % 50 &#x3D;&#x3D; 0:</span><br><span class="line">        print(&quot;After %d trainings, the cost: %f&quot; % (step, cost))</span><br><span class="line">        </span><br><span class="line">print(&#39;\nTesting ------------&#39;)</span><br><span class="line">cost &#x3D; model.evaluate(X_test, Y_test, batch_size&#x3D;40)</span><br><span class="line">print(&#39;test cost:&#39;, cost)</span><br><span class="line">W, b &#x3D; model.layers[0].get_weights()</span><br><span class="line">print(&#39;Weights&#x3D;&#39;, W, &#39;\nbiases&#x3D;&#39;, b)</span><br></pre></td></tr></table></figure>
<h3 id="使用RMSprop"><a href="#使用RMSprop" class="headerlink" title="使用RMSprop"></a>使用RMSprop</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rmsprop &#x3D; optimizers.RMSprop(lr&#x3D;0.001, rho&#x3D;0.9, epsilon&#x3D;1e-06)</span><br></pre></td></tr></table></figure>
<h3 id="使用Adagrad"><a href="#使用Adagrad" class="headerlink" title="使用Adagrad"></a>使用Adagrad</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">adagrad &#x3D; optimizers.Adagrad(lr&#x3D;0.01, epsilon&#x3D;1e-06)</span><br></pre></td></tr></table></figure>
<h3 id="使用Adadelta"><a href="#使用Adadelta" class="headerlink" title="使用Adadelta"></a>使用Adadelta</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">adadelta &#x3D; optimizers.Adadelta(lr&#x3D;1.0, rho&#x3D;0.95, epsilon&#x3D;1e-06)</span><br></pre></td></tr></table></figure>
<h3 id="使用Adam"><a href="#使用Adam" class="headerlink" title="使用Adam"></a>使用Adam</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">adam &#x3D; optimizers.Adam(lr&#x3D;0.001, beta_1&#x3D;0.9, beta_2&#x3D;0.999, epsilon&#x3D;1e-08)</span><br></pre></td></tr></table></figure>
<h2 id="BERT中使用的优化器解析"><a href="#BERT中使用的优化器解析" class="headerlink" title="BERT中使用的优化器解析"></a>BERT中使用的优化器解析</h2><p>bert使用的optimizer叫做AdamWeightDecayOptimizer，先放代码为敬：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">def create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps, use_tpu):</span><br><span class="line">  &quot;&quot;&quot;Creates an optimizer training op.&quot;&quot;&quot;</span><br><span class="line">  global_step &#x3D; tf.train.get_or_create_global_step()</span><br><span class="line"></span><br><span class="line">  learning_rate &#x3D; tf.constant(value&#x3D;init_lr, shape&#x3D;[], dtype&#x3D;tf.float32)</span><br><span class="line"></span><br><span class="line">  # Implements linear decay of the learning rate.</span><br><span class="line">  learning_rate &#x3D; tf.train.polynomial_decay(</span><br><span class="line">      learning_rate,</span><br><span class="line">      global_step,</span><br><span class="line">      num_train_steps,</span><br><span class="line">      end_learning_rate&#x3D;0.0,</span><br><span class="line">      power&#x3D;1.0,</span><br><span class="line">      cycle&#x3D;False)</span><br><span class="line"></span><br><span class="line">  # Implements linear warmup. I.e., if global_step &lt; num_warmup_steps, the</span><br><span class="line">  # learning rate will be &#96;global_step&#x2F;num_warmup_steps * init_lr&#96;.</span><br><span class="line">  if num_warmup_steps:</span><br><span class="line">    global_steps_int &#x3D; tf.cast(global_step, tf.int32)</span><br><span class="line">    warmup_steps_int &#x3D; tf.constant(num_warmup_steps, dtype&#x3D;tf.int32)</span><br><span class="line"></span><br><span class="line">    global_steps_float &#x3D; tf.cast(global_steps_int, tf.float32)</span><br><span class="line">    warmup_steps_float &#x3D; tf.cast(warmup_steps_int, tf.float32)</span><br><span class="line"></span><br><span class="line">    warmup_percent_done &#x3D; global_steps_float &#x2F; warmup_steps_float</span><br><span class="line">    warmup_learning_rate &#x3D; init_lr * warmup_percent_done</span><br><span class="line"></span><br><span class="line">    is_warmup &#x3D; tf.cast(global_steps_int &lt; warmup_steps_int, tf.float32)</span><br><span class="line">    learning_rate &#x3D; (</span><br><span class="line">        (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)</span><br><span class="line"></span><br><span class="line">  # It is recommended that you use this optimizer for fine tuning, since this</span><br><span class="line">  # is how the model was trained (note that the Adam m&#x2F;v variables are NOT</span><br><span class="line">  # loaded from init_checkpoint.)</span><br><span class="line">  optimizer &#x3D; AdamWeightDecayOptimizer(</span><br><span class="line">      learning_rate&#x3D;learning_rate,</span><br><span class="line">      weight_decay_rate&#x3D;0.01,</span><br><span class="line">      beta_1&#x3D;0.9,</span><br><span class="line">      beta_2&#x3D;0.999,</span><br><span class="line">      epsilon&#x3D;1e-6,</span><br><span class="line">      exclude_from_weight_decay&#x3D;[&quot;LayerNorm&quot;, &quot;layer_norm&quot;, &quot;bias&quot;])</span><br><span class="line"></span><br><span class="line">  if use_tpu:</span><br><span class="line">    optimizer &#x3D; tf.contrib.tpu.CrossShardOptimizer(optimizer)</span><br><span class="line"></span><br><span class="line">  tvars &#x3D; tf.trainable_variables()</span><br><span class="line">  grads &#x3D; tf.gradients(loss, tvars)</span><br><span class="line"></span><br><span class="line">  # This is how the model was pre-trained.</span><br><span class="line">  (grads, _) &#x3D; tf.clip_by_global_norm(grads, clip_norm&#x3D;1.0)</span><br><span class="line"></span><br><span class="line">  train_op &#x3D; optimizer.apply_gradients(</span><br><span class="line">      zip(grads, tvars), global_step&#x3D;global_step)</span><br><span class="line"></span><br><span class="line">  # Normally the global step update is done inside of &#96;apply_gradients&#96;.</span><br><span class="line">  # However, &#96;AdamWeightDecayOptimizer&#96; doesn&#39;t do this. But if you use</span><br><span class="line">  # a different optimizer, you should probably take this line out.</span><br><span class="line">  new_global_step &#x3D; global_step + 1</span><br><span class="line">  train_op &#x3D; tf.group(train_op, [global_step.assign(new_global_step)])</span><br><span class="line">  return train_op</span><br><span class="line"></span><br><span class="line">class AdamWeightDecayOptimizer(tf.train.Optimizer):</span><br><span class="line">  &quot;&quot;&quot;A basic Adam optimizer that includes &quot;correct&quot; L2 weight decay.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">  def __init__(self,</span><br><span class="line">               learning_rate,</span><br><span class="line">               weight_decay_rate&#x3D;0.0,</span><br><span class="line">               beta_1&#x3D;0.9,</span><br><span class="line">               beta_2&#x3D;0.999,</span><br><span class="line">               epsilon&#x3D;1e-6,</span><br><span class="line">               exclude_from_weight_decay&#x3D;None,</span><br><span class="line">               name&#x3D;&quot;AdamWeightDecayOptimizer&quot;):</span><br><span class="line">    &quot;&quot;&quot;Constructs a AdamWeightDecayOptimizer.&quot;&quot;&quot;</span><br><span class="line">    super(AdamWeightDecayOptimizer, self).__init__(False, name)</span><br><span class="line"></span><br><span class="line">    self.learning_rate &#x3D; learning_rate</span><br><span class="line">    self.weight_decay_rate &#x3D; weight_decay_rate</span><br><span class="line">    self.beta_1 &#x3D; beta_1</span><br><span class="line">    self.beta_2 &#x3D; beta_2</span><br><span class="line">    self.epsilon &#x3D; epsilon</span><br><span class="line">    self.exclude_from_weight_decay &#x3D; exclude_from_weight_decay</span><br><span class="line"></span><br><span class="line">  def apply_gradients(self, grads_and_vars, global_step&#x3D;None, name&#x3D;None):</span><br><span class="line">    &quot;&quot;&quot;See base class.&quot;&quot;&quot;</span><br><span class="line">    assignments &#x3D; []</span><br><span class="line">    for (grad, param) in grads_and_vars:</span><br><span class="line">      if grad is None or param is None:</span><br><span class="line">        continue</span><br><span class="line"></span><br><span class="line">      param_name &#x3D; self._get_variable_name(param.name)</span><br><span class="line"></span><br><span class="line">      m &#x3D; tf.get_variable(</span><br><span class="line">          name&#x3D;param_name + &quot;&#x2F;adam_m&quot;,</span><br><span class="line">          shape&#x3D;param.shape.as_list(),</span><br><span class="line">          dtype&#x3D;tf.float32,</span><br><span class="line">          trainable&#x3D;False,</span><br><span class="line">          initializer&#x3D;tf.zeros_initializer())</span><br><span class="line">      v &#x3D; tf.get_variable(</span><br><span class="line">          name&#x3D;param_name + &quot;&#x2F;adam_v&quot;,</span><br><span class="line">          shape&#x3D;param.shape.as_list(),</span><br><span class="line">          dtype&#x3D;tf.float32,</span><br><span class="line">          trainable&#x3D;False,</span><br><span class="line">          initializer&#x3D;tf.zeros_initializer())</span><br><span class="line"></span><br><span class="line">      # Standard Adam update.</span><br><span class="line">      next_m &#x3D; (</span><br><span class="line">          tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad))</span><br><span class="line">      next_v &#x3D; (</span><br><span class="line">          tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2,</span><br><span class="line">                                                    tf.square(grad)))</span><br><span class="line"></span><br><span class="line">      update &#x3D; next_m &#x2F; (tf.sqrt(next_v) + self.epsilon)</span><br><span class="line"></span><br><span class="line">      # Just adding the square of the weights to the loss function is *not*</span><br><span class="line">      # the correct way of using L2 regularization&#x2F;weight decay with Adam,</span><br><span class="line">      # since that will interact with the m and v parameters in strange ways.</span><br><span class="line">      #</span><br><span class="line">      # Instead we want ot decay the weights in a manner that doesn&#39;t interact</span><br><span class="line">      # with the m&#x2F;v parameters. This is equivalent to adding the square</span><br><span class="line">      # of the weights to the loss with plain (non-momentum) SGD.</span><br><span class="line">      if self._do_use_weight_decay(param_name):</span><br><span class="line">        update +&#x3D; self.weight_decay_rate * param</span><br><span class="line"></span><br><span class="line">      update_with_lr &#x3D; self.learning_rate * update</span><br><span class="line"></span><br><span class="line">      next_param &#x3D; param - update_with_lr</span><br><span class="line"></span><br><span class="line">      assignments.extend(</span><br><span class="line">          [param.assign(next_param),</span><br><span class="line">           m.assign(next_m),</span><br><span class="line">           v.assign(next_v)])</span><br><span class="line">    return tf.group(*assignments, name&#x3D;name)</span><br><span class="line"></span><br><span class="line">  def _do_use_weight_decay(self, param_name):</span><br><span class="line">    &quot;&quot;&quot;Whether to use L2 weight decay for &#96;param_name&#96;.&quot;&quot;&quot;</span><br><span class="line">    if not self.weight_decay_rate:</span><br><span class="line">      return False</span><br><span class="line">    if self.exclude_from_weight_decay:</span><br><span class="line">      for r in self.exclude_from_weight_decay:</span><br><span class="line">        if re.search(r, param_name) is not None:</span><br><span class="line">          return False</span><br><span class="line">    return True</span><br><span class="line"></span><br><span class="line">  def _get_variable_name(self, param_name):</span><br><span class="line">    &quot;&quot;&quot;Get the variable name from the tensor name.&quot;&quot;&quot;</span><br><span class="line">    m &#x3D; re.match(&quot;^(.*):\\d+$&quot;, param_name)</span><br><span class="line">    if m is not None:</span><br><span class="line">      param_name &#x3D; m.group(1)</span><br><span class="line">    return param_name</span><br></pre></td></tr></table></figure>
<p>可以看到里面使用了warmup+Adam+weight_dacay，现在一一进行说明。</p>
<p>（1）warmup机制在初期使用warmup_step阶段使用较小的学习率（随着warmup_step增大逐步增大到init_lr）</p>
<p>（2）Weight decay是在每次更新的梯度基础上减去一个梯度（ $\Theta$为模型参数向量，$\bigtriangledown f_t(\theta_t) $为t时刻loss函数的梯度，$\alpha$为学习率）:$\theta_t=(1-\lambda ) \theta_t - \alpha \bigtriangledown f_t(\theta_t)$</p>
<p>（3）L2 regularization是给参数加上一个L2惩罚($f_t(\theta )$为loss函数)：$f^{reg}_t(\theta )=f_t(\theta )+\frac{\lambda ‘}{2}||\theta ||^2$。(当$\lambda ‘=\frac{\lambda }{\alpha }$时，与weight decay等价，仅在使用标准SGD优化时成立)</p>
<p>（4）Adam自动调整学习率，大幅提高了训练速度，也很少需要调整学习率，但是有相当多的资料报告Adam优化的最终精度略低于SGD。问题出在哪呢，其实Adam本身没有问题，问题在于目前大多数DL框架的L2 regularization实现用的是weight decay的方式，而weight decay在与Adam共同使用的时候有互相耦合。理由如下图，其中红色是传统的Adam+L2 regularization的方式，绿色是bert使用的接入weight decay的方式，能够完成梯度下降与weight decay的解耦：</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/optimizer/5.png" alt="fig5"></p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/optimizer/6.png" alt="fig6"></p>
<p>大部分的模型都会有L2 regularization约束项，因此很有可能出现adam的最终效果没有sgd的好。如果在tf里面要对不同区域的tensor做不同的L2 regularization调整的化可以参考<a href="https://zhuanlan.zhihu.com/p/40814046" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/40814046</a> 先做adam后在手工更新L2 regularization梯度的方法。</p>
<p>推荐：一个框架看懂优化算法之异同 SGD/AdaGrad/Adam：<a href="https://zhuanlan.zhihu.com/p/32230623" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32230623</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="majsunflower.cn/2019/03/27/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8A%E4%BC%98%E5%8C%96%E5%99%A8%E2%80%94%E2%80%94%E5%8E%9F%E7%90%86%E5%8F%8A%E5%BA%94%E7%94%A8%E3%80%8B/" data-id="cklf9zmpf0012jtx96c4tdrj5"
         class="article-share-link">分享</a>
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97/" rel="tag">基础系列</a></li></ul>

    </footer>

  </div>

  
    
  <nav class="article-nav">
    
      <a href="/2019/04/23/Hello-Edge-Keyword-spotting-on-Microcontrollers%E8%AE%BA%E6%96%87%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E5%8F%8A%E5%AE%9E%E9%AA%8C%E7%BB%93%E8%AE%BA/" class="article-nav-link">
        <strong class="article-nav-caption">前一篇</strong>
        <div class="article-nav-title">
          
            Hello Edge: Keyword spotting on Microcontrollers论文源码阅读及实验结论
          
        </div>
      </a>
    
    
      <a href="/2019/03/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E3%80%8ALattice-CNNs-for-Matching-Based-Chinese-Question-Answering%E3%80%8B/" class="article-nav-link">
        <strong class="article-nav-caption">后一篇</strong>
        <div class="article-nav-title">论文阅读：《Lattice CNNs for Matching Based Chinese Question Answering》</div>
      </a>
    
  </nav>


  

  
    
  <div class="gitalk" id="gitalk-container"></div>
  
<link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">

  
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

  
<script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>

  <script type="text/javascript">
    var gitalk = new Gitalk({
      clientID: 'a0115c330d8e2a88dc59',
      clientSecret: '2e456ec13123a898d7b34ad8e117f543a6f379ea',
      repo: 'majing2019.github.io',
      owner: 'majing2019',
      admin: ['majing2019'],
      // id: location.pathname,      // Ensure uniqueness and length less than 50
      id: md5(location.pathname),
      distractionFreeMode: false,  // Facebook-like distraction free mode
      pagerDirection: 'last'
    })

  gitalk.render('gitalk-container')
  </script>

  

</article>



</section>
  <footer class="footer">
  <div class="outer">
    <div class="float-right">
      <ul class="list-inline">
  
    <li><i class="fe fe-smile-alt"></i> <span id="busuanzi_value_site_uv"></span></li>
  
    <li><i class="fe fe-bookmark"></i> <span id="busuanzi_value_page_pv"></span></li>
  
</ul>
    </div>
    <ul class="list-inline">
      <li>&copy; 2021 大嘴怪的小世界</li>
      <li>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></li>
      <li>Theme  <a href="https://github.com/zhwangart/hexo-theme-ocean" target="_blank" rel="noopener">Ocean</a></li>
    </ul>
  </div>
</footer>

</main>
<aside class="sidebar">
  <button class="navbar-toggle"></button>
<nav class="navbar">
  
    <div class="logo">
      <a href="/"><img src="/images/shark.svg" alt="大嘴怪的小世界"></a>
    </div>
  
  <ul class="nav nav-main">
    
      <li class="nav-item">
        <a class="nav-item-link" href="/">主页</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/archives">归档</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/gallery">相册</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/about">关于</a>
      </li>
    
    <li class="nav-item">
      <a class="nav-item-link nav-item-search" title="搜索">
        <i class="fe fe-search"></i>
        搜索
      </a>
    </li>
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      <div class="totop" id="totop">
  <i class="fe fe-rocket"></i>
</div>
    </li>
    <li class="nav-item">
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
</aside>

<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>


  
<script src="/fancybox/jquery.fancybox.min.js"></script>




  
<script src="/js/tocbot.min.js"></script>

  <script>
    // Tocbot_v4.7.0  http://tscanlin.github.io/tocbot/
    tocbot.init({
      tocSelector: '.tocbot',
      contentSelector: '.article-entry',
      headingSelector: 'h1, h2, h3, h4, h5, h6',
      hasInnerContainers: true,
      scrollSmooth: true,
      positionFixedSelector: '.tocbot',
      positionFixedClass: 'is-position-fixed',
      fixedSidebarOffset: 'auto',
    });
  </script>




<script src="/js/ocean.js"></script>


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  
  
  
    <meta name="description" content="è®°å½•ç”Ÿæ´»">
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <title>
    ç¥ç»ç½‘ç»œåŸºç¡€ç³»åˆ—ä¹‹ã€Šä¼˜åŒ–å™¨â€”â€”åŸç†åŠåº”ç”¨ã€‹ |
    
    å¤§å˜´æ€ªçš„å°ä¸–ç•Œ</title>
  
    <link rel="shortcut icon" href="/favicon.ico">
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<script src="/js/pace.min.js"></script>

<meta name="generator" content="Hexo 4.2.1"></head>

<body>
<main class="content">
  <section class="outer">
  

<article id="post-ç¥ç»ç½‘ç»œåŸºç¡€ç³»åˆ—ä¹‹ã€Šä¼˜åŒ–å™¨â€”â€”åŸç†åŠåº”ç”¨ã€‹" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      ç¥ç»ç½‘ç»œåŸºç¡€ç³»åˆ—ä¹‹ã€Šä¼˜åŒ–å™¨â€”â€”åŸç†åŠåº”ç”¨ã€‹
    </h1>
  
  




      </header>
    

    
      <div class="article-meta">
        <a href="/2019/03/27/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8A%E4%BC%98%E5%8C%96%E5%99%A8%E2%80%94%E2%80%94%E5%8E%9F%E7%90%86%E5%8F%8A%E5%BA%94%E7%94%A8%E3%80%8B/" class="article-date">
  <time datetime="2019-03-27T14:49:45.000Z" itemprop="datePublished">2019-03-27</time>
</a>
        
      </div>
    

    
      
    <div class="tocbot"></div>





    

    <div class="article-entry" itemprop="articleBody">
      


      

      
        <p>ç¥ç»ç½‘ç»œåœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæ¢¯åº¦ä¸‹é™æ ¹æ®â¾ƒå˜é‡å½“å‰ä½ç½®ï¼Œæ²¿ç€å½“å‰ä½ç½®çš„æ¢¯åº¦æ›´æ–°â¾ƒå˜é‡ã€‚ç„¶è€Œï¼Œå¦‚æœâ¾ƒå˜é‡çš„ è¿­ä»£â½…å‘ä»…ä»…å–å†³äºâ¾ƒå˜é‡å½“å‰ä½ç½®ï¼Œè¿™å¯èƒ½ä¼šå¸¦æ¥â¼€äº›é—®é¢˜ã€‚æ¯”å¦‚ä¸‹å›¾ï¼š</p>
<a id="more"></a>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/optimizer/1.png" alt="fig1"></p>
<p>ç»™å®šå­¦ä¹ ç‡ï¼Œæ¢¯åº¦ä¸‹é™è¿­ä»£â¾ƒå˜é‡æ—¶ä¼šä½¿â¾ƒå˜é‡åœ¨ç«–ç›´â½…å‘â½åœ¨â½”å¹³â½…å‘ç§»åŠ¨å¹…åº¦æ›´â¼¤ã€‚é‚£ä¹ˆï¼Œæˆ‘ä»¬éœ€è¦â¼€ä¸ªè¾ƒå°çš„å­¦ä¹ ç‡ä»è€Œé¿å…â¾ƒå˜é‡åœ¨ç«–ç›´â½…å‘ä¸Šè¶Šè¿‡â½¬æ ‡å‡½æ•°æœ€ä¼˜è§£ã€‚ç„¶è€Œï¼Œè¿™ä¼šé€ æˆâ¾ƒå˜é‡åœ¨â½”å¹³â½…å‘ä¸Šæœæœ€ä¼˜è§£ç§»åŠ¨å˜æ…¢ã€‚</p>
<p>ä¸ºäº†è§£å†³æ¢¯åº¦æ›´æ–°çš„é—®é¢˜ï¼Œæå‡ºäº†å¾ˆå¤šä¼˜åŒ–å™¨ç®—æ³•ï¼Œä¸‹é¢ä¸€ä¸€ä»‹ç»ã€‚</p>
<p>ç»™å®šå­¦ä¹ ç‡ï¼Œæ¢¯åº¦ä¸‹é™è¿­ä»£â¾ƒå˜é‡æ—¶ä¼šä½¿â¾ƒå˜é‡åœ¨ç«–ç›´â½…å‘â½åœ¨â½”å¹³â½…å‘ç§»åŠ¨å¹…åº¦æ›´â¼¤ã€‚é‚£ä¹ˆï¼Œæˆ‘ä»¬éœ€è¦â¼€ä¸ªè¾ƒå°çš„å­¦ä¹ ç‡ä»è€Œé¿å…â¾ƒå˜é‡åœ¨ç«–ç›´â½…å‘ä¸Šè¶Šè¿‡â½¬æ ‡å‡½æ•°æœ€ä¼˜è§£ã€‚ç„¶è€Œï¼Œè¿™ä¼šé€ æˆâ¾ƒå˜é‡åœ¨â½”å¹³â½…å‘ä¸Šæœæœ€ä¼˜è§£ç§»åŠ¨å˜æ…¢ã€‚</p>
<p>ä¸ºäº†è§£å†³æ¢¯åº¦æ›´æ–°çš„é—®é¢˜ï¼Œæå‡ºäº†å¾ˆå¤šä¼˜åŒ–å™¨ç®—æ³•ï¼Œä¸‹é¢ä¸€ä¸€ä»‹ç»ã€‚</p>
<h2 id="SGDéšæœºæ¢¯åº¦ä¸‹é™"><a href="#SGDéšæœºæ¢¯åº¦ä¸‹é™" class="headerlink" title="SGDéšæœºæ¢¯åº¦ä¸‹é™"></a>SGDéšæœºæ¢¯åº¦ä¸‹é™</h2><p>å°±æ˜¯å¯¹minibtchåšæ¢¯åº¦ä¸‹é™ã€‚å…¶ä¼˜ç¼ºç‚¹å¦‚ä¸‹ï¼š</p>
<ul>
<li><p>å¤§éƒ¨åˆ†æ—¶å€™ä½ å‘ç€å…¨å±€æœ€å°å€¼é è¿‘ï¼Œæœ‰æ—¶å€™ä½ ä¼šè¿œç¦»æœ€å°å€¼ï¼Œå› ä¸ºé‚£ä¸ªæ ·æœ¬æ°å¥½ç»™ä½ æŒ‡çš„æ–¹å‘ä¸å¯¹ï¼Œå› æ­¤éšæœºæ¢¯åº¦ä¸‹é™æ³•æ˜¯æœ‰å¾ˆå¤šå™ªå£°çš„ï¼Œå¹³å‡æ¥çœ‹ï¼Œå®ƒæœ€ç»ˆä¼šé è¿‘æœ€å°å€¼ï¼Œä¸è¿‡æœ‰æ—¶å€™ä¹Ÿä¼šæ–¹å‘é”™è¯¯ï¼Œå› ä¸ºéšæœºæ¢¯åº¦ä¸‹é™æ³•æ°¸è¿œä¸ä¼šæ”¶æ•›ï¼Œè€Œæ˜¯ä¼šä¸€ç›´åœ¨æœ€å°å€¼é™„è¿‘æ³¢åŠ¨ã€‚ä¸€æ¬¡æ€§åªå¤„ç†äº†ä¸€ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œè¿™æ ·æ•ˆç‡è¿‡äºä½ä¸‹ã€‚</p>
</li>
<li><p>å®è·µä¸­æœ€å¥½é€‰æ‹©ä¸å¤§ä¸å°çš„ mini-batchï¼Œå¾—åˆ°äº†å¤§é‡å‘é‡åŒ–ï¼Œæ•ˆç‡é«˜ï¼Œæ”¶æ•›å¿«ã€‚</p>
</li>
</ul>
<p>ä¸‹é¢ä»¥LRç®—æ³•ä¸ºä¾‹ï¼Œè¯´æ˜SGDçš„è®¡ç®—è¿‡ç¨‹ã€‚</p>
<ul>
<li><p>æ¨¡æ‹Ÿæ•°æ®å¹¶å½’ä¸€åŒ–</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">x &#x3D; [30,35,37,59,70,76,88,100]</span><br><span class="line">y &#x3D; [1100,1423,1377,1800,2304,2588,3495,4839]</span><br><span class="line"></span><br><span class="line">x_max &#x3D; max(x)</span><br><span class="line">x_min &#x3D; min(x)</span><br><span class="line">y_max &#x3D; max(y)</span><br><span class="line">y_min &#x3D; min(y)</span><br><span class="line"></span><br><span class="line">for i in range(0,len(x)):</span><br><span class="line">    x[i] &#x3D; (x[i] - x_min)&#x2F;(x_max - x_min)</span><br><span class="line">    y[i] &#x3D; (y[i] - y_min)&#x2F;(y_max - y_min)</span><br><span class="line">    </span><br><span class="line">print (x)</span><br><span class="line">print (y)</span><br></pre></td></tr></table></figure>
</li>
<li><p>æ•°æ®ç­‰é«˜çº¿</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">def calc_loss(a,b,x,y):</span><br><span class="line">    tmp &#x3D; y - (a * x + b)</span><br><span class="line">    tmp &#x3D; tmp ** 2  # å¯¹çŸ©é˜µå†…çš„æ¯ä¸€ä¸ªå…ƒç´ å¹³æ–¹</span><br><span class="line">    SSE &#x3D; sum(tmp) &#x2F; (2 * len(x))</span><br><span class="line">    return SSE</span><br><span class="line"></span><br><span class="line">def draw_hill(x,y):</span><br><span class="line">    a &#x3D; np.linspace(-20,20,100)</span><br><span class="line">    b &#x3D; np.linspace(-20,20,100)</span><br><span class="line">    x &#x3D; np.array(x)</span><br><span class="line">    y &#x3D; np.array(y)</span><br><span class="line"></span><br><span class="line">    allSSE &#x3D; np.zeros(shape&#x3D;(len(a),len(b)))</span><br><span class="line">    for ai in range(0,len(a)):</span><br><span class="line">        for bi in range(0,len(b)):</span><br><span class="line">            a0 &#x3D; a[ai]</span><br><span class="line">            b0 &#x3D; b[bi]</span><br><span class="line">            SSE &#x3D; calc_loss(a&#x3D;a0,b&#x3D;b0,x&#x3D;x,y&#x3D;y)</span><br><span class="line">            allSSE[ai][bi] &#x3D; SSE</span><br><span class="line"></span><br><span class="line">    a,b &#x3D; np.meshgrid(a, b)</span><br><span class="line">    return [a,b,allSSE]</span><br><span class="line"></span><br><span class="line">[ha,hb,hallSSE] &#x3D; draw_hill(x,y)</span><br><span class="line">hallSSE &#x3D; hallSSE.T# é‡è¦ï¼Œå°†æ‰€æœ‰çš„lossesåšä¸€ä¸ªè½¬ç½®ã€‚åŸå› æ˜¯çŸ©é˜µæ˜¯ä»¥å·¦ä¸Šè§’è‡³å³ä¸‹è§’é¡ºåºæ’åˆ—å…ƒç´ ï¼Œè€Œç»˜å›¾æ˜¯ä»¥å·¦ä¸‹è§’ä¸ºåŸç‚¹ã€‚</span><br><span class="line"></span><br><span class="line">print (ha.shape)</span><br><span class="line">print (hb.shape)</span><br><span class="line">print (hallSSE.shape)</span><br></pre></td></tr></table></figure>
</li>
<li><p>ç»˜åˆ¶å­¦ä¹ ç‡çš„æ›²çº¿</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">rate &#x3D; 0.1 # learning rate</span><br><span class="line">fig &#x3D; plt.figure(1, figsize&#x3D;(12, 8))</span><br><span class="line">fig.suptitle(&#39;learning rate: %.2f method:momentum&#39;%(rate), fontsize&#x3D;15)</span><br></pre></td></tr></table></figure>
</li>
<li><p>ç»˜åˆ¶æ›²é¢</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from mpl_toolkits.mplot3d import Axes3D</span><br><span class="line">ax &#x3D; fig.add_subplot(2, 2, 1, projection&#x3D;&#39;3d&#39;)</span><br><span class="line">ax.set_top_view()</span><br><span class="line">ax.plot_surface(ha, hb, hallSSE, rstride&#x3D;2, cstride&#x3D;2, cmap&#x3D;&#39;rainbow&#39;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>ç»˜åˆ¶ç­‰é«˜çº¿å›¾</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">plt.subplot(2,2,2)</span><br><span class="line">ta &#x3D; np.linspace(-20, 20, 100)</span><br><span class="line">tb &#x3D; np.linspace(-20, 20, 100)</span><br><span class="line">plt.contourf(ha,hb,hallSSE,15,alpha&#x3D;0.5,cmap&#x3D;plt.cm.hot)</span><br><span class="line">C &#x3D; plt.contour(ha,hb,hallSSE,15,colors&#x3D;&#39;black&#39;)</span><br><span class="line">plt.clabel(C,inline&#x3D;True)</span><br><span class="line">plt.xlabel(&#39;a&#39;)</span><br><span class="line">plt.ylabel(&#39;b&#39;)</span><br><span class="line"></span><br><span class="line">plt.ion() # iteration on</span><br></pre></td></tr></table></figure>
</li>
<li><p>æ±‚lossçš„æ¢¯åº¦</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def da(y,y_p,x):</span><br><span class="line">    return (y-y_p)*(-x)</span><br><span class="line"></span><br><span class="line">def db(y,y_p):</span><br><span class="line">    return (y-y_p)*(-1)</span><br></pre></td></tr></table></figure>
</li>
<li><p>è¿›è¡Œè¿­ä»£è®­ç»ƒ</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">import random</span><br><span class="line"></span><br><span class="line">def shuffle_data(x,y):</span><br><span class="line">    # éšæœºæ‰“ä¹±xï¼Œyçš„æ•°æ®ï¼Œå¹¶ä¸”ä¿æŒxå’Œyä¸€ä¸€å¯¹åº”</span><br><span class="line">    seed &#x3D; random.random()</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    random.shuffle(x)</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    random.shuffle(y)</span><br><span class="line"></span><br><span class="line">def get_batch_data(x,y,batch&#x3D;3):</span><br><span class="line">    shuffle_data(x,y)</span><br><span class="line">    x_new &#x3D; x[0:batch]</span><br><span class="line">    y_new &#x3D; y[0:batch]</span><br><span class="line">    return [x_new,y_new]</span><br><span class="line"></span><br><span class="line">a &#x3D; 10.0</span><br><span class="line">b &#x3D; -20.0</span><br><span class="line">rate &#x3D; 0.2</span><br><span class="line"></span><br><span class="line">all_loss &#x3D; []</span><br><span class="line">all_step &#x3D; []</span><br><span class="line">last_a &#x3D; a</span><br><span class="line">last_b &#x3D; b</span><br><span class="line">for step in range(1,200):</span><br><span class="line">    loss &#x3D; 0</span><br><span class="line">    all_da &#x3D; 0</span><br><span class="line">    all_db &#x3D; 0</span><br><span class="line">    shuffle_data(x,y)</span><br><span class="line">    [x_new,y_new] &#x3D; get_batch_data(x,y,batch&#x3D;4)</span><br><span class="line">    for i in range(0,len(x_new)):</span><br><span class="line">        y_p &#x3D; a*x_new[i] + b</span><br><span class="line">        loss &#x3D; loss + (y_new[i] - y_p)*(y_new[i] - y_p)&#x2F;2</span><br><span class="line">        all_da &#x3D; all_da + da(y_new[i],y_p,x_new[i])</span><br><span class="line">        all_db &#x3D; all_db + db(y_new[i],y_p)</span><br><span class="line">    #loss_ &#x3D; calc_loss(a &#x3D; a,b&#x3D;b,x&#x3D;np.array(x),y&#x3D;np.array(y))</span><br><span class="line">    loss &#x3D; loss&#x2F;len(x_new)</span><br><span class="line"></span><br><span class="line">    # ç»˜åˆ¶å›¾1ä¸­çš„lossç‚¹</span><br><span class="line">    ax.scatter(a, b, loss, color&#x3D;&#39;black&#39;)</span><br><span class="line">    # ç»˜åˆ¶å›¾2ä¸­çš„lossç‚¹</span><br><span class="line">    plt.subplot(2,2,2)</span><br><span class="line">    plt.scatter(a,b,s&#x3D;5,color&#x3D;&#39;blue&#39;)</span><br><span class="line">    plt.plot([last_a,a],[last_b,b],color&#x3D;&#39;aqua&#39;)</span><br><span class="line">    # ç»˜åˆ¶å›¾3ä¸­çš„å›å½’ç›´çº¿</span><br><span class="line">    plt.subplot(2, 2, 3)</span><br><span class="line">    plt.plot(x, y)</span><br><span class="line">    plt.plot(x, y, &#39;o&#39;)</span><br><span class="line">    x_ &#x3D; np.linspace(0, 1, 2)</span><br><span class="line">    y_draw &#x3D; a * x_ + b</span><br><span class="line">    plt.plot(x_, y_draw)</span><br><span class="line">    # ç»˜åˆ¶å›¾4çš„lossæ›´æ–°æ›²çº¿</span><br><span class="line">    all_loss.append(loss)</span><br><span class="line">    all_step.append(step)</span><br><span class="line">    plt.subplot(2,2,4)</span><br><span class="line">    plt.plot(all_step,all_loss,color&#x3D;&#39;orange&#39;)</span><br><span class="line">    plt.xlabel(&quot;step&quot;)</span><br><span class="line">    plt.ylabel(&quot;loss&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # print(&#39;a &#x3D; %.3f,b &#x3D; %.3f&#39; % (a,b))</span><br><span class="line">    last_a &#x3D; a</span><br><span class="line">    last_b &#x3D; b</span><br><span class="line">    a &#x3D; a - rate*all_da</span><br><span class="line">    b &#x3D; b - rate*all_db</span><br><span class="line"></span><br><span class="line">    if step%1 &#x3D;&#x3D; 0:</span><br><span class="line">        print(&quot;step: &quot;, step, &quot; loss: &quot;, loss)</span><br><span class="line">        plt.show()</span><br><span class="line">        plt.pause(0.01)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="Momentum-åŠ¨é‡ç®—æ³•"><a href="#Momentum-åŠ¨é‡ç®—æ³•" class="headerlink" title="Momentum åŠ¨é‡ç®—æ³•"></a>Momentum åŠ¨é‡ç®—æ³•</h2><p>å¦‚æœæŠŠæ¢¯åº¦ä¸‹é™æ³•æƒ³è±¡æˆä¸€ä¸ªå°çƒä»å±±å¡åˆ°å±±è°·çš„è¿‡ç¨‹ï¼Œé‚£ä¹ˆå‰é¢å‡ ç¯‡æ–‡ç« çš„å°çƒæ˜¯è¿™æ ·ç§»åŠ¨çš„ï¼šä»Aç‚¹å¼€å§‹ï¼Œè®¡ç®—å½“å‰Aç‚¹çš„å¡åº¦ï¼Œæ²¿ç€å¡åº¦æœ€å¤§çš„æ–¹å‘èµ°ä¸€æ®µè·¯ï¼Œåœä¸‹åˆ°Bã€‚åœ¨Bç‚¹å†çœ‹ä¸€çœ‹å‘¨å›´å¡åº¦æœ€å¤§çš„åœ°æ–¹ï¼Œæ²¿ç€è¿™ä¸ªå¡åº¦æ–¹å‘èµ°ä¸€æ®µè·¯ï¼Œå†åœä¸‹ã€‚ç¡®åˆ‡çš„æ¥è¯´ï¼Œè¿™å¹¶ä¸åƒä¸€ä¸ªçƒï¼Œæ›´åƒæ˜¯ä¸€ä¸ªæ­£åœ¨ä¸‹å±±çš„ç›²äººï¼Œæ¯èµ°ä¸€æ­¥éƒ½è¦åœä¸‹æ¥ï¼Œç”¨æ‹æ–æ¥æ¥æ¢æ¢å››å‘¨çš„è·¯ï¼Œå†èµ°ä¸€æ­¥åœä¸‹æ¥ï¼Œå‘¨è€Œå¤å§‹ï¼Œç›´åˆ°èµ°åˆ°å±±è°·ã€‚è€Œä¸€ä¸ªçœŸæ­£çš„å°çƒè¦æ¯”è¿™èªæ˜å¤šäº†ï¼Œä»Aç‚¹æ»šåŠ¨åˆ°Bç‚¹çš„æ—¶å€™ï¼Œå°çƒå¸¦æœ‰ä¸€å®šçš„åˆé€Ÿåº¦ï¼Œåœ¨å½“å‰åˆé€Ÿåº¦ä¸‹ç»§ç»­åŠ é€Ÿä¸‹é™ï¼Œå°çƒä¼šè¶Šæ»šè¶Šå¿«ï¼Œæ›´å¿«çš„å¥”å‘è°·åº•ã€‚momentum åŠ¨é‡æ³•å°±æ˜¯æ¨¡æ‹Ÿè¿™ä¸€è¿‡ç¨‹æ¥åŠ é€Ÿç¥ç»ç½‘ç»œçš„ä¼˜åŒ–çš„ã€‚å¯ä»¥ç”¨ä¸‹å›¾è¯´æ˜ï¼š</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/optimizer/2.png" alt="fig2"></p>
<p>æ ¹æ®å›¾ç¤ºï¼Œå…¶è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹ï¼š</p>
<ul>
<li><p>Aä¸ºèµ·å§‹ç‚¹ï¼Œé¦–å…ˆè®¡ç®—Aç‚¹çš„æ¢¯åº¦âˆ‡aï¼Œç„¶åä¸‹é™åˆ°Bç‚¹ï¼š$\Theta_{new}=\Thetaâˆ’\alphaâˆ‡a$ï¼Œå…¶ä¸­$\Theta$ä¸ºå‚æ•°ï¼Œ$\alpha$ä¸ºå­¦ä¹ ç‡ã€‚</p>
</li>
<li><p>åˆ°äº†Bç‚¹éœ€è¦åŠ ä¸ŠAç‚¹çš„æ¢¯åº¦ï¼Œè¿™é‡Œæ¢¯åº¦éœ€è¦æœ‰ä¸€ä¸ªè¡°å‡å€¼$\gamma$ï¼Œæ¨èå–0.9ã€‚è¿™æ ·çš„åšæ³•å¯ä»¥è®©æ—©æœŸçš„æ¢¯åº¦å¯¹å½“å‰æ¢¯åº¦çš„å½±å“è¶Šæ¥è¶Šå°ï¼Œå¦‚æœæ²¡æœ‰è¡°å‡å€¼ï¼Œæ¨¡å‹å¾€å¾€ä¼šéœ‡è¡éš¾ä»¥æ”¶æ•›ï¼Œç”šè‡³å‘æ•£ã€‚æ‰€ä»¥Bç‚¹çš„å‚æ•°æ›´æ–°å…¬å¼æ˜¯è¿™æ ·çš„ï¼š$v_t=\gamma v_{t-1} + \alphaâˆ‡b$ï¼Œ$\Theta_{new}=\Theta - v_t$ã€‚å…¶ä¸­$v_{t-1}$è¡¨ç¤ºä¹‹å‰æ‰€æœ‰æ­¥éª¤æ‰€ç´¯ç§¯çš„åŠ¨é‡å’Œã€‚</p>
</li>
</ul>
<p>Momentumç®—æ³•çš„ç‰¹ç‚¹å¦‚ä¸‹ï¼š</p>
<ul>
<li><p>ä¸‹é™åˆæœŸæ—¶ï¼Œä½¿ç”¨ä¸Šä¸€æ¬¡å‚æ•°æ›´æ–°ï¼Œä¸‹é™æ–¹å‘ä¸€è‡´ï¼Œä¹˜ä¸Šè¾ƒå¤§çš„$\gamma$èƒ½å¤Ÿè¿›è¡Œå¾ˆå¥½çš„åŠ é€Ÿ</p>
</li>
<li><p>ä¸‹é™ä¸­åæœŸæ—¶ï¼Œåœ¨å±€éƒ¨æœ€å°å€¼æ¥å›éœ‡è¡çš„æ—¶å€™ï¼Œ$âˆ‡b-&gt;0$ï¼Œ$\gamma$ä½¿å¾—æ›´æ–°å¹…åº¦å¢å¤§ï¼Œè·³å‡ºé™·é˜±</p>
</li>
<li><p>åœ¨æ¢¯åº¦æ”¹å˜æ–¹å‘çš„æ—¶å€™ï¼Œ$\gamma$èƒ½å¤Ÿå‡å°‘æ›´æ–° æ€»è€Œè¨€ä¹‹ï¼Œmomentumé¡¹èƒ½å¤Ÿåœ¨ç›¸å…³æ–¹å‘åŠ é€ŸSGDï¼ŒæŠ‘åˆ¶æŒ¯è¡ï¼Œä»è€ŒåŠ å¿«æ”¶æ•›</p>
</li>
</ul>
<p>ä¸‹é¢ä»¥LRç®—æ³•ä¸ºä¾‹ï¼Œè¯´æ˜åŠ¨é‡çš„è®¡ç®—è¿‡ç¨‹ï¼š</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">def da(y,y_p,x):</span><br><span class="line">    return (y-y_p)*(-x)</span><br><span class="line"></span><br><span class="line">def db(y,y_p):</span><br><span class="line">    return (y-y_p)*(-1)</span><br><span class="line"></span><br><span class="line">a &#x3D; 10.0</span><br><span class="line">b &#x3D; -20.0</span><br><span class="line">all_loss &#x3D; []</span><br><span class="line">all_step &#x3D; []</span><br><span class="line">last_a &#x3D; a</span><br><span class="line">last_b &#x3D; b</span><br><span class="line">va &#x3D; 0</span><br><span class="line">vb &#x3D; 0</span><br><span class="line">gamma &#x3D; 0.9</span><br><span class="line">for step in range(1,100):</span><br><span class="line">    loss &#x3D; 0</span><br><span class="line">    all_da &#x3D; 0</span><br><span class="line">    all_db &#x3D; 0</span><br><span class="line">    for i in range(0,len(x)):</span><br><span class="line">        y_p &#x3D; a*x[i] + b</span><br><span class="line">        loss &#x3D; loss + (y[i] - y_p)*(y[i] - y_p)&#x2F;2  </span><br><span class="line">        # loss &#x3D; (y-ax[i]-b)(y-ax[i]-b)&#x2F;2&#x3D;(y^2 + (ax[i])^2 + b^2 - 2yax[i] - 2yb + 2ax[i]b&#x2F;2</span><br><span class="line">        # å¯¹lossä¸­çš„å‚æ•°aæ±‚åå¯¼ï¼š(2ax[i]^2 - 2yx[i] + 2x[i]b)&#x2F;2 &#x3D; x[i](ax[i]+b-y) &#x3D; x[i](y[i]-y) &#x3D; -x[i](y - y[i]) </span><br><span class="line">        all_da &#x3D; all_da + da(y[i],y_p,x[i])</span><br><span class="line">        # å¯¹lossä¸­çš„å‚æ•°bæ±‚åå¯¼ï¼š(2b - 2y + 2ax[i])&#x2F;2 &#x3D; (ax[i] + b - y) &#x3D; (y - y[i]) * (-1)</span><br><span class="line">        all_db &#x3D; all_db + db(y[i],y_p)</span><br><span class="line">    #loss_ &#x3D; calc_loss(a &#x3D; a,b&#x3D;b,x&#x3D;np.array(x),y&#x3D;np.array(y))</span><br><span class="line">    loss &#x3D; loss&#x2F;len(x)</span><br><span class="line"></span><br><span class="line">    # ç»˜åˆ¶å›¾1ä¸­çš„lossç‚¹</span><br><span class="line">    ax.scatter(a, b, loss, color&#x3D;&#39;black&#39;)</span><br><span class="line">    # ç»˜åˆ¶å›¾2ä¸­çš„lossç‚¹</span><br><span class="line">    plt.subplot(2,2,2)</span><br><span class="line">    plt.scatter(a,b,s&#x3D;5,color&#x3D;&#39;blue&#39;)</span><br><span class="line">    plt.plot([last_a,a],[last_b,b],color&#x3D;&#39;aqua&#39;)</span><br><span class="line">    # ç»˜åˆ¶å›¾3ä¸­çš„å›å½’ç›´çº¿</span><br><span class="line">    plt.subplot(2, 2, 3)</span><br><span class="line">    plt.plot(x, y)</span><br><span class="line">    plt.plot(x, y, &#39;o&#39;)</span><br><span class="line">    x_ &#x3D; np.linspace(0, 1, 2)</span><br><span class="line">    y_draw &#x3D; a * x_ + b</span><br><span class="line">    plt.plot(x_, y_draw)</span><br><span class="line">    # ç»˜åˆ¶å›¾4çš„lossæ›´æ–°æ›²çº¿</span><br><span class="line">    all_loss.append(loss)</span><br><span class="line">    all_step.append(step)</span><br><span class="line">    plt.subplot(2,2,4)</span><br><span class="line">    plt.plot(all_step,all_loss,color&#x3D;&#39;orange&#39;)</span><br><span class="line">    plt.xlabel(&quot;step&quot;)</span><br><span class="line">    plt.ylabel(&quot;loss&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # print(&#39;a &#x3D; %.3f,b &#x3D; %.3f&#39; % (a,b))</span><br><span class="line">    last_a &#x3D; a</span><br><span class="line">    last_b &#x3D; b</span><br><span class="line">    va &#x3D; gamma * va+ rate*all_da</span><br><span class="line">    vb &#x3D; gamma * vb+ rate*all_db</span><br><span class="line">    a &#x3D; a - va</span><br><span class="line">    b &#x3D; b - vb</span><br><span class="line"></span><br><span class="line">    if step%1 &#x3D;&#x3D; 0:</span><br><span class="line">        print(&quot;step: &quot;, step, &quot; loss: &quot;, loss)</span><br><span class="line">        plt.show()</span><br><span class="line">        plt.pause(0.01)</span><br><span class="line">plt.show()</span><br><span class="line"># plt.pause(9999)</span><br></pre></td></tr></table></figure>
<p>æ€»ç»“ï¼šä¼˜åŒ–ç®—æ³•ä¸­ï¼Œâ½¬æ ‡å‡½æ•°â¾ƒå˜é‡çš„æ¯â¼€ä¸ªå…ƒç´ åœ¨ç›¸åŒæ—¶é—´æ­¥éƒ½ä½¿â½¤åŒâ¼€ä¸ªå­¦ä¹ ç‡æ¥â¾ƒæˆ‘è¿­ä»£ã€‚åœ¨â€œåŠ¨é‡æ³•â€â¾¥æˆ‘ä»¬çœ‹åˆ°å½“x1å’Œx2çš„æ¢¯åº¦å€¼æœ‰è¾ƒâ¼¤å·®åˆ«æ—¶ï¼Œéœ€è¦é€‰æ‹©â¾œå¤Ÿå°çš„å­¦ä¹ ç‡ä½¿å¾—â¾ƒå˜é‡åœ¨æ¢¯åº¦å€¼è¾ƒâ¼¤çš„ç»´åº¦ä¸Šä¸å‘æ•£ã€‚ä½†è¿™æ ·ä¼šå¯¼è‡´â¾ƒå˜é‡åœ¨æ¢¯åº¦å€¼è¾ƒå°çš„ç»´åº¦ä¸Šè¿­ä»£è¿‡æ…¢ã€‚åŠ¨é‡æ³•ä¾èµ–æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡ä½¿å¾—â¾ƒå˜é‡çš„æ›´æ–°â½…å‘æ›´åŠ â¼€è‡´ï¼Œä»è€Œé™ä½å‘æ•£çš„å¯èƒ½ã€‚</p>
<h2 id="AdaGradç®—æ³•"><a href="#AdaGradç®—æ³•" class="headerlink" title="AdaGradç®—æ³•"></a>AdaGradç®—æ³•</h2><p>AdaGradæ€è·¯åŸºæœ¬æ˜¯å€Ÿé‰´L2 Regularizerï¼Œä¸è¿‡æ­¤æ—¶è°ƒèŠ‚çš„ä¸æ˜¯ğ‘Šï¼Œè€Œæ˜¯ğºğ‘Ÿğ‘ğ‘‘ğ‘–ğ‘’ğ‘›ğ‘¡ã€‚AdaGradç®—æ³•æ ¹æ®â¾ƒå˜é‡åœ¨æ¯ä¸ªç»´åº¦çš„æ¢¯åº¦å€¼çš„â¼¤å°æ¥è°ƒæ•´å„ä¸ªç»´åº¦ä¸Šçš„å­¦ä¹ ç‡ï¼Œä»è€Œé¿å…ç»Ÿâ¼€çš„å­¦ä¹ ç‡éš¾ä»¥é€‚åº”æ‰€æœ‰ç»´åº¦çš„é—®é¢˜ã€‚</p>
<p>è¿™ä¸ªç®—æ³•çš„ä¼˜ç‚¹æ˜¯å¯ä»¥å¯¹ä½é¢‘çš„å‚æ•°åšè¾ƒå¤§çš„æ›´æ–°ï¼Œå¯¹é«˜é¢‘çš„åšè¾ƒå°çš„æ›´æ–°ï¼Œä¹Ÿå› æ­¤ï¼Œå¯¹äºç¨€ç–çš„æ•°æ®å®ƒçš„è¡¨ç°å¾ˆå¥½ï¼Œå¾ˆå¥½åœ°æé«˜äº† SGD çš„é²æ£’æ€§ï¼Œä¾‹å¦‚è¯†åˆ« Youtube è§†é¢‘é‡Œé¢çš„çŒ«ï¼Œè®­ç»ƒ GloVe word embeddingsï¼Œå› ä¸ºå®ƒä»¬éƒ½æ˜¯éœ€è¦åœ¨ä½é¢‘çš„ç‰¹å¾ä¸Šæœ‰æ›´å¤§çš„æ›´æ–°ã€‚</p>
<p>å®ƒçš„ç¼ºç‚¹æ˜¯å½“å­¦ä¹ ç‡åœ¨è¿­ä»£æ—©æœŸé™å¾—è¾ƒå¿«ä¸”å½“å‰è§£ä¾ç„¶ä¸ä½³æ—¶ï¼ŒAdaGradç®—æ³•åœ¨è¿­ä»£åæœŸç”±äºå­¦ä¹ ç‡è¿‡å°ï¼Œå¯èƒ½è¾ƒéš¾æ‰¾åˆ°â¼€ä¸ªæœ‰â½¤çš„è§£ã€‚å…·ä½“è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹ï¼š</p>
<ul>
<li><p>$s_t = s_{t-1} + g_t \odot g_t$</p>
</li>
<li><p>$x_t=x_{t-1}-\frac{\eta }{s_t + \varepsilon } \odot g_t$</p>
</li>
</ul>
<p>åœ¨æ—¶é—´æ­¥0ï¼ŒAdaGradå°†$s_0$ä¸­æ¯ä¸ªå…ƒç´ åˆå§‹åŒ–ä¸º0ã€‚åœ¨æ—¶é—´æ­¥tï¼Œâ¾¸å…ˆå°†å°æ‰¹é‡éšæœºæ¢¯åº¦$g_t$æŒ‰å…ƒç´ å¹³â½…åç´¯åŠ åˆ°å˜é‡$s_t$ï¼Œæ¥ç€ï¼Œæˆ‘ä»¬å°†â½¬æ ‡å‡½æ•°â¾ƒå˜é‡ä¸­æ¯ä¸ªå…ƒç´ çš„å­¦ä¹ ç‡é€šè¿‡æŒ‰å…ƒç´ è¿ç®—é‡æ–°è°ƒæ•´â¼€ä¸‹ã€‚å…¶ä¸­$\eta$æ˜¯å­¦ä¹ ç‡ï¼Œ$\varepsilon$æ˜¯ä¸ºäº†ç»´æŒæ•°å€¼ç¨³å®šæ€§è€Œæ·»åŠ çš„å¸¸æ•°ï¼Œå¦‚10çš„-6æ¬¡æ–¹ã€‚è¿™â¾¥å¼€â½…ã€é™¤æ³•å’Œä¹˜æ³•çš„è¿ç®—éƒ½æ˜¯æŒ‰å…ƒç´ è¿ç®—çš„ã€‚è¿™äº›æŒ‰å…ƒç´ è¿ç®—ä½¿å¾—â½¬æ ‡å‡½æ•°â¾ƒå˜é‡ä¸­æ¯ä¸ªå…ƒç´ éƒ½åˆ†åˆ«æ‹¥æœ‰â¾ƒâ¼°çš„å­¦ä¹ ç‡ã€‚ä¸€èˆ¬$\varepsilon$é€‰å–0.01</p>
<p>éœ€è¦å¼ºè°ƒçš„æ˜¯ï¼Œå°æ‰¹é‡éšæœºæ¢¯åº¦æŒ‰å…ƒç´ å¹³â½…çš„ç´¯åŠ å˜é‡$s_t$å‡ºç°åœ¨å­¦ä¹ ç‡çš„åˆ†âºŸé¡¹ä¸­ã€‚å› æ­¤ï¼š</p>
<ul>
<li><p>å¦‚æœâ½¬æ ‡å‡½æ•°æœ‰å…³â¾ƒå˜é‡ä¸­æŸä¸ªå…ƒç´ çš„åå¯¼æ•°â¼€ç›´éƒ½è¾ƒâ¼¤ï¼Œé‚£ä¹ˆè¯¥å…ƒç´ çš„å­¦ä¹ ç‡å°†ä¸‹é™è¾ƒå¿«ï¼›ä¹Ÿå°±æ˜¯è¯´ï¼Œåœ¨è®­ç»ƒå‰æœŸï¼Œæ¢¯åº¦è¾ƒå°ï¼Œä½¿å¾—Regularizeré¡¹å¾ˆå¤§ï¼Œæ”¾å¤§æ¢¯åº¦ã€‚[æ¿€åŠ±é˜¶æ®µ]</p>
</li>
<li><p>åä¹‹ï¼Œå¦‚æœâ½¬æ ‡å‡½æ•°æœ‰å…³â¾ƒå˜é‡ä¸­æŸä¸ªå…ƒç´ çš„åå¯¼æ•°â¼€ç›´éƒ½è¾ƒå°ï¼Œé‚£ä¹ˆè¯¥å…ƒç´ çš„å­¦ä¹ ç‡å°†ä¸‹é™è¾ƒæ…¢ï¼›ä¹Ÿå°±æ˜¯è¯´ï¼Œè®­ç»ƒåæœŸï¼Œæ¢¯åº¦è¾ƒå¤§ï¼Œä½¿å¾—Regularizeré¡¹å¾ˆå°ï¼Œç¼©å°æ¢¯åº¦ã€‚[æƒ©ç½šé˜¶æ®µ]</p>
</li>
</ul>
<p>å¯ä»¥è¿™æ ·ç†è§£ï¼šAdaGradè¿‡ç¨‹ï¼Œæ˜¯ä¸€ä¸ªé€’æ¨è¿‡ç¨‹ï¼Œæ¬¡ä»ğœ=1ï¼Œæ¨åˆ°ğœ=ğ‘¡ï¼ŒæŠŠæ²¿è·¯çš„ğºğ‘Ÿğ‘ğ‘‘ğ‘–ğ‘’ğ‘›ğ‘¡çš„å¹³æ–¹æ ¹ï¼Œä½œä¸ºRegularizerã€‚ç”±äºRegularizeræ˜¯ä¸“é—¨é’ˆå¯¹Gradientçš„ï¼Œæ‰€ä»¥æœ‰åˆ©äºè§£å†³Gradient Vanish/Expolodingé—®é¢˜ã€‚</p>
<p>ä¸‹å›¾ä¸ºAdaGradä¼˜åŒ–çš„ä¸€ä¸ªè¿‡ç¨‹ï¼š</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/optimizer/3.png" alt="fig3"></p>
<p>AdaGradçš„ç¼ºç‚¹æ˜¯ï¼š</p>
<ul>
<li><p>ç”±å…¬å¼å¯ä»¥çœ‹å‡ºï¼Œä»ä¾èµ–äºäººå·¥è®¾ç½®ä¸€ä¸ªå…¨å±€å­¦ä¹ ç‡ã€‚$\eta$è®¾ç½®è¿‡å¤§çš„è¯ï¼Œä¼šä½¿regularizerè¿‡äºæ•æ„Ÿï¼Œå¯¹æ¢¯åº¦çš„è°ƒèŠ‚å¤ªå¤§</p>
</li>
<li><p>ä¸­åæœŸï¼Œåˆ†æ¯ä¸Šæ¢¯åº¦å¹³æ–¹çš„ç´¯åŠ å°†ä¼šè¶Šæ¥è¶Šå¤§ï¼Œä½¿$g_t-&gt;0$ï¼Œä½¿å¾—è®­ç»ƒæå‰ç»“æŸ</p>
</li>
</ul>
<p>ä¸‹é¢ç»§ç»­ä»¥LRä¸ºä¾‹ï¼Œè¯´æ˜å…¶è®¡ç®—è¿‡ç¨‹ã€‚</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; 10.0</span><br><span class="line">b &#x3D; -20.0</span><br><span class="line">all_loss &#x3D; []</span><br><span class="line">all_step &#x3D; []</span><br><span class="line">last_a &#x3D; a</span><br><span class="line">last_b &#x3D; b</span><br><span class="line">n &#x3D; np.array([0,0])</span><br><span class="line">epsilon &#x3D; 1e-8</span><br><span class="line">for step in range(1,500):</span><br><span class="line">    loss &#x3D; 0</span><br><span class="line">    all_da &#x3D; 0</span><br><span class="line">    all_db &#x3D; 0</span><br><span class="line">    for i in range(0,len(x)):</span><br><span class="line">        y_p &#x3D; a*x[i] + b</span><br><span class="line">        loss &#x3D; loss + (y[i] - y_p)*(y[i] - y_p)&#x2F;2</span><br><span class="line">        all_da &#x3D; all_da + da(y[i],y_p,x[i])</span><br><span class="line">        all_db &#x3D; all_db + db(y[i],y_p)</span><br><span class="line">    #loss_ &#x3D; calc_loss(a &#x3D; a,b&#x3D;b,x&#x3D;np.array(x),y&#x3D;np.array(y))</span><br><span class="line">    loss &#x3D; loss&#x2F;len(x)</span><br><span class="line"></span><br><span class="line">    # ç»˜åˆ¶å›¾1ä¸­çš„lossç‚¹</span><br><span class="line">    ax.scatter(a, b, loss, color&#x3D;&#39;black&#39;)</span><br><span class="line">    # ç»˜åˆ¶å›¾2ä¸­çš„lossç‚¹</span><br><span class="line">    plt.subplot(2,2,2)</span><br><span class="line">    plt.scatter(a,b,s&#x3D;5,color&#x3D;&#39;blue&#39;)</span><br><span class="line">    plt.plot([last_a,a],[last_b,b],color&#x3D;&#39;aqua&#39;)</span><br><span class="line">    # ç»˜åˆ¶å›¾3ä¸­çš„å›å½’ç›´çº¿</span><br><span class="line">    plt.subplot(2, 2, 3)</span><br><span class="line">    plt.plot(x, y)</span><br><span class="line">    plt.plot(x, y, &#39;o&#39;)</span><br><span class="line">    x_ &#x3D; np.linspace(0, 1, 2)</span><br><span class="line">    y_draw &#x3D; a * x_ + b</span><br><span class="line">    plt.plot(x_, y_draw)</span><br><span class="line">    # ç»˜åˆ¶å›¾4çš„lossæ›´æ–°æ›²çº¿</span><br><span class="line">    all_loss.append(loss)</span><br><span class="line">    all_step.append(step)</span><br><span class="line">    plt.subplot(2,2,4)</span><br><span class="line">    plt.plot(all_step,all_loss,color&#x3D;&#39;orange&#39;)</span><br><span class="line">    plt.xlabel(&quot;step&quot;)</span><br><span class="line">    plt.ylabel(&quot;loss&quot;)</span><br><span class="line"></span><br><span class="line">    # print(&#39;a &#x3D; %.3f,b &#x3D; %.3f&#39; % (a,b))</span><br><span class="line">    last_a &#x3D; a</span><br><span class="line">    last_b &#x3D; b</span><br><span class="line"></span><br><span class="line">    #-- å‚æ•°æ›´æ–°</span><br><span class="line">    n[0] &#x3D; n[0]+np.square(all_da)</span><br><span class="line">    n[1] &#x3D; n[1]+np.square(all_db)</span><br><span class="line">    rate_new &#x3D; rate&#x2F;(np.sqrt(n + epsilon))</span><br><span class="line">    print(&#39;rate_new a:&#39;,rate_new[0],&#39; b:&#39;,rate_new[1])</span><br><span class="line">    a &#x3D; a - (rate&#x2F;(np.sqrt(n[0] + epsilon)))*all_da</span><br><span class="line">    b &#x3D; b - (rate&#x2F;(np.sqrt(n[1] + epsilon)))*all_db</span><br><span class="line"></span><br><span class="line">    if step%1 &#x3D;&#x3D; 0:</span><br><span class="line">        print(&quot;step: &quot;, step, &quot; loss: &quot;, loss)</span><br><span class="line">        plt.show()</span><br><span class="line">        plt.pause(0.01)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="RMSPropç®—æ³•"><a href="#RMSPropç®—æ³•" class="headerlink" title="RMSPropç®—æ³•"></a>RMSPropç®—æ³•</h2><p>å½“å­¦ä¹ ç‡åœ¨è¿­ä»£æ—©æœŸé™å¾—è¾ƒå¿«ä¸”å½“å‰è§£ä¾ç„¶ä¸ä½³æ—¶ï¼ŒAdaGradç®—æ³•åœ¨è¿­ä»£åæœŸç”±äºå­¦ä¹ ç‡è¿‡å°ï¼Œå¯èƒ½è¾ƒéš¾æ‰¾åˆ°â¼€ä¸ªæœ‰â½¤çš„è§£ã€‚ä¸ºäº†è§£å†³è¿™â¼€é—®é¢˜ï¼ŒRMSPropç®—æ³•å¯¹AdaGradç®—æ³•åšäº†â¼€ç‚¹å°å°çš„ä¿®æ”¹ï¼š</p>
<ul>
<li><p>$s_t = \lambda s_{t-1} + (1-\lambda)g_t \odot g_t$</p>
</li>
<li><p>$x_t=x_{t-1}-\frac{\eta }{s_t + \varepsilon } \odot g_t$</p>
</li>
</ul>
<p>å› ä¸ºRMSPropç®—æ³•çš„çŠ¶æ€å˜é‡stæ˜¯å¯¹å¹³â½…é¡¹$g_t \odot g_t$çš„æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡ï¼Œæ‰€ä»¥å¯ä»¥çœ‹ä½œæ˜¯æœ€è¿‘$\frac{1}{1-\lambda}$ä¸ªæ—¶é—´æ­¥çš„å°æ‰¹é‡éšæœºæ¢¯åº¦å¹³â½…é¡¹çš„åŠ æƒå¹³å‡ã€‚å¦‚æ­¤â¼€æ¥ï¼Œâ¾ƒå˜é‡æ¯ä¸ªå…ƒç´ çš„å­¦ä¹ ç‡åœ¨è¿­ä»£è¿‡ç¨‹ä¸­å°±ä¸å†â¼€ç›´é™ä½ï¼ˆæˆ–ä¸å˜ï¼‰ã€‚</p>
<p>ä¸‹å›¾ä¸ºRMSPropä¼˜åŒ–çš„ä¸€ä¸ªè¿‡ç¨‹ï¼š</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/optimizer/4.png" alt="fig4"></p>
<p>RMSPropæœ‰å¦‚ä¸‹ç‰¹ç‚¹ï¼š</p>
<ul>
<li><p>è®­ç»ƒåˆä¸­æœŸï¼ŒåŠ é€Ÿæ•ˆæœä¸é”™ï¼Œå¾ˆå¿«</p>
</li>
<li><p>è®­ç»ƒåæœŸï¼Œåå¤åœ¨å±€éƒ¨æœ€å°å€¼é™„è¿‘æŠ–åŠ¨ï¼ˆä¸‹é¢å®éªŒä¸­ï¼Œå¦‚æœstepè®¾ç½®ä¸º500ï¼Œå¯èƒ½å‡ºç°losséå¸¸å¤§çš„æƒ…å†µï¼‰</p>
</li>
<li><p>å…¶å®RMSpropä¾ç„¶ä¾èµ–äºå…¨å±€å­¦ä¹ ç‡</p>
</li>
<li><p>RMSpropç®—æ˜¯Adagradçš„ä¸€ç§å‘å±•ï¼Œå’ŒAdadeltaçš„å˜ä½“ï¼Œæ•ˆæœè¶‹äºäºŒè€…ä¹‹é—´</p>
</li>
<li><p>é€‚åˆå¤„ç†éå¹³ç¨³ç›®æ ‡ - å¯¹äºRNNæ•ˆæœå¾ˆå¥½</p>
</li>
<li><p>RMSPropåˆ©ç”¨äº†äºŒé˜¶ä¿¡æ¯åšäº†Gradientä¼˜åŒ–ï¼Œåœ¨BatchNormä¹‹åï¼Œå¯¹å…¶éœ€æ±‚ä¸æ˜¯å¾ˆå¤§ã€‚</p>
</li>
</ul>
<p>ä¸‹é¢ç»§ç»­ä»¥LRä¸ºä¾‹ï¼Œè¯´æ˜å…¶è®¡ç®—è¿‡ç¨‹ã€‚</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; 10.0</span><br><span class="line">b &#x3D; -20.0</span><br><span class="line">all_loss &#x3D; []</span><br><span class="line">all_step &#x3D; []</span><br><span class="line">last_a &#x3D; a</span><br><span class="line">last_b &#x3D; b</span><br><span class="line">lambdas &#x3D; 0.9</span><br><span class="line">n &#x3D; np.array([0,0])</span><br><span class="line">epsilon &#x3D; 1e-8</span><br><span class="line">for step in range(1,200):</span><br><span class="line">    loss &#x3D; 0</span><br><span class="line">    all_da &#x3D; 0</span><br><span class="line">    all_db &#x3D; 0</span><br><span class="line">    for i in range(0,len(x)):</span><br><span class="line">        y_p &#x3D; a*x[i] + b</span><br><span class="line">        loss &#x3D; loss + (y[i] - y_p)*(y[i] - y_p)&#x2F;2</span><br><span class="line">        all_da &#x3D; all_da + da(y[i],y_p,x[i])</span><br><span class="line">        all_db &#x3D; all_db + db(y[i],y_p)</span><br><span class="line">    #loss_ &#x3D; calc_loss(a &#x3D; a,b&#x3D;b,x&#x3D;np.array(x),y&#x3D;np.array(y))</span><br><span class="line">    loss &#x3D; loss&#x2F;len(x)</span><br><span class="line"></span><br><span class="line">    # ç»˜åˆ¶å›¾1ä¸­çš„lossç‚¹</span><br><span class="line">    ax.scatter(a, b, loss, color&#x3D;&#39;black&#39;)</span><br><span class="line">    # ç»˜åˆ¶å›¾2ä¸­çš„lossç‚¹</span><br><span class="line">    plt.subplot(2,2,2)</span><br><span class="line">    plt.scatter(a,b,s&#x3D;5,color&#x3D;&#39;blue&#39;)</span><br><span class="line">    plt.plot([last_a,a],[last_b,b],color&#x3D;&#39;aqua&#39;)</span><br><span class="line">    # ç»˜åˆ¶å›¾3ä¸­çš„å›å½’ç›´çº¿</span><br><span class="line">    plt.subplot(2, 2, 3)</span><br><span class="line">    plt.plot(x, y)</span><br><span class="line">    plt.plot(x, y, &#39;o&#39;)</span><br><span class="line">    x_ &#x3D; np.linspace(0, 1, 2)</span><br><span class="line">    y_draw &#x3D; a * x_ + b</span><br><span class="line">    plt.plot(x_, y_draw)</span><br><span class="line">    # ç»˜åˆ¶å›¾4çš„lossæ›´æ–°æ›²çº¿</span><br><span class="line">    all_loss.append(loss)</span><br><span class="line">    all_step.append(step)</span><br><span class="line">    plt.subplot(2,2,4)</span><br><span class="line">    plt.plot(all_step,all_loss,color&#x3D;&#39;orange&#39;)</span><br><span class="line">    plt.xlabel(&quot;step&quot;)</span><br><span class="line">    plt.ylabel(&quot;loss&quot;)</span><br><span class="line"></span><br><span class="line">    # print(&#39;a &#x3D; %.3f,b &#x3D; %.3f&#39; % (a,b))</span><br><span class="line">    last_a &#x3D; a</span><br><span class="line">    last_b &#x3D; b</span><br><span class="line"></span><br><span class="line">    #-- å‚æ•°æ›´æ–°</span><br><span class="line">    n[0] &#x3D; lambdas * n[0] + (1 - lambdas) * np.square(all_da)</span><br><span class="line">    n[1] &#x3D; lambdas * n[1] + (1 - lambdas) * np.square(all_db)</span><br><span class="line">    rate_new &#x3D; rate&#x2F;(np.sqrt(n + epsilon))</span><br><span class="line">    print(&#39;rate_new a:&#39;,rate_new[0],&#39; b:&#39;,rate_new[1])</span><br><span class="line">    a &#x3D; a - (rate&#x2F;(np.sqrt(n[0] + epsilon)))*all_da</span><br><span class="line">    b &#x3D; b - (rate&#x2F;(np.sqrt(n[1] + epsilon)))*all_db</span><br><span class="line"></span><br><span class="line">    if step%1 &#x3D;&#x3D; 0:</span><br><span class="line">        print(&quot;step: &quot;, step, &quot; loss: &quot;, loss)</span><br><span class="line">        plt.show()</span><br><span class="line">        plt.pause(0.01)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="AdaDeltaç®—æ³•"><a href="#AdaDeltaç®—æ³•" class="headerlink" title="AdaDeltaç®—æ³•"></a>AdaDeltaç®—æ³•</h2><p>AdaDeltaåŸºæœ¬æ€æƒ³æ˜¯ç”¨ä¸€é˜¶çš„æ–¹æ³•ï¼Œè¿‘ä¼¼æ¨¡æ‹ŸäºŒé˜¶ç‰›é¡¿æ³•ã€‚é™¤äº†RMSPropç®—æ³•ä»¥å¤–ï¼Œå¦â¼€ä¸ªå¸¸â½¤ä¼˜åŒ–ç®—æ³•AdaDeltaç®—æ³•ä¹Ÿé’ˆå¯¹AdaGradç®—æ³•åœ¨è¿­ä»£åæœŸå¯èƒ½è¾ƒéš¾æ‰¾åˆ°æœ‰â½¤è§£çš„é—®é¢˜åšäº†æ”¹è¿›ã€‚æœ‰æ„æ€çš„æ˜¯ï¼ŒAdaDeltaç®—æ³•æ²¡æœ‰å­¦ä¹ ç‡è¿™â¼€è¶…å‚æ•°ã€‚å…¶è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹ï¼š</p>
<ul>
<li><p>$s_t = \lambda s_{t-1} + (1-\lambda)g_t \odot g_t$</p>
</li>
<li><p>$gâ€™_t=\sqrt{\frac{\Delta x_t + \varepsilon }{s_t +  \varepsilon}} \odot g_t$</p>
</li>
<li><p>$\Delta x_t=px_{t-1}-(1-p)gâ€™_t \odot gâ€™_t$</p>
</li>
<li><p>$x_t = x_{t-1} - gâ€™_t$</p>
</li>
</ul>
<p>AdaDeltaç®—æ³•çš„ç¬¬ä¸€æ­¥å’ŒRMSPropç®—æ³•â¼€æ ·ï¼Œä¸RMSPropç®—æ³•ä¸åŒçš„æ˜¯ï¼ŒAdaDeltaç®—æ³•è¿˜ç»´æŠ¤â¼€ä¸ªé¢å¤–çš„çŠ¶æ€å˜é‡$\Delta x_t$æ¥è®¡ç®—è‡ªå˜é‡çš„å˜åŒ–é‡ï¼ˆæŒ‰$gâ€™_t$å…ƒç´ å¹³â½…çš„æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡ï¼‰ï¼Œå…¶å…ƒç´ åŒæ ·åœ¨æ—¶é—´æ­¥0æ—¶è¢«åˆå§‹åŒ–ä¸º0ã€‚</p>
<p>å¯ä»¥è¿™æ ·ç†è§£ï¼šç›¸æ¯”äºåŒæ ·æ˜¯åŸºäºGradientçš„Regularizerï¼Œä¸è¿‡åªå–æœ€è¿‘çš„wä¸ªçŠ¶æ€ï¼Œè¿™æ ·ä¸ä¼šè®©æ¢¯åº¦è¢«æƒ©ç½šè‡³0ã€‚</p>
<p>ä¸RMSPropç›¸æ¯”ï¼ŒAdaDeltaç®—æ³•æ²¡æœ‰å­¦ä¹ ç‡è¶…å‚æ•°ï¼Œå®ƒé€šè¿‡ä½¿ç”¨æœ‰å…³è‡ªå˜é‡æ›´æ–°é‡å¹³æ–¹çš„æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡çš„é¡¹æ¥æ›¿ä»£RMSPropç®—æ³•ä¸­çš„å­¦ä¹ ç‡ã€‚</p>
<p>AdaDeltaçš„ç‰¹ç‚¹ï¼š</p>
<ul>
<li><p>ä»å¤šä¸ªæ•°æ®é›†æƒ…å†µæ¥çœ‹ï¼ŒAdaDeltaåœ¨è®­ç»ƒåˆæœŸå’Œä¸­æœŸï¼Œå…·æœ‰éå¸¸ä¸é”™çš„åŠ é€Ÿæ•ˆæœã€‚</p>
</li>
<li><p>ä½†æ˜¯åˆ°è®­ç»ƒåæœŸï¼Œè¿›å…¥å±€éƒ¨æœ€å°å€¼é›·åŒºä¹‹åï¼ŒAdaDeltaå°±ä¼šåå¤åœ¨å±€éƒ¨æœ€å°å€¼é™„è¿‘æŠ–åŠ¨ã€‚ä¸»è¦ä½“ç°åœ¨éªŒè¯é›†é”™è¯¯ç‡ä¸Šï¼Œè„±ç¦»ä¸äº†å±€éƒ¨æœ€å°å€¼å¸å¼•ç›†ã€‚è¿™æ—¶å€™ï¼Œåˆ‡æ¢æˆåŠ¨é‡SGDï¼Œå¦‚æœæŠŠå­¦ä¹ ç‡é™ä½ä¸€ä¸ªé‡çº§ï¼Œå°±ä¼šå‘ç°éªŒè¯é›†æ­£ç¡®ç‡æœ‰2%~5%çš„æå‡ã€‚[æ³¨]ï¼šä½¿ç”¨Batch Normä¹‹åï¼Œè¿™æ ·ä»AdaDeltaåˆ‡åˆ°SGDä¼šå¯¼è‡´æ•°å€¼ä½“ç³»å´©æºƒï¼ŒåŸå› æœªçŸ¥ã€‚</p>
</li>
</ul>
<p>ä¸‹é¢ç»§ç»­ä»¥LRä¸ºä¾‹ï¼Œè¯´æ˜å…¶è®¡ç®—è¿‡ç¨‹ã€‚</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; 10.0</span><br><span class="line">b &#x3D; -20.0</span><br><span class="line">all_loss &#x3D; []</span><br><span class="line">all_step &#x3D; []</span><br><span class="line">last_a &#x3D; a</span><br><span class="line">last_b &#x3D; b</span><br><span class="line">n &#x3D; np.array([0,0])</span><br><span class="line">theta &#x3D; np.array([0,0]).astype(np.float32) # æ¯ä¸€æ¬¡a,bè¿­ä»£çš„æ›´æ–°å€¼</span><br><span class="line"></span><br><span class="line">apple &#x3D; np.array([0,0]).astype(np.float32)</span><br><span class="line">pear &#x3D; np.array([0,0]).astype(np.float32)</span><br><span class="line"># è¿­ä»£</span><br><span class="line">for step in range(1,500):</span><br><span class="line">    loss &#x3D; 0</span><br><span class="line">    all_da &#x3D; 0</span><br><span class="line">    all_db &#x3D; 0</span><br><span class="line">    all_d &#x3D; np.array([0,0]).astype(np.float32)</span><br><span class="line">    for i in range(0,len(x)):</span><br><span class="line">        y_p &#x3D; a*x[i] + b</span><br><span class="line">        loss &#x3D; loss + (y[i] - y_p)*(y[i] - y_p)&#x2F;2</span><br><span class="line">        all_da &#x3D; all_da + da(y[i],y_p,x[i])</span><br><span class="line">        all_db &#x3D; all_db + db(y[i],y_p)</span><br><span class="line">    #loss_ &#x3D; calc_loss(a &#x3D; a,b&#x3D;b,x&#x3D;np.array(x),y&#x3D;np.array(y))</span><br><span class="line">    all_d &#x3D; np.array([all_da,all_db])</span><br><span class="line">    loss &#x3D; loss&#x2F;len(x)</span><br><span class="line"></span><br><span class="line">    # ç»˜åˆ¶å›¾1ä¸­çš„lossç‚¹</span><br><span class="line">    ax.scatter(a, b, loss, color&#x3D;&#39;black&#39;)</span><br><span class="line">    # ç»˜åˆ¶å›¾2ä¸­çš„lossç‚¹</span><br><span class="line">    plt.subplot(2,2,2)</span><br><span class="line">    plt.scatter(a,b,s&#x3D;5,color&#x3D;&#39;blue&#39;)</span><br><span class="line">    plt.plot([last_a,a],[last_b,b],color&#x3D;&#39;aqua&#39;)</span><br><span class="line">    # ç»˜åˆ¶å›¾3ä¸­çš„å›å½’ç›´çº¿</span><br><span class="line">    plt.subplot(2, 2, 3)</span><br><span class="line">    plt.plot(x, y)</span><br><span class="line">    plt.plot(x, y, &#39;o&#39;)</span><br><span class="line">    x_ &#x3D; np.linspace(0, 1, 2)</span><br><span class="line">    y_draw &#x3D; a * x_ + b</span><br><span class="line">    plt.plot(x_, y_draw)</span><br><span class="line">    # ç»˜åˆ¶å›¾4çš„lossæ›´æ–°æ›²çº¿</span><br><span class="line">    all_loss.append(loss)</span><br><span class="line">    all_step.append(step)</span><br><span class="line">    plt.subplot(2,2,4)</span><br><span class="line">    plt.plot(all_step,all_loss,color&#x3D;&#39;orange&#39;)</span><br><span class="line">    plt.xlabel(&quot;step&quot;)</span><br><span class="line">    plt.ylabel(&quot;loss&quot;)</span><br><span class="line"></span><br><span class="line">    # print(&#39;a &#x3D; %.3f,b &#x3D; %.3f&#39; % (a,b))</span><br><span class="line">    last_a &#x3D; a</span><br><span class="line">    last_b &#x3D; b</span><br><span class="line"></span><br><span class="line">    #-- å‚æ•°æ›´æ–°</span><br><span class="line">    apple &#x3D; gamma*apple + (1-gamma)*(all_d**2) # apple with all_d of this step</span><br><span class="line">#     rms_apple &#x3D; np.sqrt(apple + epsilon)</span><br><span class="line">    rms_apple &#x3D; apple + epsilon</span><br><span class="line"></span><br><span class="line">    pear &#x3D; gamma*pear + (1-gamma)*(theta**2) # pear with theta of last step</span><br><span class="line">#     rms_pear &#x3D; np.sqrt(pear + epsilon)</span><br><span class="line"></span><br><span class="line">#     theta &#x3D; -(rms_pear&#x2F;rms_apple)*all_d</span><br><span class="line">    theta &#x3D; -np.sqrt(rms_pear&#x2F;rms_apple) * all_d</span><br><span class="line">    [a,b] &#x3D; [a,b] + theta</span><br><span class="line"></span><br><span class="line">    if step%1 &#x3D;&#x3D; 0:</span><br><span class="line">        print(&quot;step: &quot;, step, &quot; loss: &quot;, loss,&quot;rms_pear: &quot;,rms_pear,&quot; rms_apple&quot;,rms_apple)</span><br><span class="line">        plt.show()</span><br><span class="line">        plt.pause(0.01)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="Adamç®—æ³•"><a href="#Adamç®—æ³•" class="headerlink" title="Adamç®—æ³•"></a>Adamç®—æ³•</h2><p>Adamç®—æ³•åœ¨RMSPropç®—æ³•åŸºç¡€ä¸Šå¯¹å°æ‰¹é‡éšæœºæ¢¯åº¦ä¹Ÿåšäº†æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡ã€‚</p>
<ul>
<li><p>$v_t=\beta_1v_{t-1} + (1-\beta_1)g_t$</p>
<ul>
<li>Adamç®—æ³•ä½¿â½¤äº†åŠ¨é‡å˜é‡$v_t$å’ŒRMSPropç®—æ³•ä¸­å°æ‰¹é‡éšæœºæ¢¯åº¦æŒ‰å…ƒç´ å¹³â½…çš„æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡å˜é‡$s_t$ï¼Œå¹¶åœ¨æ—¶é—´æ­¥0å°†å®ƒä»¬ä¸­æ¯ä¸ªå…ƒç´ åˆå§‹åŒ–ä¸º0ã€‚ç»™å®šè¶…å‚æ•°0 â‰¤ $\beta_1$ &lt; 1ï¼ˆç®—æ³•ä½œè€…å»ºè®®è®¾ä¸º0.9ï¼‰ï¼Œæ—¶é—´æ­¥tçš„åŠ¨é‡å˜é‡$v_t$å³å°æ‰¹é‡éšæœºæ¢¯åº¦$g_t$çš„æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡</li>
</ul>
</li>
<li><p>$s_t=\beta_2s_{t-1}+(1-\beta_2)g_t\odot g_t$</p>
<ul>
<li>å’ŒRMSPropç®—æ³•ä¸­â¼€æ ·ï¼Œç»™å®šè¶…å‚æ•°0 â‰¤ $\beta_2$ &lt; 1ï¼ˆç®—æ³•ä½œè€…å»ºè®®è®¾ä¸º0.999ï¼‰ï¼Œå°†å°æ‰¹é‡éšæœºæ¢¯åº¦æŒ‰å…ƒç´ å¹³â½…åçš„é¡¹$g_t\odot g_t$åšæŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡å¾—åˆ°$s_t$</li>
</ul>
</li>
<li><p>$vâ€™_t=\frac{v_t}{1-\beta^t_1}$ã€$sâ€™_t=\frac{s_t}{1-\beta^t_2}$</p>
<ul>
<li>å› ä¸ºå½“tè¾ƒå°æ—¶ï¼Œè¿‡å»å„æ—¶é—´æ­¥å°æ‰¹é‡éšæœºæ¢¯åº¦æƒå€¼ä¹‹å’Œä¼šè¾ƒå°ã€‚ä¾‹å¦‚ï¼Œå½“$\beta_1=0.9$æ—¶ï¼Œ$v_1=0.1g_1$ã€‚ä¸ºäº†æ¶ˆé™¤è¿™æ ·çš„å½±å“ï¼Œå¯¹äºä»»æ„æ—¶é—´æ­¥tï¼Œæˆ‘ä»¬å¯ä»¥å°†$v_t$å†é™¤ä»¥${1-\beta^t_1}$ï¼Œä»è€Œä½¿è¿‡å»å„æ—¶é—´æ­¥å°æ‰¹é‡éšæœºæ¢¯åº¦æƒå€¼ä¹‹å’Œä¸º1ã€‚è¿™ä¹Ÿå«ä½œåå·®ä¿®æ­£ã€‚åœ¨Adamç®—æ³•ä¸­ï¼Œæˆ‘ä»¬å¯¹å˜é‡$v_t$å’Œ$s_t$å‡ä½œåå·®ä¿®æ­£å¾—åˆ°</li>
</ul>
</li>
<li><p>$gâ€™_t=\frac{\eta vâ€™_t}{\sqrt{sâ€™_t}+\varepsilon }$</p>
<ul>
<li>æ¥ä¸‹æ¥ï¼ŒAdamç®—æ³•ä½¿â½¤ä»¥ä¸Šåå·®ä¿®æ­£åçš„å˜é‡$v_t$å’Œ$s_t$ï¼Œå°†æ¨¡å‹å‚æ•°ä¸­æ¯ä¸ªå…ƒç´ çš„å­¦ä¹ ç‡é€šè¿‡æŒ‰å…ƒç´ è¿ç®—é‡æ–°è°ƒæ•´å¾—åˆ°ï¼ˆ5ï¼‰ã€‚å…¶ä¸­$\eta$æ˜¯å­¦ä¹ ç‡ï¼Œ$\varepsilon$æ˜¯ä¸ºäº†ç»´æŒæ•°å€¼ç¨³å®šæ€§è€Œæ·»åŠ çš„å¸¸æ•°ï¼Œå¦‚10çš„-8æ¬¡æ–¹ã€‚å’ŒAdaGradç®—æ³•ã€RMSPropç®—æ³•ä»¥åŠAdaDeltaç®—æ³•â¼€æ ·ï¼Œâ½¬æ ‡å‡½æ•°â¾ƒå˜é‡ä¸­æ¯ä¸ªå…ƒç´ éƒ½åˆ†åˆ«æ‹¥æœ‰â¾ƒâ¼°çš„å­¦ä¹ ç‡</li>
</ul>
</li>
<li><p>$x_t=x_{t-1}-gâ€™_t$</p>
<ul>
<li>ä½¿ç”¨$gâ€™_t$è¿­ä»£è‡ªå˜é‡</li>
</ul>
</li>
</ul>
<p>Adamç®—æ³•æœ‰å¦‚ä¸‹ç‰¹ç‚¹ï¼š</p>
<ul>
<li>é™¤äº†åƒAdadeltaå’ŒRMSpropä¸€æ ·å­˜å‚¨äº†è¿‡å»æ¢¯åº¦çš„å¹³æ–¹$s_t$çš„æŒ‡æ•°è¡°å‡å¹³å‡å€¼ ï¼Œä¹Ÿåƒmomentumä¸€æ ·ä¿æŒäº†è¿‡å»æ¢¯åº¦$v_t$çš„æŒ‡æ•°è¡°å‡å¹³å‡å€¼</li>
<li>å¦‚æœ$v_t$å’Œ$s_t$è¢«åˆå§‹åŒ–ä¸º0å‘é‡ï¼Œé‚£å®ƒä»¬å°±ä¼šå‘0åç½®ï¼Œæ‰€ä»¥åšäº†åå·®æ ¡æ­£ï¼Œé€šè¿‡è®¡ç®—åå·®æ ¡æ­£åçš„$v_t$å’Œ$s_t$æ¥æŠµæ¶ˆè¿™äº›åå·®</li>
</ul>
<p>ä¸‹é¢ç»§ç»­ä»¥LRä¸ºä¾‹ï¼Œè¯´æ˜å…¶è®¡ç®—è¿‡ç¨‹ã€‚</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; 10.0</span><br><span class="line">b &#x3D; -20.0</span><br><span class="line">all_loss &#x3D; []</span><br><span class="line">all_step &#x3D; []</span><br><span class="line">last_a &#x3D; a</span><br><span class="line">last_b &#x3D; b</span><br><span class="line">m &#x3D; 0.0</span><br><span class="line">v &#x3D; 0.0</span><br><span class="line">theta &#x3D; np.array([0,0]).astype(np.float32)</span><br><span class="line"></span><br><span class="line">rate &#x3D; 0.001</span><br><span class="line">beta1 &#x3D; 0.9</span><br><span class="line">beta2 &#x3D; 0.999</span><br><span class="line">epsilon &#x3D; 1e-8</span><br><span class="line"></span><br><span class="line">for step in range(1,500):</span><br><span class="line">    loss &#x3D; 0</span><br><span class="line">    all_da &#x3D; 0</span><br><span class="line">    all_db &#x3D; 0</span><br><span class="line">    for i in range(0,len(x)):</span><br><span class="line">        y_p &#x3D; a*x[i] + b</span><br><span class="line">        loss &#x3D; loss + (y[i] - y_p)*(y[i] - y_p)&#x2F;2</span><br><span class="line">        all_da &#x3D; all_da + da(y[i],y_p,x[i])</span><br><span class="line">        all_db &#x3D; all_db + db(y[i],y_p)</span><br><span class="line">    loss &#x3D; loss&#x2F;len(x)</span><br><span class="line">    all_d &#x3D; np.array([all_da,all_db]).astype(np.float32)</span><br><span class="line">    # ç»˜åˆ¶å›¾1ä¸­çš„lossç‚¹</span><br><span class="line">    ax.scatter(a, b, loss, color&#x3D;&#39;black&#39;)</span><br><span class="line">    # ç»˜åˆ¶å›¾2ä¸­çš„lossç‚¹</span><br><span class="line">    plt.subplot(2,2,2)</span><br><span class="line">    plt.scatter(a,b,s&#x3D;5,color&#x3D;&#39;blue&#39;)</span><br><span class="line">    plt.plot([last_a,a],[last_b,b],color&#x3D;&#39;aqua&#39;)</span><br><span class="line">    # ç»˜åˆ¶å›¾3ä¸­çš„å›å½’ç›´çº¿</span><br><span class="line">    plt.subplot(2, 2, 3)</span><br><span class="line">    plt.plot(x, y)</span><br><span class="line">    plt.plot(x, y, &#39;o&#39;)</span><br><span class="line">    x_ &#x3D; np.linspace(0, 1, 2)</span><br><span class="line">    y_draw &#x3D; a * x_ + b</span><br><span class="line">    plt.plot(x_, y_draw)</span><br><span class="line">    # ç»˜åˆ¶å›¾4çš„lossæ›´æ–°æ›²çº¿</span><br><span class="line">    all_loss.append(loss)</span><br><span class="line">    all_step.append(step)</span><br><span class="line">    plt.subplot(2,2,4)</span><br><span class="line">    plt.plot(all_step,all_loss,color&#x3D;&#39;orange&#39;)</span><br><span class="line">    plt.xlabel(&quot;step&quot;)</span><br><span class="line">    plt.ylabel(&quot;loss&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # print(&#39;a &#x3D; %.3f,b &#x3D; %.3f&#39; % (a,b))</span><br><span class="line">    last_a &#x3D; a</span><br><span class="line">    last_b &#x3D; b</span><br><span class="line"></span><br><span class="line">    m &#x3D; beta1*m + (1-beta1)*all_d</span><br><span class="line">    v &#x3D; beta2*v + (1-beta2)*(all_d**2)</span><br><span class="line"></span><br><span class="line">    m_ &#x3D; m&#x2F;(1 - beta1)</span><br><span class="line">    v_ &#x3D; v&#x2F;(1 - beta2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    theta &#x3D; -(rate*m_&#x2F;(np.sqrt(v_) + epsilon))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    [a,b] &#x3D; [a,b] + theta</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    if step%1 &#x3D;&#x3D; 0:</span><br><span class="line">        print(&quot;step: &quot;, step, &quot; loss: &quot;, loss)</span><br><span class="line">        plt.show()</span><br><span class="line">        plt.pause(0.01)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="å¦‚ä½•é€‰æ‹©ä¼˜åŒ–ç®—æ³•"><a href="#å¦‚ä½•é€‰æ‹©ä¼˜åŒ–ç®—æ³•" class="headerlink" title="å¦‚ä½•é€‰æ‹©ä¼˜åŒ–ç®—æ³•"></a>å¦‚ä½•é€‰æ‹©ä¼˜åŒ–ç®—æ³•</h2><ul>
<li><p>å¦‚æœæ•°æ®æ˜¯ç¨€ç–çš„ï¼Œå°±ç”¨è‡ªé€‚ç”¨æ–¹æ³•ï¼Œå³ Adagrad, Adadelta, RMSprop, Adamã€‚</p>
</li>
<li><p>RMSprop, Adadelta, Adam åœ¨å¾ˆå¤šæƒ…å†µä¸‹çš„æ•ˆæœæ˜¯ç›¸ä¼¼çš„ã€‚</p>
</li>
<li><p>Adam å°±æ˜¯åœ¨ RMSprop çš„åŸºç¡€ä¸ŠåŠ äº† bias-correction å’Œ momentumï¼Œ</p>
</li>
<li><p>éšç€æ¢¯åº¦å˜çš„ç¨€ç–ï¼ŒAdam æ¯” RMSprop æ•ˆæœä¼šå¥½ã€‚</p>
</li>
<li><p>æ•´ä½“æ¥è®²ï¼ŒAdam æ˜¯æœ€å¥½çš„é€‰æ‹©ã€‚</p>
</li>
<li><p>å¾ˆå¤šè®ºæ–‡é‡Œéƒ½ä¼šç”¨ SGDï¼Œæ²¡æœ‰ momentum ç­‰ã€‚SGD è™½ç„¶èƒ½è¾¾åˆ°æå°å€¼ï¼Œä½†æ˜¯æ¯”å…¶å®ƒç®—æ³•ç”¨çš„æ—¶é—´é•¿ï¼Œè€Œä¸”å¯èƒ½ä¼šè¢«å›°åœ¨éç‚¹ã€‚</p>
</li>
<li><p>å¦‚æœéœ€è¦æ›´å¿«çš„æ”¶æ•›ï¼Œæˆ–è€…æ˜¯è®­ç»ƒæ›´æ·±æ›´å¤æ‚çš„ç¥ç»ç½‘ç»œï¼Œéœ€è¦ç”¨ä¸€ç§è‡ªé€‚åº”çš„ç®—æ³•ã€‚</p>
</li>
</ul>
<h2 id="kerasä¸­ä¼˜åŒ–å™¨çš„ç”¨æ³•"><a href="#kerasä¸­ä¼˜åŒ–å™¨çš„ç”¨æ³•" class="headerlink" title="kerasä¸­ä¼˜åŒ–å™¨çš„ç”¨æ³•"></a>kerasä¸­ä¼˜åŒ–å™¨çš„ç”¨æ³•</h2><p>ä»¥å…¨è¿æ¥ç½‘ç»œåšåˆ†ç±»ä»»åŠ¡ä¸ºä¾‹å­ï¼Œè¯´æ˜kerasä¸­optimizerçš„ç”¨æ³•</p>
<h3 id="ç”Ÿæˆæ•°æ®"><a href="#ç”Ÿæˆæ•°æ®" class="headerlink" title="ç”Ÿæˆæ•°æ®"></a>ç”Ÿæˆæ•°æ®</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X &#x3D; np.linspace(-1, 1, 200) #åœ¨è¿”å›ï¼ˆ-1, 1ï¼‰èŒƒå›´å†…çš„ç­‰å·®åºåˆ—</span><br><span class="line">np.random.shuffle(X)    # æ‰“ä¹±é¡ºåº</span><br><span class="line">Y &#x3D; 0.5 * X + 2 + np.random.normal(0, 0.05, (200, )) #ç”ŸæˆYå¹¶æ·»åŠ å™ªå£°</span><br><span class="line"></span><br><span class="line">X_train, Y_train &#x3D; X[:160], Y[:160]     # å‰160ç»„æ•°æ®ä¸ºè®­ç»ƒæ•°æ®é›†</span><br><span class="line">X_test, Y_test &#x3D; X[160:], Y[160:]      #å40ç»„æ•°æ®ä¸ºæµ‹è¯•æ•°æ®é›†</span><br></pre></td></tr></table></figure>
<h3 id="æ„å»ºæ¨¡å‹"><a href="#æ„å»ºæ¨¡å‹" class="headerlink" title="æ„å»ºæ¨¡å‹"></a>æ„å»ºæ¨¡å‹</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from keras import optimizers</span><br><span class="line">from keras.layers import Dense, Activation</span><br><span class="line">from keras.models import Sequential</span><br><span class="line"></span><br><span class="line">model &#x3D; Sequential()</span><br><span class="line">model.add(Dense(input_dim&#x3D;1, units&#x3D;1))</span><br></pre></td></tr></table></figure>
<h3 id="ä½¿ç”¨SGD"><a href="#ä½¿ç”¨SGD" class="headerlink" title="ä½¿ç”¨SGD"></a>ä½¿ç”¨SGD</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># clipvalue&#x3D;0.5è¡¨ç¤ºä¿ç•™(-0.5, 0.5)ä¹‹é—´çš„æ¢¯åº¦ï¼Œå…¶ä»–çš„éœ€è¦åšæ¢¯åº¦è£å‰ª</span><br><span class="line"># momentum&#x3D;0.9ä½¿ç”¨äº†åŠ¨é‡å¹³æ»‘ï¼Œå¯ä»¥è®¾ç½®ä¸º0.0è¡¨ç¤ºçº¯SGDï¼Œnesterov&#x3D;Trueè¡¨ç¤ºä½¿ç”¨NesterovåŠ¨é‡</span><br><span class="line">sgd &#x3D; optimizers.SGD(lr&#x3D;0.01, decay&#x3D;1e-6, momentum&#x3D;0.9, nesterov&#x3D;True, clipvalue&#x3D;0.5)</span><br><span class="line">model.compile(loss&#x3D;&#39;mean_squared_error&#39;, optimizer&#x3D;sgd)</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<h3 id="è®­ç»ƒå¹¶æµ‹è¯•"><a href="#è®­ç»ƒå¹¶æµ‹è¯•" class="headerlink" title="è®­ç»ƒå¹¶æµ‹è¯•"></a>è®­ç»ƒå¹¶æµ‹è¯•</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">print(&#39;Training -----------&#39;)</span><br><span class="line">for step in range(501):</span><br><span class="line">    cost &#x3D; model.train_on_batch(X_train, Y_train)</span><br><span class="line">    if step % 50 &#x3D;&#x3D; 0:</span><br><span class="line">        print(&quot;After %d trainings, the cost: %f&quot; % (step, cost))</span><br><span class="line">        </span><br><span class="line">print(&#39;\nTesting ------------&#39;)</span><br><span class="line">cost &#x3D; model.evaluate(X_test, Y_test, batch_size&#x3D;40)</span><br><span class="line">print(&#39;test cost:&#39;, cost)</span><br><span class="line">W, b &#x3D; model.layers[0].get_weights()</span><br><span class="line">print(&#39;Weights&#x3D;&#39;, W, &#39;\nbiases&#x3D;&#39;, b)</span><br></pre></td></tr></table></figure>
<h3 id="ä½¿ç”¨RMSprop"><a href="#ä½¿ç”¨RMSprop" class="headerlink" title="ä½¿ç”¨RMSprop"></a>ä½¿ç”¨RMSprop</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rmsprop &#x3D; optimizers.RMSprop(lr&#x3D;0.001, rho&#x3D;0.9, epsilon&#x3D;1e-06)</span><br></pre></td></tr></table></figure>
<h3 id="ä½¿ç”¨Adagrad"><a href="#ä½¿ç”¨Adagrad" class="headerlink" title="ä½¿ç”¨Adagrad"></a>ä½¿ç”¨Adagrad</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">adagrad &#x3D; optimizers.Adagrad(lr&#x3D;0.01, epsilon&#x3D;1e-06)</span><br></pre></td></tr></table></figure>
<h3 id="ä½¿ç”¨Adadelta"><a href="#ä½¿ç”¨Adadelta" class="headerlink" title="ä½¿ç”¨Adadelta"></a>ä½¿ç”¨Adadelta</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">adadelta &#x3D; optimizers.Adadelta(lr&#x3D;1.0, rho&#x3D;0.95, epsilon&#x3D;1e-06)</span><br></pre></td></tr></table></figure>
<h3 id="ä½¿ç”¨Adam"><a href="#ä½¿ç”¨Adam" class="headerlink" title="ä½¿ç”¨Adam"></a>ä½¿ç”¨Adam</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">adam &#x3D; optimizers.Adam(lr&#x3D;0.001, beta_1&#x3D;0.9, beta_2&#x3D;0.999, epsilon&#x3D;1e-08)</span><br></pre></td></tr></table></figure>
<h2 id="BERTä¸­ä½¿ç”¨çš„ä¼˜åŒ–å™¨è§£æ"><a href="#BERTä¸­ä½¿ç”¨çš„ä¼˜åŒ–å™¨è§£æ" class="headerlink" title="BERTä¸­ä½¿ç”¨çš„ä¼˜åŒ–å™¨è§£æ"></a>BERTä¸­ä½¿ç”¨çš„ä¼˜åŒ–å™¨è§£æ</h2><p>bertä½¿ç”¨çš„optimizerå«åšAdamWeightDecayOptimizerï¼Œå…ˆæ”¾ä»£ç ä¸ºæ•¬ï¼š</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">def create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps, use_tpu):</span><br><span class="line">  &quot;&quot;&quot;Creates an optimizer training op.&quot;&quot;&quot;</span><br><span class="line">  global_step &#x3D; tf.train.get_or_create_global_step()</span><br><span class="line"></span><br><span class="line">  learning_rate &#x3D; tf.constant(value&#x3D;init_lr, shape&#x3D;[], dtype&#x3D;tf.float32)</span><br><span class="line"></span><br><span class="line">  # Implements linear decay of the learning rate.</span><br><span class="line">  learning_rate &#x3D; tf.train.polynomial_decay(</span><br><span class="line">      learning_rate,</span><br><span class="line">      global_step,</span><br><span class="line">      num_train_steps,</span><br><span class="line">      end_learning_rate&#x3D;0.0,</span><br><span class="line">      power&#x3D;1.0,</span><br><span class="line">      cycle&#x3D;False)</span><br><span class="line"></span><br><span class="line">  # Implements linear warmup. I.e., if global_step &lt; num_warmup_steps, the</span><br><span class="line">  # learning rate will be &#96;global_step&#x2F;num_warmup_steps * init_lr&#96;.</span><br><span class="line">  if num_warmup_steps:</span><br><span class="line">    global_steps_int &#x3D; tf.cast(global_step, tf.int32)</span><br><span class="line">    warmup_steps_int &#x3D; tf.constant(num_warmup_steps, dtype&#x3D;tf.int32)</span><br><span class="line"></span><br><span class="line">    global_steps_float &#x3D; tf.cast(global_steps_int, tf.float32)</span><br><span class="line">    warmup_steps_float &#x3D; tf.cast(warmup_steps_int, tf.float32)</span><br><span class="line"></span><br><span class="line">    warmup_percent_done &#x3D; global_steps_float &#x2F; warmup_steps_float</span><br><span class="line">    warmup_learning_rate &#x3D; init_lr * warmup_percent_done</span><br><span class="line"></span><br><span class="line">    is_warmup &#x3D; tf.cast(global_steps_int &lt; warmup_steps_int, tf.float32)</span><br><span class="line">    learning_rate &#x3D; (</span><br><span class="line">        (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)</span><br><span class="line"></span><br><span class="line">  # It is recommended that you use this optimizer for fine tuning, since this</span><br><span class="line">  # is how the model was trained (note that the Adam m&#x2F;v variables are NOT</span><br><span class="line">  # loaded from init_checkpoint.)</span><br><span class="line">  optimizer &#x3D; AdamWeightDecayOptimizer(</span><br><span class="line">      learning_rate&#x3D;learning_rate,</span><br><span class="line">      weight_decay_rate&#x3D;0.01,</span><br><span class="line">      beta_1&#x3D;0.9,</span><br><span class="line">      beta_2&#x3D;0.999,</span><br><span class="line">      epsilon&#x3D;1e-6,</span><br><span class="line">      exclude_from_weight_decay&#x3D;[&quot;LayerNorm&quot;, &quot;layer_norm&quot;, &quot;bias&quot;])</span><br><span class="line"></span><br><span class="line">  if use_tpu:</span><br><span class="line">    optimizer &#x3D; tf.contrib.tpu.CrossShardOptimizer(optimizer)</span><br><span class="line"></span><br><span class="line">  tvars &#x3D; tf.trainable_variables()</span><br><span class="line">  grads &#x3D; tf.gradients(loss, tvars)</span><br><span class="line"></span><br><span class="line">  # This is how the model was pre-trained.</span><br><span class="line">  (grads, _) &#x3D; tf.clip_by_global_norm(grads, clip_norm&#x3D;1.0)</span><br><span class="line"></span><br><span class="line">  train_op &#x3D; optimizer.apply_gradients(</span><br><span class="line">      zip(grads, tvars), global_step&#x3D;global_step)</span><br><span class="line"></span><br><span class="line">  # Normally the global step update is done inside of &#96;apply_gradients&#96;.</span><br><span class="line">  # However, &#96;AdamWeightDecayOptimizer&#96; doesn&#39;t do this. But if you use</span><br><span class="line">  # a different optimizer, you should probably take this line out.</span><br><span class="line">  new_global_step &#x3D; global_step + 1</span><br><span class="line">  train_op &#x3D; tf.group(train_op, [global_step.assign(new_global_step)])</span><br><span class="line">  return train_op</span><br><span class="line"></span><br><span class="line">class AdamWeightDecayOptimizer(tf.train.Optimizer):</span><br><span class="line">  &quot;&quot;&quot;A basic Adam optimizer that includes &quot;correct&quot; L2 weight decay.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">  def __init__(self,</span><br><span class="line">               learning_rate,</span><br><span class="line">               weight_decay_rate&#x3D;0.0,</span><br><span class="line">               beta_1&#x3D;0.9,</span><br><span class="line">               beta_2&#x3D;0.999,</span><br><span class="line">               epsilon&#x3D;1e-6,</span><br><span class="line">               exclude_from_weight_decay&#x3D;None,</span><br><span class="line">               name&#x3D;&quot;AdamWeightDecayOptimizer&quot;):</span><br><span class="line">    &quot;&quot;&quot;Constructs a AdamWeightDecayOptimizer.&quot;&quot;&quot;</span><br><span class="line">    super(AdamWeightDecayOptimizer, self).__init__(False, name)</span><br><span class="line"></span><br><span class="line">    self.learning_rate &#x3D; learning_rate</span><br><span class="line">    self.weight_decay_rate &#x3D; weight_decay_rate</span><br><span class="line">    self.beta_1 &#x3D; beta_1</span><br><span class="line">    self.beta_2 &#x3D; beta_2</span><br><span class="line">    self.epsilon &#x3D; epsilon</span><br><span class="line">    self.exclude_from_weight_decay &#x3D; exclude_from_weight_decay</span><br><span class="line"></span><br><span class="line">  def apply_gradients(self, grads_and_vars, global_step&#x3D;None, name&#x3D;None):</span><br><span class="line">    &quot;&quot;&quot;See base class.&quot;&quot;&quot;</span><br><span class="line">    assignments &#x3D; []</span><br><span class="line">    for (grad, param) in grads_and_vars:</span><br><span class="line">      if grad is None or param is None:</span><br><span class="line">        continue</span><br><span class="line"></span><br><span class="line">      param_name &#x3D; self._get_variable_name(param.name)</span><br><span class="line"></span><br><span class="line">      m &#x3D; tf.get_variable(</span><br><span class="line">          name&#x3D;param_name + &quot;&#x2F;adam_m&quot;,</span><br><span class="line">          shape&#x3D;param.shape.as_list(),</span><br><span class="line">          dtype&#x3D;tf.float32,</span><br><span class="line">          trainable&#x3D;False,</span><br><span class="line">          initializer&#x3D;tf.zeros_initializer())</span><br><span class="line">      v &#x3D; tf.get_variable(</span><br><span class="line">          name&#x3D;param_name + &quot;&#x2F;adam_v&quot;,</span><br><span class="line">          shape&#x3D;param.shape.as_list(),</span><br><span class="line">          dtype&#x3D;tf.float32,</span><br><span class="line">          trainable&#x3D;False,</span><br><span class="line">          initializer&#x3D;tf.zeros_initializer())</span><br><span class="line"></span><br><span class="line">      # Standard Adam update.</span><br><span class="line">      next_m &#x3D; (</span><br><span class="line">          tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad))</span><br><span class="line">      next_v &#x3D; (</span><br><span class="line">          tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2,</span><br><span class="line">                                                    tf.square(grad)))</span><br><span class="line"></span><br><span class="line">      update &#x3D; next_m &#x2F; (tf.sqrt(next_v) + self.epsilon)</span><br><span class="line"></span><br><span class="line">      # Just adding the square of the weights to the loss function is *not*</span><br><span class="line">      # the correct way of using L2 regularization&#x2F;weight decay with Adam,</span><br><span class="line">      # since that will interact with the m and v parameters in strange ways.</span><br><span class="line">      #</span><br><span class="line">      # Instead we want ot decay the weights in a manner that doesn&#39;t interact</span><br><span class="line">      # with the m&#x2F;v parameters. This is equivalent to adding the square</span><br><span class="line">      # of the weights to the loss with plain (non-momentum) SGD.</span><br><span class="line">      if self._do_use_weight_decay(param_name):</span><br><span class="line">        update +&#x3D; self.weight_decay_rate * param</span><br><span class="line"></span><br><span class="line">      update_with_lr &#x3D; self.learning_rate * update</span><br><span class="line"></span><br><span class="line">      next_param &#x3D; param - update_with_lr</span><br><span class="line"></span><br><span class="line">      assignments.extend(</span><br><span class="line">          [param.assign(next_param),</span><br><span class="line">           m.assign(next_m),</span><br><span class="line">           v.assign(next_v)])</span><br><span class="line">    return tf.group(*assignments, name&#x3D;name)</span><br><span class="line"></span><br><span class="line">  def _do_use_weight_decay(self, param_name):</span><br><span class="line">    &quot;&quot;&quot;Whether to use L2 weight decay for &#96;param_name&#96;.&quot;&quot;&quot;</span><br><span class="line">    if not self.weight_decay_rate:</span><br><span class="line">      return False</span><br><span class="line">    if self.exclude_from_weight_decay:</span><br><span class="line">      for r in self.exclude_from_weight_decay:</span><br><span class="line">        if re.search(r, param_name) is not None:</span><br><span class="line">          return False</span><br><span class="line">    return True</span><br><span class="line"></span><br><span class="line">  def _get_variable_name(self, param_name):</span><br><span class="line">    &quot;&quot;&quot;Get the variable name from the tensor name.&quot;&quot;&quot;</span><br><span class="line">    m &#x3D; re.match(&quot;^(.*):\\d+$&quot;, param_name)</span><br><span class="line">    if m is not None:</span><br><span class="line">      param_name &#x3D; m.group(1)</span><br><span class="line">    return param_name</span><br></pre></td></tr></table></figure>
<p>å¯ä»¥çœ‹åˆ°é‡Œé¢ä½¿ç”¨äº†warmup+Adam+weight_dacayï¼Œç°åœ¨ä¸€ä¸€è¿›è¡Œè¯´æ˜ã€‚</p>
<p>ï¼ˆ1ï¼‰warmupæœºåˆ¶åœ¨åˆæœŸä½¿ç”¨warmup_stepé˜¶æ®µä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡ï¼ˆéšç€warmup_stepå¢å¤§é€æ­¥å¢å¤§åˆ°init_lrï¼‰</p>
<p>ï¼ˆ2ï¼‰Weight decayæ˜¯åœ¨æ¯æ¬¡æ›´æ–°çš„æ¢¯åº¦åŸºç¡€ä¸Šå‡å»ä¸€ä¸ªæ¢¯åº¦ï¼ˆ $\Theta$ä¸ºæ¨¡å‹å‚æ•°å‘é‡ï¼Œ$\bigtriangledown f_t(\theta_t) $ä¸ºtæ—¶åˆ»losså‡½æ•°çš„æ¢¯åº¦ï¼Œ$\alpha$ä¸ºå­¦ä¹ ç‡ï¼‰:$\theta_t=(1-\lambda ) \theta_t - \alpha \bigtriangledown f_t(\theta_t)$</p>
<p>ï¼ˆ3ï¼‰L2 regularizationæ˜¯ç»™å‚æ•°åŠ ä¸Šä¸€ä¸ªL2æƒ©ç½š($f_t(\theta )$ä¸ºlosså‡½æ•°)ï¼š$f^{reg}_t(\theta )=f_t(\theta )+\frac{\lambda â€˜}{2}||\theta ||^2$ã€‚(å½“$\lambda â€˜=\frac{\lambda }{\alpha }$æ—¶ï¼Œä¸weight decayç­‰ä»·ï¼Œä»…åœ¨ä½¿ç”¨æ ‡å‡†SGDä¼˜åŒ–æ—¶æˆç«‹)</p>
<p>ï¼ˆ4ï¼‰Adamè‡ªåŠ¨è°ƒæ•´å­¦ä¹ ç‡ï¼Œå¤§å¹…æé«˜äº†è®­ç»ƒé€Ÿåº¦ï¼Œä¹Ÿå¾ˆå°‘éœ€è¦è°ƒæ•´å­¦ä¹ ç‡ï¼Œä½†æ˜¯æœ‰ç›¸å½“å¤šçš„èµ„æ–™æŠ¥å‘ŠAdamä¼˜åŒ–çš„æœ€ç»ˆç²¾åº¦ç•¥ä½äºSGDã€‚é—®é¢˜å‡ºåœ¨å“ªå‘¢ï¼Œå…¶å®Adamæœ¬èº«æ²¡æœ‰é—®é¢˜ï¼Œé—®é¢˜åœ¨äºç›®å‰å¤§å¤šæ•°DLæ¡†æ¶çš„L2 regularizationå®ç°ç”¨çš„æ˜¯weight decayçš„æ–¹å¼ï¼Œè€Œweight decayåœ¨ä¸Adamå…±åŒä½¿ç”¨çš„æ—¶å€™æœ‰äº’ç›¸è€¦åˆã€‚ç†ç”±å¦‚ä¸‹å›¾ï¼Œå…¶ä¸­çº¢è‰²æ˜¯ä¼ ç»Ÿçš„Adam+L2 regularizationçš„æ–¹å¼ï¼Œç»¿è‰²æ˜¯bertä½¿ç”¨çš„æ¥å…¥weight decayçš„æ–¹å¼ï¼Œèƒ½å¤Ÿå®Œæˆæ¢¯åº¦ä¸‹é™ä¸weight decayçš„è§£è€¦ï¼š</p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/optimizer/5.png" alt="fig5"></p>
<p><img src="https://xiangzaixiansheng.oss-cn-beijing.aliyuncs.com/majing_blog/optimizer/6.png" alt="fig6"></p>
<p>å¤§éƒ¨åˆ†çš„æ¨¡å‹éƒ½ä¼šæœ‰L2 regularizationçº¦æŸé¡¹ï¼Œå› æ­¤å¾ˆæœ‰å¯èƒ½å‡ºç°adamçš„æœ€ç»ˆæ•ˆæœæ²¡æœ‰sgdçš„å¥½ã€‚å¦‚æœåœ¨tfé‡Œé¢è¦å¯¹ä¸åŒåŒºåŸŸçš„tensoråšä¸åŒçš„L2 regularizationè°ƒæ•´çš„åŒ–å¯ä»¥å‚è€ƒ<a href="https://zhuanlan.zhihu.com/p/40814046" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/40814046</a> å…ˆåšadamååœ¨æ‰‹å·¥æ›´æ–°L2 regularizationæ¢¯åº¦çš„æ–¹æ³•ã€‚</p>
<p>æ¨èï¼šä¸€ä¸ªæ¡†æ¶çœ‹æ‡‚ä¼˜åŒ–ç®—æ³•ä¹‹å¼‚åŒ SGD/AdaGrad/Adamï¼š<a href="https://zhuanlan.zhihu.com/p/32230623" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32230623</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="majsunflower.cn/2019/03/27/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8A%E4%BC%98%E5%8C%96%E5%99%A8%E2%80%94%E2%80%94%E5%8E%9F%E7%90%86%E5%8F%8A%E5%BA%94%E7%94%A8%E3%80%8B/" data-id="cklf9zmpf0012jtx96c4tdrj5"
         class="article-share-link">åˆ†äº«</a>
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%9F%BA%E7%A1%80%E7%B3%BB%E5%88%97/" rel="tag">åŸºç¡€ç³»åˆ—</a></li></ul>

    </footer>

  </div>

  
    
  <nav class="article-nav">
    
      <a href="/2019/04/23/Hello-Edge-Keyword-spotting-on-Microcontrollers%E8%AE%BA%E6%96%87%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E5%8F%8A%E5%AE%9E%E9%AA%8C%E7%BB%93%E8%AE%BA/" class="article-nav-link">
        <strong class="article-nav-caption">å‰ä¸€ç¯‡</strong>
        <div class="article-nav-title">
          
            Hello Edge: Keyword spotting on Microcontrollersè®ºæ–‡æºç é˜…è¯»åŠå®éªŒç»“è®º
          
        </div>
      </a>
    
    
      <a href="/2019/03/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E3%80%8ALattice-CNNs-for-Matching-Based-Chinese-Question-Answering%E3%80%8B/" class="article-nav-link">
        <strong class="article-nav-caption">åä¸€ç¯‡</strong>
        <div class="article-nav-title">è®ºæ–‡é˜…è¯»ï¼šã€ŠLattice CNNs for Matching Based Chinese Question Answeringã€‹</div>
      </a>
    
  </nav>


  

  
    
  <div class="gitalk" id="gitalk-container"></div>
  
<link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">

  
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

  
<script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>

  <script type="text/javascript">
    var gitalk = new Gitalk({
      clientID: 'a0115c330d8e2a88dc59',
      clientSecret: '2e456ec13123a898d7b34ad8e117f543a6f379ea',
      repo: 'majing2019.github.io',
      owner: 'majing2019',
      admin: ['majing2019'],
      // id: location.pathname,      // Ensure uniqueness and length less than 50
      id: md5(location.pathname),
      distractionFreeMode: false,  // Facebook-like distraction free mode
      pagerDirection: 'last'
    })

  gitalk.render('gitalk-container')
  </script>

  

</article>



</section>
  <footer class="footer">
  <div class="outer">
    <div class="float-right">
      <ul class="list-inline">
  
    <li><i class="fe fe-smile-alt"></i> <span id="busuanzi_value_site_uv"></span></li>
  
    <li><i class="fe fe-bookmark"></i> <span id="busuanzi_value_page_pv"></span></li>
  
</ul>
    </div>
    <ul class="list-inline">
      <li>&copy; 2021 å¤§å˜´æ€ªçš„å°ä¸–ç•Œ</li>
      <li>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></li>
      <li>Theme  <a href="https://github.com/zhwangart/hexo-theme-ocean" target="_blank" rel="noopener">Ocean</a></li>
    </ul>
  </div>
</footer>

</main>
<aside class="sidebar">
  <button class="navbar-toggle"></button>
<nav class="navbar">
  
    <div class="logo">
      <a href="/"><img src="/images/shark.svg" alt="å¤§å˜´æ€ªçš„å°ä¸–ç•Œ"></a>
    </div>
  
  <ul class="nav nav-main">
    
      <li class="nav-item">
        <a class="nav-item-link" href="/">ä¸»é¡µ</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/archives">å½’æ¡£</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/gallery">ç›¸å†Œ</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/about">å…³äº</a>
      </li>
    
    <li class="nav-item">
      <a class="nav-item-link nav-item-search" title="æœç´¢">
        <i class="fe fe-search"></i>
        æœç´¢
      </a>
    </li>
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      <div class="totop" id="totop">
  <i class="fe fe-rocket"></i>
</div>
    </li>
    <li class="nav-item">
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
</aside>

<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>


  
<script src="/fancybox/jquery.fancybox.min.js"></script>




  
<script src="/js/tocbot.min.js"></script>

  <script>
    // Tocbot_v4.7.0  http://tscanlin.github.io/tocbot/
    tocbot.init({
      tocSelector: '.tocbot',
      contentSelector: '.article-entry',
      headingSelector: 'h1, h2, h3, h4, h5, h6',
      hasInnerContainers: true,
      scrollSmooth: true,
      positionFixedSelector: '.tocbot',
      positionFixedClass: 'is-position-fixed',
      fixedSidebarOffset: 'auto',
    });
  </script>




<script src="/js/ocean.js"></script>


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>